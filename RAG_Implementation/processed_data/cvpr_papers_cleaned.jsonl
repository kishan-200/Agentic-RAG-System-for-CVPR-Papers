{"id": "2508.06951v1", "title": "SLRTP2025 Sign Language Production Challenge: Methodology, Results, and Future Work", "authors": ["Harry Walsh", "Ed Fish", "Ozge Mercanoglu Sincan", "Mohamed Ilyes Lakhal", "Richard Bowden", "Neil Fox", "Bencie Woll", "Kepeng Wu", "Zecheng Li", "Weichao Zhao", "Haodong Wang", "Wengang Zhou", "Houqiang Li", "Shengeng Tang", "Jiayi He", "Xu Wang", "Ruobei Zhang", "Yaxiong Wang", "Lechao Cheng", "Meryem Tasyurek", "Tugce Kiziltepe", "Hacer Yalim Keles"], "summary": "sign language production slp task generating sign language video spoken language inputs. field seen range innovations last years, introduction deep learning-based approaches providing significant improvements realism naturalness generated outputs. however, lack standardized evaluation metrics slp approaches hampers meaningful comparisons across different systems. address this, introduce first sign language production challenge, held part third slrtp workshop cvpr competitions aims evaluate architectures translate spoken language sentences sequence skeleton poses, known text-to-pose translation, range metrics. evaluation data, use rwth-phoenix-weather-t dataset, german sign language deutsche gebardensprache dgs weather broadcast dataset. addition, curate custom hidden test set similar domain discourse. paper presents challenge design winning methodologies. challenge attracted participants submitted solutions, top-performing team achieving bleu- scores dtw-mje winning approach utilized retrieval-based framework pre-trained language model. part workshop, release standardized evaluation network, including high-quality skeleton extraction-based keypoints establishing consistent baseline slp field, enable future researchers compare work broader range methods.", "full_text": "slrtp sign language production challenge methodology, results, future work harry walsh, fish, ozge mercanoglu sincan, mohamed ilyes lakhal, richard bowden, neil fox, bencie woll, kepeng wu, zecheng li, weichao zhao, haodong wang, wengang zhou, houqiang li, shengeng tang, jiayi he, wang, ruobei zhang, yaxiong wang, lechao cheng, meryem tasyurek, tugce kiziltepe, hacer yalim keles university surrey, university birmingham, university college london, university science technology china, hefei university technology, hacettepe university harry.walsh, edward.fish, o.mercanoglusincan, m.lakhal, r.bowdensurrey.ac.uk denotes key workshop challenge organisers. abstract sign language production slp task generating sign language video spoken language inputs. field seen range innovations last years, introduction deep learning-based approaches provid- ing significant improvements realism naturalness generated outputs. however, lack standardized evaluation metrics slp approaches hampers meaning- ful comparisons across different systems. address this, introduce first sign language production challenge, held part third slrtp workshop cvpr competitions aims evaluate architectures translate spoken language sentences sequence skeleton poses, known text-to-pose transla- tion, range metrics. evaluation data, use rwth-phoenix-weather-t dataset, ger- man sign language deutsche gebardensprache dgs weather broadcast dataset. addition, curate cus- tom hidden test set similar domain discourse. paper presents challenge design win- ning methodologies. challenge attracted partici- pants submitted solutions, top-performing team achieving bleu- scores dtw-mje winning approach utilized retrieval-based framework pre-trained language model. part workshop, release standardized evaluation net- work, including high-quality skeleton extraction-based key- points establishing consistent baseline slp field, enable future researchers compare work broader range methods. introduction sign languages, like spoken languages, complex sys- tems distinct grammar vocabularies. independent fully fledged languages unique structure, using manual non-manual features asyn- chronously convey information manual features defined physical motion, location, shape hands arms, non-manual features include facial expressions, head movements, body posture. translating spoken signed languages presents significant communication challenge, typically requiring expert human interpreters rather simple word-to-sign substitution. however, scarcity interpreters limits ac- cess information deaf community. sign language production slp, task generating sign language spoken language inputs, potential part so- lution improving accessibility. last three decades, research slp made significant progress early approaches utilized graphical avatars rule-based translation systems architectures often produced robotic un- natural movement. however, recent progress deep learning-based methods significantly enhanced re- alism naturalness sign language generation approaches slp commonly decompose task several stages, using intermediate represen- tations linguistic annotation skeleton pose parametric human models despite advancements, progress field impeded absence standardized evaluation metrics, hinders meaningful comparisons across slp systems. address issue, part slrtp cs.cv aug workshop, sign language production challenge held, attracting participants submitted solutions. manuscript, present approaches top three performing teams. competition focused de- veloping robust systems spoken language sign lan- guage translation. part evaluation pipeline, propose novel metric, total distance, help score ex- pressiveness productions. given previous work noted issues regression mean, proposed met- ric provides improved qualitative measure translation quality. release evaluation pipeline, hope helps establish consistent baseline slp work, enabling future researchers conduct comprehensive com- parisons wider range methods. rest paper organized follows. section reviews related work field sign language produc- tion slp. sections describe design dataset used challenge. section details evaluation pro- tocol. section outlines methodology used top three teams, followed section presents re- sults. finally, section concludes paper suggests directions future work. related work sign language production shortage qualified sign language interpreters mo- tivated development slp systems. computational sign language research started early first approaches avatar-based how- ever, use poor quality avatars consistently re- ceived criticism deaf community due un- realistic appearance robotic motion, limits com- prehension systems looked le- gal phrases input text mapped pre-defined sign language animations another ap- proach uses motion capture mocap incorporate fluid motion dynamics, requires specialised capture equipment limits vocabulary size field progressed statistical machine trans- lation smt, allowing automatic rule learning im- proved generalizability models able use context input text solve ambiguity. however, approaches require handcrafted features extracted used ensemble models. introduction deep learning resulted data-driven approaches, learn mapping spoken sign languages. first deep slp pipeline workshop website challenge website www codabench org competitions challenge evaluation code github com walsharryslrtp-sign-production-evaluationtree main broke task three steps first, translation text-to-gloss tg, followed gloss-to-pose look-up, finally pose-to-sign video generation step however, approach unable blend motion signs, due low-resolution output key features facial expressions lost. later approaches attempted synthesize poses textual input. zelinka al. utilized recurrent neural network rnn predict -frame pose sequence word sentence. result, length predic- tion dependent number words sentence. limited realism outputs. after, saunders al. introduced progressive transformer, first learn direct mapping text pose using trans- former architecture. approach able generate realistic outputs deal variable length na- ture sign language. improvements achieved adding adversarial training mixture density net- work mdn non-autoregressive transformer also introduced improve speed model recently, diffusion applied tasks shown improvements, work relies linguistic annotation guide process. similar methods discussed far, large performance increases gained introducing gloss annotations how- ever, using gloss major limiting factor scaling larger domains discourse. models attempted directly regress pose se- quence text input often struggled regression mean. leads generation less expressive outputs. address this, works applied vector quantisation task, model used learn set discrete codes map small sequence poses. serves lexicon translation. predict codes text others start glosses instead learning units, others used dictionary pre-recorded signs. guaranteed expres- sive therefore produce comprehensible signing. simply concatenating signs create unnatural sequence, walsh al. used -step pipeline create smooth transitions, others employed transformer diffusion model task. alternatively, saunders al. proposed novel keyframe selection network learn co-articulate signs. works discussed far employed skele- ton poses representation sign language source vary. researchers utilize mediapipe whereas others opt openpose significant variation exists across methods, different authors using different sub- set keypoints. hindered meaningful comparison approaches. gloss written words associated sign. beyond skeleton pose, human parametric models also used slp task leverage smpl-x human model parameters describe face, hands, body. methods directly predicted rgb video frames substantial body research exists however, direct comparison works challenging due heterogeneity output representations. observe sig- nificant variability, particularly skeleton pose represen- tations. furthermore, extraction normalization tech- niques diverge considerably across studies. inconsis- tency impedes ability discern whether performance improvements stem novel algorithmic contributions advancements skeleton extraction normalization methodologies. issue compounded limited public availability codebases, renders direct com- parison state-of-the-art approaches exceedingly difficult highlights need standardized eval- uation metrics slp. challenge design objective challenge generate continuous sign language sequences spoken language inputs. utilized skeleton pose representation sign language. seen literature, serves common intermediate representation shown capable driving photo-realistic signer generation challenge comprised two phases. initially, development phase, teams provided train dev splits rwth-phoenix-weather-t phoenixt dataset throughout phase, partic- ipants could submit solutions test set via online platform, receiving evaluation scores skeleton pose predictions. subsequently, final week, competi- tion transitioned test phase. participants pre- sented spoken language sentences hid- den test set tasked submitting models pre- dictions. competition held codabench plat- form open-source framework running compe- titions, enabling automated evaluation code re- sults. competition spanned days, starting jan- uary finishing march time, participants contributed solutions. de- velopment phase, participants limited submis- sions per day, total submissions throughout competition. final test phase, participants limited three submissions. constraint im- plemented minimize likelihood participants over- fitting specific data portions employing random initial- izations achieve performance gains. final stage, six teams submitted solutions outperformed baseline approach. top-performing teams requested submit fact sheet detailing approach. teams highest scores, shared information code methodology, selected winners. final ranking, sec. employ pareto dom- inance. solution dominates another performs least well metrics demonstrably better least one. method ensures single metric arbitrarily favoured, providing balanced fair evaluation. challenge datasets slrtp cvpr challenge, use rwth- phoenix-weather-t phoenixt dataset custom hidden test set. provide details each. phoenixt phoenixt dataset one widely used benchmarks research sign language recognition translation. contains continuous sign language videos along corresponding gloss annotations spo- ken language subtitles. dataset derived weather forecast broadcasts german sign language dgs. dataset split training, development, test sets. training set contains videos, development set contains videos, test set contains videos. hidden test set hidden test set challenge sourced phoenix broadcast channel, consistent original phoenixt dataset. data collected part easier project. however, unlike original dataset, contains minimal manual annotation. therefore, first searched subtitles sections relevant weather- related discussions, maintain original domain dis- course. cropped sections original videos. ensure selected segments indeed weather broadcasts, manually verified subset total videos. finally, randomly selected sen- tences form hidden test set. skeleton representation video, extract mediapipe holistic keypoints use approach ivashechkin al. uplift predictions optimization leverages neural net- work, informed human body physical constraints, pre- dict skeleton joint angles keypoints. an- gles used apply canonical skeleton, thereby en- suring consistent bone lengths across signers. pro- vides keypoint representation keypoints hand, keypoints face, and, keypoints body. face subset keypoint represen- tations mediapipe, computational effi- ciency. normalise skeleton neck origin body fixed plane. example skeleton extraction seen fig. figure skeleton extraction example, left original signer, right keypoint skeleton. evaluation metrics discuss evaluation protocol used slp challenge, also publicly available. evalu- ation comprised text-based pose-based metrics. text-based metrics employ back-translation model con- vert skeletal representation back spoken language. specifically, sign language transformer utilized. input layer adapted match dimensionality keypoints. encoder decoder component comprise three layers, attention heads, embed- ding feedforward dimensions respec- tively. whereas pose-based metric calculated using skeleton itself. text-based bleu bilingual evaluation understudy bleu score compares given sentence set ref- erence sentences calculating precision word-level n-grams. use n-grams one four. chrf character-level f-score chrf cal- culates f-score based precision recall character-level n-grams two sentences. rouge recall-oriented understudy gisting evaluation rouge score measures overlap n-grams, word sequences, word pairs two sen- tences. wer word error rate wer calculates number errors insertions, deletions, substitutions divided total number words reference text. evaluation code github com walsharry slrtp-sign-production-evaluation pose-based dtw mje dynamic time warping mean joint er- ror dtw mje metric used evaluate similarity two sequences skeleton poses. calculates average error corresponding joints predicted ground truth poses, considering temporal alignment. total distance metric measures overall dis- tance signers hands moved space. judge expressive productions are. prediction normalized ground truth distance, therefore, score optimal. baseline model baseline method, use publicly available pro- gressive transformer saunders al. model translate text continuous sign pose se- quences end-to-end manner. model based transformer architecture, shown effective sequence-to-sequence tasks machine translation. introducing novel counter decoding tech- nique, model generate continuous pose sequences variable lengths. train model epochs adam optimizer learning rate line original paper, encoder decoder contain layers heads embedding size methodology section, introduce three top-performing ap- proaches slrtp sign language production challenge. team employs retrieval-based pipeline cen- tered gloss annotations, team leverages generative diffusion-based model, team presents gloss-free transformer architecture autoencoder latent pose embeddings. tab. summarizes key characteristics method. participant team- ustc-moe team- hfut-lmc team- hacettepe model architecture rule based diffusion transformer pre-trained models no. trainable parameters external datasets gloss information data augmentation handcrafted features motion constraints optimization metric bleu dtw dtw bleu- table general information top- winning approaches. following subsections detail teams respective methodologies, including training setups key design choices. figure overview figure place method ustc-moe. team ustc-moe place overall pipeline. fig. adapted teams fact sheet outlines top- approach. retrieval-based framework connects spoken language input large dictionary pose segments. method consists four key modules textgloss, signgloss, gloss-pose dic- tionary construction, sign retrieval. textgloss. multilingual pretrained language model, specifically xlm-r employed convert spoken language sentence gloss sequence. model fine- tuned cosine annealing schedule epochs using adam weight decay learning rate signgloss. train continuous sign lan- guage recognition cslr model based twostream- slt outputs gloss label every frame pose sequence. step provides robust mapping skeleton pose discrete glosses. gloss-pose dictionary construction. cslr network trained, used label segment train- ing poses. gloss therefore associated corre- sponding sub-pose sequence, producing large gloss- pose dictionary short signing segments. textsign. inference, model translates text gloss, retrieves sub-pose dictionary based gloss, iii concatenates short pose seg- ments form final sign pose sequence. report total parameter count dominated pretrained text model. additional data augmentation performed. reliance real pose segments ensures high fidelity natural transitions across retrieved signs. discussion. grounding gloss actual segment human motion, retrieval-based approach bypasses complexities motion synthesis guarantees plausi- ble, high-quality poses individual signs. concatenating sub-pose sequences requires robust gloss alignment, achieved via cslr model. simplicity reliabil- ity, coupled strong text-to-gloss translation, led first- place performance. team hfut-lmc place overall pipeline. unlike retrieval-based strategy team hfut-lmc proposes fully generative diffusion- based framework, illustrated fig. referred text- driven conditional diffusion model tcdm, method learns end-to-end mapping textual input sign lan- guage pose sequences without leveraging gloss-level super- vision large retrieval dictionaries. figure overall framework place method hfut-lmc. forward reverse processes. following standard diffusion paradigm forward process gradually adds gaussian noise ground-truth pose sequence steps, noisy sequence gamma t,p sigma t,epsilon label forward psil sim mathcal sigma training, model learns denoiser rot mathcal dbigl pt,,gbigr removes noise pt, guided condition derived text encoder figure detailed implementation details denoiser place method hfut-lmc. plus timestep mathcal dbigl pt,,gbigr label reverse inference, initially random pose iteratively de- noised small number ddim sampling steps, culminating coherent sign language motion. denoiser architecture. clarify protect mathcal operates, hfut- lmc splits procedure several sub-stages fig. linear embedding layer noisy pose pro- jected higher-dimensional space wp,pt bp, label linear embedded representation pt. positional encoding predefined sinusoidal en- coding added inject temporal information pen, label posenc indexes frame sequence. multi-head attention cross-attention embed- ded pose protect hat interacts conditioning derived text encoder current diffusion timestep via cross-attention, yielding updated pose features finally yield refined pose estimate otect hat loss functions. ensure generated poses ac- curate realistic, hfut-lmc combines two complementary objectives joint position loss protect mathcal ltext joint, mat joint frac jsum jjbigl lvert pjbigr rvert label ljoint enforcing alignment predicted ground-truth joint coordinates. bone orientation loss protect mathcal ltext bone, bone frac bsum bbbigl lvert qbbigr rvert label lbone promoting realistic limb orientations comparing direc- tion vectors bone. total training objective mat cal text total mathcal ltext joint lambda ,mathcal ltext bone, quad lambda label lfinal employ text encoder architecture baseline method layers, heads, -dimensional embedding size. forward diffusion steps set combined -step ddim inference schedule. training uses adam learning rate discussion. omitting gloss supervision, hfut-lmc avoids expense additional annotations. bone- orientation joint positioning losses, ensure poses represent natural articulations, mitigating regression-to- mean artifacts. team hacettepe place overall pipeline. hacettepe presents gloss-free transformer-based method learns compact, disentan- gled latent representation sign pose sequences via autoencoder. channel-aware regularization guides text-to- pose mapping without gloss supervision, shown fig. figure workflow diagram place method hacettepe. autoencoder latent pose. structurally disentan- gled pose autoencoder first pre-trained reconstruct poses. -dimensional input factorized four rwth-phoenix-weather-t test set teams bleu- bleu- bleu- bleu- chrf rouge wer dtw-mje total distance ground truth team ustc-moe team hfut-lmc team hacettepe progressive transformer table slp challenge results rwth-phoenix-weather-t phoenixt test set. hidden test set teams bleu- bleu- bleu- bleu- chrf rouge wer dtw-mje total distance ground truth team ustc-moe team hfut-lmc team hacettepe progressive transformer table slp challenge results hidden test set. articulatory regions, face, body, left hand, right hand, mapped -dimensional latent space. region-specific reconstruction encoder-weight regularization ensure compact semantically structured representations. transformer architecture. non-autoregressive trans- former predicts latent embeddings dimensional sentence vectors obtained pretrained bert model. text embeddings reduced dimen- sions processed -layer encoder layer decoder generate pose sequences parallel, match- ing autoencoders latent space. reconstruction training. transformer trained loss predicted latent vectors au- toencoders ground-truth codes, ensuring realistic mo- tion reconstruction. second phase, channel-wise divergence promotes articulator-aware regularization. avoids gloss annotations, model scales efficiently with- costly linguistic labelling. discussion. hacettepes gloss-free, disentangled autoencoder-based approach offers compact, inter- pretable representation slp. unlike retrieval-based methods, synthesizes entirely new sequences without gloss motion dictionary. although bridging text learned pose space present challenges highly nuanced signs, method demonstrates competitive accu- racy efficiency, securing third place challenge. details extended results provided challenge results hidden test set, three teams significantly out- performed baseline method. top-performing team achieved increase bleu-, shown tab. key limitation noted baseline method issue regression mean. observation quantified proposed total distance metric, re- veals progressive transformer generated motion traversed ground truth distance. contrast, three teams produced expressive outputs closely approximated ground truth motion. team outperformed baseline hidden test set. however, phoenixt test set, performance de- creased substantially, falling baseline. under- stand performance disparity two test sets, analyzed predicted sequences. found av- erage length team predictions times longer ground truth phoenixt test set, whereas hidden test set, close comparison, teams baseline exhibited average duration ra- tios respectively. spoken languages employ intonation, rhythm, stress convey nuance meaning, sign languages utilize prosody. thus, losing information producing longer sequences damaging performance. hypothesize team baselines transformer-based architecture, ide- ally suited sequence-to-sequence tasks, effectively cap- tured prosodic information. discrepancies emerged certain pose-based text-based metrics. specifically, dtw-mje tended favor methods produced less articulated longer sequences. observation underscores importance employing diverse range evaluation metrics assessing gener- ative models. across test sets, team consistently produced lowest word error rate wer. notably, approach in- corporated bert, model pre-trained specifically ger- man. team fine-tuned xlm-r, multilingual figure qualatative results top performing teams phoenixt test set. first row shows input spoken language sentence, second third rows show ground truth video skeleton, respectively, subsequent rows show predicted skeleton poses top teams. final row shows progressive transformer baseline. model, demonstrated superior translation quality, evi- denced higher bleu scores. given low-resource nature sign language, hypothesize models pre-existing linguistic knowledge significantly contributed performance. suggests spoken language re- sources effectively leveraged slp. correlation observed team rankings model size team model contained times pa- rameters team times team decomposing wer metric constituent com- ponents revealed team predominantly exhibited er- rors within distinct category. specifically, team encoun- tered replacement errors, team deletion errors, team insertion errors. hidden test set exhibited average sentence length eight words, range three fifteen words. negative cor- relation observed, average, sentence length error rate across three teams. suggests poten- tial tendency towards over-translation, although effect could originate either back-translation model teams respective approaches, distinction chal- lenging isolate. overall, words und, regen, suden identified frequent sources er- ror translations. qualitative results evaluation qualitative results supports previous findings. impact increased bleu- score, noted team demonstrated fig. retrieval-based approach ensures generated sequence exhibits expressive signing. observation quantified proposed total distance metric. presented table team team achieve score surpassing one. overall, outputs teams preserved features ground truth sequences. however, research necessary achieve fluency nat- uralness native deaf signers. conclusion paper summarizes findings first slp chal- lenge. final test phase, six teams submitted solutions outperformed baseline approach. presented work top three teams. top-performing team uti- lized recognition model curate dictionary signs, subsequently employed production. note gloss annotations limited resource therefore, would limit scaling method larger datasets. teams circumvented requirement type anno- tation presented models demonstrated competitive performance despite absence linguistic annotation. evaluating sign language challenging task compounded fact data normalisations rep- resentations constantly evolving. hope work helps establish consistent baseline future slp research. implore future researchers release pro- cess features data sets, without consistent in- puts, comparison approaches meaningless. acknowledgments work supported snsf project smile crsii innosuisse iict flagship pffs-- epsrc grant app signgpt-epz, google deepmind funding google.org via global goals scheme. work reflects authors views funders responsible use may made information contains. references vasileios baltatzis, rolandos alexandros potamias, evan- gelos ververas, guanxiong sun, jiankang deng, ste- fanos zafeiriou. neural sign actors diffusion model sign language production text. proceedings ieeecvf conference computer vision pattern recognition, pages andrew bangham, cox, ralph elliott, john glauert, ian marshall, sanja rankov, mark wells. virtual signing capture, animation, storage transmission-an overview visicast project. iee seminar speech language processing disabled elderly people ref. no. pages iet, jan bungeroth hermann ney. statistical sign language translation. sign-lang lrec pages cite- seer, necati cihan camgoz, simon hadfield, oscar koller, her- mann ney, richard bowden. neural sign language trans- lation. proceedings ieee conference computer vision pattern recognition, pages necati cihan camgoz, oscar koller, simon hadfield, richard bowden. sign language transformers joint end-to- end sign language recognition translation. proceed- ings ieeecvf conference computer vision pattern recognition, pages zhe cao, gines hidalgo, tomas simon, shih-en wei, yaser sheikh. openpose realtime multi-person pose estimation using part affinity fields, sheng chen, qingshan wang, wang. semantic- driven diffusion sign language production gloss- pose latent spaces alignment. computer vision image understanding, yutong chen, ronglai zuo, fangyun wei, wu, shujie liu, brian mak. two-stream network sign language recognition translation. neurips, alexis conneau, kartikay khandelwal, naman goyal, vishrav chaudhary, guillaume wenzek, francisco guzman, edouard grave, myle ott, luke zettlemoyer, veselin stoyanov. unsupervised cross-lingual representation learn- ing scale. arxiv preprint stephen cox, michael lincoln, judy tryggvason, melanie nakisa, mark wells, marcus tutt, sanja abbott. tessa, system aid communication deaf people. proceed- ings fifth international acm conference assistive technologies, pages eleni efthimiou, stavroula-evita fotinea, thomas hanke, john glauert, richard bowden, annelies braf- fort, christophe collet, petros maragos, francois lefebvre-albaret. dicta-sign wiki enabling web communication deaf. international conference computers handicapped persons, pages springer, oussama elghoul mohamed jemni. websign sys- tem make interpret signs using avatars. pro- ceedings second international workshop sign lan- guage translation avatar technology sltat, dundee, uk, sen fang, chunyu sui, xuedong zhang, yapeng tian. signdiff learning diffusion models american sign lan- guage production, sylvie gibet, francois lefebvre-albaret, ludovic hamon, remi brun, ahmed turki. interactive editing french sign language dedicated virtual signers requirements challenges. universal access information society, john glauert, ralph elliott, stephen cox, judy tryg- gvason, mary sheard. vanessaa system commu- nication deaf hearing people. technology disability, zhengsheng guo, zhiwei he, wenxiang jiao, xing wang, rui wang, kehai chen, zhaopeng tu, yong xu, min zhang. unsupervised sign language translation genera- tion, jiayi he, wang, ruobei zhang, shengeng tang, yaxiong wang, lechao cheng. text-driven diffusion model sign language production. arxiv preprint jonathan ho, ajay jain, pieter abbeel. denoising diffu- sion probabilistic models. advances neural information processing systems, pages wencan huang, wenwen pan, zhou zhao, tian. to- wards fast high-quality sign language production. proceedings acm international conference multimedia, pages eui jun hwang, huije lee, jong park. autoregres- sive sign language production gloss-free approach discrete representations. arxiv preprint maksym ivashechkin, oscar mendez, richard bowden. improving pose estimation sign language. ieee international conference acoustics, speech, signal processing workshops icasspw, pages jakub kanis, jir zahradil, filip jurccek, ludek muller. czech-sign speech corpus semantic based machine trans- lation. international conference text, speech di- alogue, pages springer, diederik kingma jimmy ba. adam method stochastic optimization. international conference learning representations, pages chin-yew lin. rouge package automatic evaluation summaries. text summarization branches out, pages matthew loper, naureen mahmood, javier romero, gerard pons-moll, michael black. smpl skinned multi- person linear model. tog, camillo lugaresi, jiuqiang tang, hadon nash, chris mc- clanahan, esha uboweja, michael hays, fan zhang, chuo- ling chang, ming guang yong, juhyun lee, al. medi- apipe framework building perception pipelines. arxiv preprint achraf othman mohamed jemni. statistical sign lan- guage machine translation english written text amer- ican sign language gloss, kishore papineni, salim roukos, todd ward, wei-jing zhu. bleu method automatic evaluation machine translation. proceedings annual meeting association computational linguistics, pages anton pelykh, ozge mercanoglu sincan, richard bow- den. giving hand diffusion models two-stage ap- proach improving conditional human image generation, maja popovic. chrf character n-gram f-score auto- matic evaluation. proceedings tenth workshop statistical machine translation, pages lisbon, portugal, association computational linguistics. ben saunders, necati cihan camgoz, richard bowden. adversarial training multi-channel sign language produc- tion. british machine vision virtual conference, ben saunders, necati cihan camgoz, richard bow- den. everybody sign translating spoken language photo realistic sign language video. arxiv preprint ben saunders, necati cihan camgoz, richard bowden. progressive transformers end-to-end sign language pro- duction. computer visioneccv european conference, glasgow, uk, august proceed- ings, part pages springer, ben saunders, necati cihan camgoz, richard bowden. mixed signals sign language production via mixture motion primitives. proceedings ieeecvf interna- tional conference computer vision, ben saunders, necati cihan camgoz, richard bowden. signing scale learning co-articulate signs large- scale photo-realistic sign language production. proceed- ings ieee conference computer vision pattern recognition cvpr, ben saunders, necati cihan camgoz, richard bowden. signing scale learning co-articulate signs large- scale photo-realistic sign language production. proceed- ings ieeecvf conference computer vision pattern recognition, pages jiaming song, chenlin meng, stefano ermon. denois- ing diffusion implicit models. international conference learning representations, stephanie stoll, necati cihan camgoz, simon hadfield, bowden. sign language production using neural machine translation generative adversarial networks. british machine vision conference, stephanie stoll, necati cihan camgoz, simon hadfield, richard bowden. textsign towards sign language produc- tion using neural machine translation generative adver- sarial networks. ijcv, stephanie stoll, armin mustafa, jean-yves guillemaut. back sign language generation text using back-translation. international conference vision dv, pages ieee, rachel sutton-spence bencie woll. linguistics british sign language introduction. cambridge univer- sity press, shinichi tamura shingo kawasaki. recognition sign language motion images. pattern recognition, shengeng tang, jiayi he, lechao cheng, jingjing wu, dan guo, richang hong. discrete continuous generat- ing smooth transition poses sign language observation. arxiv preprint shengeng tang, feng xue, jingjing wu, shuo wang, richang hong. gloss-driven conditional diffusion models sign language production. acm trans. multimedia com- put. commun. appl., accepted. sumeyye meryem tasyurek, tugce kzltepe, hacer yalim keles. disentangle regularize sign language production articulator-based disentangle- ment channel-aware regularization. arxiv preprint harry walsh, ben saunders, richard bowden. chang- ing representation examining language representation neural sign language production. lrec work- shop language resources evaluation conference june page harry walsh, abolfazl ravanshad, mariam rahmani, richard bowden. data-driven representation sign language production. proceedings interna- tional conference automatic face gesture recogni- tion institute electrical electronics engi- neers ieee, harry walsh, ben saunders, richard bowden. se- lect reorder novel approach neural sign lan- guage production. proceedings joint interna- tional conference computational linguistics, language resources evaluation lrec-coling pages harry walsh, ben saunders, richard bowden. sign stitching novel approach sign language production. british machine vision conference bmvc, pan xie, qipeng zhang, zexian li, hao tang, yao du, xiaohui hu. vector quantized diffusion model codeunet text-to-sign pose sequences generation. arxiv preprint pan xie, taiyi peng, yao du, qipeng zhang. sign lan- guage production latent motion transformer, zhen xu, sergio escalera, adrien pavao, magali richard, wei-wei tu, quanming yao, huan zhao, isabelle guyon. codabench flexible, easy-to-use, reproducible meta-benchmark platform. patterns, jan zelinka jakub kanis. neural sign language synthe- sis words glosses. ieee winter conference applications computer vision wacv, pages ronglai zuo, fangyun wei, zenggui chen, brian mak, jiao- long yang, xin tong. simple baseline spoken language sign language translation avatars. european conference computer vision, pages springer, inge zwitserlood, margriet verlinden, johan ros, sanny van der schoot, netherlands. synthetic signing deaf esign. proceedings conference workshop assistive technologies vision hearing impairment cvhi,", "published_date": "2025-08-09T11:57:33+00:00"}
{"id": "2507.13677v1", "title": "HeCoFuse: Cross-Modal Complementary V2X Cooperative Perception with Heterogeneous Sensors", "authors": ["Chuheng Wei", "Ziye Qin", "Walter Zimmer", "Guoyuan Wu", "Matthew J. Barth"], "summary": "real-world vehicle-to-everything cooperative perception systems often operate heterogeneous sensor configurations due cost constraints deployment variability across vehicles infrastructure. heterogeneity poses significant challenges feature fusion perception reliability. address issues, propose hecofuse, unified framework designed cooperative perception across mixed sensor setups nodes may carry cameras lidars both. introducing hierarchical fusion mechanism adaptively weights features combination channel-wise spatial attention, hecofuse tackle critical challenges cross-modality feature misalignment imbalanced representation quality. addition, adaptive spatial resolution adjustment module employed balance computational cost fusion effectiveness. enhance robustness across different configurations, implement cooperative learning strategy dynamically adjusts fusion type based available modalities. experiments real-world tumtraf-vx dataset demonstrate hecofuse achieves map full sensor configuration lclc, outperforming coopdetd baseline reaches even higher map llc scenario, maintaining map range across nine heterogeneous sensor configurations. results, validated first-place finish cvpr drivex challenge, establish hecofuse current state-of-the-art tum-traf dataset demonstrating robust performance across diverse sensor deployments.", "full_text": "hecofuse cross-modal complementary cooperative perception heterogeneous sensors chuheng wei, member, ieee, ziye qin, member, ieee, walter zimmer, member, ieee, guoyuan wu, senior member, ieee matthew barth, fellow, ieee, abstract real-world vehicle-to-everything cooper- ative perception systems often operate heterogeneous sensor configurations due cost constraints deployment variability across vehicles infrastructure. heterogene- ity poses significant challenges feature fusion perception reliability. address issues, propose hecofuse, unified framework designed enable effective coopera- tive perception across diverse sensor combinations, unified framework designed cooperative perception across mixed sensor setupswhere nodes may carry cameras lidars both. introducing hierarchical fusion mechanism adaptively weights features combination channel-wise spatial attention, hecofuse tackle critical challenges cross-modality feature misalignment imbalanced representation quality. addition, adaptive spatial resolution adjustment module employed balance computational cost fusion effectiveness. enhance robust- ness across different configurations, implement co- operative learning strategy dynamically adjusts fusion type based available modalities. experiments real-world tumtraf-vx dataset demonstrate hecofuse achieves map full sensor configuration lclc, outperforming coopdetd baseline reaches even higher map llc scenario, maintaining map range across nine heterogeneous sensor configurations. results, validated first-place finish cvpr drivex challenge, establish hecofuse current state-of-the-art tum-traf dataset demonstrating robust perfor- mance across diverse sensor deployments. code available github.comchuhengweihecofuse. introduction vehicle-to-everything enabled cooperative per- ception leverages information sharing vehicles roadside infrastructure extend perceptual horizon beyond line sight mitigate occlusions inherent individual sensors recent advances systems demonstrated notable gains detection range, accu- racy, robustness, especially challenging conditions dense traffic, adverse weather, complex urban scenes however, studies predominantly assume nodes possess identical similar sensor configurations, assumption barely holds real-world deployments chuheng wei, ziye qin, guoyuan wu, matthew barth college engineering, center environmental research technology, university california riverside, riverside, ca, usa. ziye qin also school transportation logistics, southwest jiaotong university, chengdu, china. walter zimmer chair robotics, artificial intelligence real-time systems, tum school computation, information technology, technical university munich, munich, germany. corresponding author. e-mail chuheng.weiemail.ucr.edu due cost constraints, incremental hardware upgrades, heterogeneous features vehicles infrastructure resulting heterogeneity creates scenarios nodes may carry lidar, others cameras, yet others sensor types variation poses three major challenges first, asymmetric sensor modalities complicate feature alignment fusion, lidar camera data differ spatial resolution, field view, information density. second, feature inconsistency arises modality absent certain nodes, rendering naive fusion strategies suboptimal. third, system robustness must guaranteed un- der partial sensor failures interacting nodes varying capabilities, requiring adaptable fusion mechanisms degrade gracefully. address challenges, propose hecofuse, unified cooperative perception framework tailored het- erogeneous vehicleinfrastructure settings. hecofuse extracts birds-eye-view bev features node via modality-specific encoders, performs inter-node fusion novel hierarchical attention-based mecha- nism dynamically weighs channel-wise spatial features according sensor quality. parallel, adaptive spatial resolution module adjusts feature-map scales based sensor configuration balance computational cost information fidelity. training, randomly sample nine representative heterogeneous configurations, enabling model learn robust fusion strategies generalized across sensor combinations. main contributions threefold introduce hecofuse, first unified framework explicitly designed cooperative perception het- erogeneous sensor configurations networks. develop two novel mechanisms, hierarchical at- tention fusion haf adaptive spatial resolution asr, dynamically integrate cross-modal features harmonize feature scales varying sensor avail- ability. conduct extensive experiments real-world tumtraf-vx dataset showing hecofuse achieves state-of-the-art performance trained scratch, maintains robust accuracy across nine het- erogeneous sensor configurations. ii. related work cooperative perception strategies cooperative perception methods systems typically categorized three main fusion strategiesearly cs.cv jul fusion, intermediate fusion, late fusionwhich corre- spond data-level fusion, feature-level fusion, fusion individual agents independent outputs, respectively strategies reflect different trade-offs commu- nication load, perception quality, system flexibility. early research primarily explored data-level output-level fusion approaches, recent work gravitated toward intermediate fusion methods due balance performance communication efficiency. early fusion involves collaboration input data level, raw sensor data multiple agents shared aggregated feature extraction. one representative method, cooper integrates data ego vehicles connected vehicles vehicular network enhance perception capabilities. leveraging lidar point clouds, cooper fuses raw sensor data multiple nodes data level. proposes object detection framework process aligned point cloud data, demonstrating feasibility performing cooperative perception point cloud data transmission using existing vehicular network technologies. method significantly improves detection accuracy driving safety. address high communication overhead raw data sharing, vx-pc introduced novel ap- proach defining point clusters message units instead transmitting raw point cloud data, significantly reducing communication requirements maintaining perception accuracy. intermediate fusion emerged mainstream approach research due ability reduce com- munication demands preserving essential perceptual information. based fusion principles, intermediate fusion techniques broadly classified three categories. traditional methods like f-cooper implement basic fea- ture aggregation techniques summation pooling operations, capitalizing compact nature features enable real-time edge computing. attention-based methods, represented attentionfusion vvformer employ various attention mechanisms dynamically inte- grate multi-sensor information weight assignment. topology-based approaches like vvnet model mul- tiple agents graph structure, using graph neural net- works facilitate information exchange. intermediate fusion frameworks include pillargrid coopercept vimi hm-vit collabgat vx-vit proposing unique strategies effective feature integra- tion. late fusion methods integrate detection results inde- pendent processing agent, typically using traditional data fusion techniques. approaches common early stages development, assigning con- fidence levels different modules combining outputs final detection. zhao al. proposed method using dempster-shafer theory update detection confidence improve lane detection accuracy, vips introduced system encoding simplified object representations efficient post-detection matching. late fusion approaches include khalifas multi-view cooperative framework pereiras poster platform cooperfuse calculates motion scale consistency across frames robust bounding box fusion. beyond three primary categories, hybrid fusion methods combine features different fusion stages, lever- aging respective strengths enhanced cooperative perception. disconet integrates knowledge distillation align early intermediate fusion strategies, ml- cooper employs reinforcement learning dynamically adjust information utilization varying bandwidth con- straints. additional hybrid approaches include arnolds dual early-late fusion method lius region-based fusion framework adapt strategies based sensor coverage environmental conditions. heterogeneous sensor fusion recent studies begun explore heterogeneous cooperative perception scenarios, vehicles infrastructure may equipped different sensor modalities detection networks. head proposes bandwidth-efficient cooperative perception framework fuses classification regression heads diverse object detection models, enabling collaboration across vehicles equipped different sensor types li- dar camera-based systems. leveraging self-attention mechanisms compact feature representations, head achieves comparable perception performance intermediate fusion significantly reducing communication overhead. method proposed luo al. vehicle-to- infrastructure cooperative perception framework proposed address beyond visual range scenarios com- bining heterogeneous sensor data. method enhances yolov centerpoint backbones novel convolutional modules performs multi-object post- fusion, demonstrating improvements urban intersection settings. hm-vit introduces unified hetero-modal vehicle-to-vehicle cooperative perception framework using heterogeneous graph transformer jointly model inter- intra-agent interactions. unlike prior approaches limited homogeneous sensor configurations, hm-vit ef- fectively integrates lidar image features across mul- tiple dynamic agents, achieving state-of-the-art performance opvv despite progress made methods, still ex- hibit notable limitations. first, focus either scenarios independently, rather addressing broader cooperative setting. importantly, treatment heterogeneous modalities often limited single-sensor configurations, fusing features one camera one lidar, without supporting diverse flexible combinations multi-camera multi-lidar setups. restricts scalability effectiveness real-world deployments sensor configurations vary significantly across agents. address gap, method aims develop generalized fusion framework accommodate arbitrary combinations camera lidar sensors across vehicles infrastructure. enabling fig. hecofuse framework overview. here, lcl case used example vehicle lidarcamera infrastructure lidar vehicleinfrastructure sensor combinations, per-node setups vary accordingly. framework integrates features via hierarchical attention fusion haf balances resolution adaptive spatial resolution asr across different scenarios. flexible hetero-modal fusion, seek improve perception robustness varied deployment conditions sys- tems. iii. methodology heterogeneous cooperative perception framework hecofuse real-world deployments rarely feature identical sen- sor configurations across nodes due cost constraints, infrastructure limitations, hardware variations. het- erogeneity creates significant challenges cooperative per- ception systems typically assume sensor consistency. introduce hecofuse, unified framework designed maintain robust perception performance across diverse sensor configurations,which framework illustrated fig. approach addresses nine distinct sensor arrangements full sensor configuration lclc homogeneous single-type ll, heterogeneous single-type lc, mixed configurations lcc, lcl, clc, llc denotes lidar denotes camera, notation symbol separates vehicle node infrastructure node convention used throughout paper. core hecofuse cooperative learning strategy enables seamless operation across heterogeneous configurations. rather training separate models sensor arrangement, employ unified learning approach network learns extract maximum information available sensors maintaining consistent fea- ture representations. achieved modular architecture feature adaptation components ensure dimensional compatibility different sensor inputs. training, randomly sample nine configu- rations, enabling model learn robust feature extraction fusion strategies generalize across sensor combi- nations. approach ensures graceful degradation high-quality sensors unavailable. instance, vehicle cameras interacts infrastructure equipped lidar sensors, system maintains effective perception adaptively fusing complementary information. fig. bev feature extraction process heterogeneous sen- sors. framework handles various sensor configurations specialized encoding paths lidar camera, along intra-node fusion multi-modal configurations pseudofusion single-sensor scenarios. hecofuse adopts modular approach feature ex- traction adapts dynamically available sensors node, illustrated fig. given node vehicle, infrastructure sensor set camera, lidar, features extracted sensor- specific pathways fusion fusef camera lidar lidar feature encoding lidar processing, transform unstructured point cloud data birds eye view bev features multi-stage pipeline. first, dynamic voxelization module converts raw point clouds structured grid, adaptively handling varying point densities across different scenes. voxelization process as- signs points voxel cells aggregates information within cell, significantly reducing computational complexity preserving essential spatial information. voxelized features processed sparse convolutional backbone efficiently extracts spatial patterns. operating non-empty voxels, sparse convolution maintains computational efficiency cap- turing detailed geometric structure. finally, features projected onto ground plane create dense birds- eye-view representation serves input subsequent fusion stages. approach effectively leverages precise spatial information provided lidar remaining computationally tractable sparse operations. camera feature encoding camera pathway transforms perspective images bev space se- quential process designed maintain rich semantic informa- tion establishing geometric consistency. starting multi-view images, apply convolutional backbone extract features capture appearance, texture, object information. initial features enhanced feature neck network e.g., fpn creates multi-scale representations, providing fine-grained details high- level semantic understanding. critical step camera bev encoding view transformation module, converts perspective view features bev space. involves first estimating depth distributions image pixel projecting fea- tures along distributions space using camera-to- lidar transformation matrices camera intrinsic param- eters. finally, height dimension collapsed form bev representation. approach addresses inherent challenges transforming perspective views coordinate system, enabling effective fusion lidar features despite fundamentally different sensing modal- ities. bev fusion node contains lidar camera sensors e.g., lclc, lcl, lcc con- figurations, apply bev fusion module combine complementary information modalities bevfusionf camera lidar bevfusion concatenates features along chan- nel dimension applies convolutional layer batch normalization activation bevfusionf, actnormconvf, intra-node fusion leverages complementary strengths modalitylidar provides accurate spa- tial information cameras offer rich semantic features. fused representation enhances detection performance combining strengths sensors. partial sensor configuration lidar-only camera- key challenge heterogeneous systems handling nodes single sensor types. configu- rations, employ pseudofusion ensure feature compat- ibility across different nodes. l-only c-only configurations, pseudofusion em- ploys sensor-specific feature adapters transform features consistent dimensionality adapted adapterf lidar, camera adapter sensor-specific convolutional layer followed normalization activation. ensures regardless input sensor, output features maintain consistent channel dimensions adapted rbcouthw handle absence specific sensor, pseudofusion checks sensor availability creates appropriate feature representations adapted sensor available fallbackb, cout, sensors available fallback creates baseline feature representation extreme cases sensor data temporarily unavail- able. approach ensures hecofuse maintains robust performance across nine possible sensor configurations guaranteeing feature compatibility regardless sensor availability. joint attention-based fusion adaptive resolution core challenge heterogeneous fusion lies ef- fectively combining features nodes different sensor configurations maintaining computational efficiency. illustrated fig. address two- pronged approach hierarchical attention fusion adaptive spatial resolution adjustment. hierarchical attention fusion haf given bev features fvehicle finfra potentially asymmetric sensor sources, channel attention mechanism first as- sesses feature channels provide reliable information wchannel heterogeneous configurations, enables network adaptively weight channels different sensor modal- ities. instance, configuration vehicle lidar, infrastructure camera, preferentially weight lidar-dominated channels distance estimation favoring camera-dominated channels appearance features fchannel fvehicle finfra shown central portion fig. spatial attention refines fusion addressing modality- specific strengths different spatial regions. nodes varying sensor configurations, spatial attention maps highlight regions nodes particular sensors provide reliable information convconvfn rbhw heterogeneous settings, attention maps become particularly importantwhen vehicle lidar interacts infrastructure cameras, spatial at- tention emphasizes lidars strengths accurate distance measurements distant objects prioritizing camera- based detections visually complex regions ffused fvehicle avehicle finfra ainfra denotes hadamard element-wise product, allowing fine-grained weighting feature element independently. adaptive spatial resolution asr different sensor configurations produce bev features varying char- acteristics, requiring different computational resources. depicted upper lower portions fig. approach incorporates adaptive resolution adjustment works concert attention fusion. heterogeneous configurations, first apply configuration-aware downsampling fn,down downsamplefn, adaptiveavgpooldfn, hsn, wsn, determined based specific sensor config- uration node shigh, node camera-only slow, node lidar-only smedium, node sensors hierarchical attention fusion applied downsampled features. fusion, apply configuration- aware upsampling prioritizes restoration detailed information precise sensor configuration ffinal upsampleffused, minsvehicle, sinfra integrated approach, combining hierarchical attention fusion adaptive resolution adjustment, enables heco- fuse effectively bridge gap different sensor configurations reducing computational overhead heterogeneous configurations. fig. architecture inter-node feature fusion combining hierarchical attention fusion haf adaptive spatial resolution asr mechanisms. shaded regions denote asr modules. iv. experimental evaluations dataset selection simulated real-world datasets offer distinct advantages cooperative perception research. simulated datasets like vxset opvv provide controlled environments, real-world datasets dair- vx-seq vxreal capture authen- tic traffic dynamics sensor noise. heterogeneous cooperative perception framework, require dataset meet- ing several criteria real-world data collection, multi-modal sensing lidar camera streams node, consistent sensor types across nodes, precise calibration vehicle infrastructure. dataset also feature diverse traffic scenarios sufficient annotations evaluate detection performance. table quantitative comparison hecofuse baseline challenge methods across different sensor configurations. method type config precision recall iou pos. rmse rot. rmse map vehicle infra. coopdetd buptmm team kaai hecofuse full sensor configuration hecofuse homogeneous single-type hecofuse hecofuse heterogeneous single-type hecofuse hecofuse mixed configurations hecofuse hecofuse hecofuse bolded values indicate best-performing model metric. results reported obtained using pre-trained camera lidar models, reproduction, training performed scratch. methods represent top- performing algorithms cvpr drivex workshop tumtraf-vx challenge excluding hecofuse. selected tumtraf-vx dataset experi- ments due advantages heterogeneous cooperative perception research. collected urban intersection munich, includes synchronized data vehicle infrastructure perspectives camera images, lidar point clouds, annotated bounding boxes across object categories. nodes feature mechan- ical lidar sensors cameras similar specifications, enabling simulation various heterogeneous configurations maintaining consistent baseline evaluation. quantitative analysis trained hecofuse framework scratch tumtraf-vx mini dataset half complete dataset epochs using single geforce rtx gpu. vehicle infrastructure nodes, employed yolov backbone encoder camera data processing, pointpillars utilized encoder lidar point cloud processing. comprehensively evaluate method, adopted multiple metrics including precision, recall, iou, position rmse, rotation rmse, map across various object categories. table presents performance comparison official tumtraf-vx baseline method, coopdetd metrics evaluate different aspects detection performance precision recall measure detection accuracy, iou quantifies spatial overlap predicted ground truth bounding boxes, position rotation rmse assess localization ac- curacy, map provides comprehensive measure detection performance across varying confidence thresholds. hecofuse method achieved first place cvpr drivex workshop tumtraf-vx challenge based map metric llc configuration vehicle lidar only, infrastructure lidar camera, reaching map. across six eval- uation metrics, hecofuse achieves best tied-for-best performance five metrics, demonstrating superiority approach. full sensor configuration lclc, hecofuse achieves map, representing improvement baselines notably, llc configuration delivers highest overall performance map superior precision high- lighting effectiveness hierarchical attention fusion mechanism heterogeneous settings. results reveal several important insights het- erogeneous sensor fusion. first, lidar sensors contribute significantly detection performance cameras framework. instance, comparing map map configurations shows substantial performance gap, indicating lidars superior spa- tial accuracy object detection. second, infrastructure sensors play pivotal role overall system performance due wider field view elevated mounting position. evident comparing llc map lcl map, former achieves higher map despite using fewer total sensors. interestingly, metrics llc configuration even surpass fully-equipped lclc case may roadside camera already provides sufficiently rich per- spectives, vehicles additional camera, limited mounting viewpoint, adds little extra benefit. findings validate adaptive fusion approachs capability dynamically leverage reliable infor- mation sources across heterogeneous configurations, main- taining robust performance even sensor asymmetry. represents critical capability real-world deployments sensor configurations often vary widely. qualitative analysis provide comprehensive understanding hecofuses performance real-world scenarios, present qualita- tive visualization results using full sensor configuration lclc tumtraf-vx test set, shown fig. visualization demonstrates systems detection capa- bilities multiple perspectives bev representation fused lidar point clouds, vehicles camera view, fig. qualitative visualization hecofuse detection results full sensor configuration lclc tumtraf- test set. figure illustrates detection results daytime first row nighttime second row conditions. a-, bev perspective showing fused lidar point clouds detection results vehicle infrastructure a-, vehicle-mounted camera view corresponding projections detections a-, a-, a-, b-, b-, three different infrastructure camera views projected detection results. three different infrastructure camera viewpoints. specif- ically selected examples daytime nighttime conditions illustrate robustness approach across varying lighting conditions. framework successfully detects multiple object cate- gories including passenger vehicles, buses, trains, pedes- trians across different viewing angles environmental conditions. notably, detection performance remains con- sistent challenging nighttime scenarios b-, camera-only methods typically struggle due poor lighting. robustness attributed hierarchical attention fusion mechanism, dynamically adjusts contri- bution sensor modality based reliability different environmental conditions. instance, nighttime scenarios, fusion mechanism automatically places greater emphasis lidar features, less affected lighting variations, still leveraging complementary semantic information cameras. multi-perspective visualization demonstrates hecofuse effectively integrates information spatially distributed sensors, ad- dressing occlusion issues extending perceptual range beyond would possible single sensor node. qualitative results reinforce quantitative findings highlight practical advantages heterogeneous sensor fusion robust cooperative perception complex urban environments. conclusion paper, introduce hecofuse, unified cross- modal cooperative perception framework specifically de- signed address challenges heterogeneous sensor configurations vehicle-infrastructure cooperative percep- tion systems. approach bridges gap ideal- ized research assumptions real-world deployments vehicles infrastructure possess varying sensor types capabilities. hecofuse incorporates several key innovations hierarchical attention-based fusion mechanism dy- namically integrates features based quality across different modalities nodes, adaptive spatial resolution adjustment module balances computational efficiency information preservation, cooperative learning strategy enables consistent performance across nine dif- ferent sensor configurations. extensive experiments real-world tumtraf-vx dataset, demonstrate hecofuse achieves state-of-the-art performance fully equipped settings maintaining robust accuracy het- erogeneous scenarios, particularly infrastructure nodes equipped multiple sensor modalities. notably, framework shows exceptional resilience asymmetric con- figurations common practical deployments, nodes may partial sensing capabilities. several promising directions remain future research. first, extending validation scenarios two nodes would demonstrate scalability approach complex urban environments multi- ple vehicles infrastructure units. second, challenging weather conditions rain, fog snow represent critical test cases heterogeneous fusion approaches, different sensor modalities exhibit complementary strengths weaknesses adverse conditions. conditions place greater demands cooperative perception systems would validate adaptive fusion capabilities framework. finally, expanding camera configurations incorporate additional viewpoints vehicle side could help address performance gap lidar camera-based detection, potentially enabling bal- anced multi-modal fusion even lidar sensors unavailable. would particularly valuable low-cost cooperative perception systems camera sensors might primary sensing modality available nodes. references yu, li, wang, chen, zhou, review cooperative perception control supported infrastructure-vehicle system, green energy intelligent transportation, vol. no. s.-w. kim, qin, chong, shen, liu, ang, frazzoli, rus, multivehicle cooperative driving using cooperative perception design experimental validation, ieee transactions intelligent transportation systems, vol. no. pp. xu, xiang, tu, xia, m.-h. yang, ma, vx-vit vehicle-to-everything cooperative perception vision transformer, european conference computer vision. springer, pp. karvat givigi, adver-city open-source multi-modal dataset collaborative perception adverse weather conditions, arxiv preprint xiang, xu, ma, hm-vit hetero-modal vehicle-to-vehicle cooperative perception vision transformer, proceedings ieeecvf international conference computer vision, pp. wei, qin, zhang, wu, barth, integrating multi- modal sensors review fusion techniques intelligent vehicles, ieee intelligent vehicles symposium iv. ieee, zimmer, wardana, sritharan, zhou, song, knoll, tumtraf cooperative perception dataset, proceedings ieeecvf conference computer vision pattern recog- nition, pp. huang, liu, zhou, nguyen, azghadi, xia, q.-l. han, sun, cooperative perception autonomous driving recent advances challenges, arxiv preprint han, zhang, li, jin, lang, li, collaborative perception autonomous driving methods, datasets, challenges, ieee intelligent transportation systems magazine, chen, tang, yang, fu, cooper cooperative percep- tion connected autonomous vehicles based point clouds, ieee international conference distributed computing systems icdcs. ieee, pp. liu, ding, fu, li, chen, zhang, zhou, vx- vehicle-to-everything collaborative perception via point cluster, arxiv preprint chen, ma, tang, guo, yang, fu, f-cooper feature based cooperative perception autonomous vehicle edge computing system using point clouds, proceedings acmieee symposium edge computing, pp. xu, xiang, xia, han, li, ma, opvv open benchmark dataset fusion pipeline perception vehicle-to-vehicle communication, international conference robotics automation icra. ieee, pp. lin, tian, duan, zhou, zhao, cao, vvformer vehicle-to-vehicle cooperative perception spatial-channel trans- former, ieee transactions intelligent vehicles, t.-h. wang, manivasagam, liang, yang, zeng, ur- tasun, vvnet vehicle-to-vehicle communication joint perception prediction, computer visioneccv european conference, glasgow, uk, august proceedings, part springer, pp. bai, wu, barth, liu, sisbot, oguchi, pillargrid deep learning-based cooperative perception object detection onboard-roadside lidar, ieee inter- national conference intelligent transportation systems itsc. ieee, pp. zhang, chen, qin, hu, hao, coopercept coop- erative perception object detection autonomous vehicles, drones, vol. no. wang, fan, huo, xu, wang, liu, chen, y.-q. zhang, vimi vehicle-infrastructure multi-view interme- diate fusion camera-based object detection, arxiv preprint ahmed, mercelis, anwar, collabgat collaborative perception using graph attention network, ieee access, zhao, mu, hui, prehofer, cooperative vehicle- infrastructure based urban driving environment perception method using theory-based credibility map, optik, vol. pp. dempster, generalization bayesian inference, journal royal statistical society series methodological, vol. no. pp. dempster, upper lower probabilities induced multi- valued mapping, classic works dempster-shafer theory belief functions. springer, pp. shi, cui, jiang, yan, xing, niu, ouyang, vips real-time perception fusion infrastructure-assisted autonomous driving, proceedings annual international conference mobile computing networking, pp. khalifa, alouani, mahjoub, rivenq, novel multi-view pedestrian detection database collaborative intelligent transportation systems, future generation computer systems, vol. pp. pereira, dorey, aguiar, poster cooperative per- ception platform intelligent transportation systems, ieee vehicular networking conference vnc. ieee, pp. zheng, xia, gao, xiang, ma, cooperfuse real-time cooperative perception fusion framework, ieee intelligent vehicles symposium iv. ieee, pp. li, ren, wu, chen, feng, zhang, learning distilled collaboration graph multi-agent perception, advances neural information processing systems, vol. pp. xie, zhou, qiu, zhang, qu, soft actorcritic-based multilevel cooperative perception connected autonomous vehicles, ieee internet things journal, vol. no. pp. arnold, dianati, temple, fallah, cooperative per- ception object detection driving scenarios using infrastructure sensors, ieee transactions intelligent transportation systems, vol. no. pp. liu, wang, yu, zhou, chen, region-based hybrid collaborative perception connected autonomous vehicles, ieee transactions vehicular technology, qu, chen, zhu, zhu, avedisov, fu, yang, head bandwidth-efficient cooperative perception approach heterogeneous connected autonomous vehicles, arxiv preprint luo, chen, luan, li, li, vehicle-to-infrastructure beyond visual range cooperative perception method based hetero- geneous sensors, energies, vol. no. bochkovskiy, c.-y. wang, h.-y. liao, yolov op- timal speed accuracy object detection, arxiv preprint yin, zhou, krahenbuhl, center-based object detec- tion tracking, proceedings ieeecvf conference computer vision pattern recognition, pp. cvpr drivex workshop tumtraf-vx cooperative perception challenge, proceedings ieeecvf conference computer vision pattern recognition cvpr workshops. ieee, cooperative perception challenge track. yu, luo, shu, huo, yang, shi, guo, li, hu, yuan al., dair-vx large-scale dataset vehicle- infrastructure cooperative object detection, proceedings ieeecvf conference computer vision pattern recognition, pp. yu, yang, ruan, yang, tang, gao, hao, shi, pan, sun al., vx-seq large-scale sequential dataset vehicle-infrastructure cooperative perception forecasting, proceedings ieeecvf conference computer vision pattern recognition, pp. xiang, zheng, xia, xu, gao, zhou, han, ji, li, meng al., vx-real largs-scale dataset vehicle-to- everything cooperative perception, arxiv preprint jocher, qiu, chaurasia, ultralytics yolo, jan. online. available lang, vora, caesar, zhou, yang, beijbom, pointpillars fast encoders object detection point clouds, proceedings ieeecvf conference computer vision pattern recognition, pp.", "published_date": "2025-07-18T06:02:22+00:00"}
{"id": "2507.08022v1", "title": "CuriosAI Submission to the EgoExo4D Proficiency Estimation Challenge 2025", "authors": ["Hayato Tanoue", "Hiroki Nishihara", "Yuma Suzuki", "Takayuki Hori", "Hiroki Takushima", "Aiswariya Manojkumar", "Yuki Shibata", "Mitsuru Takeda", "Fumika Beppu", "Zhao Hengwei", "Yuto Kanda", "Daichi Yamaga"], "summary": "report presents curiosai teams submission egoexod proficiency estimation challenge cvpr propose two methods multi-view skill assessment multi-task learning framework using sapiens-b jointly predicts proficiency scenario labels accuracy, two-stage pipeline combining zero-shot scenario recognition view-specific videomae classifiers accuracy. superior performance two-stage approach demonstrates effectiveness scenario-conditioned modeling proficiency estimation.", "full_text": "curiosai submission egoexod proficiency estimation challenge hayato tanoue hiroki nishihara yuma suzuki takayuki hori hiroki takushima aiswariya manoj kumar yuki shibata mitsuru takeda fumika beppu zhao hengwei yuto kanda daichi yamaga softbank corp. data technology planning division hayato.tanoue, hiroki.nishihara, yuma.suzuki, takayuki.hori, hiroki.takushima, aiswariya.manojkumar, yuki.shibata, mitsuru.takeda, fumika.beppu, hengwei.zhao, yuto.kanda, daichi.yamagag.softbank.co.jp abstract report presents curiosai teams submission egoexod proficiency estimation challenge cvpr propose two methods multi-view skill assessment multi-task learning framework using sapiens-b jointly predicts proficiency scenario labels accuracy, two-stage pipeline combining zero-shot scenario recognition view-specific videomae classifiers accuracy. superior performance two-stage approach demonstrates effectiveness scenario-conditioned modeling proficiency estimation. introduction recent advances wearable cameras enabled first-person video understanding applications skill assessment, sports analytics, education. egoexod dataset provides synchronized egocentric exocentric views hereafter referred ego exo, offering rich contextual information activity understanding. proficiency estimation taskassessing skill level multi-view videospresents unique challenges arising subjectivity skill assessment, high motion variability, coarse proficiency annotations. models must capture subtle execution differences yet remain robust viewpoint variations. investigate two methods multi-task learning framework built sapiens-b jointly processes views, two-stage pipeline decouples scenario recognition proficiency assessment using qwen-vlm videomae experiments demonstrate scenario-specific modeling view-aware classifiers outperforms joint table challenge leaderboard results team accuracy pcie egopose curiosai edpe baseline approach leading final challenge submission. task description challenge requires predicting demonstrators proficiency level novice, early expert, intermediate expert, late expert synchronized multi-view videos. sample contains five rgb streams one ego four exo, covering six scenarios dance, rock climbing, basketball, music, cooking, soccer. performance evaluated using top- accuracy official test split. method present two modeling strategies multi-task learning framework built sapiens-b jointly processes views, two-stage pipeline consisting scenario recognition view-specific proficiency classification. validation experiments showed two-stage approach outperformed multi-task learning leading final submission. leaderboard shows scores decimal form e.g., corresponds cs.cv jul method sapiens-b-based multi-task learning method uses -billion-parameter sapiens-b model unified video encoder joint proficiency scenario prediction, processing five camera views simultaneously. architecture. given frames per clip, sapiens-b outputs features rbt batch size number frames. temporal mean pooling yields clip representations f,t,, feed two linear heads proficiency wprof scenario wscen prediction. training. sample eight frames via linear interpolation jointly process views. multi-task objective balances losses lce wprof yprof proficiency lce wscen yscen scenario results. model achieves validation accuracy significant variation across scenarios table figure shows scenario recognition converges faster proficiency estimation, suggesting different task complexities multi-task objective. method scenario-conditioned two-stage pipeline method addresses evaluation-only track training exclusively official training split publicly available pre-trained weights. employ two-stage pipeline first, qwen-.-vlm-b performs zero-shot scenario recognition, scenario-specific classifiers estimate demonstrator proficiency conditioned predicted scenario camera view. training loss validation loss figure training validation loss multi-task model. solid total loss, dashed proficiency sub-loss, dotted scenario sub-loss. table method validation accuracy scenario accuracy basketball cooking dance music rock climbing soccer overall architecture. scenario detection, leverage zero-shot qwen-.-vlm-b without fine-tuning. proficiency classification, fine-tune videomae-v-huge end-to-end, adding single linear layer maps cls token four proficiency logits. capture scenario- view-specific patterns, instantiate specialized modelsone combination scenarios camera views. training details. input clips comprise uniformly sampled frames, resized via shorter-side resizing centre-cropping, normalised using videomae statistics. optimize parameters adamw learning rate weight decay epochs using batch size clips per gpu. inference. inference pipeline proceeds follows obtain scenario via zero-shot qwen-vlm prediction view ego, exo, run corresponding v-specific classifier obtain logits convert probabilities via softmaxv aggregate predictions using one three strategies ego pego pego exo average pexo pexoi combined pcomb pego pexoi results. table reports overall validation accuracy, five-view fusion achieving absolute improvement method performance gain demonstrates effectiveness scenario-specific modeling. ego-only configuration slightly outperforms method multi-view approach, exo averaging provides substantial gains. per-scenario analysis table reveals interesting view-dependent patterns. exo views excel activities requiring global spatial awareness dance vs. ego music vs. ego, ego views prove superior spatially constrained tasks rock climbing ego vs. exo. soccer shows viewpoint invariance both, likely table validation top- accuracy method ego exo avg combined top- acc. table per-scenario validation accuracy method scenario ego exo combined basketball cooking dance music rock climbing soccer due standardized player movements consistent field constraints. results validate hypothesis different scenarios benefit different viewpoint configurations, justifying view-specific classifier design. conclusion presented two methods egoexod proficiency estimation challenge cvpr multi-task learning sapiens-b two-stage pipeline scenario-conditioned videomae classifiers. experiments demonstrate decoupling scenario recognition proficiency estimation yields superior performance vs. highlighting importance scenario-specific modeling. analysis reveals distinct view-dependent patterns across activities, exo views excelling spatially complex tasks ego views constrained activities. findings suggest adaptive view selection based scenario characteristics could improve proficiency estimation multi-view settings. references grauman, westbury, torresani, kitani, malik, afouras, ashutosh, baiyya, bansal, boote al., ego-exod understanding skilled human activity first-and third-person perspectives, arxiv preprint khirodkar, bagautdinov, martinez, zhaoen, james, selednik, anderson, saito, sapiens foundation human vision models, arxiv preprint bai, chen, liu, wang, ge, song, dang, wang, wang, tang, zhong, zhu, yang, al., qwen.-vl technical report, arxiv preprint wang, huang, zhao, tong, he, wang, wang, qiao, videomae scaling video masked autoencoders dual masking, arxiv preprint", "published_date": "2025-07-08T12:33:02+00:00"}
{"id": "2507.04270v3", "title": "ZERO: Industry-ready Vision Foundation Model with Multi-modal Prompts", "authors": ["Sangbum Choi", "Kyeongryeol Go", "Taewoong Jang"], "summary": "foundation models revolutionized ai, yet struggle zero-shot deployment real-world industrial settings due lack high-quality, domain-specific datasets. bridge gap, superb introduces zero, industry-ready vision foundation model leverages multi-modal prompting textual visual generalization without retraining. trained compact yet representative million annotated samples proprietary billion-scale industrial dataset, zero demonstrates competitive performance academic benchmarks like lvis-val significantly outperforms existing models across diverse industrial datasets. furthermore, zero achieved place cvpr object instance detection challenge place foundational few-shot object detection challenge, highlighting practical deployability generalizability minimal adaptation limited data. best knowledge, zero first vision foundation model explicitly built domain-specific, zero-shot industrial applications.", "full_text": "zero industry-ready vision foundation model multi-modal prompts sangbum choi superb seoul, south korea sbchoisuperb-ai.com kyeongryeol superb seoul, south korea krgosuperb-ai.com taewoong jang superb seoul, south korea twjangsuperb-ai.com abstract foundation models revolutionized ai, yet struggle zero-shot deployment real-world industrial settings due lack high-quality, domain-specific datasets. bridge gap, superb introduces zero, industry- ready vision foundation model leverages multi-modal prompting textual visual generalization without retraining. trained compact yet representative million annotated samples proprietary billion-scale industrial dataset, zero demonstrates competitive perfor- mance academic benchmarks like lvis-val signifi- cantly outperforms existing models across diverse indus- trial datasets. furthermore, zero achieved place cvpr object instance detection challenge place foundational few-shot object detection challenge, highlighting practical deployability gen- eralizability minimal adaptation limited data. best knowledge, zero first vision foun- dation model explicitly built domain-specific, zero-shot industrial applications. introduction historically, improving model performance required deep domain expertise inject task-specific inductive biases model architectures. however, advent large-scale datasets shifted paradigm. empirical evidence in- creasingly supports stacking generic transformer lay- ers, rather hand-crafting domain-specific modules, yield superior results shift simplified model design reduced reliance manual heuristics. transition given rise foundation models, substantially accelerated progress across wide range tasks. text-centric models like bert vision-centric models dinov multi- modal models including clip demonstrated re- equal contribution corresponding author markable generalization capabilities. advances fueled release massive public datasets e.g., sa-b laion-b parameter-efficient fine- tuning methods like lora enable fast adaptation minimal computational cost. despite advancements, current foundation models remain ill-suited zero-shot deployment real-world, domain-specific industrial settings. datasets used train largely focus general-purpose content, high-quality, domain-specific datasets, especially areas manufacturing, logistics, medical imaging, scarce due high annotation costs long-tail data distri- butions. generative models support data augmentation, benefits often constrained diminishing re- turns, predicted scaling laws recent work active learning, data valuation, subset selection aims mitigate issues, challenge curating effective domain-specific datasets persists core bottle- neck industrial adoption. address gap, present zero, vision foun- dation model designed industry-ready data- efficient. unlike traditional foundation models rely heavily large, general-purpose pretraining require substantial downstream adaptation, zero leverages multi- modal promptingboth textual visualto enable gen- eralization across tasks domains without retraining. approach allows zero directly deployed pro- duction environments, significantly reducing time cost associated integration enterprise settings. developed superb ai, zero builds ex- tensive experience supporting end-to-end develop- ment pipelines enterprise customers across diverse ver- ticals. leveraging proprietary, billion-scale industrial dataset spanning manufacturing, retail, logistics, security, surveillance, curated compact yet highly repre- sentative dataset million annotated samples. despite relatively modest size compared vision founda- tion training corpora, dataset captures complexity variability inherent real-world industrial scenarios. experiments demonstrate zero achieves com- cs.cv jul petitive performance lvis-val, widely used aca- demic benchmark, outperforming existing models across multi-domain industrial datasets. moreover, zero achieved top ranks competitive open-world set- tings, placing object instance detection chal- lenge honorable mention foundational few- shot object detection fsod challenge, held cvpr open world vision workshop. results underscore unique position zero foundation model powerful generalizable also practically deployable minimal adaptation limited data. knowledge, zero first vision foundation model explicitly built domain-specific, zero-shot indus- trial applications, bridging persistent gap aca- demic innovation real-world enterprise needs. related work open-vocabulary object detection recent advancements contrastive learning enabled models relate images text mapping text image embeddings shared semantic space breakthrough allowed open-vocabulary object detectors trained successfully large-scale web-based image datasets without extensive relabeling. pioneering works, including owl-vit glip used contrastive learning methods align image text embeddings. groundingdino significantly im- proved performance integrating modern object detec- tion architectures language models dual-encoder framework. groundingdino became foundation subsequent research, including mm-groundingdino extended capabilities multi-modal inputs, grounded-sam combined sam seg- mentation. recently, llm-det advanced research direction incorporating large language models, enabling models understand sophisticated natural language queries. object detection visual prompts another line research extends open-vocabulary object de- tection incorporating visual prompts alleviate in- herent ambiguity text prompts. approach includes dinov t-rex t-rex dinov, built top dino enables visual in-context prompting, visual prompts come input images model searches objects. t-rex t-rex relax constraint in-context prompting, allowing mod- els use visual examples image source. dino- pushed boundaries enabling models present object masks keypoints. also introduced uni- versal prompting, model locates objects with- user prompts. zero initially built open-vocabulary object detection model progressively extended sup- port text prompts also visual prompts. section present training strategy enables model effectively leverage visual prompts preserv- ing gradually enhancing pretrained ability inter- pret text prompts. real-time visual grounding one problem previous models, despite power- ful, require substantial memory computa- tional resources, hinders practical deployment real-time inference industry. address problem, several works implemented open-set object detectors based yolo architectures. yolo-world first proposed purpose, yoloe im- proved performance terms map sophis- ticated architectural design. although zero intended real-time inference, discuss efficiency-oriented design choices deployment section ... referring expression comprehension recent improvements vision-language models vlms opened new research direction involving complex logical reasoning object detection. compared previous works, line research focuses fil- tering detected objects using vlms capabilities, enabling models account positional relationships ob- jects object attributes. works constructed datasets train models integrating outputs existing models manual labeling loop. cha- trex first attempt merge vlms object detectors, rex-seek focuses processing human attribute queries. rex-thinker improved rex-seek adding reasoning capabilities model. described section .., zero leverages vision- language model vlm solely constructing auto- labeling pipeline obtain high-quality, richly annotated data used model training inference. ex- tending use vlms incorporate visual context enhanced scene understanding video analysis left future work. zero enable efficient robust training zero, design comprehensive methodology centered around two key pil- lars data engine training strategy. data engine systematically constructs compact yet richly annotated dataset advanced collection, selection, bidi- rectional pseudo-labeling pipelines. complementing this, training strategy progressively adapts pretrained single- modal models support multi-modal prompts leverag- ing carefully orchestrated distillation contrastive learn- figure overview proposed zero training pipeline. pipeline composed data engine left, constructs compact richly annotated industrial dataset collection, selection, pseudo-labeling training strategy right progressively adapts pretrained open-vocabulary detector using distillation alignment, ultimately enabling decoupled infer- ence either text visual prompt. ing techniques. together, components form foun- dation versatile vision foundation model excels detection referring tasks maintaining in- ference efficiency. please refer overall pipeline figure data engine effectively train zero, designed data engine comprises three core components collection, selection, labeling. engine built goal constructing compact yet highly representative dataset tailored data- efficient training vision foundation models real-world industrial settings. ... data collection superb accumulated large-scale repository in- dustrial domain data years supporting end-to- end development pipelines enterprise customers. proprietary data asset spans diverse verticals, including manufacturing, retail, logistics, security, surveillance, serves foundation training zero. ... data selection building training dataset, employed advanced data selection techniques developed years op- erating commercial data curation services. selection process formulated multi-objective optimization task jointly considers sample-level uncertainty dataset- level diversity. ensure resulting subset remains informative transferable across wide range mod- els domains, leveraged embedding representations multiple foundation models. empirical results demon- strate selection strategy achieve near-equivalent training performance using fraction original dataset. based this, curated dataset million high-quality samples zero training. scale significantly smaller vision founda- tion models, highlights maturity superb ais data- centric approach demonstrates foundation models developed efficiently without requiring massive re- sources, thus offering viable path even startups academic institutions. ... data labeling maximize utility training sample enable broader task generalization, developed customized auto-labeling pipeline. pipeline enhances per- sample learning efficiency also expands annotation space include referring expression comprehension, involves complex linguistic structures beyond traditional detection tasks. unlike recent vlm-based approaches improve referring performance expense detection accuracy, pipeline designed enrich annotation quality coarse fine-grained levels, thereby en- abling single model handle detection referring tasks effectively. pipeline integrates two complementary directions. first, extract captions images using internvl identify noun phrases via spacy noun phrases filtered using wordnet isolate expres- sions corresponding physical objects. using prelimi- nary version zero trained ground-truth annotations, aggregate inference results guided tex- tual visual prompts localize bounding boxes phrase. second, reverse process generating object proposals upn extracting corresponding captions using chatrex step, captioning focuses visual attributes color, shape, material, allowing richer object-level descriptions. maintain label qual- ity, filter instances extremely small bounding boxes apply siglip assess alignment cropped image regions generated captions. low- alignment pairs removed training set. altogether, data engine integrates extends strate- gies commonly used construction foundation model datasets. empirical analysis shows bidi- rectional labeling process effectively yields high-quality su- pervision signals across different granularities plays critical role enabling zero generalize across wide range industrial domains. training strategy numerous open-source models visual ground- ing task exist, typically limited supporting single type prompt, often textual. models like yoloe support visual prompting, still un- derperform compared modern open-vocabulary detectors overcome limitation without redesigning base model architecture, focus extending pre- trained single-modal models support multiple prompt modalities. summary, approach distills open- vocabulary detection capabilities pretrained model early stages subsequently enhances performance multi-prompt synergy. final trained model efficient inference time, requiring contrastive prompt encoders. ... progressive adaptation multi-prompt naively appending visual prompt encoder lead performance degradation, particularly early training stages, due representational mismatch pre- trained text prompt encoder. mitigate this, begin training text-only prompts employ infonce loss align visual prompt encoder pre- trained text prompt encoder. prevent degradation text embeddings, limit gradient propagation dis- tillation happens one direction text en- coder visual encoder. relying solely unidirectional distillation train- ing stability limit synergy text visual prompts. address this, incorporate contrastive back- bone, clip inherently supports text- visual alignment. framework, visual prompt en- coder realized contrastive visual encoder, new contrastive text encoder introduced gradually re- place original pretrained text encoder. design yields three distinct prompt encoders pretrained text prompt encoder, contrastive visual prompt encoder, contrastive text prompt encoder. training, apply unidirectional distillation pretrained text encoder contrastive visual encoder ensure stability. parallel, promote multi-modal syn- ergy enforcing bidirectional alignment con- trastive visual text encoders. gradually reduce reliance pretrained encoder, adopt probabilistic prompt scheduling strategy. dur- ing first half training, usage probability pretrained text encoder decreases linearly probabilities contrastive prompt encoders increase values remain fixed remainder training. ... decoupled inference important note purpose distillation alignment losses promote well-structured embed- ding space multi-prompt interaction train- ing. inference, however, necessary pro- vide multiple prompts simultaneously, since forward pass involve prompt-level interaction. model accept either text prompt, visual prompt, both, optional merging ensemble-like manner. con- sequently, pretrained text prompt encoder safely removed deployment, inference proceed using contrastive backbone. experiment section presents comprehensive evaluation pro- posed zero model across wide range settings. ex- periments organized along two main axes zero-shot performance public benchmarks industrial multi- domain datasets, results competitive chal- lenges cvpr open world vision workshop. zero-shot performance evaluate zero-shot detection performance three prompting protocols introduced jiang al. text-g corresponds standard open-vocabulary setting, object categories specified using text prompts i.e., class names without visual exemplars. visual-g em- ploys category-level visual prompts derived averaging embeddings training exemplars per class, simu- lating scenarios reference images avail- able. visual-i represents interactive setting single instance crop test image serves vi- sual prompt detect similar objects within im- age. assumes access ground-truth regions test time, reflects practical applications interac- tive annotation. results summarized table note cells marked indicate corresponding prompt type supported given model. ... academic benchmark zero demonstrates strong performance widely used lvis-val benchmark, achieving visual-i score outperforms comparable open-source models yoloe approaches state-of- the-art t-rex remarkably, zero attains level accuracy using training exam- plessignificantly fewer models rely tens even hundreds millions sampleshighlighting data efficiency practical scalability. ... industrial benchmark evaluate generalization real-world settings, assess zero diverse industrial datasets spanning domains healthcare, autonomous driving, retail, gam- ing. multi-domain dataset, report three aggregate metrics. text-g visual-g scores first computed per domain averaged across domains. max metric obtained selecting higher value text-g visual-g scores domain, followed averaging maximum values across domains. table zero-shot performance lvis-val benchmark multi-domain datasets. model training dataset size lvis-val multi-domain dataset visual-i text-g visual-g max t-rex florence- yoloe ov-dino dino-x owlv zero together, metrics provide comprehensive view models performance across wide range scenarios. across domains, zero consistently outperforms previ- ous models substantial margin. ... qualitative evaluation beyond strong quantitative results, zero also demon- strates impressive qualitative performance across diverse vi- sual domains. example, precisely segments masks varying shapes sizes medical images, reliably detects cars trucks complex traffic scenes autonomous driving, maintains high accuracy retail gam- ing environments despite visual clutter style variations. representative visual examples presented figure cvpr challenge evaluate real-world utility zero general- purpose vision foundation model, participated two practically motivated object detection challenges ob- ject instance detection insdet challenge foun- dational few-shot object detection fsod challenge. benchmarks assess complementary aspects visual recognitioninstance-level retrieval open-world scenar- ios few-shot generalization across diverse industrial do- mains. tasks minimize reliance predefined cate- gories extensive supervision, aligning well zeros multi-modal, prompt-driven architecture. section, describe methods report results achieved zero challenge. ... object instance detection challenge insdet challenge presents practically motivated bench- mark built recently introduced insdet dataset. un- like conventional object detection tasks aim recog- nize instances predefined categories, task focuses detecting specific object instances provided visual ex- emplars captured multiple viewpoints. formula- tion especially relevant robotics retail automation applications, assistive robots retrieving household items fulfillment robots navigating cluttered shelves. method gain standard inference zero category refinement background filtering batched nms box refinement sam table progressive improvements zero insdet chal- lenge. row adds new component previous setting. challenge thus offers realistic high-stakes testbed evaluating models open-world, instance-level detection scenarios. zero, multi-modal prompt architecture zero-shot adaptability, well aligned nature task. instead relying fixed category labels, zero leverage visual prompts identify target objects, making particularly suitable instance-level retrieval unstruc- tured environments. end, started base pipeline using inference outputs zero. baseline, introduced series targeted improvements reflect practical deployment constraints maximizing performance zero-shot conditions. first, category refinement applied using visual em- bedding model align instance representations, combined tiled inference improve localization granularity. next, reduce spurious background predictions, com- mon issue open-world detection, trained lightweight background classifier. batched non-maximum suppres- sion nms employed handle overlapping pre- dictions scale efficiently. finally, bounding box quality enhanced using sam-based refinement pro- gressive enhancements reported table demon- strate effectiveness modular inference-time improve- ments zero require additional training fine-tuning. key innovation participation data prepa- ration strategy. insdet supports synthetic scene gen- text-prompt-based zero-shot detection examples visual-prompt-based zero-shot detection examples figure qualitative examples zero zero-shot detection settings using different types prompts. eration pasting instance crops varied backgrounds, extended approach using generative services synthesize visually rich diverse scenes. apply- ing zero generated images, could identify background-class objects originally present insdet dataset. new samples used train background classifier, effectively filtering irrele- vant detections enriching dataset semantically novel content. pipeline significantly improved instance discrimination reduced false positives, demonstrating practical synergy generative models vision foundation models open-world instance detection. overall, results, placing challenge, high- light zeros strength robust, generalizable, industry-ready foundation model. modular improve- ments propose inference-time only, lightweight, easily extensible, illustrating zero rapidly adapted domain-specific requirements without retraining. attributes make zero especially suitable real- world applications flexibility, efficiency, zero- shot generalization essential. ... foundation few-shot object detection challenge foundational fsod challenge designed rig- orously evaluate models capabilities performing object detection extremely limited annotations across diverse heterogeneous industrial domains. using rfvl- fsod dataset, encompasses distinct domains, par- ticipants tasked detecting objects given bounding box annotations per category. sparse label- ing setup realistically simulates practical constraints in- dustrial settings exhaustive annotation costly in- feasible. category supplemented detailed noun phrase descriptions clarify ambiguous domain-specific terminology, supporting better semantic understanding. challenge allows pretraining external datasets mandates fine-tuning conducted solely rfvl-fsod dataset, emphasizing adaptation target domains few-shot conditions. final performance measured mean average precision map iou thresholds ranging averaged across do- mains, thereby assessing detection accuracy gen- eralization. unlike zero-shot benchmarks, foundational fsod challenge explicitly tests models ability adapt effi- ciently robustly new domains minimal super- vision, making critical assessment foundation mod- els intended industrial deployment. address re- quirements, designed training pipeline maximizes prompt diversity textual visual levels. introduce extensive prompt diversity mitigate overfitting. text level, employ llama--b- instruction assistant specialized generating concise noun- phrase definitions paraphrasing. given list terms format term definition. term, return corresponding line format term paraphrased definition. paraphrased definitions must concise written noun phrases. preserve original meaning context. clearly distinguish term others. follow line-by-line format input. add omit terms. table instruction used text prompt augmentation. instruct paraphrase category descriptions concise noun phrases. paraphrasing process guided care- fully crafted prompt template table instructs model preserve semantic integrity, avoid redundancy, generate unambiguous distinct definitions. aug- mentation increases lexical variety prompts also improves models ability distinguish fine- grained categories. addition positive prompts, in- clude negative textual prompts encourage better dis- criminative embedding space obtained contrastive learn- ing. note number negative prompts set caution avoid introducing label noise. visual level, apply in-image out- image prompting strategies. in-image visual prompts use objects co-occurring image, reflecting con- vention common large-scale training vision foun- dation model aiding contextual reasoning. out-image visual prompts, contrast, introduce objects category images, fostering generalization re- ducing reliance narrow contextual cues. alongside prompt engineering, implement con- servative pseudo-labeling strategy model predictions high confidence scores added pseudo-labels instances unlabeled categories train split. strict filtering ensures integrity additional supervi- sion preventing error accumulation low-quality labels. inference, adopt several techniques im- prove detection performance stability. test-time aug- mentation tta applied default using combination image resizing multiple scales horizontal flipping, effectively exposing model vari- ous spatial configurations. also perform category-wise threshold search, tuning confidence threshold class individually optimize mean average precision map validation split. enhance robust- ness, considered ensemble predictions factor options text prompt original original augmented visual prompt in-image out-image annotations original original pseudo-labeled inference text visual text visual table factors considered checkpoint selection. text visual prompts. due distinctiveness domain, univer- sal strategy consistently outperforms others. therefore, obtained various model checkpoints turning proposals training inference, selected best checkpoint guided performance validation split. dataset distinct domains, best checkpoint selected based combinations four factors table important note that, similar training split, validation split partially anno- tated. therefore, checkpoint evaluation, restrict predictions categories known present image. contrast, test split inference intended final submission, predictions obtained across categories. consequently, achieved place received honorable mention challenge organizers, high- lighting zeros strong potential industry-ready vi- sion foundation model. conclusion paper introduces zero, multi-modal prompt object detection model specifically designed industry-specific zero-shot deployment. zero leverages textual visual prompts generalize across various tasks do- mains without requiring retraining. developed su- perb ai, zero utilizes proprietary industrial dataset one billion images demonstrate potential prompt- driven, data-centric scalable adaptive object de- tection industrial environments. zero achieved strong performance cvpr instance object detec- tion foundational few-shot object detection chal- lenges, placing th, respectively. zero of- fers practical advantages terms flexibility, efficiency, real-world applicability, first vision foun- dation model explicitly built domain-specific, zero- shot industrial applications, bridging persistent gap academic innovation real-world enterprise needs. references tianheng cheng, lin song, yixiao ge, wenyu liu, xing- gang wang, ying shan. yolo-world real-time open-vocabulary object detection. proceedings ieeecvf conference computer vision pattern recognition, pages jacob devlin, ming-wei chang, kenton lee, kristina toutanova. bert pre-training deep bidirectional trans- formers language understanding. proceedings conference north american chapter asso- ciation computational linguistics human language tech- nologies, volume long short papers, pages alexey dosovitskiy, lucas beyer, alexander kolesnikov, dirk weissenborn, xiaohua zhai, thomas unterthiner, mostafa dehghani, matthias minderer, georg heigold, syl- vain gelly, al. image worth words trans- formers image recognition scale. arxiv preprint lijie fan, kaifeng chen, dilip krishnan, dina katabi, phillip isola, yonglong tian. scaling laws synthetic images model training... now. proceedings ieeecvf conference computer vision pattern recognition, pages shenghao fu, qize yang, qijie mo, junkai yan, xihan wei, jingke meng, xiaohua xie, wei-shi zheng. llmdet learning strong open-vocabulary object detectors supervision large language models. proceedings computer vision pattern recognition conference, pages kyeongryeol kye-hyeon kim. transferable can- didate proposal bounded uncertainty. arxiv preprint matthew honnibal, ines montani, sofie van landeghem, adriane boyd. spacy industrial-strength natural lan- guage processing python. edward hu, yelong shen, phillip wallis, zeyuan allen- zhu, yuanzhi li, shean wang, wang, weizhu chen, al. lora low-rank adaptation large language models. iclr, qing jiang, feng li, tianhe ren, shilong liu, zhaoyang zeng, kent yu, lei zhang. t-rex counting visual prompting. arxiv preprint qing jiang, feng li, zhaoyang zeng, tianhe ren, shilong liu, lei zhang. t-rex towards generic object detec- tion via text-visual prompt synergy. european conference computer vision, pages springer, qing jiang, gen luo, yuqin yang, yuda xiong, yihao chen, zhaoyang zeng, tianhe ren, lei zhang. chatrex tam- ing multimodal llm joint perception understanding. arxiv preprint qing jiang, gen luo, yuqin yang, yuda xiong, yihao chen, zhaoyang zeng, tianhe ren, lei zhang. chatrex tam- ing multimodal llm joint perception understanding, qing jiang, xingyu chen, zhaoyang zeng, junzhi yu, lei zhang. rex-thinker grounded object referring via chain-of-thought reasoning, qing jiang, lin wu, zhaoyang zeng, tianhe ren, yuda xiong, yihao chen, qin liu, lei zhang. referring person, alexander kirillov, eric mintun, nikhila ravi, hanzi mao, chloe rolland, laura gustafson, tete xiao, spencer white- head, alexander berg, wan-yen lo, al. segment any- thing. proceedings ieeecvf international confer- ence computer vision, pages feng li, qing jiang, hao zhang, tianhe ren, shilong liu, xueyan zou, huaizhe xu, hongyang li, jianwei yang, chunyuan li, al. visual in-context prompting. pro- ceedings ieeecvf conference computer vision pattern recognition, pages liunian harold li, pengchuan zhang, haotian zhang, jian- wei yang, chunyuan li, yiwu zhong, lijuan wang, yuan, lei zhang, jenq-neng hwang, al. grounded language-image pre-training. proceedings ieeecvf conference computer vision pattern recognition, pages shilong liu, zhaoyang zeng, tianhe ren, feng li, hao zhang, jie yang, qing jiang, chunyuan li, jianwei yang, hang su, al. grounding dino marrying dino grounded pre-training open-set object detection. euro- pean conference computer vision, pages springer, anish madan, neehar peri, shu kong, deva ramanan. revisiting few-shot object detection vision-language models. advances neural information processing sys- tems, george miller. wordnet lexical database english. communications acm, matthias minderer, alexey gritsenko, austin stone, maxim neumann, dirk weissenborn, alexey dosovitskiy, aravindh mahendran, anurag arnab, mostafa dehghani, zhuoran shen, al. simple open-vocabulary object detection. european conference computer vision, pages springer, matthias minderer, alexey gritsenko, neil houlsby. scaling open-vocabulary object detection. advances neu- ral information processing systems, aaron van den oord, yazhe li, oriol vinyals. repre- sentation learning contrastive predictive coding. arxiv preprint maxime oquab, timothee darcet, theo moutakanni, huy vo, marc szafraniec, vasil khalidov, pierre fernandez, daniel haziza, francisco massa, alaaeldin el-nouby, al. dinov learning robust visual features without supervision. arxiv preprint alec radford, jong wook kim, chris hallacy, aditya ramesh, gabriel goh, sandhini agarwal, girish sastry, amanda askell, pamela mishkin, jack clark, gretchen krueger, ilya sutskever. learning transferable visual models natural language supervision. proceedings international conference machine learning, pages pmlr, nikhila ravi, valentin gabeur, yuan-ting hu, ronghang hu, chaitanya ryali, tengyu ma, haitham khedr, roman radle, chloe rolland, laura gustafson, al. sam segment anything images videos. arxiv preprint tianhe ren, yihao chen, qing jiang, zhaoyang zeng, yuda xiong, wenlong liu, zhengyu ma, junyi shen, yuan gao, xiaoke jiang, al. dino-x unified vision model open- world object detection understanding. arxiv preprint tianhe ren, shilong liu, ailing zeng, jing lin, kunchang li, cao, jiayu chen, xinyu huang, yukang chen, feng yan, al. grounded sam assembling open-world models diverse visual tasks. arxiv preprint peter robicheaux, matvei popov, anish madan, isaac robinson, joseph nelson, deva ramanan, neehar peri. roboflow-vl multi-domain object detection benchmark vision-language models. arxiv preprint christoph schuhmann, romain beaumont, richard vencu, cade gordon, ross wightman, mehdi cherti, theo coombes, aarush katta, clayton mullis, mitchell worts- man, al. laion-b open large-scale dataset training next generation image-text models. advances neural in- formation processing systems, michael tschannen, alexey gritsenko, xiao wang, muham- mad ferjad naeem, ibrahim alabdulmohsin, nikhil parthasarathy, talfan evans, lucas beyer, xia, basil mustafa, al. siglip multilingual vision-language en- coders improved semantic understanding, localization, dense features. arxiv preprint huy vo, vasil khalidov, timothee darcet, theo moutakanni, nikita smetanin, marc szafraniec, hugo tou- vron, camille couprie, maxime oquab, armand joulin, al. automatic data curation self-supervised learning clustering-based approach. arxiv preprint wang, lihao liu, hui chen, zijia lin, jungong han, guiguang ding. yoloe real-time seeing anything. arxiv preprint wang, lihao liu, hui chen, zijia lin, jungong han, guiguang ding. yoloe real-time seeing anything, hao wang, pengzhen ren, zequn jie, xiao dong, chengjian feng, yinlong qian, lin ma, dongmei jiang, yaowei wang, xiangyuan lan, al. ov-dino unified open-vocabulary de- tection language-aware selective fusion. arxiv preprint bin xiao, haiping wu, weijian xu, xiyang dai, houdong hu, yumao lu, michael zeng, liu, yuan. florence- advancing unified representation variety vision tasks. proceedings ieeecvf conference computer vision pattern recognition, pages shinya yamaguchi takuma fukuda. limitation diffusion models synthesizing training datasets. arxiv preprint xiaohua zhai, basil mustafa, alexander kolesnikov, lucas beyer. sigmoid loss language image pre-training. proceedings ieeecvf international conference computer vision, pages hao zhang, feng li, shilong liu, lei zhang, hang su, jun zhu, lionel ni, heung-yeung shum. dino detr improved denoising anchor boxes end-to-end object de- tection. eleventh international conference learn- ing representations, xiangyu zhao, yicheng chen, shilin xu, xiangtai li, xin- jiang wang, yining li, haian huang. open com- prehensive pipeline unified object grounding detec- tion. arxiv preprint jinguo zhu, weiyun wang, zhe chen, zhaoyang liu, shen- glong ye, lixin gu, hao tian, yuchen duan, weijie su, jie shao, al. internvl exploring advanced training test-time recipes open-source multimodal models. arxiv preprint", "published_date": "2025-07-06T07:03:27+00:00"}
{"id": "2506.22819v1", "title": "Prompting without Panic: Attribute-aware, Zero-shot, Test-Time Calibration", "authors": ["Ramya Hebbalaguppe", "Tamoghno Kandar", "Abhinav Nagpal", "Chetan Arora"], "summary": "vision-language models vlm demonstrated impressive performance image recognition leveraging self-supervised training large datasets. performance improved adapting test sample using test-time prompt tuning tpt. unfortunately, singular focus tpt approaches improving accuracy suffers tunnel vision, leads degradation confidence calibration. limits applicability tpt critical applications. make three contributions work. posit random naive initialization prompts leads overfitting particular test sample, main reason miscalibration vlm tpt. mitigate problem, propose careful initialization test time prompt using prior knowledge target label attributes large language model llm maintain quality prompts tpt, propose novel regularization loss reduce intraclass distance, increase inter-class distance learnt extensive experiments different clip architectures datasets, show approach effectively improve calibration tpt. report average expected calibration error ece method, tca, compared vanilla tpt, c-tpt iclr, difftpt cvpr, promptalign neurips. code publicly accessible", "full_text": "cs.cv jun prompting without panic attribute-aware, zero-shot, test-time calibration ramya hebbalaguppe, tamoghno kandar abhinav nagpal chetan arora iit delhi tcs research labs project webpage abstract. vision language models vlms become effective tools image recognition, primarily due self-supervised training large datasets. performance enhanced test- time prompt tuning tpt. however, tpts singular focus accuracy improvement often leads decline confidence calibration, restrict- ing use safety-critical applications. work, make two con- tributions posit random naive initialization prompts leads overfitting particular test sample, one rea- sons miscalibration vlms tpt. mitigate problem, propose careful initialization test time prompt using prior knowledge target label attributes large language model llm. propose novel regularization technique preserve prompt cal- ibration test-time prompt tuning tpt. method simulta- neously minimizes intraclass distances maximizing interclass dis- tances learned prompts. approach achieves significant cali- bration improvements across multiple clip architectures diverse datasets, demonstrating effectiveness tpt. report average expected calibration error ece method, tca, com- pared vanilla tpt c-tpt iclr, difftpt cvpr, promptalign neurips. code publicly accessible tcapromptwithoutpanic. introduction vlms confidence calibration. vision-language models vlms un- locked transformative applications across wide range fields, healthcare diagnostics assistive solutions visually impaired however, re- cent findings reveal vlms suffer miscalibration, hinder model trustworthiness critical applications. traditional calibration methods rely large labeled datasets, posing significant limitations settings like test- time adaptation, labeled data unavailable infeasible obtain. inspired success vlms generalizing unseen data zero-shot setting paper focus zero-shot setting, adapt models using prompt tuning. equal contribution hebbalaguppe al. fig. conceptual comparison proposed tca vs. con- temporaries. test-time prompt tuning methods, tpt learn test- time prompts parameter optimization. however, methods often face performance disadvantages calibration, struggle dynamically adapt varying textual feature distributions, limiting effective prompt cali- bration. methods, arggue, difftpt, promptalign explic- itly optimize calibration. although c-tpt introduces enhancements calibration, still falls short capturing nuanced visual attributes con- tribute precise prompt conditioning leading suboptimal prompt specificity. method termed test-time calibration via attribute alignment tca in- fuses relevant attribute information providing context via llms captures intrainter-class textual attribute spread improving prompt calibration. note tca works zero-shot test-time settings without labeled data, making practical real-world deployment data annotation infeasible. model finetuning required prompts updated test time base vi- sion text encoders kept frozen. prompt tuning. test-time prompt tuning tpt emerged promising approach improve generalization vlms, offering way adapt prompts specific contexts without requiring labeled data target domain. hard prompts often composed fixed vocabulary tokens standard templates like photo class name simplify prompt creation. however, indicate flexible prompt designs, soft prompts learned embeddings, significantly enhance models adaptability effectiveness. hand, domain-specific prompt creation image- text models requires substantial expertise time, guarantee optimal results despite extensive engineering efforts. shu al. suggested tpt technique hereinafter referred vanilla tpt vtpt aims enhance accuracy clip based models minimizing entropy prediction distribution self-supervision signal test time. however, reduction entropy leads model generate overconfident predictions, characteristic often observed models trained cross-entropy loss fig. illustrates conceptual distinction existing prompt tuning approaches method proposed work. prompting without panic attribute-aware, zero-shot, test-time calibration contributions. work focuses tpt strategy improve models cali- bration. first, may seem infeasible since various calibration techniques employed standard supervised training neural networks require substantial amounts labeled training data, restricts applicability test-time prompt tuning scenarios clip based models. here, come clever workaround, extracting label attributes using llm, leveraging tpt instead label supervision. attribute-aware prompting improved calibration unlike contemporary methods directly attach soft prompts class names, append model precise visual attributes produced llm provide rich context. visual attributes sorted relevance. may noted particular attribute may relevant one labels. hence, aligning visual embeddings chosen attributes allows model demonstrate recognizes features crucial distinguishing correct class others, also allows model express prediction uncertainty terms ambiguous at- tributes. multiple relevant attributes also enhance compositional nature visual data serve semantic anchors. incorporation soft prompt design improves image-text alignment scores establish inter- pretable correspondences visual linguistic embeddings. regularization loss proposed visual attributes-based prompt initializa- tion allows model much better starting point compared random ini- tialization prevents overfitting presence limited variations single sample augmentation based training. however, gradient- based update prompts may still overfit prompts sample. hence, propose loss text prompt embeddings minimize intra-class text feature dispersion, maximizing inter-class dispersion. idea inspired contrastive learning supervised training intra- class distance w.r.t. anchor minimized inter-class distance w.r.t. neg- ative sample maximized. proposed loss combined prompt tuning methods e.g. promptalign difftpt tda boostadapter could integrate tca prompt calibration.. supplemen- tary, show gains accuracy ece incorporate tca top promptalign difftpt superior performance perform extensive experiments across various datasets clip based models, incorporating proposed attributes aware prompt initialization, proposed loss. report average performance benchmark datasets improving model calibration baseline tpt terms ece c-tpt respectively. related works miscalibration neural network. accurate estimation predictive un- certainty, often referred model calibration, critical aspect deploying hebbalaguppe al. neural networks safety-sensitive applications. proper calibration ensures confidence associated models predictions aligns true accu- racy, thereby facilitating reliable decision-making. however, recent studies highlighted frequent instances miscalibration modern neural network architectures, indicating concerning trend despite improvements predictive performance, newer accurate models tend produce poorly calibrated probability estimates calibration techniques. calibration techniques broadly classified train-time methods post-hoc methods. train-time techniques typically used additional loss terms along nll cross-entropy loss training. representative works include tech- niques practical setting requires retraining neural network regularization terms. post-hoc calibration applied model trained often require validation set fine-tune output prob- abilities. common post-hoc calibration techniques include etc. prompt tuning vlms. efficiently adapt large foundational mod- els, prompting emerged resource-efficient method. prompt tuning typically uses static learnt prompts part input text guide model performing specific tasks zero-shot, few-shot manner. hand- crafted prompts consisting predefined vocabulary tokens, hard prompts, may optimal various settings. hence, growing focus techniques regard prompts learnable vectors optimized gradient descent instance, coop tunes prompts clip using labeled training samples improve classification accuracy. however, cocoop identified coop struggles generalizing out-of-distribution data recommends conditioning prompt input images. effective, methods require access annotated training data, limits zero- shot adaptation pre-trained models like ours. tackle challenge, recent research introduced tpt technique enables adaptive prompt learning inference time, using one test sample. tpt optimizes prompt minimizing entropy confidence selection model consistent predictions test sample. difftpt innovates test-time prompt tuning leveraging pre-trained diffusion models augment diver- sity test data samples used tpt. promptalign fine-tunes multi-modal prompts test-time aligning distribution statistics obtained mul- tiple augmented views single test image training data distribution statistics. although previous studies primarily concentrated refining prompt templates improve accuracy, largely neglected calibration except paper focuses critical under-explored challenge calibrat- ing vlms zero-short, test-time setting. maintain efficiency practicality, develop solution within prompt tuning framework. prompting without panic attribute-aware, zero-shot, test-time calibration proposed method preliminaries confidence calibration. given data distribution let denote predictive confidence predictor predictor said calibrated ex,yd intuitively, network predicts class cancer image score network calibrated, probability image actually contains cancer expected calibration error ece common metric used measuring calibration, evaluates well predicted confidence model align accuracy. compute ece, confidence interval divided fixed number bins. bin encompasses range predicted confidence. ece value computed ece accbk confbk number bins, set samples, num- ber samples, accbk prediction accuracy, confbk average predictive confidence bin lower ece preferred. zero-shot classification clip. let image space, label space. let text prompt corresponding image sample clip architecture composed two distinct encoders visual encoder denoted text encoder vanilla zero-shot inference clip, attach manually designed prompt prefix, e.g., photo possible class yk, generating class-specific textual descriptions yi. here, denotes number classes. next, generate text features gti, image features passing relevant inputs respective encoders. allows compute similarity text feature, image features fx, gti yi, refers cosine similarity. probability predicting class test image computed pyix exp gti, exp gtj, temperature softmax function. predicted class arg maxyi pyi predicted confidence maxyi pyi test-time prompt tuning. several researchers demonstrated efficacy shot prompt tuning general well clip based models test-time prompt tuning vanilla tpt vtpt introduced aims benefit rich knowledge clip boost generalization hebbalaguppe al. class name red panda attribute ranking using cosine similarity prompt please provide number words descriptive english words datasetclass name dataset. format response followsnwordnwordnwordnword multimodal foundational model eg., red panda animal fur wildlife cute small tail endangered claws nature bamboo attribute attributes red, thick fur, bushy tail, whiskers, brown eyes,. pretrained foundational model llm driven attribute extraction sorting note attributes extracted offline class training dataset, class name specified prompt. attributes eg., dataset imagenet class name red panda number words fig. visual attributes extracted prompting multimodal foundational model shown leftmost block. extracted attributes shown red ranked based similarity class name dataset e.g., top attributes red panda imagenetk dataset. offline process aids model calibration identifying relevant attributes. relevant attributes identifying attribute similarity respect class name. set attributes returned particular class pretrained llm. zero-shot manner. optimizes prompts without requiring labeled data. inference, augmented views, xj, test sample generated. predictions entropy values predefined threshold retained, higher entropy discarded confidence selection filter. entropy remaining predictions averaged, value used update prompts unsupervised manner using back-propagation following objective function ltpt pyi log pyi, pyi pyi xj. here, represents mean vector class probabilities produced model across different augmented views preserved confidence selection filter. additionally, shown test-time prompt tuning ef- fectively combined few-shot prompt tuning techniques train time, boosting vanilla vtpts performance attribute alignment using llm. vlms, attribute alignment prompt tuning guides model generate outputs matching specific visual textual attributes. authors use llms create descriptive sentences highlight- ing key features image categories. attribute extractor identifies relevant domain-specific information like color context prompt adjusted accordingly. aligned prompt improves inference accuracy tailor- ing model task. unlike train-time techniques above, approach focuses test-time calibration. prompting without panic attribute-aware, zero-shot, test-time calibration fig. calibration using test-time attribute alignment zero-shot image classification typical test time prompt tuning image classifi- cation, category label prefixed template text, photo e.g., photo red panda generate prompt tuning. approach differs following ways visual attributes extracted shown fig. approach takes image augmentations ...an input. contrast tpt utilize attribute vector concatenated template text class name initialize prompt. introduce two auxiliary terms objective function test-time calibration via at- tribute alignment linterclass maximize mean text features classes lintraclass minimize intra-class variance textual attributes prompt tuning improve alignment predicted actual class probabilities, enhancing model calibration. allows tune adaptive prompts fly single test sample, without need additional training data annotations. visual text encoders kept frozen prompt tuning. test time calibration via attribute alignment proposed attribute-aware prompt tuning procedure comprises two steps, namely, relevant attribute extraction see fig enhancing calibration via test-time loss textual features separationcontraction see fig fig. depicts first step, obtain visual attributes provide context prompting llms inquiries visual characteristics specific classes. llm input exclusively consists class names dataset. for- mally, given label retrieve corresponding class name, ci, list attributes ayi language model like gpt. template prompting llm pre-defined see fig. attributes subsequently ranked descending order relevance sorting based cosine similarity class name attribute names. store relevant attributes attribute vector use top attribute hebbalaguppe al. algorithm test-time calibration via attribute alignment inference initialize manual prompt, photo attribute class class attribute form text embedding tij end compute mean text embeddings class tyi gtij, clip text encoder. calculate mean text attribute spread mtas class mtasyi gtij tyi lintra-classyi mtasyi end compute mean text embeddings classes, tyi calculate average text feature dispersion atfd across classes atfd tyi linter-class atfd ltotal ltpt .linter-class .lintra-class. implementation based ablation study. fig. illustrate example red panda image. attributes thus generated appended tunable prompt, along class names, tunable prompt concatenatep, also see block diagram corresponding yellow box fig. full prompt text including attributes shown json file caltech dataset included supplementary material. step enforce effective calibration, employ contrastive loss test-time, test-time calibration process specified algorithm start initialized prompts described earlier, every class attribute form text embedding paj compute centroid text embeddings. minimize distance class centroid textual embeddings corresponding class generated using different class attributes. referred intra-class loss serves learn discriminative features class. similar c-tpt also in- crease distance text embeddings distinct classes loss referred inter-class loss. this, first take mean embeddings corresponding different attributes specific class. represents tex- tual feature corresponding class. maximise distance representative features class classes well separated. overall loss used tune prompts summation vanilla test time prompt tuning loss ltpt, two loss terms. note back- propagated gradients update tokens corresponding whereas tokens remain frozen, prevent overfitting test sample. recall generated manual template text, photo prompting without panic attribute-aware, zero-shot, test-time calibration understanding role tca enhancing calibration tca improves representation quality leveraging contrastive learning principles thus enabling generation high-quality, meaningful, discriminative em- beddings effectively capture semantic similarity. achieved contrastive test-time loss inter-class linter-class intra-class lintra-class loss terms. model classifies new samples aligning closest class embeddings simultaneously distinguishing classes. believe alignment enhances calibration test-time. specifically, recall calibration aims align predictive probabilities true likelihood event. tca addresses aligning similar representa- tions simultaneously mitigating overconfidence, key factor contributing miscalibration. use term see algorithm line plays criti- cal role process explicitly penalizing embedding overlap dissimilar classes. discourages model assigning overly confident probabilities incorrect predictions, ensuring extreme predictive probabilities close assigned different classes well-separated. see algorithm lines takes care aligning similar textual embeddings. difference tca contemporary techniques although prompt tuning c-tpt introduces enhancements cali- bration, still falls short capturing nuanced class specific features important disambiguate classes, thus necessary uncertainty calibration. though sample specific labels absent test time set- ting ours, however make observation, note even class specific information indeed available. make use llms generate class attributes use proposed technique choose representative attributes. another big difference, choose update attribute fea- tures. c-tpt, firstly text prompt initialization classes, get updated updated test-time loss, leading over- fitting sample, less ideal calibration. case, frozen attribute based features provide adequate grounding prevent overfitting, whereas learnable prompts allow adapt particular sample, thus leading better calibration proposed tca current state-of- the-art, c-tpt. approach also differs tpt incorporate attribute auxiliary information llms, explicitly op- timize calibration. result, method exhibits sub-optimal calibration performance. experiments section outlines benchmarks assessing method experi- mental results. consistent previous research prompt tuning vision- language models evaluation centered two primary aspects hebbalaguppe al. method metric imagenet caltech pets cars flower food aircraft sun dtd eurosat ucf average acc. clip-rnhardprompt ece acc. tpthardprompt ece acc. tpthardpromptc-tpt ece acc. tpthardprompttca attribute ece acc. tptensemble ece acc. tptensemblec-tpt ece acc. tptensembletca attributes ece acc. clip-vit-bhardprompt ece acc. tpthardprompt ece acc. tpthardpromptc-tpt ece acc. tpthardprompttca attribute ece acc. clip-vit-bensemble ece acc. tptensemble ece acc. tptensemblec-tpt ece acc. tptensembletca attributes ece table fine-grained classification. results clip-rn clip- vit-b reported, providing accuracy ece metrics different experimental configuration please see main test configuration de- tails. values highlighted bold indicate lowest ece achieved following test-time prompt tuning underline second best. note full ta- ble, includes comparisons contemporary methods, found supplementary material due space limitations main paper ablate tca loss promptalign difftpt show gains top contemporary methods promptalign neurips difftpt iccv range fine-grained classifications natural distribution shift. note particular, given objective enhance calibration context test-time prompt tuning, experimental framework emphasizes prompt opti- mization absence labeled training data. datasets. fine-grained classification, utilize diverse set datasets, in- cluding imagenet caltech oxfordpets stanfordcars flow- ers food fgvcaircraft sun ucf dtd eurosat out-of-distribution ood generalization task, define imagenet in-distribution source dataset extend evaluation four ood variants imagenetv imagenet-sketch imagenet-a, imagenet-r. implementation details. report results following experimental con- figurations. initialized prompt set hard prompt photo cliphardprompt corresponding tokens optimized based single test image using tpt tpthardprompt jointly using tpt pro- posed technique tca tpthardprompttca. also include ensemble set- ting average logits different hard-prompt initialization us- ing photo photo the, picture picture clipensemble. similarly, optimize using tpt well tptensemble, jointly prompting without panic attribute-aware, zero-shot, test-time calibration using tpt tca tptensembletca hard-prompt initializa- tion average resulting logits. tried use attribute initialization. hyperparameters employ test-time prompt tun- ing strategy, allow access data hyperparameter tuning. perform grid search balance calibration loss least ece using caltech dataset apply values datasets fol- lowing setup similar c-tpt. obtain respectively. using attributes gave best ece values majority datasets finegrained classification. natural distribution shifts, obtained, tpt optimize prompt one step using adamw optimizer learning rate method runs single nvidia tesla gpu memory, except imagenet, imagenet-a, imagenet datasets, use two gpus evaluation. comparison fine grained classification fine-grained classification task, compare contemporary methods hard prompt benchmark approaches, tpt c-tpt tab. summarizes results accuracy ece values. evaluation includes multi- ple clip architectures, specifically clip rn- vit-b. results show method significantly outperforms hard prompt configuration. comparing average performance c-tpt across datasets, method achieves similar average predictive accuracy notably reducing av- erage ece. clip rn-, ece decreases similarly, vit-b, ece reduced robustness natural distribution shifts follow setting radford al. evaluate models robustness natural distribution shifts imagenet variants considered ood imagenet previous works. report results table table shows outperform contemporary methods tpt, c-tpt terms ece datasets. ablation study investigate factors contributing calibration whether driven inclusion attributes choice loss function. examine this, conducted experiment two conditions. first condition, incorporate attributes prompts evaluate method using tpt loss function. second, incorporate attributes prompts evaluate using combined tpt tca loss function datasets. relative contribution attribute initialization proposed loss. better understand contribution conduct ablation experiments dtd dataset using resnet feature extractor report acc ece hebbalaguppe al. methods metric in-a in-v in-r in-s avg. acc. clip-rnhardprompt ece acc. tpthardprompt ece acc. tpthardpromptc-tpt ece acc. tpthardprompttca attributes ece acc. clip-rnensemble ece acc. tptensemble ece acc. tptensemblec-tpt ece acc. tptensembletca attributes ece acc. clip-vit-bhardprompt ece acc. tpthardprompt ece acc. tpthardpromptc-tpt ece acc. tpthardprompttca attributes ece acc. clip-vit-bensemble ece acc. tptensemble ece acc. tptensemblec-tpt ece acc. tptensembletca attributes ece table natural distribution shifts. results clip-rn clip-vit- reported, providing acc. ece metrics different experimental configurations please refer main text details con- figurations. dataset abbreviations imagenet-v in-v, imagenet-a in-a, imagenet-r in-r, imagenet-sketch in-s. values highlighted bold indicate lowest ece achieved test-time prompt tuning. variants tpthardp rompt tpthardp rompt initialization attributes tpthardp rompt initialization at- tributes proposed tca loss key observations ablation follows attribute initialization initialized attributes, reduction ece compared hard prompt model tca loss introduction tca loss resulted reduction ece, bringing ece significantly improving models cali- bration. combined effect attribute initialization tca loss used together, ece reduction even pronounced, overall reduction ece, yielding lowest ece value maximum accuracy thus, proposed contributions, attribute initialization strategy, well proposed loss play significant roles im- proving model calibration. proposed loss particularly effective reducing ece, combining attribute initialization leads significant improvement accuracy calibration. refer fig. illustrates prompting without panic attribute-aware, zero-shot, test-time calibration flowers oxfordpets fig. t-sne plot shows class-specific text embeddings tuned prompts. conduct ablation term ltotal arg minpltpt.linter-class .lintra-class understand relative contribution empirically. notice incorporating three terms ltotal results lowest ece highest feature dispersion spead. comparison feature dispersion, found inversely correlated ece. inter- intra-loss terms utilized, observe maximum class-specific text embedding dispersion lowest ece, consistent findings see suppl. details plot obtained. discussion confidence calibration tca here, provide intuitive understanding proposed loss function, formulated ltotal arg minpltpt .linter-class .lintra-class. assess significance component within formulation, conduct systematic ablation study. includes t-sne visualizations, facilitate analysis impact individual loss terms feature separability clustering. additionally, compare approach state-of-the-art test-time calibration methods zero-shot setting, thereby demonstrating effectiveness robustness. need intra-inter class losses tca improves representation quality leveraging contrastive principles thus enabling generation high-quality, dis- criminative embeddings effectively capture semantic similaritydissimilarity. tca addresses calibration aligning similar classes, use disper- sion term explicitly penalizes embedding overlap dissimilar classes. discourages model assigning overly confident probabilities incorrect predictions, ensuring extreme predictive probabilities near assigned different classes well-separated. fig. shows ablation hebbalaguppe al. individual loss terms impact calibration using lintra linter ltotal leads lowest ece greatest text feature dispersion. conceptual differences tca loss contemporaries re- cent contemporary method, dapt targets improved accuracy few-shot set- tings, whereas focus zero-shot calibration. dapt uses exponential inter- intra-dispersion vision text embeddings, method re- lies norm distance test sample mean text embeddings. norm easier interpret measures euclidean distance em- beddings, making intuitive transparent, especially comparing distances high-dimensional spaces, less sensitive outliers compu- tationally efficient. facilitates calibration using temperature scaling imagenet validation set. however, applying tca loss cal- tech dataset vit b-, observe degradation accuracy,ece vs. without ts, suggesting decrease per- formance ts. uses distribution aware calibration fine-tuned vlm calibration, focus zero-shot settings like c-tpt. finally, involves few-shot finetuning, making directly comparable approach. vizualisation class-specific text embeddings tuned prompts. please refer supplemental materials t-sne plots across multiple datasets, illustrate lower ece highest dispersion indicating better class separability tca relative contemporaneous methods. supplementary material details factors behind tcas superior perfor- mance, datasets, metrics, feature extractor, experimental setup, hyperparame- ters, t-sne comparisons promptalign difftpt conclusions future directions work, introduced two key insights enhance effectiveness test time prompt tuning. first, demonstrated attribute-aware prompting, wherein relevant visual attributes appended prompts. allows model better align visual embeddings discriminative features, result- ing improved predictive uncertainty handling class-separation. second, proposed regularization loss encourages model minimize intra- class text feature dispersion maximizing inter-class dispersion, inspired contrastive learning principles. ensures learned prompts overfit individual samples, even limited data available. work opens new possibilities leveraging unsupervised attribute information improve model performance low-data test-time settings, paving way robust adaptable models real-world applica- tions. future, would interesting study effectiveness vlm architectures apart clip flamingo. bibliography bossard, l., guillaumin, m., gool, food-mining discriminative com- ponents random forests. eccv chen, g., yao, w., song, x., li, x., rao, y., zhang, plot prompt learning optimal transport vision-language models. iclr cho, e., kim, j., kim, h.j. distribution-aware prompt tuning vision- language models. proceedings ieeecvf iccv. pp. cimpoi, m., maji, s., kokkinos, i., s., vedaldi, describing textures wild. cvpr. pp. dawid, a.p. well-calibrated bayesian. journal american statis- tical association deng, j., dong, w., socher, r., li, l., li, k., fei-fei, imagenet large- scale hierarchical image database. cvpr. pp. dosovitskiy, image worth words transformers image recognition scale. arxiv preprint fei-fei, l., fergus, r., perona, learning generative visual models training examples incremental bayesian approach tested object categories. cvpr workshops. feng, c.m., yu, k., liu, y., khan, s., zuo, diverse data augmentation diffusions effective test-time prompt tuning. iccv ghosal, s., hebbalaguppe, r., manocha, better features, better calibra- tion simple fix overconfident networks. joint european confer- ence machine learning knowledge discovery databases. springer guo, c., pleiss, g., sun, y., weinberger, k.q. calibration modern neural networks. icml. vol. pp. hantao yao, rui zhang, c.x. visual-language prompt tuning knowledge-guided context optimization. cvpr hebbalaguppe, r., baranwal, m., anand, k., arora, calibration trans- fer via knowledge distillation. proceedings asian conference computer vision. pp. hebbalaguppe, r., ghosal, s.s., prakash, j., khadilkar, h., arora, novel data augmentation technique out-of-distribution sample detection using compounded corruptions. joint european conference machine learning knowledge discovery databases. pp. springer hebbalaguppe, r., prakash, j., madan, n., arora, stitch time saves nine train-time regularizing loss improved neural network calibration. hebbalaguppe al. proceedings ieeecvf conference computer vision pattern recognition cvpr. pp. june helber, p., bischke, b., dengel, a., borth, eurosat novel dataset deep learning benchmark land use land cover classification. ieee sel. top. appl. earth obs. remote. sens. hendrycks, d., basart, s., mu, n., kadavath, s., wang, f., dorundo, e., desai, r., zhu, t., parajuli, s., guo, m., song, d., steinhardt, j., gilmer, many faces robustness critical analysis out-of-distribution generalization. iccv hendrycks, d., zhao, k., basart, s., steinhardt, j., song, natural ad- versarial examples. cvpr jia, m., tang, l., chen, b.c., cardie, c., belongie, s., hariharan, b., lim, s.n. visual prompt tuning. eccv. pp. karmanov, a., guan, d., lu, s., saddik, a., xing, efficient test-time adaptation vision-language models. proceedings ieeecvf conference computer vision pattern recognition. pp. khosla, p., teterwak, p., wang, c., sarna, a., tian, y., isola, p., maschinot, a., liu, c., krishnan, supervised contrastive learning. neurips koo, g., yoon, s., yoo, c.d. wavelet-guided acceleration text inversion diffusion-based image editing. arxiv preprint krause, j., stark, m., deng, j., fei-fei, object representations fine-grained categorization. ieee workshop representation recognition drr- kull, m., perello nieto, m., kngsepp, m., silva filho, t., song, h., flach, beyond temperature scaling obtaining well-calibrated multi-class prob- abilities dirichlet calibration. neurips lester, b., al-rfou, r., constant, power scale parameter- efficient prompt tuning. emnlp. pp. nov levine, w., pikus, b., raja, p., gil, f.a. enabling calibration zero-shot inference large vision-language models. arxiv preprint li, f.f., andreeto, m., ranzato, m., perona, caltech apr liu, p., yuan, w., fu, j., jiang, z., hayashi, h., neubig, pre-train, prompt, predict systematic survey prompting methods natural language processing. acm computing surveys liu, x., ji, k., fu, y., tam, w.l., du, z., yang, z., tang, p-tuning prompt tuning comparable fine-tuning universally across scales tasks van der maaten, accelerating t-sne using tree-based algorithms. journal machine learning research maji, s., kannala, j., rahtu, e., blaschko, m., vedaldi, fine-grained visual classification aircraft. tech. rep. prompting without panic attribute-aware, zero-shot, test-time calibration manli, s., weili, n., de-an, h., zhiding, y., tom, g., anima, a., chaowei, test-time prompt tuning zero-shot generalization vision-language models. neurips naeini, m.p., cooper, g.f., hauskrecht, obtaining well calibrated prob- abilities using bayesian binning. aaai. nilsback, m., zisserman, automated flower classification large number classes. icvgip. pp. niu, s., wu, j., zhang, y., chen, y., zheng, s., zhao, p., tan, effi- cient test-time model adaptation without forgetting. iclr. vol. pp. jul oh, c., lim, h., kim, m., han, d., yun, s., choo, j., hauptmann, a., cheng, z.q., song, towards calibrated robust fine-tuning vision- language models. advances neural information processing systems park, h., noh, j., oh, y., baek, d., ham, acls adaptive conditional label smoothing network calibration. iccv. pp. parkhi, o., vedaldi, a., a.zisserman, jawahar, c.v. cats dogs. cvpr patra, r., hebbalaguppe, r., dash, t., shroff, g., vig, calibrating deep neural networks using explicit regularisation dynamic data pruning. proceedings ieeecvf winter conference applications computer vision wacv. pp. january pereyra, g., tucker, g., chorowski, j., kaiser, hinton, regularizing neural networks penalizing confident output distributions. arxiv preprint platt, j.c. probabilistic outputs support vector machines com- parisons regularized likelihood methods. advances large margin classifiers. pp. mit press pratt, s., covert, i., liu, r., farhadi, platypus look like? generating customized prompts zero-shot image classification. iccv. pp. radford, a., kim, j.w., hallacy, c., ramesh, a., goh, g., agarwal, s., sastry, g., askell, a., mishkin, p., clark, j., al. learning transferable visual models natural language supervision. icml. pp. rawat, m., hebbalaguppe, r., vig, pnpood out-of-distribution de- tection text classification via plug andplay data augmentation. arxiv preprint recht, b., roelofs, r., schmidt, l., shankar, imagenet classifiers generalize imagenet? icml. pp. s., g., basu, s., feizi, s., manocha, intcoop interpretability-aware vision-language prompt tuning. emnlp samadh, align prompts test-time prompting distribution alignment zero-shot generalization. neurips hebbalaguppe al. soomro, k., zamir, a.r., shah, ucf dataset human ac- tions classes videos wild. corr abs. tian, x., zou, s., yang, z., zhang, argue attribute-guided prompt tuning vision-language models. cvpr. pp. tu, w., deng, w., campbell, d., gould, s., gedeon, empirical study matters calibrating vision-language models wang, d.b., feng, l., zhang, m.l. rethinking calibration deep neural networks afraid overconfidence. neurips wang, h., ge, s., lipton, z., xing, e.p. learning robust global represen- tations penalizing local predictive power. neurips wang, s., wang, j., wang, g., zhang, b., zhou, k., wei, open- vocabulary calibration fine-tuned clip. arxiv preprint xiao, j., hays, j., ehinger, k.a., oliva, a., torralba, sun database large-scale scene recognition abbey zoo. cvpr. pp. y., n.t. multimodal healthcare identifying designing clinically rele- vant vision-language applications radiology. chi conference hu- man factors computing systems yi, z., yilin, z., rong, x., jing, l., hillming, vialm survey benchmark visually impaired assistance large models. arxiv preprint yoon, e., yoon, h.s., harvill, j., hasegawa-johnson, m., yoo, c.d. in- tapt information-theoretic adversarial prompt tuning enhanced non- native speech recognition yoon, h.s., yoon, e., tee, j.t.j., hasegawa-johnson, m.a., li, y., yoo, c.d. c-tpt calibrated test-time prompt tuning vision-language mod- els via text feature dispersion. iclr yoon, s., koo, g., hong, j.w., yoo, c.d. neutral editing framework diffusion-based video editing. arxiv preprint zhang, t., wang, j., guo, h., dai, t., chen, b., xia, s.t. boostadapter improving vision-language test-time adaptation via regional bootstrapping. arxiv preprint zhou, k., yang, j., loy, c.c., liu, conditional prompt learning vision-language models. cvpr zhou, k., yang, j., loy, c.c., liu, learning prompt vision- language models. ijcv prompting without panic attribute-aware, zero-shot, test-time calibration supplemental material keep main manuscript self-contained, include following details test-time prompt tuning present detailed description loss function provide insights formulation. additionally, provide intuitive explanation integrating loss function poten- tial enhance calibration. datasets provide comprehensive description datasets utilized fine-grained classification natural distribution shift see table table main text. reproducible research facilitate reproducible research, following ac- ceptance, make source code publicly available. additional results study, present results applica- tion promptalign test-time prompt tuning technique c-tpt across datasets, compare performance pro- posed approach, tca. findings demonstrate integrating tca promptalign leads reduction calibration error improve- ment accuracy. additionally, provide t-sne visualizations investigate distribution text features, complement datasets discussed main text. test-time prompt tuning background test time prompt tuning tpt short adapts pre-trained language model llmvlm specific tasks domains inference, eliminating need retraining full fine-tuning. aims enhance models performance given task adjusting input prompts, without modifying models core parameters. setting, aim learn adaptively prompts fly single test sample tpt attractive? tpt particularly appealing due ability operate single test sam- ple without need large training datasets extensive computational resources typically required training-time calibrators. additionally, tpt of- fers significant advantages terms efficiency, requires less time computational effort adapt sample generalization calibration. challenges contemporary tpt approaches despite key advantages tpt dynamic adaptation, improved ro- bustness distributional shifts, resource efficiency, methods often encounter challenges calibration, particularly dynamically adapting hebbalaguppe al. diverse textual feature distributions encountered real-world data. limi- tation restricts ability achieve effective prompt calibration. several methods illustrate shortcomings. instance, arggue uti- lizes argument-guided prompt learning refine task-specific tuning explicitly address calibration concerns. similarly, difftpt focuses gener- ating diverse image variants improve task adaptability however, overlooks specific optimization calibration metrics. promptalign aligns prompts semantic features enhance task performance explicitly ac- count calibration. end, goal enhance calibration without much trade-off accuracy. tca insights proposed loss function calibration mentioned main text, enforce calibration, apply contrastive loss textual attributes. first follow attribute extraction ranking mentioned fig main text. subsequently, follow alg. main text induce test-time calibration. within class, enforce minimization textual attribute distances respect centroid among different classes maximize distance per class mean embeddings. list terms introduce top enforce calibration here. loss combination interclass attribute dispersion intraclass attribute contraction term loss function called test-time calibration via attribute alignment tca, incorporates inter- intra-class terms facilitate prompt learning. let total number classes total number attributes seeking inspiration contrastive training, compute mean encoded text embeddings class follows tyi gtij clip text encoder, index class attributes re- spectively. subsequently, calculate mean text attribute spread mtas class mtasyi gtij tyi mtas analogous atfd defined however, mtas also incorporates attribute information prompt initialization differentiating lintra-classyi mtasyi note average textual feature dispersion atfd refers metric used evaluate spread diversity textual features across different instances given dataset. specifically, measures varied dispersed features textual data mapped feature space. idea that, high-quality representation space, features corresponding similar texts close together, features prompting without panic attribute-aware, zero-shot, test-time calibration impose inter-class distance first computing mean text embed- dings class. approach ensures class representations well-separated, promoting distinctiveness across classes. process formally described follows tyi now, calculate average text feature dispersion atfd across classes follows atfd tyi similar contrastive training, aim maximize distance representations different classes, formulated linter-class atfd total loss total loss test-time calibration zero-shot classification formulated ltotal arg min ltpt .linter-class .lintra-class. here, pis optimal prompt achieved backpropagation using stochastic gradient descent aimed optimize calibration. loss terms, lintra-class linter-class used enforce intra-class feature contraction maximize intraclass text feature dispersion note linter-class atfd. hyperparameters control relative importance respect inter-class intra-class losses. understanding role tca enhancing calibration tca improves representation quality leveraging contrastive learning principles thus enabling generation high-quality, meaningful, discriminative em- beddings effectively capture semantic similarity. achieved contrastive test-time loss inter-class linter-class intra-class lintra-class loss terms. model classifies new samples aligning closest class embeddings simultaneously distinguishing classes. believe alignment enhances calibration test-time. specifically, recall calibration aims align predictive probabilities true likelihood event. tca addresses aligning similar representa- tions simultaneously mitigating overconfidence, key factor contributing dissimilar texts distant. atfd, case, helps quantify dispersed clustered features average. hebbalaguppe al. miscalibration. use term see eq. plays critical role process explicitly penalizing embedding overlap dissimilar classes. discourages model assigning overly confident probabilities incorrect predictions, ensuring extreme predictive probabilities close assigned different classes well-separated. eq. takes care aligning similar textual embeddings. datasets non-semanticnatural distribution shifts datasets. order evaluate robustness wrt distribution shifts occur naturally real-world scenarios, follow setting proposed radford al. evaluate models robustness natural distribution shifts imagenet variants. considered out-of-distribution ood data imagenet previous work. imagenet-sketch dataset black white sketches, collected in- dependently original imagenet validation set. dataset includes images total, covering imagenet categories. imagenet-r collects images imagenet categories artistic renditions. images total, including imagenet cate- gories. imagenet-v independent test set containing natural images, collected different source, including images imagenet categories. imagenet-a challenging test set natural adversarial examples consisting images imagenet categories. datasets finegrained classification fine-grained classification experimental setup comprises datasets follow- ing mentioned summarize number classes test-set size dataset table additional experiments tab. shows results proposed method tca comparison contemporary methods. add promptalign individually also combine tca promptalign ablation. outperform promptalign terms achieving lowest ece accuracy. prompting without panic attribute-aware, zero-shot, test-time calibration dataset classes test set size imagenet caltech oxfordpets stanfordcars flowers food fgvcaircraft sun dtd eurosat ucf imagenet-a imagenetv imagenet-r imagenet-sketch table detailed statistics datasets used experiments datasets highlighted lavender color designated fine-grained clas- sification, whereas without highlight intended classification natural distribution shifts assess robustness. t-sne vizualization additional datasets figs. shows t-sne plot visualize class-specific text embeddings tuned prompts, demonstrating varying levels calibration. result in- dicates prompts generated methods like tpt tpt individual terms loss either intra interclass losses exhibit less dispersion demonstrate lower calibration unlike technique. tca surpasses improving calibration strategically enhancing diversity text features tar- geted attribute application. additional t-sne plots different datasets contemporary method, please see supplementary material. present additional t-sne plots illustrating visualization class- specific text embeddings generated tuned prompts across three datasets flowers oxfordpets ucf shown fig. flowers ucf datasets, tca-tuned prompts demonstrate better calibration, characterized dispersed cluster. contrast, oxfordpets dataset, tpt c-tpt performs better, resulting dispersed tuned prompts, indicative improved embedding separa- tion. hebbalaguppe al. method metric caltech pets cars flower food aircraft sun dtd eurosat ucf average acc. clip-vit-bhardprompt ece acc. promptalignhardprompt ece acc. promptalignhardpromptc-tpt ece acc. promptalignhardprompttca attributes ece acc. table fine-grained classification. results clip-vit-b re- ported, providing accuracy represented acc. ece metrics initialization, applying promptalign, jointly employing promptalign proposed tca loss please see main text configuration details. note baseline method promptalign initialized photo manual prompt. values highlighted bold indicate low- est ece achieved following test-time prompt tuning underline second best. note first rows-pairs acc,ece borrowed c-tpt paper. outperform promptalign c-tpt terms achieving low- est ece accuracy. flowers oxfordpets ucf fig. t-sne visualization class-specific text embeddings shown via sne tuned prompts flowers oxfordpets ucf datasets. color figure denotes unique prompt. see tca exhibits lowest ece flower ucf datasets, showing maximum dispersion hence better calibration. experiments vit-b model prompting without panic attribute-aware, zero-shot, test-time calibration fig. t-sne plot prompt visualizations proposed tca com- pared recent state-of-the-art method, c-tpt. observed tca demonstrates highest class dispersion, indicating superior class separability. hebbalaguppe al. method metric caltech pets cars flower food aircraft sun dtd eurosat ucf average acc. difftpt cliprn ece acc. difftpt tca -attribute ece acc. acc. difftpt vit-b ece acc. difftpt tca -attribute ece table fine-grained classification using difftpt results clip- clip-vit-b reported, providing accuracy represented acc. ece metrics initialization, applying difftpt jointly employing difftpt proposed tca loss please see main text configuration details. note baseline method difftpt initialized photo manual prompt. values highlighted bold indicate lowest ece achieved following test-time prompt tuning underline second best. outperform difftpt terms achieving lowest ece average. method metric in-a in-v in-r in-s avg. acc. difftpt clip-rn ece acc. difftpt tca -attribute ece acc. difftpt clip-vit-b ece acc. difftpt tca -attribute ece table natural distribution shifts difftpt. results clip-rn clip-vit-b reported difftpt providing acc. ece metrics different experimental configurations please refer main text details configurations. dataset abbreviations imagenet-v in-v, imagenet-a in-a, imagenet-r in-r, imagenet-sketch in- values highlighted bold indicate lowest ece achieved test-time prompt tuning.", "published_date": "2025-06-28T08:57:57+00:00"}
{"id": "2506.21891v1", "title": "DIVE: Deep-search Iterative Video Exploration A Technical Report for the CVRR Challenge at CVPR 2025", "authors": ["Umihiro Kamoto", "Tatsuya Ishibashi", "Noriyuki Kugo"], "summary": "report, present winning solution achieved place complex video reasoning robustness evaluation challenge challenge evaluates ability generate accurate natural language answers questions diverse, real-world video clips. uses complex video reasoning robustness evaluation suite cvrr-es benchmark, consists unique videos question-answer pairs spanning categories. method, dive deep-search iterative video exploration, adopts iterative reasoning approach, input question semantically decomposed solved stepwise reasoning progressive inference. enables system provide highly accurate contextually appropriate answers even complex queries. applied cvrr-es benchmark, approach achieves accuracy test set, securing top position among participants. report details methodology provides comprehensive analysis experimental results, demonstrating effectiveness iterative reasoning framework achieving robust video question answering. code available", "full_text": "cs.cv jun dive deep-search iterative video exploration technical report cvrr challenge cvpr umihiro kamoto tatsuya ishibashi noriyuki kugo panasonic connect co., ltd. kamoto.umihiro, ishibashi.tatsuya, kugou.noriyukijp.panasonic.com abstract report, present winning solution achieved place complex video reasoning robustness evaluation challenge challenge evaluates ability generate accurate natural language answers questions diverse, real-world video clips. uses complex video reasoning robustness eval- uation suite cvrr-es benchmark, consists unique videos question-answer pairs spanning categories. method, dive deep-search itera- tive video exploration, adopts iterative reasoning ap- proach, input question semantically decom- posed solved stepwise reasoning progres- sive inference. enables system provide highly accurate contextually appropriate answers even complex queries. applied cvrr-es benchmark, approach achieves accuracy test set, se- curing top position among participants. re- port details methodology provides comprehensive analysis experimental results, demonstrating ef- fectiveness iterative reasoning framework achiev- ing robust video question answering. code available dive. introduction video understanding critical research challenge computer vision natural language processing. re- cent years, video question answering vqa task, requires models answer question video clips, emerged comprehensive benchmark as- sessing video understanding capabilities. approaches vqa become increasingly diverse, extensive re- search end-to-end methods ap- proaches convert video content image captions leverage large language models llms generate answers recently, methods incorporate agents also gained attention. ex- ample, approaches dynamically generate specialized agents based question, others employ agents select important video frames it- erative reasoning approaches, openais deepre- search employ multi-step inference strategies, advanced llms enabling deeper robust understanding complex questions. however, tech- niques still rarely applied vqa, especially video understanding. result, many existing vqa methods answer questions straightforward manner, often over- looking underlying intent producing less contextu- ally appropriate responses. address gaps, pro- pose incorporating intent estimation iterative reasoning improve video question answering. paper, present dive deep-search iterative video exploration, novel framework combines se- mantic decomposition, intent estimation, iterative infer- ence generate accurate answers questions given video clips. specifically, dive breaks ques- tion sub-questions based underlying intent, solves iterative process generate con- textually appropriate precise answers. additionally, propose object-centric video summarization method. method uses object detection techniques create sum- maries focused appearance spatio-temporal tran- sitions key objects scene. leads improved overall performance video understanding. main con- tributions work follows propose dive, new framework answers com- plex video questions breaking sub- questions solving iterative reasoning process. incorporate intent estimation vqa task en- able answers better capture underlying intent be- hind question. develop novel object-centric video summarization method enhances video comprehension capturing key object transitions. results demonstrate method effectively robustly addresses complex queries featured cvrr-es benchmark. methodology figure presents overall architecture system, incrementally iteratively enhances video understanding answer accuracy six-step process intent estimation, question breakdown sub-questions, agent-based answering sub-questions, refinement remaining sub-questions, loop continuation judgment, final answer generation integrating answers sub-questions. following subsections describe step detail. step intent estimation step intent estimation performed input question. system interprets question fundamentally asking considering question text video context. specifically, video summary used key visual information video. question video summary combined estimate describe detail underlying intent question. step question breakdown step question decomposed multiple sub-questions. step, question text video summary information provided input, relevant sub-questions generated based original question. decomposition enables system focus specific parts aspects video. furthermore, information video analysis tools used agent step also considered generating sub-questions. tailoring sub-questions match analytical capabilities subsequent agent, system enables effective targeted video analysis. step question answering step agent employed answer sub-question highest priority loop. remaining sub-questions retained refinement next step. sub-question, agent utilizes following dedicated video analysis tools produce appropriate responses gemini pro tool analyzes one frame per second along corresponding audio. tool excels temporal reasoning, recognizing audio cues. processes entire video consistent temporal resolution, particularly useful achieving comprehensive understanding overall video content. gpt-. tool based sub-question video summary information, tool identifies key temporal question identify women hit person video? video summary intent estimation question breakdown question answering question refinement continuation judgment final answer generation yes tool call agent tools figure overall architecture dive achieving robust accurate videoqa iteratively solving sub-questions decomposed question breakdown module. segments interest, samples frames segments, performs detailed analysis using gpt-.. selecting analyzing frames relevant sub-question, tool enables detailed analysis specific temporal segments within video. leveraging tools, agent generate optimal answers sub-question. step question refinement step remaining sub-questions refined adjusted needed based agents responses. example, sub-question regarding existence object receives negative answer, additional follow-up sub-questions may generated reconfirm result. way, sub-questions flexibly modified supplemented according situation, enabling confirmation necessary information precise video understanding optimized question flow content. step continuation judgment step system decides whether continue loop. need additional information assessed based agents responses current set sub-questions. additional information required confidently answer original question, process returns step contrast, sufficient information gathered, particularly straightforward questions, system terminates loop avoid overthinking maintain quality answers. iterative procedure enables continuous adaptive information gathering, facilitating deeper understanding video avoiding unnecessary analysis. step final answer generation step final answer generated. system synthesizes agents responses sub-questions together video summary produce comprehensive answer original question. integrating sources information, system provides contextually accurate precise response. video summarization using object detection video summarization process leverages object detection results generate context-aware summaries video. procedure consists three main steps object label extraction evenly spaced frames sampled video, object labels extracted frames using gpt-.. object detection using extracted object labels prompts, perform object detection video frames using grounding dino capturing presence spatial locations objects throughout video. video summary generation video summary generated using gpt-. integrating sampled frames object detection results, describing key objects, transitions, interactions throughout video. approach enables creation video summaries reflect global context fine-grained object-level information, thereby supporting accurate video understanding downstream tasks. experiments section, present evaluation proposed dive cvrr-es benchmark. first provide implementation details dive architecture section section compare method existing approaches. conduct in-depth ablation study section analyze impact component include detailed case studies illustrate effectiveness characteristics approach. implementation details proposed method, dive, implemented using open-source langgraph library steps, langgraph, table comparison evalai leaderboard entries paper-reported baselines cvrr-es validation set. method acc. baseline gptv baseline gpt-o fri host team njust kmg pciegogogo dive table comparison leaderboard entries announced organizing committee cvrr-es test set. method acc. aaa vlm pciego pciegogogo pcie love liang njust kmg dive used agent llm based openais gpt-. version gpt-.---. tools invoked agent step question answering, utilized gemini pro, provided google vertex version gemini-.-pro-preview--, gemini pro tool video analysis. frame-level image analysis, employed openais gpt-. within gpt-. tool. temperature parameter llms used tool set agent itself. additionally, maximum number reasoning steps dive set main results table summarizes performance existing methods participants evalai public leaderboard cvrr-es validation set, table presents results test set leaderboard announced organizing committee. shown table proposed method achieves accuracy validation set, significantly surpassing highest previously reported score cvrr-es paper scores teams public leaderboard. likewise, shown table method achieves best performance test set accuracy outperforming competing approaches leaderboard. results demonstrate superior openai, vertex ai, table ablation study cvrr-es validation set. gpt-. refers direct api use agent employs gpt-. gemini pro tools. checkmarks indicate enabled components, values parentheses show incremental accuracy gains added component. method breakdown intent summary acc. gpt-. gpt-. agent agent robustness accuracy approach cvrr-es benchmark compared existing methods. ablation study conduct ablation study evaluate effectiveness key components proposed method. progressively integrate individual modules base pipeline assess contribution component overall performance. specifically, investigate three key components agent-based answering sub-questions, intent estimation step video summarization using object detection step question breakdown. experiments conducted cvrr-es validation set following evaluation protocol table shows results ablation study cvrr-es validation set. starting gpt-. baseline, adding question breakdown module increases accuracy incorporating intent estimation agent boosts performance addition video summarization yields best result findings highlight incremental contribution component overall effectiveness approach. addition quantitative results, figure presents representative example cvrr-es benchmark gemini pro gpt-. fail produce correct answer. contrast, method, dive, successfully solves problem breaking complex question sub-questions applying iterative reasoning. qualitative result underscores strength modular approach, particularly question breakdown iterative reasoning components, addressing complex queries require deeper understanding iterative inference. conclusion report, proposed evaluated robust video question answering method achieved place complex video reasoning robustness evaluation challenge approach combines iterative sub-q boy girl present video? sub-a based visual cues frames, boy short hair blue shirt girl long hair, tying hair back present video. distinction hairstyle, clothing, appearance supports conclusion. sub-q person said boy blue shirt identified male, person long hair female? sub-a person blue shirt appears male one long hair appears female based visual cues, assumptions based stereotypes definitive without context. sub-q visible hair dryer present frame video? sub-a hair dryer visible frame video? bottles toiletries appear, confirming absence. sub-q boy hold handle object resembles hair dryer point? sub-a boy never handles object resembling hair dryer instead, uses something like hairbrush girls hair, confirming hair dryer present video. final question boy using hair dryer girls hair? final answer boy using hair dryer girls hair. device appears frame, uses hands possibly simple brush, confirming hes drying hair hair dryer. question boy using hair dryer girls hair? answer based frames, boy holding object visually resembles hair dryer, pointing girls hair manner consistent using hair dryer. therefore, yes, boy using hair dryer girls hair. question boy using hair dryer girls hair? answer yes, appears boy using hair dryer girls hair. holding object resembles hair dryer pointing hair, holds hair up. gpt-. gemini pro ... ... ... figure example result cvrr-es benchmark gemini pro gpt-. fail, dive achieves correct answer breaking question solving iteratively. reasoning, intent estimation, video summarization using object detection, enabling highly accurate contextually appropriate answers complex questions cvrr-es dataset. experimental results show method achieves accuracies validation set test set, significantly outperforming existing methods participating teams. furthermore, ablation studies confirmed significant contribution module, question breakdown, intent estimation, video summarization, overall performance. future work focus optimizing computational efficiency, exploring dynamic module selection, evaluating generalizability approach video benchmarks. references yue fan, xiaojian ma, rujie wu, yuntao du, jiaqi li, zhi gao, qing li. videoagent memory-augmented multimodal agent video understanding. arxiv, abs., muhammad uzair khattak, muhammad ferjad naeem, jameel hassan, naseer muzzamal, federcio tombari, fahad shahbaz khan, salman khan. good video lmm? complex video reasoning robustness evaluation suite video-lmms. noriyuki kugo, tatsuya ishibashi, kosuke ono, yuji sato. vdma video question answering dynamically generated multi-agents. arxiv, abs., noriyuki kugo, xiang li, zixin li, ashish gupta, arpandeep khatua, nidhish jain, chaitanya patel, yuta kyuragi, yasunori ishii, masamoto tanabiki, kazuki kozuka, ehsan adeli. videomultiagents multi-agent framework video question answering. arxiv, abs., junnan li, dongxu li, silvio savarese, steven hoi. blip- bootstrapping language-image pre-training frozen image encoders large language models. icml, yanwei li, chengyao wang, jiaya jia. llama-vid image worth tokens large language models. bin lin, bin zhu, yang ye, munan ning, peng jin, yuan. video-llava learning united visual representation alignment projection. conference empirical methods natural language processing, shilong liu, zhaoyang zeng, tianhe ren, feng li, hao zhang, jie yang, qing jiang, chunyuan li, jianwei yang, hang su, al. grounding dino marrying dino grounded pre-training open-set object detection. european conference computer vision, pages springer, muhammad maaz, hanoona rasheed, salman khan, fahad shahbaz khan. video-chatgpt towards detailed video understanding via large vision language models. proceedings annual meeting association computational linguistics acl openai. introducing deep research. openai com index introducing deep research, accessed --. shuhuai ren, linli yao, shicheng li, sun, hou. timechat time-sensitive multimodal large language model long video understanding. arxiv, abs., chuyi shang, amos you, sanjay subramanian, trevor darrell, roei herzig. traveler modular multi-lmm agent framework video question-answering. conference empirical methods natural language processing, chao-hong wang al. videoagent long-form video understanding large language model agent. neurips, wang, kunchang li, yizhuo li, yinan he, bingkun huang, zhiyu zhao, hongjie zhang, jilan xu, liu, zun wang, sen xing, guo chen, junting pan, jiashuo yu, yali wang, limin wang, qiao. internvideo general video foundation models via generative discriminative learning. arxiv, abs., ying wang, yanlai yang, mengye ren. lifelongmemory leveraging llms answering queries long-form egocentric videos. arxiv, ziyang wang, shoubin yu, elias stengel-eskin, jaehong yoon, feng cheng, gedas bertasius, mohit bansal. videotree adaptive tree-based video representation llm reasoning long videos. arxiv, abs., antoine yang, antoine miech, josef sivic, ivan laptev, cordelia schmid. zero-shot video question answering via frozen bidirectional language models. arxiv, abs., zhang, taixi lu, mohaiminul islam, ziyang wang, shoubin yu, mohit bansal, gedas bertasius. simple llm framework long-range video question-answering. conference empirical methods natural language processing, haoyu zhang, yuquan xie, yisen feng, zaijing li, meng liu, liqiang nie. hcqa egod egoschema challenge arxiv, abs., zhuo zhi, qiangqiang wu, minghe shen, wenbo li, yinchuan li, kun shao, kaiwen zhou. videoagent enhancing llm-based agent system long-form video understanding uncertainty-aware cot. arxiv, abs.,", "published_date": "2025-06-27T04:05:12+00:00"}
{"id": "2506.21851v1", "title": "End-to-End RGB-IR Joint Image Compression With Channel-wise Cross-modality Entropy Model", "authors": ["Haofeng Wang", "Fangtao Zhou", "Qi Zhang", "Zeyuan Chen", "Enci Zhang", "Zhao Wang", "Xiaofeng Huang", "Siwei Ma"], "summary": "rgb-irrgb-infrared image pairs frequently applied simultaneously various applications like intelligent surveillance. however, number modalities increases, required data storage transmission costs also double. therefore, efficient rgb-ir data compression essential. work proposes joint compression framework rgb-ir image pair. specifically, fully utilize cross-modality prior information accurate context probability modeling within modalities, propose channel-wise cross-modality entropy model ccem. among ccem, low-frequency context extraction block lceb low-frequency context fusion block lcfb designed extracting aggregating global low-frequency information modalities, assist model predicting entropy parameters accurately. experimental results demonstrate approach outperforms existing rgb-ir image pair single-modality compression methods llvip kaist datasets. instance, proposed framework achieves bit rate saving llvip dataset compared state-of-the-art rgb-ir image codec presented cvpr", "full_text": "cs.cv jun end-to-end rgb-ir joint image compression channel-wise cross-modality entropy model haofeng wang,,, fangtao zhou, zhang, zeyuan chen,, enci zhang, zhao wang,, xiaofeng huang, siwei abstract rgb-irrgb-infrared image pairs fre- quently applied simultaneously various applications like intelligent surveillance. however, number modalities increases, required data storage transmission costs also double. therefore, efficient rgb-ir data compression essential. work proposes joint compression framework rgb-ir image pair. specifically, fully utilize cross- modality prior information accurate context probability modeling within modalities, propose channel- wise cross-modality entropy model ccem. among ccem, low-frequency context extraction block lceb low-frequency context fusion block lcfb designed extracting aggregating global low-frequency in- formation modalities, assist model predicting entropy parameters accurately. experimental results demonstrate approach outperforms existing rgb-ir image pair single-modality compression methods llvip kaist datasets. instance, proposed framework achieves bit rate saving llvip dataset compared state-of-the-art rgb-ir image codec presented cvpr introduction recently, rgb-ir images pairs captured within scene jointly applied various practical scenarios, largely due fact advantages rgb modalities complementary. rgb images, known high resolution ability capture fine details textures, limited reliance ambient lighting. however, limitation mitigated incorporating images low sensitivity illumination changes. nevertheless, use rgb-ir image pairs significantly increases amount data needs transmitted stored. consequently, developing efficient joint compression method rgb- image pairs become crucial challenging task. past decades, deep learning-based image com- pression methods, exten- sively developed, pushing boundaries rate-distortion performance. intuitive compress rgb modal- ities independently using neural codecs. however, corresponding authors. school electronic computer engineering, peking university, shenzhen, china national engineering research center visual technology, school computer science, peking university, beijing, china school communication engineering, hangzhou dianzi university, zhejiang, china pengcheng laboratory, shenzhen, china advanced institute information technology, peking university, zhe- jiang, china redundancy rgb modalities fully ex- ploited compression, thereby limiting overall rate-distortion performance. recent years, several multi-modality data compression methods, proposed. however, methods specifically designed visible images paired depth hyperspectral images, suitable compressing rgb-ir image pairs due dif- ferent distributions modalities. example, unlike depth images use spatial geometry, images capture thermal properties less sensitive lighting. rgb-ir image pairs, learning-based multimodal image compression framework leverages one modality anchor assist encoding decoding process modality. approach enhances com- pression performance one modality, leverage cross-modality correlation context-based entropy model, thereby limiting rate-distortion performance modalities, often necessary practical appli- cations rgb-ir image pairs used together, besides, compression two modalities cannot performed simultaneously, one modality decoded first serve anchor compressing other, lowers computation efficiency. therefore, designing framework capable jointly compressing rgb-ir image pairs fully exploiting cross-modality correlations prior information enhance performance remains challenge. paper, main contribution propose dual-branch learning-based rgb-ir joint image compression framework simultaneously compress rgb-ir image pairs, leveraging correlation modalities save bit rate. design channel-wise cross-modality entropy model ccem fully utilize cross-modality prior information accurate context probability modeling within be- tween modalities. within ccem, propose low-frequency context extraction blocklceb low-frequency con- text fusion blocklcfb extract aggregate low- frequency prior information reveal dependency modalities. besides, unlike previous learning- based method rgb-ir image pair compression, approach require decoding one modalitys image anchor compressing another. according experimental results, proposed framework attains state- of-the-art performance compared existing rgb-ir image pair single-modality compression methods llvip kaist datasets fig. overall framework proposed method. network consists encoder, channel-wise cross-modality entropy model decoder. ae, denote arithmetic encoding decoding, respectively. denotes quantizer, denote concat split operation, denote upsampling downsampling factor two, respectively. ii. proposed method overall architecture overall architecture rgb-ir joint compression framework illustrated fig. use transformer-based encoder-decoder architecture. compression, rgb image converted yuv format, channels used inputs model. first, input channels individually fed encoder fea- ture extraction. use residual network combined self-attention-based module obtain feature maps yy, yu, yv, yir input channel. feature maps channels concatenated form unified yuv feature yyuv. use cross-attention embed cross-modality information within latent representations yyuv yir. subsequently, yyuv yir quantized yyuv yir, fed proposed channel-wise context-based cross-modality entropy model accurate symbol probability prediction. finally, yyuv yir input decoder upsampling image reconstruction. denote encoder, quantizer, decoder ga, sa, respectively. overall process formulated gaxi qyi, represents one input output channels learnable parameters. channel-wise cross-modality entropy model entropy model plays key role boosting com- pression performance estimating distribution latent representation. minnen al. introduced entropy model based spatial autoregressive prediction, surpass- ing compression performance h.. accelerate decoding, another work proposed split la- tent representation multiple slices leveraging inter- channel correlations autoregressively predict entropy model parameters slice. based this, mlic incorporates multiple perspectives context information multi-references predict entropy model parameters accurately. rgb-ir image pairs, leveraging cross- modality information, fully utilized, prior context enhance accuracy entropy model fig. architecture low-frequency context fusion blocklcfb. pe, pr, represent patch embedding, patch recovery, layernorm, respectively. fig. architecture proposed channel-wise cross-modality entropy model. latent representations split slices sent hyperprior model. encoded slices fed low-frequency context extraction block lceb low-frequency context fusion block lcfb extract global low-frequency prior, slice entropy model ei, hyperprior context global low-frequency context used predict entropy parameters. lrp represents latent residual prediction module. denotes concatenate operation. parameters prediction natural worthwhile problem explore. global low-frequency information rgb images images scene highly similar. therefore, reasonable infer that, compression rgb-ir image pairs, extracting aggregating global low-frequency information modalities condi- tional prior enable context-based entropy model predict parameters entropy model accurately, thereby effectively reducing bit rate. verify this, design low-frequency context extraction block lceb low-frequency context fusion block lcfb. role lceb extract global low-frequency information within modality, since low-frequency information typically distributed large regions, requires cross- regional global information exchange. therefore, adopt lite transformer architecture, model long- range dependencies globally, making particularly suitable capturing low-frequency information. shown fig. instead using concatenation operation, designed lcfb based agent-attention better aggregate global low-frequency information two modalities. agent-attention allows dynamic selective weighting information modalities, enables effec- tive context-aware fusion low-frequency features, thereby enhancing prediction cross-modal entropy model parameters. pipeline processing latent representations two modalities lcfb follows frgbwq, firwk, firwv poolingq, softmax softmax ffusion dwcv frgb fir represent feature input slices. wq, wk, linear projection matrices map input features query key value spaces, respectively. represent dimension relative po- sition bias respectively. dwc depth-wise convolution module. see output ffusion aggregated global low-frequency information two modalities use context entropy model get accurate entropy parameters. combining lceb lcfb, design channel-wise cross-modality entropy model ccem accurate probability estimation. architecture ccem shown left side fig. latent representation gen- erated encoder fed hyperprior model obtain spatial prior information. additionally, latent representation divided slices represents one input modalities. latent representation, first slice uses hyperprior context predict entropy model parameters. ith slice, use previous slices exact context predict entropy parameters. particular, slices concatenated processed lceb extract global low-frequency information. global low- frequency context hyperprior context used predict entropy parameters. rgb latent represen- tation, addition above, jth slice processed concatenating preceding slices global low-frequency information previously obtained latent representation input lcfb derive cross-modality information. additional cross-modality information used improve accuracy entropy model parameters prediction. specifically, denote yir latent representation two modalities. represents side information extracted hyperprior. probability distribution latent variables pyir pyr formulated pyirziryirzir pyi iryi ,ziryi iryi zir, pyryir,zryryir, pyiryi ,yir,zryi ryi yir, zr. fig. experimental results different image compression approaches llvip kaist datasets. loss function loss function framework described rir rrgb dir drgb. rir rrgb bit rate cost two modalities, calculated probability distribution latent representations. drgb dir calculated pixel- wise mean square error mse compressed original image. iii. experiments experiment details baseline metric introduce state-of-the-art rgb- image compression method hereafter, referred cvpr, comparison model. addition- ally, compare model best-performing single- modality codec kodak dataset, mlic, classic end-to-end codec, cheng, traditional single- modality image compression method bpg. ensure fair comparison, fine-tuned end-to-end compression methods llvip kaist datasets. single- modality codecs, primarily trained rgb-ir images, duplicated single-channel images three channels preserve original model structure training images. approach follows methods used previous learning-based multimodality compression studies, existing codecs directly fine- tuned using images. additionally, bpg codec, encoding images, set pixel format option grayscale. use psnr ms-ssim assess quality compressed images bjontegaard delta rate bd-rate evaluate rate-distortion performance. considering ultimate goal joint compression rgb-ir image pairs, compare average psnr modalities corresponding bd-rate baseline models. note evaluation metrics computed yuv domain. training details considering joint training modalities beginning would require model fig. visualization reconstructions .png llvip using different methods. simultaneously process multiple channels two modali- ties, difficult learn features modality cross-modality correlations. model training, propose two-stage training method. first stage, focus training compressing rgb data. specifically, converting rgb modality yuv, input channel data proposed model training. approach ensures model effectively extract features rgb modality early stages. completing first stage, proceed jointly optimize rgb modalities. experimental results show adopting training method improves models performance approximately additionally, set different hyperparameters control bit rate, following settings compressai training, use adam optimizer, learning rate gradually decreases throughout stage. conduct training testing llvip kaist pedestrian two widely used rgb-ir datasets. llvip, training performed datasets training images epochs stage, testing carried pairs test images. kaist pedestrian dataset, use set set training rest testing. datasets tested col- lected different lighting conditions day night. additionally, knowledge, llvip dataset currently highest resolution rgb-ir image pair dataset. kaist dataset contains significant number high-dynamic scenes includes many fast-moving objects real-time traffic scenes. two-stage training process, followed training schemes setting values fol- lowed treating two modalities equally important. therefore, introduce additional weight loss function balance distortion two modalities. training process, images randomly cropped size batch-size set number channels latent representation modality output encoder additionally, ccem, latent representation divided blocks. structure hyperprior consistent training goal initial stage training maintain consistency overall model architecture enhance models learning capability rgb modality early stages training, temporarily exclude modality first stage. thus, loss function initial stage similar second stage, except terms rir dir second stage removed, hyperparameters remaining same. experiment results quantitative results make comparison com- pression performance among various single-modality codecs state-of-the-art rgb-ir compression framework. com- pared single-modality compression frameworks, proposed framework shows significant improvement bd-rate performance. specifically, method outperforms cvpr mlic respectively. plot corresponding curves fig. intuitively illustrate performance gap different codecs. results clearly demonstrate proposed method significantly outperforms methods terms compression performance. qualitative results depicted fig. method exhibits superior subjective visual quality premise using less bit rate. specifically, local details enlarged, method still retain semantic information text roadside sign license plate number original image. modality-specific performance shown table proposed method achieves significant performance improve- ments rgb modalities. due efficient utilization low-frequency information modalities proposed ccem, makes parameter estimation entropy model accurate. running time complexity model proposed parameters, compressing rgb-ir image pair size requires .gb gpu memory. flops ccem module reach milpixel. model proposed focus real-time performance, rather emphasizes rate-distortion performance. tested datasets using single nvidia machine, models average encoding time rgb- image pair ms, average decoding time table modality-specific bd-rate comparisons llvip dataset kaist dataset bpg. methods llvip kaist rgb rgb cheng cvpr mlic ms. comparison, prior sequential approaches, cvpr, reach encoding time decoding time ms. anchor-based methods, like mlic, reach encoding time decoding time ms. method introduces slightly running time costs, achieves significant improvement performance. ablation study demonstrate effectiveness proposed lceb lcfb modules, conducted experi- ments removing module individually comparing results. table shows proposed modules contribute bd-rate performance, proposed ccem significantly enhances compression efficiency. table ablation study component channel-wise cross-modality entropy model model bd-rate baseline channel-wise cross-modality entropy model baseline lceb baseline lcfb exploration analysis ccem, extract contextual information decoded features series transformations assist entropy parameters prediction rgb features. fact, also conducted comparative experiments rgb features used assist entropy parameters prediction features. ultimately, llvip dataset, former approach achieved approximately bit rate savings compared latter. investigate underlying reasons, visualized rgb features entropy models two approaches. illustrated second line fig. rgb features obtained assistance decoded features exhibit distinct structural information com- pared features without assistance. additionally, regions experience texture enhancement, enabling accurate entropy parameters prediction. improvement attributed low-frequency information extracted designed lceb. conversely, illustrated first line fig. using decoded features assist rgb feature, resulting feature contain richer texture information compared features without assistance. however, approach also introduces noise certain regions marked figure, interferes accuracy entropy parameters prediction. fig. visualization rgb feature ccem iv. conclusions paper, propose joint compression frame- work rgb-ir image pair. specifically, remove cross-modality redundancy save bit-rate, introduce channel-wise cross-modality entropy model ccem. within ccem, design low-frequency context ex- traction block lceb low-frequency context fu- sion block lcfb based similarity low-frequency information rgb images. blocks effectively capture intra-modality cross-modality priors, thus assisting entropy model predicting symbol probability estimates accurately. comparative experi- ments ablation studies confirm effectiveness proposed method. references g.-a. wang, zhang, yang, cheng, chang, liang, z.-g. hou, cross-modality paired-images generation rgb-infrared person re-identification, proceedings aaai conference artificial intelligence, vol. no. pp. zhang, zhang, wang, ying, sheng, yu, li, h.- shen, tfdet target-aware fusion rgb-t pedestrian detection, ieee transactions neural networks learning systems, lee, kim, shin, kim, choi, insanet intra-inter spectral attention network effective feature fusion multispectral pedestrian detection, sensors, vol. no. wang, zhang, cheng, liu, yang, hou, rgb- infrared cross-modality person re-identification via joint pixel feature alignment, proceedings ieeecvf international conference computer vision, pp. balle, laparra, simoncelli, end-to-end optimized image compression, arxiv preprint cheng, sun, takeuchi, katto, learned image com- pression discretized gaussian mixture likelihoods attention modules, proceedings ieeecvf conference computer vision pattern recognition, pp. hu, yang, liu, coarse-to-fine hyper-prior modeling learned image compression, proceedings aaai conference artificial intelligence, vol. no. pp. zhu, yang, cohen, transformer-based transform coding, international conference learning representations, he, yang, peng, ma, qin, wang, elic efficient learned image compression unevenly grouped space- channel contextual adaptive coding, proceedings ieeecvf conference computer vision pattern recognition, pp. jiang, yang, zhai, ning, gao, wang, mlic multi-reference entropy model learned image compression, proceedings acm international conference multimedia, pp. kong, ren, hu, li, hu, mixture autoregressive spectral attention network multispectral image compression based variational autoencoder, visual computer, vol. no. pp. zheng gao, end-to-end rgb-d image compression via exploiting channel-modality redundancy, proceedings aaai conference artificial intelligence, vol. no. pp. lu, zhong, geng, hu, xu, learning based multi-modality image video compression, proceedings ieeecvf conference computer vision pattern recognition, pp. liu, zhang, wang, metaxas, multispec- tral deep neural networks pedestrian detection, arxiv preprint zhao, yuan, jiang, wang, wei, removal selection improving rgb-infrared object detection via coarse-to-fine fusion, arxiv preprint jia, zhu, li, tang, zhou, llvip visible-infrared paired dataset low-light vision, proceedings ieeecvf international conference computer vision, pp. choi, kim, hwang, park, yoon, an, kweon, kaist multi-spectral daynight data set autonomous assisted driving, ieee transactions intelligent transportation systems, vol. no. pp. liu, lin, cao, hu, wei, zhang, lin, guo, swin transformer hierarchical vision transformer using shifted windows, proceedings ieeecvf international conference computer vision, pp. minnen, balle, toderici, joint autoregressive hierarchical priors learned image compression, advances neural information processing systems, vol. minnen singh, channel-wise autoregressive entropy models learned image compression, ieee international confer- ence image processing icip. ieee, pp. zhao, bai, zhang, zhang, xu, lin, timofte, van gool, cddfuse correlation-driven dual-branch feature decomposition multi-modality image fusion, proceedings ieeecvf conference computer vision pattern recognition, pp. han, ye, han, xia, pan, wan, song, huang, agent attention integration softmax linear attention, european conference computer vision. springer, pp. han, pan, han, song, huang, flatten transformer vision transformer using focused linear attention, proceedings ieeecvf international conference computer vision, pp. bellard, bpg image format, available accessed oct. bjontegaard, calculation average psnr differences rd- curves, itu-t, tech. rep. vceg-m, begaint, racape, feltman, pushparaja, compressai pytorch library evaluation platform end-to-end compression research, arxiv preprint", "published_date": "2025-06-27T02:04:21+00:00"}
{"id": "2506.14265v1", "title": "Exploring Non-contrastive Self-supervised Representation Learning for Image-based Profiling", "authors": ["Siran Dai", "Qianqian Xu", "Peisong Wen", "Yang Liu", "Qingming Huang"], "summary": "image-based cell profiling aims create informative representations cell images. technique critical drug discovery greatly advanced recent improvements computer vision. inspired recent developments non-contrastive self-supervised learning ssl, paper provides initial exploration training generalizable feature extractor cell images using methods. however, two major challenges large difference distributions cell images natural images, causing view-generation process existing ssl methods fail unlike typical scenarios representation based single image, cell profiling often involves multiple input images, making difficult effectively combine available information. overcome challenges, propose sslprofiler, non-contrastive ssl framework specifically designed cell profiling. introduce specialized data augmentation representation post-processing methods tailored cell images, effectively address issues mentioned result robust feature extractor. improvements, sslprofiler cell line transferability challenge cvpr", "full_text": "cs.cv jun exploring non-contrastive self-supervised representation learning image-based profiling siran dai, qianqian xu, peisong wen, yang liu qingming huang,, institute information engineering, chinese academy sciences school cyber security, university chinese academy sciences institute computing technology, chinese academy sciences school computer science technology, university chinese academy sciences daisiraniie.ac.cn xuqianqian, wenpeisongzict.ac.cn liuyangmails.ucas.ac.cn qmhuangucas.ac.cn abstract image-based cell profiling aims create informative repre- sentations cell images. technique critical drug discovery greatly advanced recent improve- ments computer vision. inspired recent developments non-contrastive self-supervised learning ssl, pa- per provides initial exploration training gener- alizable feature extractor cell images using meth- ods. however, two major challenges large difference distributions cell im- ages natural images, causing view-generation pro- cess existing ssl methods fail unlike typical scenarios representation based single image, cell profiling often involves multiple input images, making difficult effectively combine available infor- mation. overcome challenges, propose sslpro- filer, non-contrastive ssl framework specifically designed cell profiling. introduce specialized data augmenta- tion representation post-processing methods tailored cell images, effectively address issues mentioned result robust feature extractor. improvements, sslprofiler cell line transferabil- ity challenge cvpr introduction cell profiling aims create meaningful representations cells, utilized validating compounds drug discovery understanding disease mechanisms among various cell profiling methods, image-based profil- ing using microscope images cost-effective ap- proach generating high-dimensional representations. corresponding authors generating representations without human annotations long important goal computer vision, recent advances self-supervised learning ssl brought closer. although previous research made significant progress image-based profiling using com- puter vision techniques obtaining generalizable feature extractor self-supervised methods remains open challenge. although ssl highly successful natural image processing, directly applying existing ssl methods cell profiling suitable. main difficulty arises distribution gap fluorescently dyed mi- croscope cell images natural images. gap intro- duces two challenges state-of-the-art ssl methods aim learn representations remain consistent across differ- ent views objects, resulting informative general- izable representations. however, methods depend carefully designed image augmentations. applying augmentations directly cell images appropriate degrades performance due dimensional collapse unlike natural images, representations generated -channel image, cell images require generating one representation multiple images several channels. channels, may include fluorescent bright- field images, contain lower information density compared natural images. effectively extracting merging infor- mation channels therefore crucial. address first challenge, propose augmentations specifically tailored cell images. given channels dependent fluorescent dyes, introduce channel- aware color jitter augmentation. additionally, simulate imaging noise microscope noise augmentation. enhance robustness learned representations consid- ering cell morphology anisotropy, include elastic transformations random rotations. second challenge, make model adaptable varying inputs, use single-cell image input feature extractor. then, design sequence fea- ture post-processing steps merge outputs multiple images within well, producing final representa- tion. specifically, first perform multi-granularity merg- ing concatenating individual image representation averaged dense representations. merge represen- tations different sites within well, implement interpolation-based merging. strengthen causal re- lationship representations perturbation com- pounds, apply cross-plate concentration scheme. empirically validate method, named sslprofiler, pretraining evaluating vit model cell image dataset also examine impact key components proposed method. approach achieved first place cell line transferability challenge cvdd workshop, cvpr related works self-supervised learning initially, ssl aimed solving pretext tasks designed manu- ally humans later, contrastive learning methods significantly outper- formed supervised pretraining approaches image-level tasks, making ssl mainstream method model pre- training. recently, non-contrastive learning, aims achieve consistency across different views attracted increasing interest due better ability generalize. non-contrastive methods success- fully applied various domains, including video represen- tation learning medical image processing nevertheless, applying ssl dense prediction tasks long-tailed scenarios remains challenging. im- portantly, effectively integrate ssl, especially non- contrastive approaches, image-based cell profiling re- mains open research question. image-based cell profiling image-based cell profiling become powerful method measuring phenotypic differences across various cellu- lar states. utilizing high-throughput microscopy ad- vanced computational methods, approach extracts de- tailed morphological features cell images, providing valuable insights cellular responses different con- ditions. earlier research image-based profiling primar- ily focused supervised weakly supervised approaches recent developments enhanced profiling integrating contrastive ssl methods ob- tain richer informative representations. nonethe- less, exploring non-contrastive methods cell profiling may improves performance generalization. method task definition paper focuses extracting meaningful representa- tions cell images. specifically, use images cpjump dataset provided dataset includes chemically genetically perturbed cells two cell lines uos experiment performed within plate contains multiple wells. well represents distinct experimental condition unique perturbations. thus, define dataset total number wells. within well, images captured either distinct positions, represented xjpi number positions well images position channels, consisting five fluorescent three brightfield channels. goal train robust feature extractor well xw, enabling learned representation capture cellular phenotypic features causal effects applied perturbations. although ssl methods successful natural images, applying methods cell images challenging two main reasons first, cell images provide multiple types information compared natural images. instance, natural image datasets imagenet representations generated based single image. contrast, cell images require integration data various positions multiple channels. second, significant distribution gap exists cell images natural images. effective ssl methods heavily rely data augmentation strategies, random cropping color jittering must carefully modified cell im- ages due distinct properties channels. data preprocessing original dataset consists -bit tiff images. re- duce disk space usage accelerate data loading training, first convert images -bit format, aligning natural image standards, using ix, ix, mini maxi mini next, compute mean variance channel, denoted training inference, input images normalized follows self-supervised model pretraining ... model framework adopt classical non-contrastive ssl framework based siamese network, comprises student model teacher model ft. consistent standard self- distillation methods student model learns distilling knowledge teacher model, teacher model updated using exponential moving av- erage ema student model parameters. vision transformer vit used backbone architec- ture. pretraining, position image treated single input unit. unlike natural images, cell im- ages consist channels rather three. due signifi- cant distribution differences fluorescent bright- field channels, datasets include channels, separately train two models chan- nel types. features combined testing. empirical results indicate approach performs better using channels simultaneously. loss computation, directly employ dino framework without modification. includes three main components ldino loss instance-level information distillation libot loss patch-level information reconstruction lkoleo loss regularization overall loss function ltotal ldino libot lkoleo, hyperparameters. since loss func- tions remain unchanged training, details omitted due space limitations. readers may refer original dino paper details. ... data augmentation effectiveness cross-view consistency ssl heavily depends view generation, specifically data augmenta- tion. proper data augmentation promotes clustering sim- ilar examples whereas excessively strong augmenta- tions lead dimensional collapse thus, carefully designing augmentations cell images critical. adapted color jitter. considering channels fluores- cent images independent rgb images, propose channel-aware color jitter augmentation independently adjusts brightness contrast per channel. given input image rhw augmented image computed follows. let represent c-th channel input image. channel brightness factors contrast factors independently drawn uniform distributions umax, umax, control maximum brightness con- trast adjustments, respectively. augmented image channel first adjusted brightness subsequently adjusted contrast relative mean icx, additional augmentations cell images. simulate realistic microscopy imaging noise, apply microscope noise augmentation, includes shot noise, dark cur- rent, read noise. noises simulated using pois- son gaussian distributions. address cell morpholog- ical variability anisotropy, also use elastic transfor- mations random rotations additional augmentations. detailed description presented appendix due space limitations. feature post processing far, obtained feature extractors. however, extractors produce representation position within well. subsection, investigate methods combine embeddings effectively obtain strong representations well. ... multi-granularity information merging since use vit models backbone, naturally provide representations different levels image-level cls token dense-level patch tokens. ef- fectively use multi-level features, concatenate cls token average patch tokens, forming final output position concatzcls zpatch ... multi-position information merging next, combine representations multiple posi- tions single well-level representation. cpjump dataset, images captured ei- ther positions. typically, representations merged averaging concatenation. however, direct concatenation suitable cell profiling tasks representations different wells would varying dimensions, complicating downstream tasks. solve issue, assume positions sampled symmetrical pattern. based assumption, wells images first reshaped matrix, inter- polated matrix. adjustment, wells contain number representations representa- tions, allowing effective concatenation. practically, observe concatenation typically provides better results compared averaging. ... cross-plate representation alignment enhance consistency learned representa- tions across different experiments, apply simple yet ef- fective method. specifically, observed dataset perturbations linked specific well positions, meaning well positions across different plates af- fected identical compounds. thus, aligning representa- tions well positions improves causal re- lationship learned representations compound- induced perturbations. considering representations well position clusters, first calculate cluster centers number plates, denotes well representation position plate then, shift well representation toward cluster center follows hyperparameter controlling degree align- ment. find strategy enhances causal rela- tionship cell representations compounds, par- ticularly excelling cvdd challenge however, may slightly decrease generalization ability re- ducing phenotypic information captured cell repre- sentations. experiments experiment setup model pretraining. pretrained vit-small- model scratch epochs cpjump batch feed-forward network, employed swiglu set batch size used learn- ing rate warmup period epochs. trained two separate models fluorescent brightfield channels, respectively, combined outputs using post-processing method described sec evaluation protocol. adopted evaluation setup cell line transferability challenge cvpr uses k-nn classifier downstream task. specifically, trained k-nn classifier learned representations wells associate representations compound perturbations. applied k-fold cross- validation split dataset training evaluation subsets. assess robustness representations, evaluation included tests within across different cell lines. details found main results present analysis key components tab. table, local refers reimplemented evaluation, table ablation study key differences dino sslprofiler. local leader board baseline submission dino channels input patch representations separate training training res adapted augmentations vit-base- oom epochs training oom leader board indicates official results challenge. baseline submission refers example rep- resentations provided challenge organizers. below, detail component included analysis channels input indicates adaptation dino framework cell image dataset, number channels changed. patch representations refers methods sec ... separate training means trained two separate models fluorescent brightfield channels. approach improved training stability model performance. training res means increased training resolu- tion global view significantly im- proved results since higher resolution images nec- essary capture detailed information small cells. adapted augmentations refers augmentation tech- niques described sec .., confirming effective- ness approach. vit-base- epochs means increased backbone model size training duration, achieving bet- ter performance. cross place alignment refers post-processing method described sec. ... method sig- nificantly improved results challenge, may lead overfitting reduced generalization. therefore, recommend using specific challenge. conclusion paper, presented initial attempt utilize success ssl enhance cell profiling capabilities. how- ever, considerable differences natural images cell images make task challenging. using dino starting point, investigated several important fac- tors, including data preprocessing, model pretraining, representation post-processing. experiments confirmed effectiveness component. future work, plan explore ways improve training algorithms cell profiling. references pulkit agrawal, joao carreira, jitendra malik. learning see moving. proceedings ieee international conference computer vision, pages saeid asgari taghanaki, kumar abhishek, joseph paul co- hen, julien cohen-adad, ghassan hamarneh. deep se- mantic segmentation natural medical images re- view. artificial intelligence review, kumar ayush, burak uzkent, chenlin meng, kumar tan- may, marshall burke, david lobell, stefano ermon. geography-aware self-supervised learning. proceedings ieeecvf international conference computer vi- sion, pages adriana borowa, ana sanchez-fernandez, dawid ry- marczyk. cell line transferability challenge cvdd. kaggle com competitions cell line-transferability-challenge-cvdd, kaggle. mark-anthony bray, shantanu singh, han han, chad- wick davis, blake borgeson, cathy hartland, maria kost- alimova, sigrun gustafsdottir, christopher gibson, anne carpenter. cell painting, high-content image-based assay morphological profiling using multiplexed fluores- cent dyes. nature protocols, juan caicedo, samuel cooper, florian heigwer, scott warchal, peng qiu, csaba molnar, alexey vasilevich, john barry, harsimrat bansal, oliver kraus, al. data- analysis strategies image-based cell profiling. nature methods, mathilde caron, hugo touvron, ishan misra, herve jegou, julien mairal, piotr bojanowski, armand joulin. emerg- ing properties self-supervised vision transformers. pro- ceedings ieeecvf international conference com- puter vision, pages srinivas niranj chandrasekaran, hugo ceulemans, justin boyd, anne carpenter. image-based profiling drug discovery due machine-learning upgrade? nature re- views drug discovery, srinivas niranj chandrasekaran, beth cimini, amy goodale, lisa miller, maria kost-alimova, nasim jamali, john doench, briana fritchman, adam skepner, michelle melanson, al. three million images morphological profiles cells treated matched chemical genetic perturbations. nature methods, ting chen, simon kornblith, mohammad norouzi, ge- offrey hinton. simple framework contrastive learning visual representations. international conference ma- chine learning, pages pmlr, xinlei chen kaiming he. exploring simple siamese rep- resentation learning. proceedings ieeecvf con- ference computer vision pattern recognition, pages xinlei chen, haoqi fan, ross girshick, kaiming he. improved baselines momentum contrastive learning. arxiv preprint siran dai, qianqian xu, zhiyong yang, xiaochun cao, qingming huang. drauc instance-wise distributionally robust auc optimization framework. advances neural in- formation processing systems, carl doersch, abhinav gupta, alexei efros. unsuper- vised visual representation learning context prediction. proceedings ieee international conference com- puter vision, pages alexey dosovitskiy, lucas beyer, alexander kolesnikov, dirk weissenborn, xiaohua zhai, thomas unterthiner, mostafa dehghani, matthias minderer, georg heigold, syl- vain gelly, al. image worth words trans- formers image recognition scale. iclr, spyros gidaris, praveer singh, nikos komodakis. un- supervised representation learning predicting image rota- tions. arxiv preprint jean-bastien grill, florian strub, florent altche, corentin tallec, pierre richemond, elena buchatskaya, carl doersch, bernardo avila pires, zhaohan guo, mohammad ghesh- laghi azar, al. bootstrap latent-a new approach self-supervised learning. advances neural information processing systems, kaiming he, haoqi fan, yuxin wu, saining xie, ross girshick. momentum contrast unsupervised visual rep- resentation learning. proceedings ieeecvf con- ference computer vision pattern recognition, pages weiran huang, mingyang yi, xuyang zhao, zihao jiang. towards generalization contrastive self- supervised learning. iclr, simon jenni paolo favaro. self-supervised feature learn- ing learning spot artifacts. proceedings ieee conference computer vision pattern recognition, pages jing, pascal vincent, yann lecun, yuandong tian. understanding dimensional collapse contrastive self- supervised learning. arxiv preprint alex krizhevsky, ilya sutskever, geoffrey hinton. imagenet classification deep convolutional neural net- works. neurips, gustav larsson, michael maire, gregory shakhnarovich. colorization proxy task visual understanding. proceedings ieee conference computer vision pattern recognition, pages yang liu, qianqian xu, peisong wen, siran dai, qing- ming huang. pairs equal hierarchical learning average-precision-oriented video retrieval. proceed- ings acm international conference multime- dia, pages yang liu, qianqian xu, peisong wen, siran dai, qing- ming huang. future becomes past taming temporal correspondence self-supervised video represen- tation learning. arxiv preprint claire mcquin, allen goodman, victor chernyshev, lee kamentsky, beth cimini, kyle karhohs, minh doan, ding, susanne rafelski, david thirstrup, al. cell- profiler next-generation image processing biology. plos biology, ishan misra laurens van der maaten. self-supervised learning pretext-invariant representations. proceedings ieeecvf conference computer vision pattern recognition, pages mehdi noroozi paolo favaro. unsupervised learning visual representations solving jigsaw puzzles. eccv, pages aaron van den oord, yazhe li, oriol vinyals. repre- sentation learning contrastive predictive coding. arxiv preprint aaron van den oord, yazhe li, oriol vinyals. repre- sentation learning contrastive predictive coding. arxiv preprint maxime oquab, timothee darcet, theo moutakanni, huy vo, marc szafraniec, vasil khalidov, pierre fernandez, daniel haziza, francisco massa, alaaeldin el-nouby, al. dinov learning robust visual features without supervision. arxiv preprint alexandre sablayrolles, matthijs douze, cordelia schmid, herve jegou. spreading vectors similarity search. arxiv preprint ana sanchez-fernandez, elisabeth rumetshofer, sepp hochreiter, gunter klambauer. contrastive learning image-and structure-based representations drug discovery. iclr machine learning drug discovery, sanchez-fernandez al. cloome contrastive learning unlocks bioimaging databases drug discovery. nature communications, noam shazeer. glu variants improve transformer. arxiv preprint shantanu singh anne carpenter. deepprofiler image- based profiling using deep learning. github repository, yemin yu, neil tenenholtz, lester mackey, ying wei, david alvarez-melis, ava amini, alex lu. causal integra- tion chemical structures improves representations mi- croscopy images morphological profiling. arxiv preprint jinghao zhou, chen wei, huiyu wang, wei shen, cihang xie, alan yuille, tao kong. ibot image bert pre-training online tokenizer. arxiv preprint adrian ziegler yuki asano. self-supervised learning object parts semantic segmentation. proceedings ieeecvf conference computer vision pattern recognition, pages exploring non-contrastive self-supervised representation learning image-based profiling supplementary material details additional augmentations cell images. microscope noise data augmentation microscope noise augmentation introduces realistic imaging noise components original microscopy im- age, modeled mathematically follows given input image ix, intensity values nor- malized range augmented noisy image inoisyx, defined incorporating three types noise shot noise, dark current, read noise. specifically, augmented image computed iphotonx, ix, shot iphotonx, poisson iphotonx, inoisyx, iphotonx, shot dark read, inoisyx, clip inoisyx, read represents gaussian noise zero- mean variance read. hyper-parameters shot denotes shot noise scaling factor default dark represents dark current noise level default read denotes standard deviation gaussian read noise default augmentation simulates realistic microscopy imag- ing conditions, providing robustness various noise artifacts commonly encountered practical scenarios. elastic deformation augment training image rhw prob- ability elastic transformation. first, two indepen- dent displacement fields gn, generated convolving zero-mean, unit-variance gaussian noise gaussian kernel standard devia- tion subsequently scaling elasticity coefficient denotes gaussian smoothing operator convolution. dense mesh grid base coordi- nates constructed xu, perturbed obtain deformed sampling locations xu, xu, xu, yu, augmented image obtained applying channel- wise bilinear interpolation icu, xu, hyper-parameters magnitude deformation default spatial smoothness displacement field pixels default probability applying sample default operation preserves image topology introduc- ing smooth, non-linear distortions, effectively enlarging training distribution simulating natural variations ob- ject shape.", "published_date": "2025-06-17T07:25:57+00:00"}
{"id": "2506.05815v1", "title": "NTIRE 2025 Challenge on HR Depth from Images of Specular and Transparent Surfaces", "authors": ["Pierluigi Zama Ramirez", "Fabio Tosi", "Luigi Di Stefano", "Radu Timofte", "Alex Costanzino", "Matteo Poggi", "Samuele Salti", "Stefano Mattoccia", "Zhe Zhang", "Yang Yang", "Wu Chen", "Anlong Ming", "Mingshuai Zhao", "Mengying Yu", "Shida Gao", "Xiangfeng Wang", "Feng Xue", "Jun Shi", "Yong Yang", "Yong A", "Yixiang Jin", "Dingzhe Li", "Aryan Shukla", "Liam Frija-Altarac", "Matthew Toews", "Hui Geng", "Tianjiao Wan", "Zijian Gao", "Qisheng Xu", "Kele Xu", "Zijian Zang", "Jameer Babu Pinjari", "Kuldeep Purohit", "Mykola Lavreniuk", "Jing Cao", "Shenyi Li", "Kui Jiang", "Junjun Jiang", "Yong Huang"], "summary": "paper reports ntire challenge depth images specular transparent surfaces, held conjunction new trends image restoration enhancement ntire workshop cvpr challenge aims advance research depth estimation, specifically address two main open issues field high-resolution non-lambertian surfaces. challenge proposes two tracks stereo single-image depth estimation, attracting registered participants. final testing stage, participating teams submitted models fact sheets two tracks.", "full_text": "cs.cv jun ntire challenge depth images specular transparent surfaces pierluigi zama ramirez fabio tosi luigi stefano radu timofte alex costanzino matteo poggi samuele salti stefano mattoccia zhe zhang yang yang chen anlong ming mingshuai zhao mengying shida gao xiangfeng wang feng xue jun shi yong yang yong yixiang jin dingzhe aryan shukla liam frija-altarac matthew toews hui geng tianjiao wan zijian gao qisheng kele zijian zang jameer babu pinjari kuldeep purohit mykola lavreniuk jing cao shenyi kui jiang junjun jiang yong huang abstract paper reports ntire challenge depth images specular transparent surfaces, held conjunction new trends image restora- tion enhancement ntire workshop cvpr challenge aims advance research depth es- timation, specifically address two main open is- sues field high-resolution non-lambertian sur- faces. challenge proposes two tracks stereo single-image depth estimation, attracting regis- tered participants. final testing stage, partic- ipating teams submitted models fact sheets two tracks. introduction reversing image formation process model structure world represents one quintessential tasks studied computer vision. purpose, estimat- ing depth images often lays foundation pro- cess, well entry point higher-level applications augmented reality, autonomous assisted driving, robotics, more. recovering information im- ages represents cheaper viable alternative use active depth sensors radars, lidars, time-of-flight tof, others known pierluigi zama ramirez pierluigi.zamaunibo.it, alex costanzino, fabio tosi, matteo poggi, samuele salti, stefano mat- toccia, luigi stefano radu timofte ntire depth images specular transparent surfaces challenge organizers. authors participated challenge. sections contains authors team names affiliations. ntire website higher cost multiple limitations preventing unconstrained deployment environment. further- more, disruptive advent deep learning computer vi- sion made former strategy preferable active sensors, also thanks recent development first foundational models depth estimation and, general, vision. although brought rapid evo- lution depth estimation models observed last decade, task remains far solved pres- ence particularly challenging conditions. among many, argue two matters interest common different approaches devoted estimating depth images even active sensors. first longstanding challenge, common computer vision task spatial resolution. indeed, although nowadays color cameras capable capturing frames dozens megapixels mpx whereas active sensors fall far behind, processing high-resolution images poses several challenges, terms computational require- ments well data training methodologies deploy deep models capable exploiting rich information. second consists ambiguity may occur images may assume different forms, lack texture absence perspective cues, af- fect different kinds depth estimation strategies. specif- ically, presence non-lambertian surfaces represents challenge approaches, well active sensors since materials featuring property often vi- olate basic assumptions upon active sensors built, e.g., lidars beams refracted surpass- ing transparent surfaces. makes ground-truth depth an- notations, often sourced active sensors, hard collect and, therefore, rare training data lever- aged state-of-the-art image-based depth estimation models, making latter failing estimate distance transparent surface favor distance objects behind it, surface mirror place depth reflected objects. although latter examples might represent real failure cases, since definition depth becomes ambiguous circumstances, argue popular applications, instance prop- erly perceiving real depth transparent objects may crucial accomplishing higher level task, like grasping glassy objects navigating indoor environment glass doors may common. ntire challenge depth images specular transparent surfaces aims pushing development state-of-the-art methodologies answering aforementioned challenges. following previous, successful editions build challenge booster dataset benchmark peculiarly encom- passing high-resolution non-lambertian surfaces, thanks mpx images abundant presence transparent reflective objects. following tradition, challenge organized two tracks one devoted stereo approaches, depth measured trian- gulation disparity estimated pixels two rectified frames, focusing single-image frameworks mono. challenge attracted reg- istered participants. among them, teams, respec- tively, monocular stereo tracks, submitted models fact sheets final phase. adopt recent foundational models field off-the- shelf solutions, whereas others develop custom frameworks. outcome edition challenge reported discussed detail section related work deep stereo matching. ten years ago already, com- munity started facing stereo depth estimation deep neural networks becoming standard approach task years first, two main fami- lies end-to-end models developed, respectively, architectures. advent new paradigms deal dense matching tasks, use optimization- based frameworks transformers ignited development two new lines research. former particular, starting raft-stereo rapidly become popular approach translated steady saturation pop- ular benchmarks, kitti ethd middlebury ap- proaches try refine disparity maps high-resolution predictions. lately, first foundational models stereo depth estimation appeared achieving consistent step forward terms zero-shot generalization robustness handling non-lambertian surfaces. indeed, notice remainder paper, solutions successfully deployed booster dataset well. monocular depth estimation. deep learning allowed facing highly ill-posed tasks, estimating depth single image thanks increas- ing availability large-scale, annotated datasets emergence self-supervised paradigms replac- ing need explicit depth annotation principles multi-view geometry, instance casting depth es- timation process image reconstruction problem dur- ing training thanks availability either paired stereo images monocular videos. major trend emerging twenties consists development affine-invariant depth estimation models capable generalizing beyond single-dataset domain. midas took direction first, training deep network mixture multiple datasets achieve cross-domain generalization, followed dpt others focusing recov- ering real shapes deformed point cloud ob- tained monocular depth maps restoring high- frequency details higher resolution. affine- invariant models recently converged first founda- tional models single-image depth estimation, depth anything series newest diffusion- derived frameworks marigold geowizard lotus others, extended deal video depth estimation lately, ability single-image depth estimation effectively handle transparent reflective surfaces gained relevance, also thanks advent benchmarks dedicated purpose track, ap- proaches developed annotation pipeline obtain reli- able pseud-labels non-lambertian objects, using pre- trained monocular depth estimation models jointly ma- terial segmentation masks diffusion models whereas others employed depth completion approaches fill holes depth maps occurring cor- respondence transparent surfaces. furthermore, latest foundational models depth anything expose surprising effectiveness perceiving transpar- ent mirroring surfaces, shown remainder. competitionschallenges depth estimation. depth estimation task, stereo monocular im- ages, object several challenges taking place previous years, even concurrently ours. among them, robust vision challenge rob covering both, dense depth autonomous driving challenge ddad, fast accurate single-image depth estimation mobile devices challenge mai argoverse stereo challenge monocular depth estimation challenge mdec finally, re- call previous editions challenge part ntire workshop cvpr tricky workshop eccv ntire challenges. challenge one ntire workshop associated challenges ambi- ent lighting normalization reflection removal wild shadow removal event-based image de- blurring image denoising xgc quality assess- ment ugc video enhancement night photogra- phy rendering image super-resolution real- world face restoration efficient super-resolution depth estimation efficient burst hdr restora- tion cross-domain few-shot object detection short-form ugc video quality assessment enhance- ment text image generation model quality as- sessment day night raindrop removal dual- focused images video quality assessment video conferencing low light image enhancement light field super-resolution restore image model raim wild raw restoration super- resolution raw reconstruction rgb smart- phones ntire challenge depth im- ages specular transparent surfaces organize ntire challenge depth images specular transparent surfaces push community toward developing newer, state-of-the- art solutions properly deal high-resolution images non-lambertian surfaces mirrors glasses. outline main characteristics challenge. tracks. challenge composed two tracks stereo, devoted methods estimating disparity pairs rectified images, mono, instead allows estimating depth single input image only. track stereo. tracks demands participants obtain high-quality, high-resolution dense disparity maps mpx stereo frames. resolution repre- sents one main challenges, prohibitive state-of-the-art existing stereo networks. further- more, presence non-lambertian objects violating common assumptions made stereo matching makes track even harder. track mono. parallel, track requires esti- mate depth single mpx frame. case, inherent ill-posed nature problem represents one main challenges. additionally, presence ob- jects belonging long-tail training data used task transparent objects mirrors fur- ther makes complex. datasets. build challenge around booster dataset composed high-resolution bal- anced unbalanced stereo pairs, captured different scenes respectively distributed pairs training testing purposes two sets respectively. newer version dataset extended second testing split, tailored evaluat- ing monocular depth estimation approaches single frames, captured different environments. previous editions use origi- nal training stereo pair shared training split, com- mon tracks. select two distinct validation splits, selecting frames different illuminations scenes stereo monocular testing splits i.e., microwave, mirror, pots stereo track, desk, mirror, sanitaries mono track respectively, total validation samples track total available selected scenes. remaining images two original testing splits become official stereo mono testing splits challenge, total samples. evaluation protocol. track, stereo mono respectively, adopt official metrics reported booster benchmark stereo track, com- pute percentage pixels disparity errors larger threshold bad-, well mean absolute error mae root mean squared error rmse pixel. mono track, compute percentage pixels maximum predictionground-truth ground-truthprediction ratios lower threshold absolute error relative ground truth value abs rel., well mean absolute error mae, root mean squared error rmse. following lat- est edition compute metrics three different sets pixels tom regions i.e., belonging non-lambertian surfaces pixels others i.e., difference tom sets. rank submissions determine winner, use bad- respectively stereo mono tracks averaged pixels, highlighted red tables. specifically, define two rankings based performance tom regions, respectively. finally, state-of-the-art monocular networks estimate depth unknown pair scale shift factors, computing metrics re- cover metric depth predicted maps scale shift factors. following estimated least square estimation lse regression ground truth depth map highlight two coincide mono track tom team rank bad- bad- bad- bad- mae rmse rank bad- bad- bad- bad- mae rmse bad- bad- bad- bad- mae rmse src-b robot-vrobotit njust-kmg weouibaguette crestereo baseline table stereo track evaluation challenge test set. predictions evaluated full resolution pixels pixels belonging tom transparent mirror materials. gold silver bronze show first, second, third-rank approaches, respectively. rank methods two metrics, computed either tom pixels. rgb crestereo src-b robot-vrobotit njust-kmg weouibaguette figure qualitative results stereo track. left right rgb reference image, ground-truth disparity, predictions crestereo src-b, robot-vrobotit, njust-kmg, weouibaguette. eqrescaling alpha ,beta text argmin alpha ,beta sum big alpha hat beta big pixel locations predictions ground truth depths available. challenge results track, four teams participated final evalua- tion phase, outcomes detailed sections brief explanation approach stereo mono tracks provided section section team composition detailed sections track stereo table reports results first track. bottom, report baseline i.e., crestereo left right, report bad- metrics, mae, rmse metrics tom, all, pixels respectively. right teams name, report overall rank, computed according bad- errors restrictive metric tom regions. submitted methods outperformed baseline tom pixels, src-b achieving lowest error rates tom pixels weouibaguette pixels, baseline still performs best pixels. interestingly, unlike previous iterations challenge, somewhat clear trend tom pixels, low bad- errors correlating low mae rmse scores, results consistently mixed up, making hard identify jack-of-all-trades model. fig. depicts qualitative results stereo testing set. appreciate submitted methods tend deal better specific challenges, bottles row crestereo falters infer without discontinu- ities, book column submitted meth- ods produce much smoother results. track mono table shows results second track. bottom, report results achieved baseline method i.e., zoedepth model using weights provided authors. left right, report deltas, abs rel., mae, rmse metrics tom, all, pixels respectively. report two different rankings, ac- cording performance observed restrictive metric computed tom pixels. unlike stereo track, submitted methods con- sistently outperformed zoedepth baseline, lavre- niuk achieving best accuracy values tom, pixels. indeed, conversely observed tom team rank abs rel. mae rmse rank abs rel. mae rmse abs rel. mae rmse lavreniuk colab prerdw ipcv zoedepth baseline table mono track evaluation challenge test set. predictions evaluated full resolution pixels pixels belonging tom transparent mirror materials. gold silver bronze show first, second, third-rank approaches, respectively. rank methods two metrics, computed either tom pixels. rgb zoedepth lavreniuk colab prerdw ipcv figure qualitative results mono track. left right rgb reference image, ground-truth disparity, predictions zoedepth lavreniuk, colab, prerdw, ipcv. stereo track, lavreniuk represents versatile method top performer pixel categories. concerns tom regions, top methods able push strictest accuracy metric beyond remarkable improvement respect last year, well reduce abs rel. improvements reflected pixels well. despite minor gain respect baseline, com- pared observed tom regions, improve- ment yet consistent. fig. shows qualitative examples mono testing set. similarly last edition, submitted models properly handle tom regions, oven third row, still struggling mirrors water surfaces, first second rows. challenge methods track stereo baseline crestereo first track, set state-of-the-art crestereo architecture baseline. model consists hierarchical network employing recurrent refinement process, designed update predicted disparity map coarse-to-fine manner. process implemented based adaptive group correlation layer agcl, alternate d-d local search strategy deformable win- dows employed robust matching even presence imperfect rectification. agcl module computes correlations pixels local search windows, con- trast all-pairs correlation module raft- stereo does, reducing computational requirements. obtain final predictions, process images quar- ter resolution using original weights released au- thors, upsample predicted disparity maps original resolution bilinear interpolation. team njust-kmg njust-kmg team codalab chenyin adapts defom-stereo integrating depth anything approach improves cnn encoders depth foun- dation model, introduces scale update module delta update module, leverages reflective data multi-scale sampling training. feature encoder fuses dpt cnn feature maps resolution matching, context encoder combines multi-level dpt cnn features. disparity estimation initialized depth anything depth, followed scale correction detail refine- ment using pyramid lookup. system pre-trained figure network architecture team robot-vrobotit. kitti middlebury ethd fine-tuned booster crestereo datasets. quarter-resolution inference ensures efficient processing high-resolution inputs. team robot-vrobotit robot-vrobotit team codalab bupt-chenwu in- troduces improved monster based depth any- thing approach leverages complementary strengths monocular depth estimation stereo match- ing dual-branch architecture, monocular depth provides global structural information stereo match- ing refines pixel-level geometric details. training process divided two stages first, depth anything v-large model fine-tuned improve accuracy tom regions booster dataset sec- ond, monster network fine-tuned using improved monocular model first stage. training, ex- tensive data augmentation applied, including resizing, random cropping, saturation adjustment, color jittering, spatial scaling. fine-tuning performed nvidia rtx gpus epochs, learning rate batch size team src-b stereo src-b team codalab pixinsight presents multi- scale-mono-stereo, method integrates monocular depth network features improve depth estimation high- resolution images non-lambertian surfaces. ap- proach adopts data augmentation strategy similar as- grasp leveraging blender generate additional stereo training samples aithor dataset. ef- fectively use pre-trained monocular model, adopt stereo branch structure monster incorpo- rating pretrained vit encoder depth anything frozen parameters. feature transfer network in- troduced downsample transform vit-extracted features multi-scale feature pyramid, figure network architecture team src-b stereo. concatenated features extracted igev en- hance performance high-resolution images, team in- tegrates stacked cascaded architecture training, en- abling network adaptively propagate depth informa- tion across different scales. network implemented pytorch trained nvidia rtx gpus, depth anything modules weights kept fixed fine- tuning stereo module additional steps. team weouibaguette weouibaguette team employs foundationstereo finding original network outperforms custom-trained models developed. using booster training set, evaluated various parameter con- figurations observed, similar last years ntire win- ner mimcalgo, inference yielded better overall met- rics down-sampled images. team experimented different down-sampling factors tested inference without hierarchical processing. output resized using linear interpolation. results indicate optimal performance achieved resizing factor hierarchical inference enabled. track mono baseline zoedepth second track, set zoedepth model baseline, state-of-the-art framework single-image depth estimation. builds dpt backbone enhanced metric bins module implemented learning metric depth rather affine-invariant output. stereo track, obtain predicted depth maps using original weights made available authors. team lavreniuk lavreniuk teams deepblend method uses depth anything dav model primary depth estimator, additional modifications improve accuracy chal- lenging scenarios. training, images categorized two groups transparent objects mirrors. cate- gories, pseudo-labeling depth masks created using re- fined blending technique fuses original image figure network architecture team lavreniuk. figure network architecture team colab. transparent object mask, improving upon mirror surfaces, additional restoration step applied, in- painted images generated using fast fourier convolution- based model serve auxiliary input depth estima- tion. experiments various depth estimation models led selecting depth anything depth anything pseudo-labeling. interestingly, averaging out- puts yields better results using dav alone, performs better transparent mirror surfaces. blending inpainting remove transparent mirror surfaces images, effectively complements dav there. final dav model fine-tuned carefully selected subset datasets, including transcg clear- grasp midepth hammer housecatd msd transk booster dur- ing training, extensive data augmentations applied en- hance robustness varying lighting conditions. infer- ence, test-time augmentations flipping color jittering refine final depth predictions. figure network architecture team prerdw. team colab colab team codalab presents depthinpaint, efficient depth estimation optimization framework tom objects combining two-stage tom mask optimization in-paint mechanism. two-stage approach, overcomes physical limitations traditional monoc- ular depth perception significantly improves accu- racy depth estimation tom surfaces. method first generated rough tom surfaces mask based pdnet model combines rgb image depth information mirror segmentation. then, depthinpaint refines rough mask segment anything model sam based image initial mask. following this, physics-guided image inpaint strategy applied masked regions, eliminating artifacts specular high- lights medium refraction. finally, depth pro used generate depth metric inpaint images. key in- novations approach second-stage mask op- timization strategy tom surfaces, mask-guided image-in-paint mechanism. team ipcv ipcv team codalab jameerbabu employs marigold monocular depth estimation model leverages visual knowledge embedded diffusion- based image generators. specifically, marigold built upon architecture stable diffusion latent diffusion model. core component architecture de- noising u-net, operates within latent space model. adapt stable diffusion depth estimation, marigold employs fine-tuning protocol focuses denoising u-net preserving integrity latent space. fine-tuning performed using synthetic rgb-d datasets, hypersim virtual kitti completed within days single gpu. implementation, team directly used inference pretrained weights marigold repository. team prerdw prerdw team codalab jingc presents reflective depth wizard, method takes advantage depth anything inherent generalization capability. team fine-tuned model accurately estimating depth challenging reflective transparent materials, initializ- ing training using pre-trained hypersim model foundation. prepare training data, converted booster depth annotations usable depth maps com- puted scale shift parameters properly align models predictions ground truth depths training. input images resized pix- els processing. inference, team implemented several enhancements color calibration applied gray world algorithm normalize input image color distributions depth refinement processed raw depth predictions bilateral filter smooth surfaces preserving edge details iii ensemble optimization com- bined results across varying lighting conditions applied median filtering reduce noise final depth maps. acknowledgments work partially supported humboldt foun- dation. thank ntire sponsors bytedance, meituan, kuaishou, university wurzburg computer vision lab. ntire organizers title ntire challenge depth images specular transparent surfaces members pierluigi zama ramirez pierluigi.zamaunibo.it, alex costanzino, fabio tosi, matteo poggi, samuele salti, stefano mattoccia, luigi stefano, radu timofte affiliations university bologna, italy computer vision lab, university wurzburg, germany track teams affiliations njust-kmg members zhe zhang zhe.zhangnjust.edu.cn, yang yang yyangnjust.edu.cn affiliations nanjing university science technology, china robot-vrobotit members chen chenwbupt.edu.cn, anlong ming malbupt.edu.cn, mingshuai zhao mingshuai zbupt.edu.cn, mengying yumengyingbupt.edu.cn, shida gao gaostarbupt.edu.cn, xiangfeng wang xi- angfeng wfoxmail.com, feng xue feng.xueunitn.it affiliations beijing university posts telecommunications, china university trento, italy samsung institute china-beijing src-b members jun shi jun.shisamsung.com, yong yang, yong yixiang jin, dingzhe affiliations samsung institute china-beijing src-b weouibaguette members aryan shukla aryan.shukla.ens.etsmtl.ca, liam frija- altarac liam.frija-altarac.ens.etsmtl.ca, matthew toews affiliations ecole technologie superieure ets, montreal, canada track teams affiliations colab members hui geng gengh.com, tianjiao wan, zi- jian gao, qisheng xu, kele xu, zijian zang affiliations national university defense technology, changsha, china fudan university ipcv members jameer babu pinjari jameer.jbgmail.com, kuldeep purohit kuldeeppurohitgmail.com affiliations independent researchers lavreniuk members mykola lavreniuk nick ukr.net affiliations space research institute nasu-ssau, kyiv, ukraine prerdw members jing cao caojingstu.hit.edu.cn, shenyi qq.com, kui jiang jiangkuihit.edu.cn, junjun jiang jiangjunjunhit.edu.cn, yong huang huangyonghit.edu.cn affiliations harbin institute technology, china references stable diffusion model card, filippo aleotti, fabio tosi, pierluigi zama ramirez, mat- teo poggi, samuele salti, stefano mattoccia, luigi stefano. neural disparity refinement arbitrary res- olution stereo. international conference vision, dv. luca bartolomei, fabio tosi, matteo poggi, stefano mattoccia. stereo anywhere robust zero-shot deep stereo matching even either stereo mono fail. pro- ceedings ieeecvf conference computer vision pattern recognition cvpr, shariq farooq bhat, reiner birkl, diana wofk, peter wonka, matthias muller. zoedepth zero-shot trans- fer combining relative metric depth, aleksei bochkovskii, ama delaunoy, hugo germain, marcel santos, yichao zhou, stephan richter, vladlen koltun. depth pro sharp monocular metric depth less second. arxiv preprint yohann cabon, naila murray, martin humenberger. virtual kitti arxiv preprint jia-ren chang yong-sheng chen. pyramid stereo matching network. ieeecvf conference computer vision pattern recognition cvpr, pages weifeng chen, zhao fu, dawei yang, jia deng. single-image depth perception wild. proc. neurips, zheng chen, kai liu, jue gong, jingkai wang, lei sun, zongwei wu, radu timofte, yulun zhang, al. ntire challenge image super-resolution methods results. proceedings ieeecvf conference computer vision pattern recognition cvpr work- shops, zheng chen, jingkai wang, kai liu, jue gong, lei sun, zongwei wu, radu timofte, yulun zhang, al. ntire challenge real-world face restoration methods results. proceedings ieeecvf conference computer vision pattern recognition cvpr work- shops, junda cheng, longliang liu, gangwei xu, xianqi wang, zhaoxing zhang, yong deng, jinliang zang, yurui chen, zhipeng cai, xin yang. monster marry monodepth stereo unleashes power. arxiv preprint xinjing cheng, peng wang, ruigang yang. learn- ing depth convolutional spatial propagation network. ieee transactions pattern analysis machine intelli- gence, xuelian cheng, yiran zhong, mehrtash harandi, yuchao dai, xiaojun chang, hongdong li, tom drummond, zongyuan ge. hierarchical neural architecture search deep stereo matching. advances neural information pro- cessing systems, jaehoon choi, dongki jung, yonghan lee, deokhwa kim, dinesh manocha, donghwan lee. selfdeco self- supervised monocular depth completion challenging in- door environments. ieee international conference robotics automation icra, pages ieee, marcos conde, radu timofte, al. ntire challenge raw image restoration super-resolution. proceed- ings ieeecvf conference computer vision pattern recognition cvpr workshops, marcos conde, radu timofte, al. raw image reconstruc- tion rgb smartphones. ntire challenge re- port. proceedings ieeecvf conference com- puter vision pattern recognition cvpr workshops, alex costanzino, pierluigi zama ramirez, matteo poggi, fabio tosi, stefano mattoccia, luigi stefano. learning depth estimation transparent mirror sur- faces. ieee international conference computer vision, iccv. shivam duggal, shenlong wang, wei-chiu ma, rui hu, raquel urtasun. deeppruner learning efficient stereo matching via differentiable patchmatch. proceedings ieeecvf international conference computer vi- sion, pages david eigen, christian puhrsch, rob fergus. depth map prediction single image using multi-scale deep network. proc. neurips, egor ershov, sergey korchagin, alexei khalin, artyom panshin, arseniy terekhin, ekaterina zaychenkova, georgiy lobarev, vsevolod plokhotnyuk, denis abramov, elisey zhdanov, sofia dorogova, yasin mamedov, nikola banic, georgii perevozchikov, radu timofte, al. ntire challenge night photography rendering. pro- ceedings ieeecvf conference computer vision pattern recognition cvpr workshops, hongjie fang, hao-shu fang, sheng xu, cewu lu. transcg large-scale real-world dataset transparent object depth completion grasping baseline. ieee robotics automation letters, xiao fu, wei yin, hu, kaixuan wang, yuexin ma, ping tan, shaojie shen, dahua lin, xiaoxiao long. ge- owizard unleashing diffusion priors geometry estimation single image. eccv, yuqian fu, xingyu qiu, bin ren yanwei fu, radu timofte, nicu sebe, ming-hsuan yang, luc van gool, al. ntire challenge cross-domain few-shot object detection methods results. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, adrien gaidon, greg shakhnarovich, rares ambrus, vi- tor guizilini, igor vasiljevic, matthew walter, sudeep pil- lai, nick kolkin. dense depth autonomous driving ddad challenge viewmonod-workshop, andreas geiger, martin roser, raquel urtasun. effi- cient large-scale stereo matching. asian conference computer vision, pages springer, clement godard, oisin mac aodha, gabriel bros- tow. unsupervised monocular depth estimation left- right consistency. proc. cvpr, clement godard, oisin mac aodha, michael firman, gabriel brostow. digging self-supervised monocular depth estimation. proc. iccv, juan luis gonzalezbello munchurl kim. forget lidar self-supervised depth estimators med prob- ability volumes. larochelle, ranzato, hadsell, balcan, lin, editors, advances neural in- formation processing systems, volume pages curran associates, inc., weiyu guo, zhaoshuo li, yongkui yang, zheng wang, russell taylor, mathias unberath, alan yuille, ying- wei li. context-enhanced stereo transformer. european conference computer vision, pages springer, xiaoyang guo, hongsheng li, shuai yi, jimmy ren, xiaogang wang. learning monocular depth distilling cross-domain stereo networks. proc. eccv, xiaoyang guo, kai yang, wukui yang, xiaogang wang, hongsheng li. group-wise correlation stereo network. proceedings ieeecvf conference computer vision pattern recognition, pages shuhao han, haotian fan, fangyuan kong, wenjie liao, chunle guo, chongyi li, radu timofte, al. ntire challenge text image generation model qual- ity assessment. proceedings ieeecvf confer- ence computer vision pattern recognition cvpr workshops, jing he, haodong li, wei yin, yixun liang, leheng li, kaiqiang zhou, hongbo zhang, bingbing liu, ying- cong chen. lotus diffusion-based visual foundation model high-quality dense prediction. arxiv preprint wenbo hu, xiangjun gao, xiaoyu li, sijie zhao, xi- aodong cun, yong zhang, long quan, ying shan. depthcrafter generating consistent long depth sequences open-world videos. proceedings ieeecvf conference computer vision pattern recognition cvpr, andrey ignatov, grigory malivenko, david plowman, samarth shukla, radu timofte. fast accurate single-image depth estimation mobile devices, mobile challenge report. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, pages june varun jain, zongwei wu, quan zou, louis florentin, henrik turbell, sandeep siddhartha, radu timofte, al. ntire challenge video quality enhancement video conferencing datasets, methods results. pro- ceedings ieeecvf conference computer vision pattern recognition cvpr workshops, huaizu jiang, gustav larsson, michael maire greg shakhnarovich, erik learned-miller. self- supervised relative depth learning urban scene understanding. proc. eccv, hualie jiang, zhiqiang lou, laiyan ding, rui xu, minglang tan, wenjie jiang, rui huang. defom- stereo depth foundation model based stereo matching. arxiv preprint junpeng jing, jiankun li, pengfei xiong, jiangyu liu, shuaicheng liu, yichen guo, xin deng, mai xu, lai jiang, leonid sigal. uncertainty guided adaptive warping robust efficient stereo matching. proceedings ieeecvf international conference computer vision iccv, pages october adrian johnston gustavo carneiro. self-supervised monocular trained depth estimation using self-attention discrete disparity volume. proc. cvpr, hyunjun jung, patrick ruhkamp, guangyao zhai, nikolas brasch, yitong li, yannick verdie, jifei song, yiren zhou, anil armagan, slobodan ilic, al. importance accurate geometry data dense vision tasks. pro- ceedings ieeecvf conference computer vision pattern recognition, pages hyunjun jung, shun-cheng wu, patrick ruhkamp, guangyao zhai, hannah schieber, giulia rizzoli, pengyuan wang, hongcheng zhao, lorenzo garattoni, sven meier, al. housecatd-a large-scale multi-modal category level object perception dataset household objects realistic scenarios. proceedings ieeecvf conference computer vision pattern recognition, pages bingxin ke, dominik narnhofer, shengyu huang, lei ke, torben peters, katerina fragkiadaki, anton obukhov, konrad schindler. video depth without video models. proceedings ieeecvf conference computer vi- sion pattern recognition cvpr, bingxin ke, anton obukhov, shengyu huang, nando met- zger, rodrigo caye daudt, konrad schindler. re- purposing diffusion-based image generators monocular depth estimation. proceedings ieeecvf confer- ence computer vision pattern recognition cvpr, alex kendall, hayk martirosyan, saumitro dasgupta, pe- ter henry, ryan kennedy, abraham bachrach, adam bry. end-to-end learning geometry context deep stereo regression. ieee international conference computer vision iccv, oct sameh khamis, sean fanello, christoph rhemann, adarsh kowdle, julien valentin, shahram izadi. stereonet guided hierarchical refinement real-time edge-aware depth prediction. proceedings european confer- ence computer vision eccv, pages alexander kirillov, eric mintun, nikhila ravi, hanzi mao, chloe rolland, laura gustafson, tete xiao, spencer whitehead, alexander berg, wan-yen lo, al. segment anything. proceedings ieeecvf international conference computer vision, pages eric kolve, roozbeh mottaghi, winson han, eli vander- bilt, luca weihs, alvaro herrasti, matt deitke, kiana ehsani, daniel gordon, yuke zhu, al. ai-thor interactive environment visual ai. arxiv preprint henrik kretzschmar, alex liniger, jose alvarez, yan wang, vincent casser, fisher yu, marco pavone, li, andreas geiger, peter ondruska, erran li, dragomir angelov, john leonard, luc van gool. argov- erse stereo competition vision, iro laina, christian rupprecht, vasileios belagiannis, fed- erico tombari, nassir navab. deeper depth prediction fully convolutional residual networks. fourth international conference vision dv, pages ieee, sangmin lee, eunpil park, angel canelo, hyunhee park, youngjo kim, hyungju chun, xin jin, chongyi li, chun- guo, radu timofte, al. ntire challenge efficient burst hdr restoration datasets, methods, results. proceedings ieeecvf conference computer vision pattern recognition cvpr work- shops, jiankun li, peisen wang, pengfei xiong, tao cai, ziwei yan, lei yang, jiangyu liu, haoqiang fan, shuaicheng liu. practical stereo matching via cascaded recurrent net- work adaptive correlation. proceedings ieeecvf conference computer vision pattern recognition, pages xin li, yeying jin, xin jin, zongwei wu, bingchen li, yufei wang, wenhan yang, li, zhibo chen, bihan wen, robby tan, radu timofte, al. ntire challenge day night raindrop removal dual-focused images methods results. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, xin li, xijun wang, bingchen li, kun yuan, yizhen shao, suhang yao, ming sun, chao zhou, radu timofte, zhibo chen. ntire challenge short-form ugc video quality assessment enhancement kwaisr dataset study. proceedings ieeecvf conference computer vision pattern recognition cvpr work- shops, xin li, kun yuan, bingchen li, fengbin guan, yizhen shao, zihao yu, xijun wang, yiting lu, wei luo, suhang yao, ming sun, chao zhou, zhibo chen, radu timofte, al. ntire challenge short-form ugc video qual- ity assessment enhancement methods results. proceedings ieeecvf conference computer vi- sion pattern recognition cvpr workshops, zhenyu li, shariq farooq bhat, peter wonka. patch- fusion end-to-end tile-based framework high- resolution monocular metric depth estimation. proceed- ings ieeecvf conference computer vision pattern recognition cvpr, zhaoshuo li, xingtong liu, nathan drenkow, andy ding, francis creighton, russell taylor, mathias un- berath. revisiting stereo depth estimation sequence- to-sequence perspective transformers. proceedings ieeecvf international conference computer vision, pages jie liang, radu timofte, qiaosi yi, zhengqiang zhang, shuaizheng liu, lingchen sun, rongyuan wu, xindong zhang, hui zeng, lei zhang, al. ntire restore image model raim wild challenge. proceedings ieeecvf conference computer vi- sion pattern recognition cvpr workshops, yuan liang, zitian zhang, chuhua xian, shengfeng he. delving multi-illumination monocular depth esti- mation new dataset method. ieee transactions multimedia, zhengfa liang, yiliu feng, yulan guo, hengzhu liu, wei chen, linbo qiao, zhou, jianfeng zhang. learning disparity estimation feature constancy. pro- ceedings ieee conference computer vision pattern recognition cvpr, june lahav lipson, zachary teed, jia deng. raft-stereo multilevel recurrent field transforms stereo match- ing. arxiv preprint xiaohong liu, xiongkuo min, qiang hu, xiaoyun zhang, jie guo, al. ntire xgc quality assessment challenge methods results. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, xiaoning liu, zongwei wu, florin-alexandru vasluianu, hailong yan, bin ren, yulun zhang, shuhang gu, zhang, zhu, radu timofte, al. ntire challenge low light image enhancement methods results. proceedings ieeecvf conference computer vi- sion pattern recognition cvpr workshops, jieming lou, weide liu, zhuo chen, fayao liu, jun cheng. elfnet evidential local-global fusion stereo matching. proceedings ieeecvf inter- national conference computer vision, pages nikolaus mayer, eddy ilg, philip hausser, philipp fischer, daniel cremers, alexey dosovitskiy, thomas brox. large dataset train convolutional networks dispar- ity, optical flow, scene flow estimation. ieee conference computer vision pattern recognition cvpr, june haiyang mei, dong, wen dong, pieter peers, xin yang, qiang zhang, xiaopeng wei. depth-aware mirror seg- mentation. proceedings ieeecvf conference computer vision pattern recognition, pages moritz menze andreas geiger. object scene flow autonomous vehicles. conference computer vision pattern recognition cvpr, mahdi miangoleh, sebastian dille, long mai, sylvain paris, yagiz aksoy. boosting monocular depth estima- tion models high-resolution via content-adaptive multi- resolution merging. proceedings ieeecvf con- ference computer vision pattern recognition, pages anton obukhov, matteo poggi, fabio tosi, ripu- daman singh arora, jaime spencer, chris russell, si- mon hadfield, richard bowden, shuaihang wang, zhenxin ma, weijie chen, baobei xu, fengyu sun, xie, jiang zhu, mykola lavreniuk, haining guan, qun wu, yupei zeng, chao lu, huanran wang, guangyuan zhou, hao- tian zhang, jianxiong wang, qiang rao, chunjie wang, xiao liu, zhiqiang lou, hualie jiang, yihao chen, rui xu, minglang tan, zihan qin, yifan mao, jiayang liu, jialei xu, yifan yang, wenbo zhao, junjun jiang, xi- anming liu, mingshuai zhao, anlong ming, chen, feng xue, mengying yu, shida gao, xiangfeng wang, gbenga omotara, ramy farag, jacket dembys, seyed mo- hamad ali tousi, guilherme desouza, tuan-anh yang, minh-quang nguyen, thien-phuc tran, albert luginov, muhammad shahzad. fourth monocular depth es- timation challenge. proceedings ieeecvf con- ference computer vision pattern recognition work- shops, jiahao pang, wenxiu sun, jimmy sj. ren, chengxi yang, qiong yan. cascade residual learning two-stage convolutional neural network stereo matching. ieee international conference computer vision iccv, oct matteo poggi, filippo aleotti, fabio tosi, stefano mat- toccia. uncertainty self-supervised monocular depth estimation. proc. cvpr, matteo poggi, seungryong kim, fabio tosi, sunok kim, filippo aleotti, dongbo min, kwanghoon sohn, ste- fano mattoccia. confidence stereo matching deep-learning era quantitative evaluation. ieee transac- tions pattern analysis machine intelligence, matteo poggi fabio tosi. federated online adaptation deep stereo. cvpr, matteo poggi, fabio tosi, konstantinos batsos, philippos mordohai, stefano mattoccia. synergies be- tween machine learning binocular stereo depth esti- mation images survey. ieee transactions pat- tern analysis machine intelligence, michael ramamonjisoa, yuming du, vincent lep- etit. predicting sharp accurate occlusion boundaries monocular depth estimation using displacement fields. proc. cvpr, pierluigi zama ramirez, fabio tosi, luigi stefano, radu timofte, alex costanzino, matteo poggi, samuele salti, stefano mattoccia, jun shi, dafeng zhang, al. ntire challenge depth images specular transparent surfaces. proceedings ieeecvf conference computer vision pattern recognition, pages pierluigi zama ramirez, fabio tosi, luigi ste- fano, radu timofte, alex costanzino, matteo poggi, samuele salti, stefano mattoccia, yangyang zhang, cailin wu, zhuangda he, shuangshuang yin, jiaxu dong, yangchenxu liu, hao jiang, jun shi, yong yixiang jin, dingzhe li, bingxin ke, anton obukhov, tinafu wang, nando metzger, shengyu huang, konrad schindler, yachuan huang, jiaqi li, junrui zhang, yiran wang, zihao huang, tianqi liu, zhiguo cao, pengzhi li, jui-lin wang, wenjie zhu, hui geng, yuxin zhang, long lan, kele xu, tao sun, qisheng xu, sourav saini, aashray gupta, sa- haj mistry, aryan shukla, vinit jakhetiya, sunil jaiswal, yuejin sun, zhuofan zheng, ning, jen-hao cheng, hou-i liu, hsiang-wei huang, cheng-yen yang, zhongyu jiang, yi-hao peng, aishi huang, jenq-neng hwang. ntire challenge depth images specular transparent surfaces. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, pages june rene ranftl, alexey bochkovskiy, vladlen koltun. vi- sion transformers dense prediction. iccv, rene ranftl, alexey bochkovskiy, vladlen koltun. vi- sion transformers dense prediction. proceedings ieeecvf international conference computer vi- sion iccv, pages october rene ranftl, katrin lasinger, david hafner, konrad schindler, vladlen koltun. towards robust monocu- lar depth estimation mixing datasets zero-shot cross- dataset transfer. ieee transactions pattern analysis machine intelligence, bin ren, hang guo, lei sun, zongwei wu, radu tim- ofte, yawei li, al. tenth ntire efficient super-resolution challenge report. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, mike roberts, jason ramapuram, anurag ranjan, at- ulit kumar, miguel angel bautista, nathan paczan, russ webb, joshua susskind. hypersim photorealistic synthetic dataset holistic indoor scene understanding. proceedings ieeecvf international conference computer vision, pages nickolay safonov, alexey bryntsev, andrey moskalenko, dmitry kulikov, dmitriy vatolin, radu timofte, al. ntire challenge ugc video enhancement meth- ods results. proceedings ieeecvf confer- ence computer vision pattern recognition cvpr workshops, tonmoy saikia, yassine marrakchi, arber zela, frank hut- ter, thomas brox. autodispnet improving disparity estimation automl. proceedings ieeecvf international conference computer vision, pages shreeyak sajjan, matthew moore, mike pan, ganesh na- garaja, johnny lee, andy zeng, shuran song. clear grasp shape estimation transparent objects ma- nipulation. ieee international conference robotics automation icra, pages ieee, daniel scharstein, heiko hirschmuller, york kitajima, greg krathwohl, nera nesic, wang, porter west- ling. high-resolution stereo datasets subpixel-accurate ground truth. german conference pattern recognition, pages springer, thomas schops, johannes schonberger, silvano galliani, torsten sattler, konrad schindler, marc pollefeys, an- dreas geiger. multi-view stereo benchmark high- resolution images multi-camera videos. ieee con- ference computer vision pattern recognition, pages ieee, jiahao shao, yuanbo yang, hongyu zhou, youmin zhang, yujun shen, vitor guizilini, yue wang, matteo poggi, yiyi liao. learning temporally consistent video depth video diffusion priors. proceedings ieeecvf conference computer vision pattern recognition cvpr, zhelun shen, yuchao dai, zhibo rao. cfnet cas- cade fused cost volume robust stereo matching. proceedings ieeecvf conference computer vi- sion pattern recognition cvpr, pages june jun shi, yixiang jin, dingzhe li, haoyu niu, zhezhu jin, wang, al. asgrasp generalizable transparent object reconstruction grasping rgb-d active stereo cam- era. arxiv preprint xiao song, zhao, hanwen hu, liangji fang. edgestereo context integrated residual pyramid network stereo matching. accv, jaime spencer, stella qian, chris russell, simon had- field, erich graf, wendy adams, andrew schofield, james elder, richard bowden, heng cong, stefano mattoccia, matteo poggi, zeeshan khan suri, yang tang, fabio tosi, hao wang, youmin zhang, yusheng zhang, chaoqiang zhao. monocular depth estimation challenge. proceedings ieeecvf winter con- ference applications computer vision wacv work- shops, pages january jaime spencer, stella qian, michaela trescakova, chris russell, simon hadfield, erich graf, wendy adams, an- drew schofield, james elder, richard bowden, ali an- war, hao chen, xiaozhi chen, kai cheng, yuchao dai, huynh thai hoa, sadat hossain, jianmian huang, mo- han jing, li, chao li, baojun li, zhiwen liu, ste- fano mattoccia, siegfried mercelis, myungwoo nam, mat- teo poggi, xiaohua qi, jiahui ren, yang tang, fabio tosi, linh trinh, nadim uddin, khan muhammad umair, kaixuan wang, yufei wang, yixing wang, mochu xiang, guangkai xu, wei yin, jun yu, zhang, chaoqiang zhao. second monocular depth estimation challenge. proceedings ieeecvf conference computer vision pattern recognition workshops, jaime spencer, fabio tosi, matteo poggi, ripu- daman singh arora, chris russell, simon hadfield, richard bowden, guangyuan zhou, zhengxin li, qiang rao, yiping bao, xiao liu, dohyeong kim, jinseong kim, myunghyun kim, mykola lavreniuk, rui li, qing mao, jiang wu, zhu, jinqiu sun, yanning zhang, suraj patni, aradhye agarwal, chetan arora, pihai sun, kui jiang, gang wu, jian liu, xianming liu, junjun jiang, xidan zhang, jianing wei, fangjun wang, zhiming tan, jiabao wang, albert luginov, muhammad shahzad, seyed hosseini, aleksander trajcevski, james elder. third monocular depth estimation challenge. proceedings ieeecvf conference computer vision pattern recognition workshops, lei sun, andrea alfarano, peiqi duan, shaolin su, kaiwei wang, boxin shi, radu timofte, danda pani paudel, luc van gool, al. ntire challenge event-based image deblurring methods results. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, lei sun, hang guo, bin ren, luc van gool, radu timo- fte, yawei li, al. tenth ntire image denoising challenge report. proceedings ieeecvf confer- ence computer vision pattern recognition cvpr workshops, vladimir tankovich, christian hane, yinda zhang, adarsh kowdle, sean fanello, sofien bouaziz. hitnet hierar- chical iterative tile refinement network real-time stereo matching. proceedings ieeecvf conference computer vision pattern recognition cvpr, pages june zachary teed jia deng. raft recurrent all-pairs field transforms optical flow. european conference computer vision, pages springer, alessio tonioni, fabio tosi, matteo poggi, stefano mat- toccia, luigi stefano. real-time self-adaptive deep stereo. proceedings ieeecvf conference computer vision pattern recognition, pages fabio tosi, filippo aleotti, matteo poggi, stefano mat- toccia. learning monocular depth estimation infusing tradi- tional stereo knowledge. proceedings ieeecvf conference computer vision pattern recognition cvpr, june fabio tosi, filippo aleotti, pierluigi zama ramirez, mat- teo poggi, samuele salti, luigi stefano, stefano mattoccia. distilled semantics comprehensive scene un- derstanding videos. proceedings ieeecvf conference computer vision pattern recognition cvpr, june fabio tosi, filippo aleotti, pierluigi zama ramirez, mat- teo poggi, samuele salti, stefano mattoccia, luigi stefano. neural disparity refinement ieee transactions pattern analysis machine intelligence, fabio tosi, luca bartolomei, matteo poggi. sur- vey deep stereo matching twenties. international journal computer vision, pages fabio tosi, yiyi liao, carolin schmitt, andreas geiger. smd-nets stereo mixture density networks. confer- ence computer vision pattern recognition cvpr, fabio tosi, alessio tonioni, daniele gregorio, mat- teo poggi. nerf-supervised deep stereo. conference computer vision pattern recognition cvpr, pages june fabio tosi, pierluigi zama ramirez, matteo poggi. diffusion models monocular depth estimation over- coming challenging conditions. european conference computer vision eccv, florin-alexandru vasluianu, tim seizinger, zhuyun zhou, cailian chen, zongwei wu, radu timofte, al. ntire image shadow removal challenge report. proceed- ings ieeecvf conference computer vision pattern recognition cvpr workshops, florin-alexandru vasluianu, tim seizinger, zhuyun zhou, zongwei wu, radu timofte, al. ntire ambi- ent lighting normalization challenge. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, lijun wang, jianming zhang, yifan wang, huchuan lu, xiang ruan. cliffnet monocular depth estimation hierarchical embedding loss. proc. eccv, yan wang, zihang lai, gao huang, brian wang, lau- rens van der maaten, mark campbell, kilian wein- berger. anytime stereo image depth estimation mobile devices. international conference robotics automation icra, pages yingqian wang, zhengyu liang, fengyuan zhang, lvli tian, longguang wang, juncheng li, jungang yang, radu timofte, yulan guo, al. ntire challenge light field image super-resolution methods results. pro- ceedings ieeecvf conference computer vision pattern recognition cvpr workshops, jamie watson, michael firman, gabriel brostow, daniyar turmukhambetov. self-supervised monocular depth hints. proc. iccv, bowen wen, matthew trepte, joseph aribido, jan kautz, orazio gallo, stan birchfield. foundationstereo zero- shot stereo matching. arxiv, enze xie, wenjia wang, wenhai wang, mingyu ding, chunhua shen, ping luo. segmenting transparent ob- jects wild. computer visioneccv european conference, glasgow, uk, august proceedings, part xiii pages springer, gangwei xu, xianqi wang, xiaohuan ding, xin yang. iterative geometry encoding volume stereo matching. proceedings ieeecvf conference computer vision pattern recognition, pages gangwei xu, xianqi wang, xiaohuan ding, xin yang. iterative geometry encoding volume stereo matching. proceedings ieeecvf conference computer vision pattern recognition, pages haofei xu, jing zhang, jianfei cai, hamid rezatofighi, fisher yu, dacheng tao, andreas geiger. unifying flow, stereo depth estimation. ieee transactions pattern analysis machine intelligence, gengshan yang, joshua manela, michael happold, deva ramanan. hierarchical deep stereo matching high- resolution images. proceedings ieeecvf con- ference computer vision pattern recognition, pages guorun yang, hengshuang zhao, jianping shi, zhidong deng, jiaya jia. segstereo exploiting semantic infor- mation disparity estimation. eccv, pages kangning yang, jie cai, ling ouyang, florin-alexandru vasluianu, radu timofte, jiaming ding, huiming sun, lan fu, jinlong li, chiu man ho, zibo meng, al. ntire challenge single image reflection removal wild datasets, methods results. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, lihe yang, bingyi kang, zilong huang, xiaogang xu, ji- ashi feng, hengshuang zhao. depth anything un- leashing power large-scale unlabeled data. cvpr, lihe yang, bingyi kang, zilong huang, zhen zhao, xiao- gang xu, jiashi feng, hengshuang zhao. depth any- thing advances neural information processing sys- tems, xin yang, haiyang mei, xu, xiaopeng wei, baocai yin, rynson lau. mirror? proceedings ieeecvf international conference computer vision, pages wei yin, jianming zhang, oliver wang, simon niklaus, long mai, simon chen, chunhua shen. learning recover scene shape single image. proceed- ings ieeecvf conference computer vision pattern recognition, pages zhichao yin, trevor darrell, fisher yu. hierarchi- cal discrete distribution decomposition match density estimation. proceedings ieeecvf conference computer vision pattern recognition, pages pierluigi zama ramirez, alex costanzino, fabio tosi, mat- teo poggi, luigi stefano, jean-baptiste weibel, do- minik bauer, doris antensteiner, markus vincze, jiaqi li, yachuan huang, junrui zhang, yiran wang, jinghong zheng, liao shen, zhiguo cao, ziyang song, zerong wang, ruijie zhu, hao zhang, rui li, jiang wu, xian li, zhu, jinqiu sun, yanning zhang, pihai sun, yuanqi yao, wenbo zhao, kui jiang, junjun jiang, mykola lavre- niuk, jui-lin wang. tricky challenge monocu- lar depth images specular transparent surfaces. european conference computer vision workshops, eccvw. pierluigi zama ramirez, alex costanzino, fabio tosi, mat- teo poggi, samuele salti, luigi stefano, stefano mattoccia. booster benchmark depth im- ages specular transparent surfaces. arxiv preprint pierluigi zama ramirez, matteo poggi, fabio tosi, ste- fano mattoccia, luigi stefano. geometry meets semantics semi-supervised monocular depth estima- tion. computer visionaccv asian confer- ence computer vision, perth, australia, december revised selected papers, part iii pages springer, pierluigi zama ramirez, fabio tosi, luigi stefano, radu timofte, alex costanzino, matteo poggi, samuele salti, stefano mattoccia, al. ntire challenge depth images specular transparent surfaces. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, pierluigi zama ramirez, fabio tosi, matteo poggi, samuele salti, stefano mattoccia, luigi stefano. open challenges deep stereo booster dataset. proceedings ieeecvf conference computer vi- sion pattern recognition cvpr, pages june jure zbontar, yann lecun, al. stereo matching training convolutional neural network compare image patches. mach. learn. res., oliver zendel, angela dai, xavier puig fernandez, andreas geiger, vladen koltun, peter kontschieder, adam kortylewski, tsung-yi lin, torsten sattler, daniel scharstein, hendrik schilling, jonas uhrig, jonas wulff. robust vision challenge robustvision.net, jiaxi zeng, chengtang yao, lidong yu, yuwei wu, yunde jia. parameterized cost volume stereo matching. proceedings ieeecvf international conference computer vision iccv, pages october feihu zhang, victor prisacariu, ruigang yang, philip torr. ga-net guided aggregation net end- to-end stereo matching. ieeecvf conference com- puter vision pattern recognition cvpr, chaoqiang zhao, matteo poggi, fabio tosi, lei zhou, qiyu sun, yang tang, stefano mattoccia. gasmono geometry-aided self-supervised monocular depth estima- tion indoor scenes. proceedings ieeecvf in- ternational conference computer vision, pages chao zhou, hong zhang, xiaoyong shen, jiaya jia. unsupervised learning stereo matching. ieee in- ternational conference computer vision iccv. ieee, october", "published_date": "2025-06-06T07:27:15+00:00"}
{"id": "2506.05782v1", "title": "GazeNLQ @ Ego4D Natural Language Queries Challenge 2025", "authors": ["Wei-Cheng Lin", "Chih-Ming Lien", "Chen Lo", "Chia-Hung Yeh"], "summary": "report presents solution egod natural language queries nlq challenge cvpr egocentric video captures scene wearers perspective, gaze serves key non-verbal communication cue reflects visual attention offer insights human intention cognition. motivated this, propose novel approach, gazenlq, leverages gaze retrieve video segments match given natural language queries. specifically, introduce contrastive learning-based pretraining strategy gaze estimation directly video. estimated gaze used augment video representations within proposed model, thereby enhancing localization accuracy. experimental results show gazenlq achieves riou. riou. scores respectively. code available", "full_text": "cs.cv jun gazenlq egod natural language queries challenge wei-cheng lin, chih-ming lien, chen lo, chia-hung yeh, national taiwan normal university national sun yat-sen university linwc, lien, chyeh, clontnu.edu.tw abstract report presents solution egod natural lan- guage queries nlq challenge cvpr egocen- tric video captures scene wearers perspective, gaze serves key non-verbal communication cue reflects visual attention offer insights human intention cognition. motivated this, propose novel approach, gazenlq, leverages gaze retrieve video segments match given natural language queries. specifically, introduce contrastive learning-based pre- training strategy gaze estimation directly video. estimated gaze used augment video representa- tions within proposed model, thereby enhancing localiza- tion accuracy. experimental results show gazenlq achieves riou. riou. scores respectively. code available github.comstevenlingazenlq. introduction goal egod natural language queries nlq challenge temporally localize segment egocentric video corresponds given natural lan- guage query. existing approaches generally fall two categories pretraining foundation model learn transfer- able representations suitable various downstream tasks developing specialized grounding model tailored nlq task. pretraining foundation models large-scale dataset yielded impressive results numerous downstream tasks. instance, internvideo explores three types feature extractors backbone fine-tunes egod training set. egovlp constructs large-scale egocentric training dataset adapts video-text contrastive learning explore representations. egovideo enhances training data quality filtering selecting samples multiple existing datasets, leveraging video-text contrastive learn- ing model training. alternatively, task-specific models equal contributions. groundnlq adopt two-stage pretraining strat- egy framework introduces multi-modal multi-scale grounding module enables early fusion video text features. objectnlq enhances video representation incorporating object-level information extracted object detection model. despite advancements, methods focus vi- sual textual modalities, limited exploration aux- iliary sensor data head motion gaze signals egocentric video understanding. recently, egodistill demonstrated utility head motion signals captured inertial measurement unit imu head-mounted camera facilitate efficient egocentric video understand- ing. given imu data shown improve classification accuracy egocentric action recognition, raises question whether characteristics egocen- tric video similarly benefit egocentric video-language grounding. egocentric video, gaze aligns closely camera wearers field view, serving natural informative cue providing valuable information visual attention, cognitive process, underlying inten- tions. understanding gaze behavior essential many applications, including cognitive science psychology, human-robot interaction, virtual augmented reality. recognizing central role gaze revealing attention intention egocentric contexts, aim leverage cue advance understanding egocentric video analysis. therefore, propose gazenlq, novel framework incorporates gaze enhance natural language grounding egocentric videos. use contrastive learning strategy train gaze estimator, predicts gaze directly video. estimated gaze used augment video features, leading promising results nlq task. method section presents gazenlq detailing multi-modal feature representation proposed model architecture. multi-modal feature representation text video representation. following groundnlq extract textual token representations using figure proposed training framework gaze estimator using contrastive learning. figure proposed model video temporal grounding. clip text encoder construct video representation concatenating features intervideo egovlp gaze representation. since gaze annotations available video nlq dataset, train gaze estimator using annotated data. gaze estima- tor directly estimates gaze video. approach utilizes dual-branch structure contrastive learning training, illustrated fig. architecture in- cludes video encoder, gated linear unit glu layers, self-attention layer, video projection head. video features first extracted omnivore video encoder, processed glu attention layer projected aligned gaze embed- ding space. gaze branch, follow preprocess- ing procedure provided generate gaze map frame raw gaze data. gaze maps processed convolution block gaze pro- jection head produce corresponding gaze embeddings. align video embeddings gaze embeddings, employ contrastive loss lnce log expvi expvi gj, video embedding, positive gaze em- bedding, negative samples temperature hy- perparameter. additionally, regression module predicts gaze map gpred, compared ground truth ggt using divergence loss lkl dklggt gpred, dkl denote divergence ggt gpred. total loss lgaze defined lgaze lnce lkl. model architecture overall architecture proposed method illus- trated fig. framework extracts gaze, video, textual embeddings using gaze estimator, video encoder, text encoder, respectively. two cross-attention mod- ules employed align integrate gaze text embeddings video embeddings. resulting figure visualization gaze estimation. top row shows ground-truth gaze heatmaps, bottom row shows predicted heatmaps. embeddings combined via element-wise addition refined using self-attention. next, leverages multi-scale transformer encoder architecture introduced enhance modeling hierarchical temporal dependencies. final predictions produced clas- sification head, scores interval feature pyramid, regression head, estimate bound- ary distances interval, similar approach de- scribed model training employs binary classifi- cation loss lcls intersection union iou regres- sion loss lreg. total loss video temporal grounding llocalization defined llocalization lcls lreg. experiment implementation details gaze estimator training. begin pretraining gaze estimator using features extracted pretrained omnivore model omnivore processes video segments window size frames stride frames, yielding single feature vector per temporal window. align ground-truth supervision temporal resolution, average corresponding gaze heatmaps -frame segment resolution illustrated fig. contrastive learning, video gaze representations projected embedding size aligns dimensionality subsequent finetuning stage. gaze estimator trained using learning rate batch size grounding model finetuning. adopt groundnlq architecture initialize pretrained weights model trained narration data establish strong starting point. following pretraining gaze estimator, incorporate groundnlq pipeline end-to-end finetuning. additionally, investigate model variant called gazenlqthat employs negative gaze embedding, directing models attention regions outside gaze area. phase, freeze gaze table performance comparison nlq test split. method test private groundnlq groundnlq objectnlq groundvqa egovideo egovideo gazenlq gazenlq gazenlq ensemble results table performance comparison nlq val split. method validation groundnlq groundvqa egovideo gazenlq gazenlq estimators weights train combined model ten epochs, incorporating warm-up period four epochs. finetuning process, utilize learning rate batch size experiments conducted using single nvidia rtx gpu. inference, apply soft-nms merge overlapping moment predictions, optimizing final localization outputs. ensemble. combines predictions groundvqa followed strategy egovideo ground- vqa incorporates question-answering data video grounding task using large language model. performance comparison tab. reports comparison results nlq test split. ensemble approach achieves score score demonstrating competitive performance. notably, variant incorporating negative gaze embeddings slightly outperforms standard posi- tive gaze formulation. interesting finding plan explore future work understand implications potential enhancing grounding perfor- mance. tab. presents results nlq val split without en- sembling. method improves score compared groundnlq, results slight decrease score. indicates approach table ablation study whether freeze weights gaze estimator nlq val split. weights validation unfreeze freeze effective retrieving relevant segments within relaxed temporal threshold less accurate stricter align- ment constraints. ablation study conducted ablation study evaluate whether freez- ing weights pretrained gaze estimation module af- fects grounding performance tab. interestingly, freez- ing gaze models weights results better performance compared finetuning. since gaze estimator trained relatively small dataset may generalize well finetuned jointly grounding model, trained large-scale narration dataset. case analysis fig. shows successful examples nlq, model accurately locates target text description. however, failure examples presented fig. top figure, error arises imprecise tempo- ral boundarygazenlq captures first half ground truth event chop vegetables, indicating diffi- culty handling long-duration actions. bottom fig- ure, model fails due misunderstanding object involved activity. however, believe ground truth annotation may accurately reflect subject, action placing bulb performed someone camera wearer. discussion study represents early stage research ego- centric video grounding gaze, remains room future improvement. first, exists feature discrep- ancy gaze training stage video ground- ing stage due use different video feature extrac- tors. video features gaze estimator om- nivore grounding stage employs video features. mismatch may hinder seamless transfer learned representations, potentially impacting grounding performance. second, gaze information serves strong spatial prior gaze estimation phase, capturing precise loca- tions visual attention. however, grounding stage, video features lack explicit spatial information. non-spatial feature structure limits ability directly successful cases failure cases figure four examples gazenlq nlq val split two suc- cessful cases two failure cases leverage spatial cues provided gaze tokens, neces- sitating additional processing fusion strategies align gaze video features, may introduce inefficien- cies loss spatial detail. third, approach relies finetuning pretrained groundnlq model rather training scratch us- ing narration data. finetuning strategy may constrain models ability fully adapt nuances dataset, particularly integrating gaze information text queries. training scratch narration data could potentially yield robust model pursued due resource time constraints stage. conclusion report presents gazenlq, proposed method egod natural language queries challenge cvpr gazenlq employs contrastive learning-based pretraining strategy gaze estimation, core component overall framework. incorporation estimated gaze video representation enhances models ability localize relevant content response natural language queries, demonstrated experimental results. improvements highlight promise leveraging gaze advance egocentric video understanding. future work focus developing consistent feature extractors across stages, incorporating spatial information ground- ing features, exploring training scratch enhance model adaptability. references navaneeth bodla, bharat singh, rama chellappa, larry davis. soft-nms improving object detection one line code. ieee international conference computer vision iccv, pages guo chen, sen xing, zhe chen, wang, kunchang li, yizhuo li, liu, jiahao wang, yin-dong zheng, bingkun huang, al. internvideo-egod pack champion solu- tions egod challenges. arxiv preprint., shangzhe weidi xie. grounded question-answering long egocentric videos. cvpr, pages yisen feng, haoyu zhang, yuquan xie, zaijing li, meng liu, liqiang nie. objectnlq egod episodic memory challenge arxiv preprint rohit girdhar, mannat singh, nikhila ravi, laurens van der maaten, armand joulin, ishan misra. omnivore sin- gle model many visual modalities. cvpr, kristen grauman, andrew westbury, eugene byrne, zachary chavis, antonino furnari, rohit girdhar, jackson hamburger, hao jiang, miao liu, xingyu liu, al. egod around world hours egocentric video. cvpr, pages zhijian hou, lei ji, difei gao, wanjun zhong, kun yan, chao li, wing-kwong chan, chong-wah ngo, nan duan, mike zheng shou. groundnlq egod natural language queries challenge arxiv preprint bolin lai, miao liu, fiona ryan, james rehg. eye transformer globallocal correlation egocentric gaze estimation beyond. ijcv, pages kevin qinghong lin, jinpeng wang, mattia soldan, michael wray, rui yan, eric xu, difei gao, rong-cheng tu, wen- zhe zhao, weijie kong, al. egocentric video-language pretraining. neurips, pages naiyuan liu, xiaohan wang, xiaobo li, yang, yuet- ing zhuang. reler zju-alibaba submission egod natural language queries challenge arxiv preprint baoqi pei, guo chen, jilan xu, yuping he, yicheng liu, kanghua pan, yifei huang, yali wang, tong lu, limin wang, al. egovideo exploring egocentric founda- tion model downstream adaptation. arxiv preprint shraman pramanick, yale song, sayan nag, kevin qinghong lin, hardik shah, mike zheng shou, rama chellappa, pengchuan zhang. egovlpv egocentric video-language pre-training fusion backbone. iccv, pages alec radford, jong wook kim, chris hallacy, aditya ramesh, gabriel goh, sandhini agarwal, girish sastry, amanda askell, pamela mishkin, jack clark, gretchen krueger, ilya sutskever. learning transferable visual models natural language supervision. proceedings international conference machine learning, pages santhosh ramakrishnan, ziad al-halah, kristen grauman. naq leveraging narrations queries su- pervise episodic memory. computer vision pat- tern recognition cvpr, ieee conference on. ieee, shuhan tan, tushar nagarajan, kristen grauman. egodistill egocentric head motion distillation efficient video understanding. neurips,", "published_date": "2025-06-06T06:21:57+00:00"}
{"id": "2506.04115v1", "title": "Multi-view Surface Reconstruction Using Normal and Reflectance Cues", "authors": ["Robin Bruneau", "Baptiste Brument", "Yvain Qu\u00e9au", "Jean M\u00e9lou", "Fran\u00e7ois Bernard Lauze", "Jean-Denis Durou", "Lilian Calvet"], "summary": "achieving high-fidelity surface reconstruction preserving fine details remains challenging, especially presence materials complex reflectance properties without dense-view setup. paper, introduce versatile framework incorporates multi-view normal optionally reflectance maps radiance-based surface reconstruction. approach employs pixel-wise joint re-parametrization reflectance surface normals, representing vector radiances simulated, varying illumination. formulation enables seamless incorporation standard surface reconstruction pipelines, traditional multi-view stereo mvs frameworks modern neural volume rendering nvr ones. combined latter, approach achieves state-of-the-art performance multi-view photometric stereo mvps benchmark datasets, including diligent-mv, luces-mv skoltechd. particular, method excels reconstructing fine-grained details handling challenging visibility conditions. present paper extended version earlier conference paper brument al. proceedings ieeecvf conference computer vision pattern recognition cvpr, featuring accelerated robust algorithm well broader empirical evaluation. code data relative article available", "full_text": "cs.cv jun multi-view surface reconstruction using normal reflectance cues robin bruneau, baptiste brument, yvain queau, jean melou, francois bernard lauze, jean-denis durou, lilian calvet dqbm, university zurich, switzerland. irit, umr cnrs universite toulouse, france. greyc, cnrs, unicaen, ensicaen, normandie universite, france. fittingbox, toulouse, france. diku, university copenhagen, denmark. rocs, university zurich, or-x, balgrist university hospital, switzerland. corresponding authors. e-mails robin.bruneauuzh.ch baptiste.brumentirit.fr contributing authors yvain.queauensicaen.fr jean.melouirit.fr francoisdi.ku.dk jean-denis.durouirit.fr lilian.calvetbalgrist.ch authors contributed equally work. abstract achieving high-fidelity surface reconstruction preserving fine details remains challenging, especially presence materials complex reflectance properties without dense-view setup. paper, introduce versatile framework incorporates multi-view normal optionally reflectance maps radiance-based surface reconstruction. approach employs pixel- wise joint re-parametrization reflectance surface normals, representing vector radiances simulated, varying illumination. formulation enables seamless incorporation standard surface reconstruction pipelines, traditional multi-view stereo mvs frameworks modern neural volume rendering nvr ones. combined latter, approach achieves state-of-the-art performance multi-view photometric stereo mvps benchmark datasets, including diligent-mv, luces-mv skoltechd. particular, method excels reconstructing fine- grained details handling challenging visibility conditions. present paper extended version earlier conference paper brument al. proceedings ieeecvf conference computer vision pattern recognition cvpr, featuring accelerated robust algorithm well broader empirical evaluation. code data relative article available keywords surface reconstruction, neural volume rendering, multi-view photometric stereo, multi-view normal integration. introduction surface reconstruction essential various fields, including cultural heritage preservation, medical imaging, virtual augmented reality, digital twinning, content creation games film production. despite significant advancements, performance state-of-the-art surface recon- struction methods remains highly dependent scene characteristics, notably presence fine- scale geometric details possibly complex reflectance properties surface. recovery fine-grained structures long-standing bottleneck surface recon- struction. traditional multi-view stereo mvs methods furukawa ponce, schonberger al, often produce overly smoothed sur- faces, struggling sharp edges intricate geometries seitz al, furukawa al, recent neural approaches advanced surface reconstruction direction shift- ing local, patch-based optimization global, pixel-wise optimization yariv al, oechsle al, wang al, notably, methods hf-neus wang al, pet-neus wang al, neus wang al, neuralangelo al, excel capturing fine surface details least, density viewpoints high brument al, logothetis al, hand, approaches leveraging multi-light information demonstrated strong performance recovering intricate surface geome- tries, even sparse-view scenario. principle photometric stereo ps, achieves monocular reconstruction form normal map, vary- ing illumination woodham, several recent multi-view, multi-light setups supernor- mal cao taketomi, previ- ous work rnb-neus brument al, thus employ viewpoint, resorting multi-view normal integration complete surface reconstruction. apart fine details, complex materi- als metallic, specular, translucent, rough, etc. represent another challenge due strong view-dependence reflectance hence radi- ance, breaks brightness consistency assumption upon mvs fundamentally based seitz al, furukawa al, although recent single-image approaches demonstrated promising results reconstruct- ing accurate surface normals even complex materials, leveraging inductive biases dif- fusion mechanisms bae davison, al, within vision transformers vits kolesnikov al, natural way cope complex materials remains active illumination i.e., ps. recent deep learning-based tech- niques, also based vits, indeed recover detailed surface normal maps fine geometric structures even highly complex materials, illumination calibrated wei al, unknown possibly spatially- varying so-called universal setup ikehata, hardy al, besides, methods ikehata, even extend beyond normal estimation estimat- ing reflectance properties diffuse colour, roughness, metalness, thereby enabling virtual relighting applications. motivated strong performance recent methods estimating detailed surface nor- mals reflectance properties, present work proposes multi-view surface reconstruc- tion approach designed inherit advance- ments. approach follows two-stage decom- position strategy proves effective even with- dense multi-view inputs. specifically, first estimate per-view surface normals option- ally reflectance properties multi-light input data, subsequently reconstruct surface best aligns estimates across multiple views. paper builds upon significantly extends previous conference publication bru- ment al, foundational con- cepts two-stage approach intro- duced. therein, presented key idea re-parametrize normal reflectance priors simulated radiance values per-pixel basis, facilitating seamless incorporation existing surface reconstruction pipelines, notably neural volume rendering nvr frameworks based signed distance functions sdf. innova- tive way perform multi-view normal reflectance integration yields state-of-the-art multi-view photometric stereo mvps technique, summarized figure evaluated diligent-mv dataset al, using sdm-unips method ikehata, nor- mal reflectance estimation. present extension, substantially improve upon initial contribution terms speed, robustness, versatility, evaluation. particular implement approach within neus framework wang al, achieving speedup compared original imple- mentation see section improve robustness approach reflectance singularities introducing reflectance embedding see section addition volume rendering, validate re-parametrization traditional patch-based mvs framework, demonstrating broad applicability ability achieve exact integration noiseless setup see section alternative use case explored, sub- stituting photometric stereo-estimated normals ones derived dense mvs fixed illumination schonberger al, thereby improving fine-structure reconstruction dtu dataset jensen al, see section much deeper evaluation mvps solu- tion conducted see section notably sparse-view setup additional benchmarks luces-mv logothetis al, skoltechd voynov al, using various results input ike- hata, hardy al, also compare results recent supernormal method cao taketomi, hap- pens special case approach see section rest article organized fol- lows. first review relevant literature section then, show section jointly re-parametrize normal reflectance data simulated radiance values. embed re-parametrization traditional mvs frame- work section demonstrate feasibility exact surface reconstruction normals reflectance cues. ensure robustness, section turn attention replacing frame- work volume rendering one based neural implicit surfaces. extensive experiments validat- ing proposed approach conducted section conclusions drawn section related work normal reflectance integration approach mind, let review estimation nor- mals, reflectance, integration techniques. normal estimation normal maps carry high-frequency geometric information. estimation achieved differ- ently depending amount input data. single-image normal estimation highly beneficial scene reconstruction, since preserves local geometry without metric ambigu- ity al, wang al, recent approaches predominantly leverage learning-based methodologies al, wang al, al, liao al, al, bae al, yang al, however, direct training real data remains challenging, normal labels cannot directly captured current sensor technology. typically, normals thus inferred depth maps silberman al, eigen fergus, inaccuracies persist despite various correction strategies bae al, long al, therefore, recent works employed large-scale datasets syn- thesized data omnidata eftekhar al, kar al, along smart architec- tural choices, e.g. per-pixel ray direction modelling dsine bae davison, vits diffusion models stablenormal al, photometric stereo woodham, hand, also monocular technique yet analyzes multiple images captured vary- ing lighting. historical inverse problem-based approach, fits normals reflectance physics-based image formation model, employed calibrated known illumination scenario goldman al, al, shi al, ikehata al, ikehata aizawa, uncalibrated one hayakawa, chandraker al, alldrin al, favaro papadhimitri, queau al, well unknown spatially- varying lighting basri al, queau al, al, haefner al, guo al, however, switching inverse fig. overview surface reconstruction pipeline rnb-neus proposed brument multi-view multi-light data, photometric stereo noted first estimates per-view normal maps optionally reflectance maps rk. estimates re-parametrized radiance vectors frk, nk, via physically-based rendering function simulated lightings lk. implicit neural representation, consisting signed distance function reflectance learned minimising discrepancy volume-rendered counterpart vk, final surface mesh extracted zero-level set optimisation. problem framework deep learning paradigm recently revolutionized ps. limitations tra- ditional physics-based models, especially regard- ing non-lambertian effects, thereby addressed calibrated santo al, ikehata, chen al, ikehata, wei al, uncalibrated taniai maehara, chen al, kaya al, li, mod- ern techniques make lighting context implicit, improves performances al, allows tackling universal unknown spatially-varying illumination ikehata, hardy al, multi-view single-light contexts, tradi- tional methods, colmaps multi-view stereo schonberger al, infer normal maps analysing patch orientations bleyer al, differently, calvet esti- mate normals image grey level variations use slanted plane-sweeping algo- rithm. alternative paradigm computes normals post-mvs deriving depth maps, demonstrated regsdf zhang al, utilizes depth vis-mvsnet zhang al, depth-based normal estimation mvs-method agnostic, applicable tradi- tional furukawa ponce, al, deep learning-driven methods yao al, al, zhang al, reflectance estimation reflectance estimation involves recovering mate- rial properties defining surface-light interactions, critical realistic relighting applica- tions. normals, various methods employed depending whether single image, mul- tiple views, multiple illuminations available. single-image inverse rendering decom- poses single image reflectance, geometry, illumination using instance multi-view self-supervision al, diffusion models infer physically-based rendering pbr materials materialpalette lopes al, multi-view frameworks, methods ref-nerf verbin al, extend neural radi- ance fields nerf explicitly model reflections, improving reconstruction specular glossy surfaces. techniques like shinobi engel- hardt al, concurrently optimize geometry, reflectance, illumination, enabling detailed relightable assets. recently, diffusion- renderer liang al, provided unified solution high-quality normal reflectance reconstruction video sequences, bridg- ing monocular multi-view estimation diffusion models neural inverse rendering. finally, aforementioned photometric stereo technique photographic tech- nique designed reflectance recovery wood- ham, addition, multi-light approaches offer two key advantages single-light meth- ods enhance reflectance estimation shad- owed regions self- cast shadows improve robustness areas affected strong non-linearities, saturation specular highlights, shift across different lighting conditions. multi-view normal integration although individual normal maps carry valuable high-frequency geometry information, may inconsistent across varying viewpoints, making fusion challenging. multi-view single-light setting, monosdf al, combined nor- mal depth consistency loss derived monocular predictions eftekhar al, radiance constraint, common approach nerf-like mildenhall al, methods. similarly, gaussian surfels dai al, uses monocular normal predictions constrain gaussian splatting dgs kerbl al, yet, multi-view normal integration much studied prism multi-view photometric stereo. problem first addressed hernandez optimization loss combining render- ing term discrepancy photometric stereo normals optimized mesh. note first approach required neither prior knowl- edge camera poses illumination conditions. similarly, park simultaneously estimated reflectance, normals, illumination uncalibrated ps, leveraging mesh normals resolve ambiguities refining sur- face details. logothetis later formu- lated problem within sdf representation, achieving superior surface detail reconstruction compared park refine- ments made enhanced mesh propagating sfm points following method nehab validat- ing method introduction publicly available dataset diligent-mv. neural surface reconstruction methods recently emerged promising alterna- tive, enforcing alignment per-view normal maps gradient neural sdf. kaya instance constrained sdf optimization cnn-ps normals ike- hata, mvs depths wang al, incorporating uncertainty measures mitigate conflicts predictions. kaya added neural volume rendering loss, benefit- ing latters robustness handling various material types. resulted multi-objective optimization comprising three loss terms. how- ever, kaya reliance uncertainty-based hyperparameter tuning fully resolve conflicts objectives, lead- ing potential loss fine-grained details. ps- nerf yang al, introduced two-stage solution, first stage achieves multi-view normal integration aligning surface gradients sdps-net normals chen al, second stage leverages unisurf oechsle al, optimize geometry, reflectance illumination, modelled multi-layer perceptrons mlps. however, reliance directional illumination assumption limited generalizabil- ity. npl-mvps logothetis al, relaxed assumption considering near-light model, initially enforcing alignment sdf gradi- ents unims-ps normals hardy al, subsequently processing joint shape reflectance optimization considering light attenuation cast shadows. recently, supernormal cao taketomi, adopted similar initialization approach ps-nerf npl-mvps, utilizing sdm-unips normals ike- hata, significantly improved efficiency multi-resolution hash encoding direc- tional finite differences, achieving nearly double training speed. contrast prior methods, novel approach propose rest article for- mulates multi-view normal integration single- objective optimization problem, joint re- parametrization normals reflectance. let introduce re-parametrization. reflectance normal re-parametrization reflectance surface normals traditionally rep- resent distinct types information reflectance captures intrinsic photometric properties sur- face materials, normals describe geomet- ric orientation surfaces. inherent hetero- geneity complicates simultaneous optimiza- tion, typically necessitating separate processing steps, e.g., depth triangulation multi-view data followed fusion depth infor- mation normal maps using multi-objective optimization frameworks kaya al, address issue, introduce re- parametrization approach designed unify sur- face normals reflectance homogeneous quantities, mapping simu- lated radiance values varying illumination conditions. radiance-based parametrization enables unified optimization reducing need additional regularization, commonly required multi-objective contexts al, zhang al, consequently enhancing consistency computational efficiency. indeed, key moti- vation radiance-based re-parametrization compatibility existing photometric cost- minimization frameworks employed classical multi-view stereo furukawa ponce, schonberger al, neural volume ren- dering methods wang al, yariv al, al, incorporation facilitates optimization photometric consistency across viewpoints input rendered images. input data given set viewpoints, assume avail- ability corresponding reflectance maps normal maps ni, indexed viewpoint map comprises pixels, indexed mathsf hbf ri,kk dots quad mathsf mathbf ni,kk dots therein, reflectance k-th pixel i-th view given parameter vector ri,k dimension corresponding spe- cific reflectance model used, outward normal unit vector ni,k latter expressed, using known camera poses, world coordinates. reflectance data unavail- able, simply setting ri,k allows framework used exclusively multi-view normal integration. re-parametrization re-parametrization approach transforms couple reflectance normal vectors ri,k, ni,k vector vi,k homogeneous radiance val- ues, simulated using physically-based rendering pbr model thbf mathsf fmathbf ri,k, mathbf ni,k, mathsf li,k, label eqreparamgeneral li,k rnl illumination conditions cho- sen specifically k-th photosite i-th view rnl rnq pbr function simplicity, model used pixels. here, stands dimensionality light radiance represen- tation, depend choice particular model. general framework supports arbi- trary pbr illumination models, practical con- siderations regarding computational efficiency implementation simplicity guided choice towards lambertian reflectance directional illumination. former assumption simplifies reflectance albedo i.e., ri,k ri,k depending whether images grey scale rgb. pbr function reads lon imes mathbb times mathbb times mathbb times nonumber mathbf r,mathbf n,mathsf mapsto mathsf fmathbf r,mathbf n,mathsf mathsf mathbf mathbf rtop label eqparama illumination vectors intensity direction stored row-wise ensure bijectivity mapping employ three distinct illumination vectors two additional constraints ensure bijectivity non-zero reflectance ri,k valid long surface perfectly black, linear independence illumination directions non-singular li,k. conditions, matrix li,k invertible. inverse given ri,k i,kvi,k ni,k i,kvi,k i,k vi,k. vectors ni,k ri,k deduced matrix i,kvi,k ni,kr i,k using svd. employing illumination vectors may interesting, especially considering advanced pbr models including specularity, roughness, anisotropy uncertain input scenarios, yet expense bijectivity. leave avenue future research. optimal illumination directions although mapping ri,k, ni,k vi,k bijec- tive non-singular illumination matrix li,k, choices equally adapted task. indeed, input reflectances normals prone estimation errors, radiance values considered noisy. since next invert pbr model noisy values, condition- ing illumination matrices sub- stantial impact accuracy numerical inversion. therefore chose widely accepted con- figuration suggested drbohlav chantler employs three illumination configu- rations equal intensity, directions spaced equally azimuth, con- stant slant .relative normal vector ni,k. alternative illumination arrangements, embedded supernormal cao taketomi, assessed section addition numerical considerations, particular choice also avoids self-shadowing, namely negative dot products nor- mals illumination vectors. nevertheless, method could accommodate non-physical nega- tive radiance long rendering function downstream reconstruction technique con- sistent. next section, explore first possibility latter downstream technique, surface sweeping. surface sweeping-based reconstruction first proof concept, let apply re-parametrization proposed section triangulation-based mvs, method esti- mates depth pixel reference view maximizing photometric consistency control views. particular, show leveraging information pro- vided normals, plane sweeping-based algo- rithms collins, turned exact surface sweeping-based method integrating multi-view normal data, eliminating inherent approximation errors due local planar sur- face assumptions, e.g., fronto-parallel slanted patches furukawa ponce, bleyer al, schonberger al, objective function aim compute depth values pixels first view, considered reference. with- loss generality, reference camera frame aligns world coordinate system. views serve control views enforc- ing multi-view consistency reflectance normals. optimization multi-view consis- tency usually carried locally, splitting pixels reference view patches mathcal subset mathbb containing mmathcal pixels. denote v,jj,...,mp re-parametrized reflectance normals patch. formulated terms re-parametrization, finding depth patch centre maximizes multi-view con- sistency reflectance normal maps amounts solving lab bje tive reparam min sum sum jmmathcal left mathbf v,j mathbf vi,jz right mathb vi,jz mathbb times stands re- parametrization i-th view, sampled using bilinear interpolation pixel position pi,jz calculated given depth hypothesis position obtained first back-projecting reference patch pixel hbf p,j mathcal projecting point control view labe project mathbf pi,jz circ z-mathbf p,j, quad forall mathbf p,j mathcal denotes projection world coordinates i-th control views image plane, inverse projection ref- erence image pixel corresponding point, given depth hypothesis proceed actual optimization function must made explicit, typically involves local geometric approx- imation e.g., fronto-parallel slanted patches. shall see next, availability normal information provides natural way avoid approximation. p,j reference camera control camera pi,j p,j reference camera control camera pi,j n,j p,j reference camera control camera pi,j fig. three types patches. left right fronto-parallel patches, slanted patches proposed surface patches. exact multi-view integration common approximations used clas- sical plane-sweeping mvs methods, illustrate first two diagrams figure fronto-parallel patches zero-order approxi- mation patch points assumed lie plane parallel image plane, located hypothesized depth inverse projec- tion simply thb p,j mathsf mathbf p,jtop top intrinsics matrix reference camera assumed known priori. simplest model, often inaccurate non-planar surfaces slanted views furukawa ponce, furukawa al, slanted patches first-order approximation finer approximation retains depth patch centre, yet assumes points patch lie slanted plane, whose orientation determined nor- mal patch centre. depth thus adjusted based plane equation bleyer al, schonberger al, calvet al, generally provides better accuracy fronto-parallel patches, approximation errors remain curved surfaces. contrast, framework detailed local geometry fully encoded normal map mathsf allows move beyond simple planar approximations. knowing cameras intrinsics, normals n,j pixels p,j indeed integrated depth values patch center. means surface patch reconstructed scale factor queau al, none depth patch centre. inverse projection thus becomes abel nverse ojec ion urface z-mathbf p,j alpha mathsf mathbf p,jtop top quad forall mathbf p,j mathcal searching optimal therefore amounts achieve surface sweeping, without planar approximation. using exact local surface represen- tation, triangulation-based mvs frame- work leveraging re-parametrization, enforce consistency appearance reflectance detailed geometry integrated normals across views, also avoid low-order approximation errors inherent plane sweeping- based methods. empirically validated experiments presented hereafter. empirical validation evaluate re-parametrization loss surface sweeping strategy, generated synthetic benchmark compared results obtained using traditional methods. synthetic dataset created comprises two superimposed gaussian functions, whose ground truth normals computed analytically. reflectance surface set piecewise-linear function. rendered nor- mal reflectance maps surface, using camera parameters first five views buddha dataset diligent-mv al, example normal reflectance maps two viewpoints shown figure dataset allows evaluate surface reconstruction accu- racy terms mean depth error reference view known, ideal conditions noise, perfect normalreflectance estimates study effect controlled noise. fig. synthetic normals top reflectance bottom used experiments, refer- ence view left one control view right. single-objective validation first consid- ered, comparing results straightforward solution combining reflectance discrepancy loss lphoto geometric one lgeom, weighting hyperparameter lab bje eco bined sum mmathcal big underbrace mathbf n,j cdot mathbf ni,jz mathcal ltext geom underbrace mathbf r,j mathbf ri,jz mathcal ltext photo big therein, tuning often difficult, since optimal value depends noise level. unifying geometric photometric cues single, parameter-free objective, proposed re- parametrization circumvents issue. figure compares mean depth error obtained using approaches, presence fixed amount additive gaussian noise reflectance values standard deviation maximum reflectance value increas- ing one normals. focus evaluation re-parametrization, slanted patch approximation adopted cases. optimal value depends noise level, hyperparameter-free approach always repre- sents reasonable compromise performance, regardless noise level. fig. mean depth estimation error function gaussian noise added input normal maps. multi-objective approach highly sensitive tuning hyperparameter maintains stable results without tuning. surface sweeping validation assessed, comparing various patch geome- try approximations. focus sweep- ing method, time considered single-objective loss, noise added normals, reflectance. also compare patch-based mvs methods modern frameworks, included comparison previous implementation rnb-neus brument al, based volumetric rendering. results presented figure ideal, noiseless scenario, proposed surface sweeping method leveraging normal inte- gration achieves essentially exact recon- struction, demonstrating capacity elimi- nate errors due low-order patch geometry approximation. errors indeed clearly visible first-order approximation purple curve, even zero-order one dark blue curve. volumetric rendering approach orange curve, hand, presents small, non-null error, potentially stemming inherent approximations within neural fig. mean depth estimation error func- tion gaussian noise added input nor- mals, patch-based mvs method employing fronto-parallel plane sweeping dark blue, slanted plane sweeping purple, normal-aware sur- face sweeping pink, well volumetric rendering method rnb-neus brument al, orange. surface sweeping achieves exact reconstruction noiseless case, yet volumet- ric rendering much robust high noise levels. implicit representation volume rendering pro- cess may include local planarity assump- tions wang al, zhang al, adding noise normals reveals critical dif- ferences robustness. accuracy patch- based variants degrades noise increases. notably, surface sweeping approach, per- fect initially, rapidly deteriorates level lower-order approximations. indicates accuracy local surface derived normal integration highly vulnerable inac- curacies input normal field. hand, volumetric rendering demonstrates consid- erably better robustness high noise levels. robustness likely benefits global optimiza- tion framework implicit regularization provided optimization sdf. overall, empirical findings validate hyperparameter-free re-parametrization strategy, feasibility exact reconstruction ideal conditions. however, given state-of- the-art photometric stereo methods often yield normals mean angular errors range circ ,circ hardy al, believe robustness noise prevails theoretical accu- racy. therefore, next section turn attention coupling re-parametrization volumetric rendering frameworks. volume rendering-based reconstruction contrast previous section focused single depth map, present one introduces method estimating full model geometry reflectance consis- tent input normal reflectance data. so, embed homogeneous radiance- based re-parametrization introduced section unified objective function derived neu- ral volume rendering nvr principles. actual implementation builds upon neus frame- work wang al, yielding highly effec- tive method reconstruction multi-view normal reflectance maps. surface parametrization aim infer model defined two func- tions geometric map photometric map rq. function rep- resents signed distance surface, surface zero-level set function assigns reflectance value point. since actual form must consistent assumptions section limit lamber- tian model rgb data, yet extensions complex brdfs conceiv- able long consistent pbr model used re-parametrization. objective function clarity within subsection, focus data associated single camera view omit index input reflectance normal values therefore denoted simply rkk nkk, joint re-parametrization vkk. core idea nvr approach opti- mize scene representation rendering, conditions section matches input-derived radiance vectors vk, minimizing mathc text pboldsymbol rho sum tilde mathbf vkboldsymbol rho mathbf pp, label eqphotometriclossrevised choice discussed experiments, vk, nvr- based radiance pixel computation latter draws inspira- tion neus wang al, denoting camera centre viewing direction associated k-th pixel, points along ray write xkt volume rendering amounts integrating individual colour contributions along ray labe nder initrev ised mathbf vkboldsymbol rho int !!! wt, fmathbf xkt mathbf cmathbf xkt, boldsymbol rho mathrm dt, integration range, occlusion-aware weighting function ensuring con- centration around surface wang al, unlike original neus directly opti- mized apparent colour, optimize underlying surface properties taking account pbr model crucial use model assimilating unit outward surface nor- mal gradient sdf, apparent colour writes label qln ormfinal revised mat hbf cmathbf xkt, boldsymbol rho mathsf fleft boldsymbol rho mathbf xkt,nabla fmathbf xkt,mathsf right provided simplify opti- mization process, followed neus relaxed hard constraint eikonal regularizer encouraging fto close unity, con- trolled hyperparameter label alrevised mathc ext regf lambda dfrac sum int ttext nabla mathbf xkt !-! mathrm dtmleft ttext tright combining yields nvr loss single view. averaging con- tributions views, adding regulariza- tion well silhouette consistency regularization neus wang al, obtain complete loss function. formula- tion enables end-to-end optimization sdf reflectance using gradient-based meth- ods, typically representing mlps employing hierarchical sampling along rays, similar neus wang al, supernormal specific case stated section absence reflectance data, framework still used, setting missing reflectance constant white. addi- tion, instead optimal triplet discussed section one chooses triplet lights follow- ing vectors canonical basis, framework actually comes particular case happens supernormal cao taketomi, indeed, using ri,k li,k re-parametrized inputs sim- plify input normals vi,k ni,k similarly, using yields volumetric rendered normals nkf wt, fxkt fxkt dt. two simplifications nvr loss becomes mat ltex sum left tilde mathbf nkf mathbf right precisely normal consistency loss introduced supernormal. upcoming experi- ments, presented section examine performance obtained using particular set- ting, comparison proposed joint opti- mization reflectance normals using opti- mal triplets light sources. reflectance singularities comparison supernormal, approach advantage taking account normals, also reflectance. however, may sometimes turn drawback. indeed, actual reflectance values weight optimization process, potentially leading singularities e.g., presence dark materials. let demonstrate lambertian assump- tion, case grey scale images simplicity, let consider ideal sam- pling scenario weight fxk dirac function tk, i.e. null every- except xktk lies surface combining volumetric rendering simplifies vk, fxk xk. introduc- ing scalar variable nvr loss rewritten loss nvr pr,f mathbf left rmathbf mathsf nabla fmathbf mathsf mathbf right pp. label eqrgb rewriting emphasizes reflectance acts weighting factor loss function. conse- quently, singular reflectance values may substan- tially influence optimization geometry. particular, dark colours essentially leave geom- etry locally unconstrained, may result slow, sub-optimal convergence loss fine details see figure -b. fig. effect reflectance embedding. one input reflectance maps. reconstruction without reflectance embedding deformed dark reflectance area. reflectance embed- ding, much closer ground truth mitigate issue, reflectance embed- ding approach introduced, ensures reflectance norm remains constant. is, augment input estimated reflectance values -dimensional vectors frac math top left q-left mathbf rkright ppright pright top forall boldsymbol rho cdot frac left boldsymbol rho cdot top left q-left boldsymbol rho cdot right ppright pright top easy check norm vectors equal whatever values reflectance embedding strategy prevents unin- tended influence singular reflectance values geometry optimization. qualitatively illus- trated figure synthetic experiment, using l-norm grey level data therein, dark regions faithfully recon- structed standard case, whereas reflectance embedding significantly improves results. fur- ther experiments evaluating interest embedding conducted section rnb-neus rnb-neus one main challenges initial approach, rnb-neus brument al, built upon neus wang al, sig- nificant computation time required sin- gle reconstruction. scene-dependent process indeed usually takes approximately hours nvidia rtx gpu. address issue, restructured algorithm follow neus wang al, architecture. results solution faster, reducing computation time around minutes. neus builds nerf mildenhall al, introducing volumetric rendering trans- fer function converts sdf density function. allows direct optimization scene geometry extraction mesh via sdfs zero-level set. neus applies princi- ples replaces nerf instant-ngp muller al, enables real-time rendering thanks cuda acceleration use optimizable hash grid sampling volu- metric rendering. cuda contributes significant performance improvement, hash grid reduces computation time focusing surface-adjacent regions, enhancing detail preservation. neus retains key opti- mizations also including sdf-based density representation neus. transition rnb-neus neus frame- work, several modifications required. first one involved modifying input structure accommodate normal albedo maps. unlike neus, predicts colour function viewing direction, rnb-neus estimates albedo, independent viewing direction. consequently, dependency removed inputs albedo prediction network. adjustment, albedo could retrieved point xkt allowing colour prediction pixel following equation next phase involved implementing nec- essary loss functions. eikonal loss silhouette consistency one remained unchanged, adapting colour loss challenging. indeed, presence surface- aware pbr model colour prediction fundamentally altered gradient computations required back-propagation. unlike pytorch, supports automatic differentiation, neus built entirely cuda, requiring manual spec- ification gradient computations. details derivatives supplementary material bruneau al, outlined technical ingredients proposed method, let present series experiments designed evaluate effectiveness across range scenarios. experimental results section presents thorough empirical evalu- ation approach. multi-view multi-light scenario mvps first considered. then, carry ablation study individ- ual components framework. eventually, demonstrate use method multi- view single-light mvs setup. thorough qualita- tive quantitative results provided supplementary material bruneau al, mvps materials approach uses normal reflectance maps, derivable techniques. multi-view multi- light datasets thus key, enable inde- pendent per view, providing inputs integration process. evaluation datasets diligent-mv al, bench- mark five real-world objects, complex reflectance surface profiles. object imaged calibrated viewpoints using classical turntable mvps acquisition setup hernandez al, direc- tional lighting conditions. given acquisition characteristics, relatively low image resolution pixels corresponds approximately per pixel. luces-mv logothetis al, features objects larger reflectance diversity diligent-mv. object captured different angles using turntable setup, different near-light non-directional conditions. considering low number views light- ing conditions, luces-mv considered sparse dataset. however, image resolu- tion pixels significantly higher diligent-mv. related actual object size camera distance, results finer scene resolution, pixel corresponds approximately mm. skoltechd voynov al, multi- sensor dataset designed multi-view surface reconstruction. includes objects, among selected focusing chal- lenging reflectance properties transparency, spec- ularity, uniform texture. acquisitions performed highly challenging lighting condi- tions led panels significantly deviating directional assumption, often present over- exposed regions. evaluation, used views right camera, tis cam- era imaging source industrial type, illuminations per view. although image resolu- tion high increased object-to- camera distance results scene representation approximately per pixel, placing diligent-mv luces-mv terms scene resolution. photometric stereo methods given variety reflectance properties illumination conditions selected mvps datasets, state-of-the-art uncalibrated meth- ods selected, particularly transformers-based ones exhibit strong performance even unknown, general lighting conditions. among methods, sdm-unips ike- hata, referred sdm result subsections provides normal reflectance maps, enabling complete evaluation pipeline. unims-ps hardy al, predicts slightly accurate normals see table yet reflectance. thus allows normal inte- gration assessment, similar supernormal cao taketomi, establish upper bound achievable reconstruction quality, exper- iments also conducted using ground truth normals. forthcoming experiments, notation resp. indicates inputs normals alone resp. normals reflectance, estimated method since architecture constraints limit sdm- unips use images, sampling strategy employed. specifically, computed median estimated reflectances normals across random trials, trial involving randomly chosen images. unims-ps scaling better thanks multi-scale enhancement, sampling necessary. normal mae sdm-unips unims-ps diligent-mv mean std. luces-mv mean std. skoltechd mean std. table normal mean angular error two methods, three benchmarks. benchmark, mean indicates averaged error datasets std. standard deviation. assessment ps-estimated normals, con- ducted table reveals methods exhibit significant disparity across datasets diligent-mv maintains average mae around luces-mv shows much higher error skoltechd even higher one input normals provided method therefore considered rather noisy. baselines approach compared various mvps techniques including, diligent-mv, classical methods park park al, al, neural approaches ps-nerf yang al, mvpsnet zhao al, kaya kaya al, kaya kaya al, recent methods supernormal cao taketomi, npl-mvps logothetis al, com- parisons limited luces-mv skoltechd, methods originally benchmarked datasets, either proprietary complex reproduce. evaluations also conducted single-light mvs diligent-mv. required generating lambertian-like images view, computing viewpoint median intensity across lighting conditions, following approach kaya metrics assess overall reconstruction fine details, quantitative evaluations rely chamfer distance mean angular error mae. highlight ability capture fine geometric details robustness poorly constrained scenarios, also provide focus clusters particular interest namely high cur- vature low visibility areas, illustrated figure clusters segmented using vcglib lab, meshlab cignoni al, retaining vertices maximal absolute principal curvature higher visible less views, respectively. fig. high curvature left low visibility right areas, buddha reading datasets diligent-mv. mvps results unless otherwise specified, subsequent evalu- ations carried using nvr loss optimal lighting triplet reflectance embedding, consistently shown best performance across evaluated datasets see ablation study section results diligent-mv table presents evaluation diligent-mv dataset. preliminary obser- vation, let remark significant discrepancy around results conference paper rnb-neus, using sdm-unips normal reflectance maps. chamfer distance methods bear buddha cow pot reading mean park kaya ps-nerf kaya mvpsnet supernormal sdm nlp-mvps rnb-neus sdm sdm sdm unims-ps table chamfer distance lower better averaged overall vertices diligent-mv dataset. best results second best since requires manual efforts al, ranked. normal mae methods bear buddha cow pot reading mean park kaya ps-nerf kaya mvpsnet supernormal nlp-mvps rnb-neus sdm sdm sdm unims-ps table normal mae lower better averaged views diligent-mv dataset. discrepancy due change norm nvr loss severely impacts scores. shown ablation study table using nvr loss yields scores comparable conference paper. another notable observation whatever method, results par state- of-the-art, even better. nevertheless, combining method unims-ps yields better results sdm-unips, consistent higher accuracy formers normals see first row table highlights versa- tility proposed method, adapts well advancements. dataset, reflectance seems really improve results, yet importance become clearer later, considering datasets exhibiting challenging materials. conclusions mae evaluation table remain consistent one. last interesting observation using ground truth normals yields non-null error, indicating possible bias within volumetric approach, already noticed section lastly, figure permits qualitative assessment overall reconstruction fine details. results luces-mv experiments luces-mv dataset table confirmed state-of-the-art results, yet reflectance visibly improves results. interest- ingly, scores largely increase comparison diligent-mv despite better pixel resolu- tion bowl object, even using normals, mm. believe could due sparser number views figure indeed shows strong correlation visibility inaccuracies datasets camera calibration process. fig. reconstructed mesh corresponding mae four objects diligent-mv. chamfer distance methods bowl buddha bunny cup die hippo house owl queen squirrel mean supernormal sdm sdm sdm unims-ps table chamfer distance lower better averaged overall vertices luces-mv. fig. correlation visibility. left right mm, number cameras observing vertex luces-mvs bowl object, visibility graph entire dataset. results skoltechd luces-mv, compared results skoltechd supernormal, rep- resents state-of-the-art mvps. reported table emphasize significant superiority results, well inter- est accounting reflectance. nevertheless, results dataset globally disappoint- ing, seen figure two examples exhibiting severe breaks surface. confirmed quantitatively observing strikingly high values. similar spatial resolution number views diligent- mv, scores drastically worse best-performing baselines methods dragon golden snail plush bear jin chan green carved. moon pillow painted cup red ceramic. painted sam. green tea. blue boxing. neus supernormal sdm sdm sdm unims-ps golden bust small wooden. amber vase green bucket white human. orange mini. pink boot skate white castle. wooden trex mean neus supernormal sdm sdm sdm unims-ps table chamfer distance lower better averaged overall vertices skoltechd. fig. reconstruction two objects skoltechd. clear surface ruptures observed. around higher diligent-mv mm. time, believe quality normals over-saturated areas, notably bottleneck high errors table tend indicate. key indicator dataset suit- able yet multi-light studies comparison neus, chosen single-light mvs baseline. neus achieves better reconstructions mvps-based methods using views. thus still room improvement psmvps research, particularly presence highly challenging illumination conditions. high curvature low visibility areas illustrated figure proposed method successfully reconstructs low- high- frequency geometric details. however, metrics averaged entire surface fail report accuracy high curvature low visibil- ity areas. therefore, targeted evaluations specific regions conducted. results diligent-mv reported table fig. focus high-frequency details diligent-mv reconstructions. chamfer distance high curv. diff. vertices park kaya ps-nerf kaya mvpsnet supernormal sdm npl-mvps rnb-neus sdm sdm sdm unims-ps low vis. diff. vertices park kaya ps-nerf kaya mvpsnet supernormal sdm npl-mvps rnb-neus sdm sdm sdm unims-ps table chamfer distance high curvature top low visibility bottom areas, diligent-mv dataset. key insight table found difference column, compares error entire point set selected subsets. seen, proposed method significantly stabler competitors challenging areas. sparse-view scenario one key advantage using photometric stereo data richness normal information. normals sufficiently high quality, case diligent-mv dataset, impose strong constraints optimization pro- cess, enabling proposed approach perform well even sparse-view scenario, highly challenging single-light mvs methods. assessed table reports evolution reducing number views panel recent mvs methods proposed one. chamfer distance methods views views views neus neus pet-neus gaussiansurfels rnb-neus sdm sdm sdm unims-ps table chamfer distance lower better averaged overall vertices diligent-mv, decreasing number views. expected, mvs methods experience significant drop performance number views decreases. contrast, proposed one remains much stable terms cd. primary effect reduced number views method loss fine details, shown figure ablations lastly, ablation studies conducted diligent-mv table luces-mv table quantify impact architectural choices within method. specifically, compared choice nvr loss nvr see equation considering reflectance maps addition normal maps, using pixel-wise optimal lighting triplets canonical ones using reflectance embedding instead standard reflectance parametrization. lighting optimization appears important, yet less extent choice using nvr reduces scores however, nvr per- forms better normals reflectance used together, reflectance always con- tributes positively using nvr. finally, reflectance embedding slightly influences met- rics, expected since singular reflectance points sparse. mvs reconstruction addition mvps use case, also con- ducted series experiments single-light scenario. therein, reflectance input fed method, normals obtained using colmap schonberger al, estimates fig. qualitative comparison single-light neus method, sparse-view scenario two objects diligent-mv. chamfer distance methods bear buddha cow pot reading mean lor lcr lor lor lcr lor table ablation study diligent-mv, com- paring choice nvr loss, benefit reflectance optimal canonical light triplets finally reflectance embedding sake clarity, nvr shown lp. using patchmatch bleyer al, addi- tional tests conducted using single-view normal prediction method dsine bae davi- son, however include results due significantly lower quality. methodology compared results neus wang al, colmap schonberger al, views subset objects dtu jensen al, state-of-the-art methods. complementing overall chamfer distance, employ specific evaluation protocol assess reconstruction accuracy fine geometric details, averaged global met- rics. therefore, previously focus global reconstruction, also high curva- ture areas. however, dtu objects less curvy benchmarks, high curvature areas segmented slightly differently, order focus fine structures. ground truth points first meshed using screened poisson surface reconstruction, mesh smoothed via laplacian filter- ing dawson-haggerty al., displace- ment magnitude vertex smooth- ing eventually served proxy geometric frequency content, displacements hand- tuned threshold indicating fine struc- tures. figure illustrates segmentation approach. results values dtu reported table observed, proposed method globally par alternative mvs frameworks. however, results much stabler sparse-view scenario, indicated table number views decreased results also much accurate high curvature areas, indicated tables. notably, although use colmaps normals, cds lower areas, highlights interest proposed normal integration framework. overall, mvs experiments pro- vide proof concept incorporation framework beyond mvps. chamfer distance methods bowl buddha bunny cup die hippo house owl queen squir. mean lor lcr lor lor lcr lor table ablation study luces-mv, notations table reference mesh smoothed mesh displacement map fig. high curvature segmentation scan object dtu. ground truth mesh obtained via poisson reconstruction, smoothed using laplacian operator displacement magnitude thresholded identify finest structures. conclusion perspectives presented versatile efficient frame- work high-fidelity surface reconstruction leveraging multi-view normal cues, optional use reflectance information. novel reparametrization normals reflectance radiance vectors simulated varying illu- mination, method fits seamlessly traditional mvs nvr pipelines. also fast, thanks cuda acceleration optimizable hash grid. extensive experiments recent benchmarks demonstrate state-of- the-art performance, particularly capturing fine details handling challenging reflectance visibility conditions. however, challenges remain. observe bias volume rendering version using ground truth normals, robustness noisy input normals still needs improvement. lim- itations underline importance better estimation, especially limited number light sources. moreover, findings suggest need reliable mvps datasets. notably, recon- struction accuracy paradoxically decreases increasing image resolution current bench- marks see example results diligent-mv luces-mv, observation questions ground truth quality recent datasets. broadly, work highlights prac- tical promise mvps traditional mvs single-light methods. recent tech- niques e.g., sdm-unips, unims-ps deliver- ing accurate normals complex reflectance, multi-view normal integration methods e.g., supernormal, preserving fine details, mvps emerges compelling option accu- rate, affordable scanning especially semi- controlled environments. also significantly out- performs single-illumination methods objects deep concavities see figure trans- parent parts see figure leveraging rich geometric cues offered multi-light vit-based priors. chamfer distance methods colmap neus schonberger schonberger mean high curvatures methods colmap neus schonberger schonberger mean table chamfer distance objects dtu, globally top high curvature areas bottom. sake clarity, nvr shown lp. chamfer distance methods views views views views views colmap neus schonberger schonberger high curvatures methods views views views views views colmap neus schonberger schonberger table chamfer distance dtu averaged overall vertices top high curvature areas bottom, decreasing number views. sake clarity, nvr shown lp. hope work encourages explo- ration paradigm, improving robust- ness dataset quality enabling broader applicability real-world scenarios. funding robin bruneaus postdoctoral fellowship funded department quantitative biomedicine university zurich. bap- tiste bruments doctoral student fellowship input image reference mesh neus fig. advantage mvps mvs beyond fine detail reconstruction handling deep concavities wooden trex skoltechd dataset associated low visibility areas. input image reference mesh neus fig. advantage mvps mvs beyond fine detail reconstruction han- dling transparency orange mini vacuum skoltechd dataset see transparent cover. enabled high diversity light reflec- tions, effectively leveraged recent techniques. funded cnrs project open- dopamin. lilian calvets postdoctoral fellow- ship supported university zurich, university hospital balgrist, or-x swiss national research infrastructure trans- lational surgery. research leading results received funding french national research agency anr labcom project alicia-vision, department computer science university copen- hagen diku copenhagen data project phylorama. data availability code data relative arti- cle available robinbruneaurnb-neus. also, thorough qualitative quantitive study avail- able supplementary material bruneau al, kdfckedixnpostl qldauwsnkcd view?uspdrive link. diligent-mv dataset al, available photometricstereodatamv. luces-mv dataset logothetis al, available yweyuplvnpcqeghrpmhtxvtfrli? uspsharing. skoltechd dataset voynov al, available references alldrin ng, mallick sp, kriegman resolving generalized bas-relief ambiguity entropy minimization. proceedings ieee conference computer vision pattern recognition, bae davison rethinking induc- tive biases surface normal estimation. proceedings ieeecvf conference computer vision pattern recognition bae budvytis cipolla estimating exploiting aleatoric uncertainty sur- face normal estimation. proceedings ieeecvf international conference computer vision, bae budvytis cipolla irondepth iterative refinement single-view depth using surface normal uncertainty. proceedings british machine vision conference basri jacobs kemelmacher pho- tometric stereo general, unknown light- ing. international journal computer vision bleyer rhemann rother patch- match stereo-stereo matching slanted sup- port windows. proceedings british machine vision conference, brument bruneau queau rnb-neus reflectance normal- based multi-view reconstruction. pro- ceedings ieeecvf conference com- puter vision pattern recognition bruneau brument queau multi-view surface reconstruction using nor- mal reflectance cues supplementary material. kdfckedixnpostl qldauwsnkcd view?uspdrive link calvet maignan brument multi-view normal estimation application slanted plane-sweeping. proceedings international conference scale space variational methods computer vision, cao taketomi supernormal neural surface reconstruction via multi-view normal integration. proceedings ieeecvf conference computer vision pattern recognition, chandraker mk, kahl kriegman reflections generalized bas-relief ambi- guity. proceedings ieee conference computer vision pattern recognition, chen han wong kyk ps-fcn flexible learning framework photometric stereo. proceedings european conference computer vision, chen han shi self-calibrating deep photometric stereo networks. proceed- ings ieeecvf conference computer vision pattern recognition, chen waechter shi learned deep uncalibrated photometric stereo? proceedings european conference computer vision, cignoni callieri corsini meshlab open-source mesh processing tool. proceedings eurographics italian chapter conference, collins space-sweep approach true multi-image matching. proceedings ieee conference computer vision pattern recognition, dai xie high-quality surface reconstruction using gaussian surfels. acm siggraph conference papers, dawson-haggerty al. trimesh. url vuong roumeliotis si, surface normal estimation tilted images via spatial rectifier. proceedings european conference computer vision, drbohlav chantler optimal light configurations photometric stereo. pro- ceedings ieee international con- ference computer vision, eftekhar sax malik omnidata scalable pipeline making multi-task mid-level vision datasets scans. proceedings ieeecvf international conference computer vision, eigen fergus predicting depth, sur- face normals semantic labels com- mon multi-scale convolutional architecture. proceedings ieee international conference computer vision, engelhardt raj boss shi- nobi shape illumination using neural object decomposition via brdf optimization in-the-wild. proceedings ieeecvf conference computer vision pattern recognition favaro papadhimitri closed-form solution uncalibrated photometric stereo via diffuse maxima. proceedings ieee conference computer vision pattern recognition, furukawa ponce accurate, dense, robust multi-view stereopsis. proceedings ieee conference computer vision pattern recognition furukawa hernandez multi-view stereo tutorial. foundations trends computer graphics vision goldman db, curless hertzmann shape spatially-varying brdfs photometric stereo. ieee transactions pattern analysis machine intelligence fan zhu cascade cost volume high-resolution multi-view stereo stereo matching. proceedings ieeecvf conference computer vision pattern recognition, guo shi patch-based uncalibrated photometric stereo natu- ral illumination. ieee transactions pattern analysis machine intelligence haefner gao variational uncalibrated photometric stereo general lighting. proceedings ieeecvf international conference computer vision, hardy queau tschumperle uni ms-ps multi-scale encoder-decoder transformer universal photometric stereo. computer vision image understanding hayakawa photometric stereo light source arbitrary motion. journal optical society america hernandez vogiatzis cipolla mul- tiview photometric stereo. ieee transactions pattern analysis machine intelligence ikehata cnn-ps cnn-based photomet- ric stereo general non-convex surfaces. proceedings european conference computer vision, ikehata ps-transformer learning sparse photometric stereo network using self-attention mechanism. proceedings british machine vision conference ikehata universal photometric stereo network using global lighting contexts. proceedings ieeecvf conference computer vision pattern recognition, ikehata scalable, detailed mask-free universal photometric stereo. proceedings ieeecvf conference computer vision pattern recognition, ikehata aizawa photometric stereo using constrained bivariate regression gen- eral isotropic surfaces. proceedings ieee conference computer vision pat- tern recognition, ikehata wipf matsushita robust photometric stereo using sparse regres- sion. proceedings ieee conference computer vision pattern recognition, jensen dahl vogiatzis large scale multi-view stereopsis evaluation. pro- ceedings ieee conference computer vision pattern recognition, shi wen revisiting one- stage deep uncalibrated photometric stereo via fourier embedding. ieee transactions pattern analysis machine intelligence kar of, yeo atanov com- mon corruptions data augmentation. proceedings ieeecvf conference computer vision pattern recognition, kaya kumar oliveira uncali- brated neural inverse rendering photometric stereo general surfaces. proceedings ieeecvf conference computer vision pattern recognition, kaya kumar oliveira uncertainty-aware deep multi-view photomet- ric stereo. proceedings ieeecvf conference computer vision pattern recognition, kaya kumar oliveira multi-view photometric stereo revisited. proceedings ieeecvf winter confer- ence applications computer vision, kerbl kopanas leimkuhler gaussian splatting real-time radi- ance field rendering. acm transactions graphics kolesnikov dosovitskiy weissenborn image worth words trans- formers image recognition scale. proceedings international conference learning representations lab portable templated library manipulation, processing triangle tetrahedral meshes shen dai depth sur- face normal estimation monocular images using regression deep features hierarchi- cal crfs. proceedings ieee conference computer vision pattern recognition, self-calibrating photometric stereo neural inverse rendering. pro- ceedings european conference computer vision, zhou multi-view pho- tometric stereo robust solution bench- mark dataset spatially varying isotropic materials. ieee transactions image pro- cessing muller evans neu- ralangelo high-fidelity neural surface recon- struction. proceedings ieeecvf conference computer vision pattern recognition, liang gojcic ling dif- fusionrenderer neural inverse forward rendering video diffusion models. proceedings ieeecvf conference computer vision pattern recognition liao gavves snoek spher- ical regression learning viewpoints, surface normals rotations n-spheres. proceedings ieeecvf conference computer vision pattern recognition, logothetis mecca cipolla differ- ential volumetric approach multi-view pho- tometric stereo. proceedings ieeecvf international conference com- puter vision, logothetis budvytis liwicki luces-mv multi-view dataset near- field point light source photometric stereo. arxiv preprint logothetis budvytis cipolla npl-mvps neural point-light multi-view photometric stereo. proceedings ieeecvf winter conference applications computer vision, long zheng zheng adaptive surface normal constraint geometric estima- tion monocular images. ieee transactions pattern analysis machine intelligence lopes pizzati charette material palette extraction materials sin- gle image. proceedings ieeecvf conference computer vision pattern recognition mildenhall srinivasan pp, tancik nerf representing scenes neural radiance fields view synthesis. communi- cations acm shi uncalibrated pho- tometric stereo natural illumination. proceedings ieeecvf conference computer vision pattern recognition, muller evans schied instant neural graphics primitives multireso- lution hash encoding. acm transactions graphics nehab rusinkiewicz davis efficiently combining positions normals precise geometry. acm transactions graphics oechsle peng geiger unisurf unifying neural implicit surfaces radiance fields multi-view reconstruction. pro- ceedings ieeecvf international conference computer vision, park sinha sn, matsushita mul- tiview photometric stereo using planar mesh parameterization. proceedings ieee international conference computer vision, park sinha sn, matsushita robust multiview photometric stereo using pla- nar mesh parameterization. ieee transactions pattern analysis machine intelligence liao liu geonet geomet- ric neural network joint depth surface normal estimation. proceedings ieee conference computer vision pattern recognition, liu liao geonet iter- ative geometric neural network edge-aware refinement joint depth surface nor- mal estimation. ieee transactions pattern analysis machine intelligence queau lauze durou l-tv algorithm robust perspective photomet- ric stereo spatially-varying lightings. proceedings international confer- ence scale space variational methods computer vision, queau durou jd, non- convex variational approach photometric stereo inaccurate lighting. proceed- ings ieee conference computer vision pattern recognition, queau durou jd, aujol normal integration survey. journal mathematical imaging vision santo samejima sugano deep photometric stereo network. proceed- ings ieee international conference computer vision workshops, schonberger jl, zheng frahm jm, pixelwise view selection unstructured multi-view stereo. proceedings european conference computer vision, seitz sm, curless diebel comparison evaluation multi-view stereo reconstruction algorithms. proceedings ieee conference computer vision pattern recognition, shi tan matsushita biquadratic reflectance model radiometric image analysis. proceedings ieee conference computer vision pattern recognition, silberman hoiem kohli indoor segmentation support inference rgbd images. proceedings european conference computer vision, taniai maehara neural inverse rendering general reflectance photometric stereo. proceedings international conference machine learning, verbin hedman mildenhall ref-nerf structured view-dependent appearance neural radiance fields. proceedings ieeecvf conference computer vision pattern recognition voynov bobrovskikh karpyshev multi-sensor large-scale dataset multi-view reconstruction. proceedings ieeecvf conference computer vision pattern recognition wang galliani vogel patchmatchnet learned multi-view patch- match stereo. proceedings ieeecvf conference computer vision pattern recognition, wang wang long neuris neural reconstruction indoor scenes using normal priors. proceedings european conference computer vision, wang liu liu neus learning neural implicit surfaces volume rendering multi-view reconstruction. proceedings annual conference neural information processing systems wang fouhey gupta designing deep networks surface normal estimation. proceedings ieee conference computer vision pattern recognition, wang skorokhodov wonka hf- neus improved surface reconstruction using high-frequency details. advances neural information processing systems wang han habermann neus fast learning neural implicit surfaces multi-view reconstruction. proceedings ieeecvf international confer- ence computer vision, wang skorokhodov wonka pet- neus positional encoding triplanes neural surfaces. proceedings ieeecvf conference computer vision pattern recognition wei ding revisiting super- vised learning-based photometric stereo net- works. ieee transactions pattern analysis machine intelligence woodham photometric method determining surface orientation multiple images. optical engineering ganesh shi robust pho- tometric stereo via low-rank matrix completion recovery. proceedings asian conference computer vision, kong tao multi-scale geometric consistency guided planar prior assisted multi-view stereo. ieee transactions pattern analysis machine intelligence yang chen chen ps-nerf neural inverse rendering multi-view pho- tometric stereo. proceedings european conference computer vision, yang yuan wilber poly- max general dense prediction mask trans- former. proceedings ieeecvf win- ter conference applications computer vision, yao luo mvsnet depth inference unstructured multi-view stereo. proceedings european conference computer vision, yariv kasten moran multi- view neural surface reconstruction disen- tangling geometry appearance. advances neural information processing systems yariv kasten volume rendering neural implicit surfaces. pro- ceedings annual conference neural information processing systems qiu stablenor- mal reducing diffusion variance stable sharp normal. acm transactions graphics meka elgharib self- supervised outdoor scene relighting. pro- ceedings european conference computer vision, peng niemeyer monosdf exploring monocular geometric cues neu- ral implicit surface reconstruction. advances neural information processing systems zhang yao critical regu- larizations neural surface reconstruction wild. proceedings ieeecvf conference computer vision pattern recognition, zhang luo vis- mvsnet visibility-aware multi-view stereo net- work. international journal computer vision zhang towards unbiased volume rendering neural implicit surfaces geometry priors. proceedings ieeecvf conference computer vision pattern recognition, zhao lichy perrin pn, mvpsnet fast generalizable multi-view pho- tometric stereo. proceedings ieeecvf international conference com- puter vision,", "published_date": "2025-06-04T16:09:16+00:00"}
{"id": "2506.03710v1", "title": "OSGNet @ Ego4D Episodic Memory Challenge 2025", "authors": ["Yisen Feng", "Haoyu Zhang", "Qiaohui Chu", "Meng Liu", "Weili Guan", "Yaowei Wang", "Liqiang Nie"], "summary": "report, present champion solutions three egocentric video localization tracks egod episodic memory challenge cvpr tracks require precise localization interval within untrimmed egocentric video. previous unified video localization approaches often rely late fusion strategies, tend yield suboptimal results. address this, adopt early fusion-based video localization model tackle three tasks, aiming enhance localization accuracy. ultimately, method achieved first place natural language queries, goal step, moment queries tracks, demonstrating effectiveness. code found", "full_text": "cs.cv jun osgnet egod episodic memory challenge yisen feng, haoyu zhang qiaohui chu meng liu, weili guan, yaowei wang liqiang nie harbin institute technology shenzhen pengcheng laboratory shandong jianzhu university yisenfeng.hit, zhang.hy., qiaohuichu, mengliu.sdu, honeyguan, nieliqianggmail.com wangywpcl.ac.cn abstract report, present champion solutions three egocentric video localization tracks egod episodic memory challenge cvpr tracks require precise localization interval within untrimmed egocentric video. previous unified video lo- calization approaches often rely late fusion strategies, tend yield suboptimal results. address this, adopt early fusion-based video localization model tackle three tasks, aiming enhance localization accuracy. ultimately, method achieved first place natural language queries, goal step, mo- ment queries tracks, demonstrating effectiveness. code found fengosgnet. introduction egocentric video moment localization basis many egocentric video understanding tasks, underpins range innovative applications intelligent assistant systems smart glasses memory retrieval modules embodied how- ever, unlike traditional exocentric video moment localiza- tion task faces unique challenges due rapid viewpoint shifts emphasis fine-grained ob- jects episodic memory challenge egod ego- centric video moment localization framed three task variants. natural language queries nlq, involves lo- cating relevant moment egocentric video given natural language query. goalstep-step grounding requires identifying egocentric video moment corre- sponding described step. moment queries mq, focuses localizing predefined actions within egocen- tric video. although tasks differ input formulation, share common objective precisely identifying temporal intervals within egocentric videos often rely similar model architectures. recent works explore unified frameworks capa- ble handling different localization tasks simultaneously. however, pursuit efficiency, approaches typically adopt late fusion multimodal fusion, resulting subop- timal localization performance. address limitation, adapt previously proposed state-of-the-art early fu- sion methods egocentric moment localization, aim- ing achieve better localization performance. based powerful model, surpassed last years champion solution three tracks first place years challenge. specifically, nlq, model outperforms egovideo achieving im- provement rank iou.. goal step, model surpasses bayesianvslnet in- crease rank iou.. mq, model achieves improvement mean average precision map causaltad methodology task definition video moment localization task formally defined follows given video consisting frames natural language query step descrip- tion, action category, etc. contain- ing words, objective identify temporal seg- ment ts, within video best corresponds semantic content query. track natural language queries approach. illustrated figure adopt previ- ously proposed grounding model, osgnet address challenge insufficient fine-grained object information nlq task. osgnet enhances video representations incorporating object-level details extraction predict head multi-scale network bimamba mlp cross attention object attention ffn ffn gate fusion main branch text encoder query location see shovel? video object encoder text backbone clip video backbone feature extraction mlp shot aggregator text aggregator mlp narration puts plywood dustbin looks around compound holds miter saw turns around walks towards stairs shot branch positive shot negative shot video frame object selection query location see shovel? object shovel object annotations object extraction co-detr multi-modal fusion contrastive learning image class shovel bucket jar score regress head object extraction sim matrix figure framework osgnet encoding features video, text, detected objects, thereby enabling accurate moment localiza- tion. framework consists two key components main branch fuses video, text, object features improve localization precision, shot branch pro- cesses video-only features generate shot-level representa- tions, leveraging contrastive learning textual inputs strengthen cross-modal alignment. architectural specifics, please refer original work implementation details. feature extraction utilize egovideo internvideo extract video features dividing video multiple non-overlapping snip- pets, consisting frames. text feature extrac- tion, use egovideo clip independently, represent- ing word sentence corresponding fea- ture discarding additional cls token. train- ing setup following experimental setup osgnet, first train osgnet without shot branch object features named osgnet-baseline utilizing naq pretraining strategy improve efficiency. pretrain- ing phase, learning rate set batch size main training phase, learning rate in- creased .e- maintaining batch size. model trained epochs total, first epochs designated warm-up phase. results. shown table model outperforms last years champion solution using training set success example drill machine? failure example put fire gun? ground truth osgnet figure two examples validation set nlq. single model. subsequently, improved perfor- mance integrating multiple model variants incorpo- rating validation set training process, achiev- ing gain iou. single model. specifically, integrated ensemble comprises four model variants original osgnet, osgnet without shot branches, osgnet utilizing internvideo features, osgnet augmented additional cross-modal atten- tion layers multi-scale network. case analysis. shown figure first example, model accurately identifies time interval drill machine appears, demonstrating effective use table performance nlq. results utilize ensemble strategy. method validation test egovideo egovideo osgnet osgnet table performance goal step. results utilize ensemble strategy. method validation test bayesianvslnet osgnet osgnet fine-grained object information. however, second ex- ample, model fails correctly localize fire gun, object detector trained object cate- gory. highlights methods limitation relying coverage capability underlying object detec- tor. track goalstep step grounding approach. similar nlq, utilize osgnet-baseline grounding, employ egovideo extracting video text features. implementation details. follow feature ex- traction pretraining settings used nlq. main training phase, increase learning rate .e- use batch size given excessive length in- put videos, apply random cropping cap number video frame features manage computational cost. inference, incorporate order prior introduced bayesianvslnet results. table presents performance comparison goal step. single model outperforms last years champion solution substantial margin iou.. performance enhanced additional model ensemble in- corporation validation set. specifically, ensem- ble combines three model variants osgnet-baseline, osgnet-baseline full-length video feature input, osgnet-baseline augmented additional cross- modal attention layers multi-scale network denoted osgnet-baseline. case analysis. shown figure first exam- ple, model accurately localizes action pouring flour mixer, demonstrating strong localiza- success example pours flour mixer. failure example organize table. ground truth osgnet figure two examples validation set goal step. table performance mq. results utilize ensemble strat- egy. represents iou. metric. method validation test map map causaltad osgnet osgnet tion capability. however, second example, model fails correctly identify action organizing table. speculate model recognizes ac- tion organize object table, misinterprets action organizing drawer organizing table, in- dicating limited understanding subject-object rela- tionship within action. track moment queries approach. employ model features grounding goalstep. since temporal action lo- calization task, convert text-based video mo- ment localization task treating predefined action cat- egories text. implementation details. follow settings nlq extract features pretrain. training, learning rate set batch size results. table presents performance comparison benchmark. single model, trained solely example take photo record video camera ground truth prediction take photo record video camera prediction use phone figure example validation set mq. training set, outperforms last years winning solution. incorporating validation set applying model en- sembling, performance improved map. specifically, ensemble includes three model vari- ants osgnet-baseline, osgnet-baseline trained training set, osgnet-baseline variant, includes additional cross-modal attention layers also trained solely training set. case analysis. shown figure model demon- strates strong localization capabilities, accurately identify- ing video moment corresponding correct label. however, also assigns wrong label seg- ments, suspect stems insufficient under- standing video content. conclusion report presents methods results nlq, goal step, tracks egod challenge cvpr primary contribution application state- of-the-art retrieval-based model temporal action localiza- tion tasks, achieves superior performance compared conventional localization methods. despite effective- ness, approach certain limitations. reformulating action localization retrieval problem leads increased training inference time, suggesting future work explore strategies better balance performance computational efficiency. references guo chen, sen xing, zhe chen, wang, kunchang li, yizhuo li, liu, jiahao wang, yin-dong zheng, bingkun huang, al. internvideo-egod pack champion solu- tions egod challenges. arxiv preprint pages qiaohui chu, haoyu zhang, yisen feng, meng liu, weili guan, yaowei wang, liqiang nie. technical report egod long-term action anticipation challenge arxiv preprint yisen feng, haoyu zhang, yuquan xie, zaijing li, meng liu, liqiang nie. objectnlq egod episodic mem- ory challenge arxiv preprint yisen feng, haoyu zhang, meng liu, weili guan, liqiang nie. object-shot enhanced grounding network egocentric video. arxiv preprint kristen grauman, andrew westbury, eugene byrne, zachary chavis, antonino furnari, rohit girdhar, jackson hamburger, hao jiang, miao liu, xingyu liu, al. egod around world hours egocentric video. pro- ceedings ieeecvf conference computer vision pattern recognition, pages weili guan, xuemeng song, haoyu zhang, meng liu, chung-hsing yeh, xiaojun chang. bi-directional het- erogeneous graph hashing towards efficient outfit recom- mendation. proceedings acm international conference multimedia, pages zhijian hou, lei ji, difei gao, wanjun zhong, kun yan, chao li, wing-kwong chan, chong-wah ngo, nan duan, mike zheng shou. groundnlq egod natural language queries challenge arxiv preprint pages meng liu, xiang wang, liqiang nie, xiangnan he, bao- quan chen, tat-seng chua. attentive moment retrieval videos. international acm sigir conference research development information retrieval, pages meng liu, xiang wang, liqiang nie, tian, baoquan chen, tat-seng chua. cross-modal moment localiza- tion videos. proceedings acm international conference multimedia, pages shuming liu, lin sui, chen-lin zhang, fangzhou mu, chen zhao, bernard ghanem. harnessing temporal causal- ity advanced temporal action detection. arxiv preprint baoqi pei, guo chen, jilan xu, yuping he, yicheng liu, kanghua pan, yifei huang, yali wang, tong lu, limin wang, al. egovideo exploring egocentric founda- tion model downstream adaptation. arxiv preprint pages carlos plou, lorenzo mur-labadia, ruben martinez-cantin, ana murillo. carlor egod step grounding challenge bayesian temporal-order priors test time re- finement. arxiv preprint pages santhosh kumar ramakrishnan, ziad al-halah, kris- ten grauman. naq leveraging narrations queries su- pervise episodic memory. proceedings ieeecvf conference computer vision pattern recognition, pages ieee computer society, yale song, eugene byrne, tushar nagarajan, huiyu wang, miguel martin, lorenzo torresani. egod goal-step to- ward hierarchical understanding procedural activities. ad- vances neural information processing systems, yunxiao wang, meng liu, rui shao, haoyu zhang, bin wen, fan yang, tingting gao, zhang, liqiang nie. time temporal-sensitive multi-dimensional instruc- tion tuning benchmarking video-llms. arxiv preprint yingsen zeng, yujie zhong, chengjian feng, lin ma. unimd towards unifying moment retrieval temporal ac- tion detection. european conference computer vision, pages springer, chen-lin zhang, lin sui, shuming liu, fangzhou mu, zhangcheng wang, bernard ghanem. timeloc uni- fied end-to-end framework precise timestamp localiza- tion long videos. arxiv preprint haoyu zhang, meng liu, zan gao, xiaoqiang lei, yinglong wang, liqiang nie. multimodal dialog system rela- tional graph-based context-aware question understanding. proceedings acm international conference multimedia, page association computing ma- chinery, haoyu zhang, meng liu, zan gao, xiaoqiang lei, yinglong wang, liqiang nie. multimodal dialog system rela- tional graph-based context-aware question understanding. proceedings acm international conference multimedia, pages haoyu zhang, meng liu, yuhong li, ming yan, zan gao, xiaojun chang, liqiang nie. attribute-guided collab- orative learning partial person re-identification. ieee transactions pattern analysis machine intelligence, haoyu zhang, meng liu, yaowei wang, cao, weili guan, liqiang nie. uncovering hidden connections iterative tracking reasoning video-grounded dialog. arxiv preprint haoyu zhang, meng liu, zixin liu, xuemeng song, yaowei wang, liqiang nie. multi-factor adaptive vision selec- tion egocentric video question answering. proceedings international conference machine learning, pages pmlr, haoyu zhang, yuquan xie, yisen feng, zaijing li, meng liu, liqiang nie. hcqa egod egoschema challenge arxiv preprint haoyu zhang, qiaohui chu, meng liu, yunxiao wang, bin wen, fan yang, tingting gao, zhang, yaowei wang, liqiang nie. exoego exocentric knowledge guided mllm egocentric video understanding. arxiv preprint haoyu zhang, yisen feng, qiaohui chu, meng liu, weili guan, yaowei wang, liqiang nie. hcqa- egod egoschema challenge arxiv preprint", "published_date": "2025-06-04T08:41:42+00:00"}
{"id": "2506.02875v1", "title": "NTIRE 2025 XGC Quality Assessment Challenge: Methods and Results", "authors": ["Xiaohong Liu", "Xiongkuo Min", "Qiang Hu", "Xiaoyun Zhang", "Jie Guo", "Guangtao Zhai", "Shushi Wang", "Yingjie Zhou", "Lu Liu", "Jingxin Li", "Liu Yang", "Farong Wen", "Li Xu", "Yanwei Jiang", "Xilei Zhu", "Chunyi Li", "Zicheng Zhang", "Huiyu Duan", "Xiele Wu", "Yixuan Gao", "Yuqin Cao", "Jun Jia", "Wei Sun", "Jiezhang Cao", "Radu Timofte", "Baojun Li", "Jiamian Huang", "Dan Luo", "Tao Liu", "Weixia Zhang", "Bingkun Zheng", "Junlin Chen", "Ruikai Zhou", "Meiya Chen", "Yu Wang", "Hao Jiang", "Xiantao Li", "Yuxiang Jiang", "Jun Tang", "Yimeng Zhao", "Bo Hu", "Zelu Qi", "Chaoyang Zhang", "Fei Zhao", "Ping Shi", "Lingzhi Fu", "Heng Cong", "Shuai He", "Rongyu Zhang", "Jiarong He", "Zongyao Hu", "Wei Luo", "Zihao Yu", "Fengbin Guan", "Yiting Lu", "Xin Li", "Zhibo Chen", "Mengjing Su", "Yi Wang", "Tuo Chen", "Chunxiao Li", "Shuaiyu Zhao", "Jiaxin Wen", "Chuyi Lin", "Sitong Liu", "Ningxin Chu", "Jing Wan", "Yu Zhou", "Baoying Chen", "Jishen Zeng", "Jiarui Liu", "Xianjin Liu", "Xin Chen", "Lanzhi Zhou", "Hangyu Li", "You Han", "Bibo Xiang", "Zhenjie Liu", "Jianzhang Lu", "Jialin Gui", "Renjie Lu", "Shangfei Wang", "Donghao Zhou", "Jingyu Lin", "Quanjian Song", "Jiancheng Huang", "Yufeng Yang", "Changwei Wang", "Shupeng Zhong", "Yang Yang", "Lihuo He", "Jia Liu", "Yuting Xing", "Tida Fang", "Yuchun Jin"], "summary": "paper reports ntire xgc quality assessment challenge, held conjunction new trends image restoration enhancement workshop ntire cvpr challenge address major challenge field video talking head processing. challenge divided three tracks, including user generated video, generated video talking head. user-generated video track uses finevd-gc, contains user generated videos. user-generated video track total registered participants. total submissions received development phase, submissions received test phase. finally, participating teams submitted models fact sheets. generated video track uses q-eval-video, contains ai-generated videos aigvs generated popular text-to-video models. total participants registered track. total submissions received development phase, submissions received test phase. finally, participating teams submitted models fact sheets. talking head track uses thqa-ntire, contains talking heads. total participants registered track. total submissions received development phase, submissions received test phase. finally, participating teams submitted models fact sheets. participating team every track proposed method outperforms baseline, contributed development fields three tracks.", "full_text": "cs.cv jun ntire xgc quality assessment challenge methods results xiaohong liu xiongkuo min qiang xiaoyun zhang jie guo guangtao zhai shushi wang yingjie zhou liu jingxin liu yang farong wen yanwei jiang xilei zhu chunyi zicheng zhang huiyu duan xiele yixuan gao yuqin cao jun jia wei sun jiezhang cao radu timofte baojun jiamian huang dan luo tao liu weixia zhang bingkun zheng junlin chen ruikai zhou meiya chen wang hao jiang xiantao yuxiang jiang jun tang yimeng zhao zelu chaoyang zhang fei zhao ping shi lingzhi heng cong shuai rongyu zhang jiarong zongyao wei luo zihao fengbin guan yiting xin zhibo chen mengjing wang tuo chen chunxiao shuaiyu zhao jiaxin wen chuyi lin sitong liu ningxin chu jing wan zhou baoying chen jishen zeng jiarui liu xianjin liu xin chen lanzhi zhou hangyu han bibo xiang zhenjie liu jianzhang jialin gui renjie shangfei wang donghao zhou jingyu lin quanjian song jiancheng huang yufeng yang changwei wang shupeng zhong yang yang lihuo jia liu yuting xing tida fang yuchun jin abstract paper reports ntire xgc quality assessment challenge, held conjunction new trends image restoration enhance- ment workshop ntire cvpr challenge address major challenge field video talk- ing head processing. challenge divided three tracks, including user generated video, generated video talking head. user-generated video track uses finevd-gc, contains user generated videos. user- generated video track total registered partic- ipants. total submissions received de- velopment phase, submissions received test phase. finally, participating teams submitted models fact sheets. generated video track uses q-eval-video, contains ai-generated videos aigvs gen- erated popular text-to-video models. total participants registered track. total submissions received development phase, submissions received test phase. finally, par- organizers ntire xgc quality challenge. ticipating teams submitted models fact sheets. talking head track uses thqa-ntire, contains talking heads. total participants registered track. total submissions received development phase, submissions received test phase. finally, par- ticipating teams submitted models fact sheets. participating team every track proposed method outperforms baseline, con- tributed development fields three tracks. introduction rapid development video generation technolo- gies, user-generated videos ugvs, ai-generated videos aigvs, talking head become widely used various applications. however, quality videos vary significantly due differences capture condi- tions, generation models, animation techniques. there- fore, crucial develop effective video quality as- sessment vqa methods accurately evaluate visual quality ugvs, aigvs, talking head, ensuring bet- ter user experience reliable performance real-world scenarios. robust quality assessment framework help identify distortions, enhance generation techniques, op- timize models improved visual fidelity realism. ntire xgc quality assessment challenge aims promote development quality assessment methods videos talking heads guide improve- ment enhancement video capture, compression, processing techniques performance generative models. challenge divided three tracks, includ- ing user generated video track, generated video track talking head track. user generated video track, use finevd-gc contains user generated videos. subjects invited produce accurate mean opinion scores moss. generated video track uses q-eval-video popular text-to- video models used generate videos. sample scrutinize strategy employed dur- ing dataset annotation process make sure quality accuracy dataset. talking head track uses thqa-ntire contains talking heads. challenge total registered participants, user generated video track, gener- ated video track talking head track. total submissions received development phase, prediction results submitted fi- nal testing phase. finally, valid participating teams user generated video track, valid participating teams generated video track valid participating teams talking head track submitted final models fact sheets. provided detailed introductions quality assessment methods. provide detailed results challenge section section hope challenge promote development quality assessment video talking head. challenge one ntire work- shop associated challenges ambient lighting normaliza- tion reflection removal wild shadow re- moval event-based image deblurring image de- noising xgc quality assessment ugc video en- hancement night photography rendering image super-resolution real-world face restoration efficient super-resolution depth estimation efficient burst hdr restoration cross-domain few- shot object detection short-form ugc video quality assessment enhancement text image gen- eration model quality assessment day night rain- drop removal dual-focused images video quality assessment video conferencing low light image enhancement light field super-resolution restore image model raim wild raw restoration super-resolution raw reconstruction rgb smartphones related work user generated vqa dataset years, researchers developed various video quality assessment vqa datasets analyze human visual perception characteristics. initial datasets primarily exam- ined synthetic degradations, employing restricted original content manually simulated degradation patterns rise user-generated content ugc plat- forms, contemporary research shifted toward creating vqa databases capture genuine quality issues encoun- tered practical scenarios. multiple studies specifically addressed real-world quality deterioration occurring content capture natural viewing envi- ronments. comprehensive datasets incorporated simulated authentic distortion types broaden research scope. existing ugc col- lections predominantly source materials conventional platforms like youtube, emerging datasets like kvq specifically target short-format video content. pro- posed finevd-gc expands landscape encompass- ing diverse video formats including on-demand streaming, conventional ugc, short-form media. unlike existing databases providing singular quality ratings, finevd-gcs multi-dimensional annotations enable broader practical im- plementations detailed quality characterization. generated vqa dataset compared user generated video quality assessment datasets, number proposed generated video aigv datasets small. chivileva al. proposes dataset videos generated models. users involved subjective study. evalcrafter builds dataset using prompts models, re- sulting videos total. however, users involved subjective study. similarly, fetv uses prompts, models, users annota- tion well. vbench larger scale to- tal prompts models. continuing exploration aigv quality assessment, tvqa- emerges significant addition landscape. dataset videos generated different models. subjects invited collect moss. track, use latest dataset, q-eval-video contains approximately videos generated dif- ferent models. meanwhile, sample scrutinize strat- egy employed dataset annotation process. vqa model traditional vqa models usually designed user- generated videos certain attribute videos. example, simplevqa trains end-to-end spatial feature extraction network directly learn quality-aware spatial features video frames, extracts motion features measure temporally related distortions time predict video qual- ity. fast-vqa proposes fragments sampling strategies fragment attention network fanet accommodate fragments inputs. light-vqa light-vqa provide methods assessing quality videos enhanced low-light conditions. dover evaluates quality videos technical aes- thetic perspectives respectively. q-align also ad- dress vqa task relying ability multi-modal large models vqa explores approach utilizing multi-modal large models video quality assessment visual question answering. several works targeting vqa tasks aigvs. vbench evalcrafter q-bench- video build benchmarks aigvs designing multi-dimensional metrics. maxvqa fetv propose separate metrics assessment video-text alignment video fidelity, tvqa handles features two dimensions whole. newly emerged model, q-eval-score explores use multimodal large language models mllms assess- ing quality aigv. vqa tasks aigv, fur- ther research still needed. believe development models aigv certainly benefit generation high-quality videos. talking head talking heads emerging form human-centered me- dia, distinguished integration realistic facial vocal features conventional approach de- signing talking heads predominantly relies facial cap- ture technology, wherein designers utilize software map facial bones fine-tune facial details specific digital persona, based captured facial data manual technique yield high-quality talk- ing heads, substantial costs associated required equipment, coupled complexity operation, significantly constrain efficiency design process. address challenges associated talking head design, range ai-based methods developed. methods categorized based type data used generate talking heads, distinguishing generative generative talk- ing heads furthermore, genera- tion techniques classified vision-driven speech-driven approaches, depending fundamental principles be- hind generation. given current prominence talking head generation active area research, antici- pated effective methods continue emerge. however, existing quality assessments talking heads often limited subjective evaluations traditional objective quality metrics, psnr ssim approaches provide insights qual- ity talking heads, notable limitations. sub- jective assessments typically time-consuming conducive large-scale quantitative analysis, objec- tive metrics like psnr ssim fail capture human visual experiences inadequate evaluating gener- ative talking heads due absence reference data. therefore, development accurate reliable objective quality assessment framework talking heads crucial advancing field talking head generation. digital human quality assessment rapid advancement digital human technology, quality digital humans garnered significant atten- tion. explore issue greater depth, zhang al. developed several datasets, including dhhqa ddh-qa sjtu-hd focusing cap- tured digital humans. datasets provide rich data assessing quality static heads, dynamic full-body digital humans static full-body digital humans. addi- tionally, designed full-reference reduced- reference no-reference evaluation methods datasets, incorporating siamese networks multi- task learning multi-modal information fusion techniques. approaches offer re- liable assessment frameworks various types digital humans also account different applicability scenar- ios. furthermore, investigate potential quality degrada- tion communication transmission, zhou al. zhang al. conducted user experience quality assess- ments talking heads talking digital humans, respectively. first established thqa-d g-dtqa datasets proposed corresponding ob- jective evaluation algorithms integrate channel param- eters, visual features, audio features. despite ad- vancements, existing datasets digital human quality as- sessment often constrained limited data size in- sufficient diversity digital human models, turn restricts generalizability assessment algorithms. recent years, rapid growth generative en- abled efficient solutions designing acquiring digital humans response, zhou al. developed first thqa dataset speech-driven talking heads. dataset includes talking heads generated applying eight representative speech-driven algorithms images. dataset introduces talking head quality assessment challenge, unfortu- nately provide reliable quality assessment frame- work. address gap, present work seeks estab- lish comprehensive evaluation scheme emerging media engaging experts discussions develop- ment appropriate assessment methodology. ntire xgc quality assessment challenge organize ntire xgc quality assessment challenge, including user generated video quality assess- ment, generated video quality assessment talking head quality assessment, order promote develop- ment objective quality assessment methods. main goal challenge predict perceptual quality videos talking heads. details challenge follows overview challenge three tracks, i.e. user generated video track, generated video track talking head track. task predict perceptual quality video talking head based set prior examples perceptual quality labels. challenge uses finevd-gc eval-video thqa dataset splits training, validation, testing sets. final result, participants challenge asked submit predicted scores given testing set. datasets user-generated video track, use new dataset called fine-grained video database generated content finevd-gc comprises total web- crawled ugc videos sourced youtube, tiktok, bilibili. video randomly clipped -second segments. initially employ fastvqa assess video quality. based distribution qual- ity scores, uniformly sample videos span wide range categories exhibit diverse spectrum qual- ity. videos subsequently manually filtered en- sure comprehensive representation various distortion types. subjects invited rate videos finevd- gc. normalizing averaging subjective opinion scores, mean opinion score mos video obtained. furthermore, randomly split finevd-gc training set, validation set, testing set accord- ing ratio numbers videos training set, validation set, testing set respectively. generated video track, use q-eval- video dataset contains generated videos cogvideox gen- gen- latte kling dreamina luma pix- verse pika svd vidu videos generated using approximately prompts sam- pled vbench, evalcrafter, tvcompench, vide- ofeedback. every video resolution unified video length talking head track, utilize thqa-ntire dataset training, validation, testing. dataset in- tegrates extends existing thqa thqa- datasets, comprising total talking heads. specifically, includes generative talk- ing heads talking heads, providing com- prehensive dataset development unified talking head quality assessment framework. talking heads dataset contain audio information exhibit diverse range resolutions durations, thereby posing increased challenges accurate robust quality assessment. evaluation protocol tracks, main scores utilized determine rankings participating teams. ignore sign cal- culate average spearman rank-order correlation co- efficient srcc person linear correlation coefficient plcc main score athrm main core mathrm srcc mathrm plcc. srcc measures prediction monotonicity, plcc measures prediction accuracy. better quality assessment methods larger srcc plcc values. be- fore calculating plcc index, perform third-order polynomial nonlinear regression. combining srcc plcc, main scores comprehensively measure performance participating methods. challenge phases tracks consist two phases developing phase testing phase. developing phase, participants access generated imagesvideos training set corresponding prompts moss. participants familiar dataset structure develop meth- ods. also release generated images videos validation set corresponding prompts without corresponding moss. participants utilize meth- ods predict quality scores validation set upload results server. participants receive immediate feedback analyze effectiveness methods validation set. validation leaderboard available. testing phase, participants ac- cess images videos testing set corre- sponding prompts without corresponding moss. partic- ipants need upload final predicted scores testing set challenge deadline. participating team needs submit source codeexecutable fact sheet, detailed description file proposed method corresponding team information. final results sent participants. table quantitative results ntire xgc quality assessment challenge track user generated video. color, noise, artifact, blur, temporal, overall indicate main scores dimension. rank team leader color noise artifact blur temporal overall main score srcc plcc slcv baojun sjtu-moe-ai weixia zhang mivqa ruikai zhou xgc-go xiantao foodvqa yimeng zhao baseline fastvqa table quantitative results ntire quality assessment ai-generated content challenge track generated video. rank team leader main score srcc plcc slcv baojun cuc-imc zelu opdai lingzhi magnolia zongyao aigc vqa wei luo sjtu-moe-ai bingkun zheng baseline q-eval-score dover tvqa table quantitative results ntire xgc quality assessment track talking head. rank team leader main score srcc plcc team mengjing mediaforensics baoying chen autohome aigc xin chen ustc-ac zhenjie liu sjtu-moe-ai junlin chen focusq donghao zhou njust-kmg shupeng zhong xidian-vqateam lihuo baseline simplevqa challenge results teams user generated video track, teams generated video track teams talking head track submitted final codesexecutables fact sheets. table table tabel summarize main results important information valid teams. de- tailed information participating teams al- gorithms found supplementary materials. baselines compare performance submitted methods several quality assessment methods testing set, in- cluding fastvqa q-eval-score dover tvqa simplevqa three tracks. result analysis main results teams methods baseline methods shown table table table slcv sjtu-moe-ai mivqa xgc-go foodvqa figure scatter plots predicted scores vs. moss user-generated video track. curves obtained four-order polynomial nonlinear fitting. seen three tracks, results baseline methods ideal testing set three datasets, submitted methods achieved better results. means methods closer human visual perception used evaluate content. user generated video track, teams achieve main score higher teams higher generated video track, teams achieve main score higher teams higher champi- onship team higher talking head track, teams achieve main score higher baseline, teams higher meantime, top-ranked teams small difference main score. fig- ures show scatter plots predicted scores versus moss teams methods testing set. curves obtained polynomial nonlinear fitting. observe predicted scores obtained top team methods higher correlations moss. track figure intuitively shows performance teams methods. results demonstrate ef- fectiveness submitted methods improving quality assessment across tracks, highlighting potential figure scatter plots predicted scores vs. moss generated video track. curves obtained four-order polynomial nonlinear fitting. better alignment human perception. challenge winner methods user-generated video track team slcv wins championship user-generated video track. unlike conventional approaches rely regression classification video quality assessment e.g., liqe q-align fast-vqa simplevqa method leverages multimodal large language model mllm estimate video quality. internvl effective data filtering process introduced, leveraging large language model llm scoring evaluate remove low-quality samples, thereby improving overall quality training data. inspired capability internvl assess data quality using llm-based scoring, adopt multimodal large language model mllm estimating video quality work. specifically, directly utilize internvl model mllm achieve robust reliable video quality assessment overcome limitation spatial domain, introduce spatial window sampling data augmentation strategy. specifically, employ sliding window approach crops original video frames window size set videos longest side. method effectively triples amount training data, thereby enhancing models ability learn fine-grained spatial features. employ lora low-rank adaptation method efficiently fine-tune internvl model, enabling perform six fine-grained quality assessments. inference, data processing strategy used training applied test videos. specifically, model inde- pendently predicts quality scores three sub-videos generated sliding window sampling process. final prediction obtained averaging results across sub-videos. approach ensures robust training also facilitates accurate reliable evaluation fine-grained video quality. generated video track team slcv final winner generated video track. propose temporal pyramid sampling, ad- team mediaforensics autohome aigc ustc-ac sjtu-moe-ai focusq njust kmg xidian-vqateam team performance srcc plcc figure performance methods proposed different teams talking head track. dress unique challenges posed ai-generated videos quality assessment. unlike user generated video, quality assessment generated videos primar- ily focuses two core aspects smoothness object motion authenticity content. effectively capture critical metrics, team design temporal pyramid sampling method capture dynamic charac- teristics videos multiple temporal resolutions. achieved performing multi-scale frame interval sampling varying frequencies. original video sampled dif- ferent frame rates lengths, generating multiple subsets data diverse temporal granularities. subset used independently train model, enabling learn distinct motion smoothness content authenticity features different temporal scales. talking head track team wins champion talking head track. proposed novel video quality assess- ment model based multimodal feature representations, comprising four modules spatial feature extraction, tempo- ral feature extraction, audio feature extraction, audio- visual fusion. visual distortions categorized spa- tial motion distortions. types visual distortions videos roughly divided two categories spa- tial distortion motion distortion. first, talking head videos split clips spatial temporal feature extraction.whole clip utilized temporal fea- ture extraction fixed pretrained d-cnn backbone slowfast.the first frame clip used spatial feature extraction.the spatial feature extraction module uti- lizes efficient channel attention module eca-net, effectively achieve cross-channel interaction,and uti- lize swintransformer-tiny extract visual features first frame. audio feature extraction, audio aligned visual frames, four techniqueschromagram, cqt, mfcc, gfccare used extract time-frequency features. features stacked channels fed separable convolution network frequency, time, fusion blocks, consisting convd layers, batchnorm, maxpool. frequency time blocks use kernels, respectively, perform spa- tially separable convolutions, reducing parameters. tem- poral information processed using bi-lstm, cap- tures context past future sequences. finally, features fused quality score using fully con- nected layers. videos divided -second clips, clips se- lected via cyclic sampling. swin transformer extracts spatial features patches, slowfast extracts temporal features resized clips. acknowledgments work partially supported humboldt foun- dation. thank ntire sponsors bytedance, meituan, kuaishou, university wurzburg computer vision lab. references introducing gen- alpha new frontier video genera- tion. alpha, madhav agarwal, rudrabha mukhopadhyay, vinay namboodiri, jawahar. audio-visual face reen- actment. proceedings ieeecvf winter confer- ence applications computer vision, pages luma ai. dream machine video generator. pixverse ai. pixverse video creation platform. vidu team. vidu ai. andreas blattmann, tim dockhorn, sumith kulal, daniel mendelevitch, maciej kilian, dominik lorenz, yam levi, zion english, vikram voleti, adam letts. stable video diffusion scaling latent video diffusion models large datasets. arxiv preprint., stella bounareli, vasileios argyriou, georgios tz- imiropoulos. finding directions gans latent space neural face reenactment. arxiv preprint stella bounareli, christos tzelepis, vasileios argyriou, ioannis patras, georgios tzimiropoulos. stylemask disentangling style space stylegan neural face reenactment. ieee international conference automatic face gesture recognition fg, pages ieee, dreamina capcut. dreamina. zheng cai, maosong cao, haojiong chen, kai chen, keyu chen, xin chen, xun chen, zehui chen, zhi chen, pei chu, xiaoyi dong, haodong duan, fan, zhaoye fei, yang gao, jiaye ge, chenya gu, yuzhe gu, tao gui, aijia guo, qipeng guo, conghui he, yingfan hu, ting huang, tao jiang, penglong jiao, zhenjiang jin, zhikai lei, jiax- ing li, jingwen li, linyang li, shuaibin li, wei li, yining li, hongwei liu, jiangning liu, jiawei hong, kaiwen liu, kuikun liu, xiaoran liu, chengqi lv, haijun lv, kai lv, ma, runyuan ma, zerun ma, wenchang ning, linke ouyang, jiantao qiu, yuan qu, fukai shang, yunfan shao, demin song, zifan song, zhihao sui, peng sun, sun, huanze tang, bin wang, guoteng wang, jiaqi wang, jiayu wang, rui wang, yudong wang, ziyi wang, xingjian wei, qizhen weng, fan wu, yingtong xiong, chao xu, ruil- iang xu, hang yan, yirong yan, xiaogui yang, haochen ye, huaiyuan ying, jia yu, jing yu, yuhang zang, chuyu zhang, zhang, pan zhang, peng zhang, ruijie zhang, shuo zhang, songyang zhang, wenjian zhang, wenwei zhang, xingcheng zhang, xinyue zhang, hui zhao, qian zhao, xiaomeng zhao, fengzhe zhou, zaida zhou, jing- ming zhuo, yicheng zou, xipeng qiu, qiao, dahua lin. internlm technical report, shi chen, zicheng zhang, yingjie zhou, wei sun, xiongkuo min. no-reference quality assessment metric dynamic digital human. displays, zheng chen, kai liu, jue gong, jingkai wang, lei sun, zongwei wu, radu timofte, yulun zhang, al. ntire challenge image super-resolution methods results. proceedings ieeecvf conference computer vision pattern recognition cvpr work- shops, zheng chen, jingkai wang, kai liu, jue gong, lei sun, zongwei wu, radu timofte, yulun zhang, al. ntire challenge real-world face restoration methods results. proceedings ieeecvf conference computer vision pattern recognition cvpr work- shops, kun cheng al. videoretalking audio-based lip synchro- nization talking head video editing wild. sig- graph asia, iya chivileva, philip lynch, tomas ward, alan smeaton. measuring quality text-to-video model outputs metrics dataset. arxiv preprint marcos conde, radu timofte, al. ntire challenge raw image restoration super-resolution. proceed- ings ieeecvf conference computer vision pattern recognition cvpr workshops, marcos conde, radu timofte, al. raw image reconstruc- tion rgb smartphones. ntire challenge re- port. proceedings ieeecvf conference com- puter vision pattern recognition cvpr workshops, francesca simone, marco tagliasacchi, matteo nac- cari, stefano tubaro, touradj ebrahimi. avc video database evaluation quality metrics. ieee international conference acoustics, speech signal processing icassp, pages ieee, yunlong dong, xiaohong liu, yixuan gao, xunchu zhou, tao tan, guangtao zhai. light-vqa multi- dimensional quality assessment model low-light video enhancement. proceedings acm interna- tional conference multimedia, pages yunlong dong, xiaohong liu, yixuan gao, xunchu zhou, tao tan, guangtao zhai. light-vqa multi- dimensional quality assessment model low-light video enhancement. proceedings acm interna- tional conference multimedia, michail christos doukas, stefanos zafeiriou, viktoriia sharmanska. headgan one-shot neural head synthesis editing. proceedings ieeecvf international con- ference computer vision, pages huiyu duan, qiang hu, jiarui wang, liu yang, zitong xu, liu, xiongkuo min, chunlei cai, tianxiao ye, xiaoyun zhang, al. finevq fine-grained user gen- erated content video quality assessment. arxiv preprint egor ershov, sergey korchagin, alexei khalin, artyom panshin, arseniy terekhin, ekaterina zaychenkova, georgiy lobarev, vsevolod plokhotnyuk, denis abramov, elisey zhdanov, sofia dorogova, yasin mamedov, nikola banic, georgii perevozchikov, radu timofte, al. ntire challenge night photography rendering. pro- ceedings ieeecvf conference computer vision pattern recognition cvpr workshops, christoph feichtenhofer, haoqi fan, jitendra malik, kaiming he. slowfast networks video recognition. proceedings ieeecvf international conference computer vision, pages yuqian fu, xingyu qiu, bin ren yanwei fu, radu timofte, nicu sebe, ming-hsuan yang, luc van gool, al. ntire challenge cross-domain few-shot object detection methods results. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, yixuan gao, yuqin cao, tengchuan kou, wei sun, yun- long dong, xiaohong liu, xiongkuo min, guangtao zhai. vdpve vqa dataset perceptual video enhance- ment. arxiv preprint yue gao, yuan zhou, jinglu wang, xiao li, xiang ming, yan lu. high-fidelity freely controllable talking head video generation. proceedings ieeecvf conference computer vision pattern recognition, pages anastasis germanidis. gen- generate novel videos text, images video clips. deepti ghadiyaram, janice pan, alan bovik, anush kr- ishna moorthy, prasanjit panda, kai-chieh yang. in- capture mobile video distortions study subjective behavior objective algorithms. ieee transactions circuits systems video technology tcsvt, shihan guo, jiachen guo, han wang, haibo wang, xiaol- ing huang, lin zhang. efficient ophthalmic dis- ease system integrated knowledge graphs digi- tal humans. international conference infor- mation communication signal processing icicsp, pages ieee, sungjoo ha, martin kersner, beomsu kim, seokjun seo, dongyoung kim. marionette few-shot face reenact- ment preserving identity unseen targets. proceed- ings aaai conference artificial intelligence, vol- ume pages jinliang han, xiongkuo min, jun jia, yixuan gao, xi- aohong liu, guangtao zhai. full-reference no- reference quality assessment video frame interpolation. ieee transactions circuits systems video technology, shuhao han, haotian fan, fangyuan kong, wenjie liao, chunle guo, chongyi li, radu timofte, al. ntire challenge text image generation model qual- ity assessment. proceedings ieeecvf confer- ence computer vision pattern recognition cvpr workshops, fa-ting hong, longhao zhang, shen, dan xu. depth-aware generative adversarial network talking head video generation. proceedings ieeecvf conference computer vision pattern recognition, pages vlad hosu, franz hahn, mohsen jenadeleh, hanhe lin, hui men, tamas sziranyi, shujun li, dietmar saupe. konstanz natural video database konvid-k. pro- ceedings ninth international conference quality multimedia experience qomex, pages ieee, qiang hu, qihan he, houqiang zhong, guo lu, xiaoyun zhang, guangtao zhai, yanfeng wang. varfvv view- adaptive real-time interactive free-view video streaming edge computing. ieee journal selected areas communications, pages ziqi huang, yinan he, jiashuo yu, fan zhang, chenyang si, yuming jiang, yuanhan zhang, tianxing wu, qingyang jin, nattapol chanpaisit, yaohui wang, xinyuan chen, limin wang, dahua lin, qiao, ziwei liu. vbench comprehensive benchmark suite video generative mod- els. proceedings ieeecvf conference com- puter vision pattern recognition, varun jain, zongwei wu, quan zou, louis florentin, henrik turbell, sandeep siddhartha, radu timofte, al. ntire challenge video quality enhancement video conferencing datasets, methods results. pro- ceedings ieeecvf conference computer vision pattern recognition cvpr workshops, ziheng jia, zicheng zhang, jiaying qian, haoning wu, wei sun, chunyi li, xiaohong liu, weisi lin, guang- tao zhai, xiongkuo min. vqa visual question an- swering video quality assessment. arxiv preprint tengchuan kou, xiaohong liu, jun jia, wei sun, guangtao zhai, ning liu. stablevqa deep no-reference quality assessment model video stability. proceedings acm international conference multimedia, tengchuan kou, xiaohong liu, wei sun, jun jia, xiongkuo min, guangtao zhai, ning liu. stablevqa deep no- reference quality assessment model video stability. proceedings acm international conference multimedia, pages tengchuan kou, xiaohong liu, zicheng zhang, chunyi li, haoning wu, xiongkuo min, guangtao zhai, ning liu. subjective-aligned dateset metric text-to-video quality assessment. arxiv preprint pika labs. pika video generation platform. sangmin lee, eunpil park, angel canelo, hyunhee park, youngjo kim, hyungju chun, xin jin, chongyi li, chun- guo, radu timofte, al. ntire challenge efficient burst hdr restoration datasets, methods, results. proceedings ieeecvf conference computer vision pattern recognition cvpr work- shops, chunyi li, tengchuan kou, yixuan gao, yuqin cao, wei sun, zicheng zhang, yingjie zhou, zhichao zhang, haon- ing wu, weixia zhang, xiaohong liu, xiongkuo min, guangtao zhai. aigiqa-k large database ai-generated image quality assessment. proceedings ieeecvf conference computer vision pattern recognition workshops, xin li, yeying jin, xin jin, zongwei wu, bingchen li, yufei wang, wenhan yang, li, zhibo chen, bihan wen, robby tan, radu timofte, al. ntire challenge day night raindrop removal dual-focused images methods results. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, xin li, xijun wang, bingchen li, kun yuan, yizhen shao, suhang yao, ming sun, chao zhou, radu timofte, zhibo chen. ntire challenge short-form ugc video quality assessment enhancement kwaisr dataset study. proceedings ieeecvf conference computer vision pattern recognition cvpr work- shops, xin li, kun yuan, bingchen li, fengbin guan, yizhen shao, zihao yu, xijun wang, yiting lu, wei luo, suhang yao, ming sun, chao zhou, zhibo chen, radu timofte, al. ntire challenge short-form ugc video qual- ity assessment enhancement methods results. proceedings ieeecvf conference computer vi- sion pattern recognition cvpr workshops, yang li, shengbin meng, xinfeng zhang, shiqi wang, yue wang, siwei ma. ugc-video perceptual quality as- sessment user-generated videos. ieee confer- ence multimedia information processing retrieval mipr, pages ieee, jie liang, radu timofte, qiaosi yi, zhengqiang zhang, shuaizheng liu, lingchen sun, rongyuan wu, xindong zhang, hui zeng, lei zhang, al. ntire restore image model raim wild challenge. proceedings ieeecvf conference computer vi- sion pattern recognition cvpr workshops, xiaohong liu, xiongkuo min, qiang hu, xiaoyun zhang, jie guo, al. ntire xgc quality assessment challenge methods results. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, xiaohong liu, radu timofte, yunlong dong, zhiliang ma, haotian fan, chunzheng zhu, xiongkuo min, guangtao zhai, ziheng jia, mirko agarla, al. ntire quality assessment video enhancement challenge. proceed- ings ieeecvf conference computer vision pattern recognition, pages xiaoning liu, zongwei wu, florin-alexandru vasluianu, hailong yan, bin ren, yulun zhang, shuhang gu, zhang, zhu, radu timofte, al. ntire challenge low light image enhancement methods results. proceedings ieeecvf conference computer vi- sion pattern recognition cvpr workshops, yaofang liu, xiaodong cun, xuebo liu, xintao wang, yong zhang, haoxin chen, yang liu, tieyong zeng, ray- mond chan, ying shan. evalcrafter benchmark- ing evaluating large video generation models. arxiv preprint yuanxin liu, lei li, shuhuai ren, rundong gao, shicheng li, sishuo chen, sun, hou. fetv bench- mark fine-grained evaluation open-domain text-to- video generation. advances neural information process- ing systems, liu, yutong lin, yue cao, han hu, yixuan wei, zheng zhang, stephen lin, baining guo. swin transformer hierarchical vision transformer using shifted windows. proceedings ieeecvf international conference computer vision, pages yiting lu, xin li, yajing pei, kun yuan, qizhi xie, yun- peng qu, ming sun, chao zhou, zhibo chen. kvq kwai video quality assessment short-form videos. proceedings ieeecvf conference computer vi- sion pattern recognition, xin ma, yaohui wang, gengyun jia, xinyuan chen, ziwei liu, yuanfang li, cunjian chen, qiao. latte la- tent diffusion transformer video generation. arxiv preprint., yifeng al. dreamtalk expressive talking head generation meets diffusion probabilistic models. arxiv preprint anush krishna moorthy, lark kwon choi, alan conrad bovik, gustavo veciana. video quality assessment mobile devices subjective, behavioral objective studies. ieee journal selected topics signal pro- cessing, antonio moura, ingrida mazonaviciute, joao nunes, justinas grigaravicius. human lips synchronisation au- todesk maya. international workshop systems, signals image processing eurasip conference focused speech image processing, mul- timedia communications services, pages ieee, prajwal, rudrabha mukhopadhyay, vinay nambood- iri, jawahar. lip sync expert need speech lip generation wild. acm mm, bin ren, hang guo, lei sun, zongwei wu, radu tim- ofte, yawei li, al. tenth ntire efficient super-resolution challenge report. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, yurui ren, li, yuanqi chen, thomas li, liu. pirenderer controllable portrait image generation via semantic neural rendering. proceedings ieeecvf international conference computer vision, pages nickolay safonov, alexey bryntsev, andrey moskalenko, dmitry kulikov, dmitriy vatolin, radu timofte, al. ntire challenge ugc video enhancement meth- ods results. proceedings ieeecvf confer- ence computer vision pattern recognition cvpr workshops, lei sun, andrea alfarano, peiqi duan, shaolin su, kaiwei wang, boxin shi, radu timofte, danda pani paudel, luc van gool, al. ntire challenge event-based image deblurring methods results. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, lei sun, hang guo, bin ren, luc van gool, radu timo- fte, yawei li, al. tenth ntire image denoising challenge report. proceedings ieeecvf confer- ence computer vision pattern recognition cvpr workshops, wei sun, xiongkuo min, wei lu, guangtao zhai. deep learning based no-reference quality assessment model ugc videos. proceedings acm interna- tional conference multimedia, pages wei sun, xiongkuo min, wei lu, guangtao zhai. deep learning based no-reference quality assessment model ugc videos. proceedings acm interna- tional conference multimedia, page kuaishou team. kling ai. soumya tripathy, juho kannala, esa rahtu. facegan facial attribute controllable reenactment gan. proceed- ings ieeecvf winter conference applications computer vision, pages florin-alexandru vasluianu, tim seizinger, zhuyun zhou, cailian chen, zongwei wu, radu timofte, al. ntire image shadow removal challenge report. proceed- ings ieeecvf conference computer vision pattern recognition cvpr workshops, florin-alexandru vasluianu, tim seizinger, zhuyun zhou, zongwei wu, radu timofte, al. ntire ambi- ent lighting normalization challenge. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, haiqiang wang, weihao gan, sudeng hu, joe yuchieh lin, lina jin, longguang song, ping wang, ioannis kat- savounidis, anne aaron, c-c jay kuo. mcl-jcv jnd-based avc video quality assessment dataset. proceedings ieee international conference im- age processing, pages ieee, qilong wang, banggu wu, pengfei zhu, peihua li, wang- meng zuo, qinghua hu. eca-net efficient channel attention deep convolutional neural networks. ieeecvf conference computer vision pattern recognition cvpr, pages qiulin wang, zhang, li. safa structure aware face animation. international conference vision dv, pages ieee, suzhen wang, lincheng li, ding, changjie fan, xin yu. audiohead audio-driven one-shot talking- head generation natural head motion. arxiv preprint yuhan wang, chen, junwei zhu, wenqing chu, ying tai, chengjie wang, jilin li, yongjian wu, feiyue huang, rongrong ji. hififace shape semantic prior guided high fidelity face swapping. arxiv preprint yilin wang, junjie ke, hossein talebi, joong gon yim, neil birkbeck, balu adsumilli, peyman milanfar, feng yang. rich features perceptual quality assessment ugc videos. proceedings ieeecvf conference computer vision pattern recognition, pages yingqian wang, zhengyu liang, fengyuan zhang, lvli tian, longguang wang, juncheng li, jungang yang, radu timofte, yulan guo, al. ntire challenge light field image super-resolution methods results. pro- ceedings ieeecvf conference computer vision pattern recognition cvpr workshops, zhou wang, alan bovik, hamid sheikh, eero simoncelli. image quality assessment error visibility structural similarity. ieee transactions image pro- cessing, olivia wiles, koepke, andrew zisserman. xface network controlling face generation using images, au- dio, pose codes. proceedings european con- ference computer vision eccv, pages haoning wu, chaofeng chen, jingwen hou, liang liao, annan wang, wenxiu sun, qiong yan, weisi lin. fast-vqa efficient end-to-end video quality assessment fragment sampling. proceedings computer visioneccv european conference, tel aviv, israel, october proceedings, part vi, pages springer, haoning wu, chaofeng chen, jingwen hou, liang liao, annan wang, wenxiu sun, qiong yan, weisi lin. fast- vqa efficient end-to-end video quality assessment fragment sampling, haoning wu, erli zhang, liang liao, chaofeng chen, jing- wen hou, annan wang, wenxiu sun, qiong yan, weisi lin. towards explainable in-the-wild video quality assess- ment database language-prompted approach. proceedings acm international conference multimedia, pages haoning wu, erli zhang, liang liao, chaofeng chen, jing- wen hou hou, annan wang, wenxiu sun sun, qiong yan, weisi lin. exploring video quality assessment user generated contents aesthetic technical per- spectives. international conference computer vision iccv, haoning wu, zicheng zhang, weixia zhang, chaofeng chen, chunyi li, liang liao, annan wang, erli zhang, wenxiu sun, qiong yan, xiongkuo min, guangtao zhai, weisi lin. q-align teaching lmms visual scoring via discrete text-defined levels. arxiv preprint equal contribution wu, haon- ing zhang, zicheng. corresponding authors zhai, guangtao lin, weisi. haoning wu, hanwei zhu, zicheng zhang, erli zhang, chaofeng chen, liang liao, chunyi li, al. towards open-ended visual quality comparison. arxiv preprint wayne wu, yunxuan zhang, cheng li, chen qian, chen change loy. reenactgan learning reenact faces via boundary transfer. proceedings european con- ference computer vision eccv, pages kangning yang, jie cai, ling ouyang, florin-alexandru vasluianu, radu timofte, jiaming ding, huiming sun, lan fu, jinlong li, chiu man ho, zibo meng, al. ntire challenge single image reflection removal wild datasets, methods results. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, zhuoyi yang, jiayan teng, wendi zheng, ming ding, shiyu huang, jiazheng xu, yuanming yang, wenyi hong, xiaohan zhang, guanyu feng. cogvideox text-to- video diffusion models expert transformer. arxiv preprint., guangming yao, yuan, tianjia shao, kun zhou. mesh guided one-shot face reenactment using graph convo- lutional networks. proceedings acm interna- tional conference multimedia, pages mateusz zajac szczepan paszkiel. using brain- computer interface technology modeling objects blender software. journal automation mobile robotics intelligent systems, egor zakharov, aliaksandra shysheya, egor burkov, victor lempitsky. few-shot adversarial learning real- istic neural talking head models. proceedings ieeecvf international conference computer vision, pages pierluigi zama ramirez, fabio tosi, luigi stefano, radu timofte, alex costanzino, matteo poggi, samuele salti, stefano mattoccia, al. ntire challenge depth images specular transparent surfaces. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, wenxuan zhang al. sadtalker learning realistic mo- tion coefficients stylized audio-driven single image talk- ing face animation. ieeecvf cvpr, weixia zhang, guangtao zhai, ying wei, xiaokang yang, kede ma. blind image quality assessment via vision- language correspondence multitask learning perspec- tive. ieee conference computer vision pattern recognition, pages zicheng zhang, fan, wei sun, xiongkuo min, xiao- hong liu, chunyi li, haoning wu, weisi lin, ning liu, guangtao zhai. paps-ovqa projection-aware patch sampling omnidirectional video quality assessment. ieee international symposium circuits systems, zhimeng zhang, zhipeng hu, wenjin deng, changjie fan, tangjie lv, ding. dinet deformation inpainting network realistic face visually dubbing high resolu- tion video. arxiv preprint zicheng zhang, ziheng jia, haoning wu, chunyi li, zijian chen, yingjie zhou, wei sun, xiaohong liu, xiongkuo min, weisi lin, guangtao zhai. q-bench-video benchmarking video quality understanding lmms. arxiv preprint zicheng zhang, tengchuan kou, shushi wang, chunyi li, wei sun, wei wang, xiaoyu li, zongyu wang, xuezhi cao, xiongkuo min, xiaohong liu, guangtao zhai. q-eval-k evaluating visual quality align- ment level text-to-vision content. arxiv preprint zicheng zhang, chunyi li, wei sun, xiaohong liu, xiongkuo min, guangtao zhai. perceptual qual- ity assessment exploration aigc images. ieee international conference multimedia expo work- shops icmew, pages ieee, zicheng zhang, wei sun, yingjie zhou, haoning wu, chunyi li, xiongkuo min, xiaohong liu, guangtao zhai, weisi lin. advancing zero-shot digital human qual- ity assessment text-prompted evaluation. arxiv preprint zicheng zhang, haoning wu, zhongpeng ji, chunyi li, erli zhang, wei sun, xiaohong liu, al. q-boost vi- sual quality assessment ability low-level multi-modality foundation models. arxiv preprint zicheng zhang, wei wu, wei sun, danyang tu, wei lu, xiongkuo min, ying chen, guangtao zhai. md-vqa multi-dimensional quality assessment ugc live videos. proceedings ieeecvf conference computer vision pattern recognition cvpr, pages zicheng zhang, yingjie zhou, chunyi li, kang fu, wei sun, xiaohong liu, xiongkuo min, guangtao zhai. reduced-reference quality assessment metric textured mesh digital humans. icassp ieee inter- national conference acoustics, speech signal pro- cessing icassp, pages ieee, zicheng zhang, yingjie zhou, chunyi li, baixuan zhao, yixuan gao, zicheng zhang, chunyi li, haoning wu, guangtao zhai. quality assessment era large mod- els survey. arxiv preprint zicheng zhang, yingjie zhou, wei sun, wei lu, xiongkuo min, wang, guangtao zhai. ddh-qa dynamic digital humans quality assessment database. ieee international conference multimedia expo icme, pages ieee, zicheng zhang, yingjie zhou, wei sun, xiongkuo min, guangtao zhai. geometry-aware video quality as- sessment dynamic digital human. ieee in- ternational conference image processing icip, pages ieee, zicheng zhang, yingjie zhou, long teng, wei sun, chunyi li, xiongkuo min, xiao-ping zhang, guangtao zhai. quality-of-experience evaluation digital twins net- work environments. ieee transactions broadcasting, weizhi zhong, chaowei fang, yinqi cai, pengxu wei, gangming zhao, liang lin, guanbin li. identity- preserving talking face generation landmark ap- pearance priors. ieeecvf cvpr, xunchu zhou, xiaohong liu, yunlong dong, tengchuan kou, yixuan gao, zicheng zhang, chunyi li, haoning wu, guangtao zhai. light-vqa video quality assess- ment model exposure correction vision-language guidance. arxiv preprint yingjie zhou, yaodong chen, kaiyue bi, lian xiong, hui liu. implementation multimodal fusion sys- tem intelligent digital human generation. arxiv preprint zhou, weikang gong, yanjing sun, leida li, jinjian wu, xinbo gao. pyramid feature aggregation hi- erarchical quality prediction stitched panoramic images. ieee transactions multimedia, yang zhou, xintong han, eli shechtman, jose echevar- ria, evangelos kalogerakis, dingzeyu li. makelttalk speaker-aware talking-head animation. acm tog, yingjie zhou, zicheng zhang, jiezhang cao, jun jia, yan- wei jiang, farong wen, xiaohong liu, xiongkuo min, guangtao zhai. memo-bench multiple benchmark text-to-image multimodal large language models hu- man emotion analysis. arxiv preprint yingjie zhou, zicheng zhang, wei sun, xiaohong liu, xiongkuo min, zhihua wang, xiao-ping zhang, guangtao zhai. thqa perceptual quality assessment database talking heads. ieee international conference image processing icip, pages ieee, yingjie zhou, zicheng zhang, wei sun, xiaohong liu, xiongkuo min, guangtao zhai. subjective objec- tive quality-of-experience assessment talking heads. proceedings acm international conference multimedia, pages yingjie zhou, zicheng zhang, wei sun, xiongkuo min, xi- anghe ma, guangtao zhai. no-reference quality as- sessment method digital human head. ieee in- ternational conference image processing icip, pages ieee, yingjie zhou, zicheng zhang, farong wen, jun jia, yan- wei jiang, xiaohong liu, xiongkuo min, guangtao zhai. dgcqa quality assessment database ai- generated contents. icassp ieee interna- tional conference acoustics, speech signal pro- cessing icassp, pages ieee, yingjie zhou, zicheng zhang, farong wen, jun jia, xiongkuo min, jia wang, guangtao zhai. reli-qa multidimensional quality assessment dataset relighted human heads. ieee international conference visual communications image processing vcip, pages ieee, xilei zhu, huiyu duan, liu yang, yucheng zhu, xiongkuo min, guangtao zhai, patrick callet. esvqa percep- tual quality assessment egocentric spatial videos. arxiv preprint peiye zhuang, liqian ma, sanmi koyejo, alexander schwing. controllable radiance fields dynamic face synthesis. international conference vision dv, pages ieee,", "published_date": "2025-06-03T13:39:57+00:00"}
{"id": "2506.02550v2", "title": "Technical Report for Ego4D Long-Term Action Anticipation Challenge 2025", "authors": ["Qiaohui Chu", "Haoyu Zhang", "Yisen Feng", "Meng Liu", "Weili Guan", "Yaowei Wang", "Liqiang Nie"], "summary": "report, present novel three-stage framework developed egod long-term action anticipation lta task. inspired recent advances foundation models, method consists three stages feature extraction, action recognition, long-term action anticipation. first, visual features extracted using high-performance visual encoder. features fed transformer predict verbs nouns, verb-noun co-occurrence matrix incorporated enhance recognition accuracy. finally, predicted verb-noun pairs formatted textual prompts input fine-tuned large language model llm anticipate future action sequences. framework achieves first place challenge cvpr establishing new state-of-the-art long-term action prediction. code released", "full_text": "cs.cv jun technical report egod long-term action anticipation challenge qiaohui chu haoyu zhang yisen feng, meng liu, weili guan, yaowei wang liqiang nie harbin institute technology shenzhen pengcheng laboratory shandong jianzhu university qiaohuichu, zhang.hy., yisenfeng.hit, mengliu.sdu, honeyguan, nieliqianggmail.com wangywpcl.ac.cn abstract report, present novel three-stage framework developed egod long-term action anticipation lta task. inspired recent advances foundation models, method consists three stages feature ex- traction, action recognition, long-term action antici- pation. first, visual features extracted using high- performance visual encoder. features fed transformer predict verbs nouns, verbnoun co-occurrence matrix incorporated enhance recognition accuracy. finally, predicted verbnoun pairs formatted textual prompts input fine-tuned large language model llm anticipate fu- ture action sequences. framework achieves first place challenge cvpr establishing new state- of-the-art long-term action prediction. code released challenge-. introduction egocentric video understanding emerged critical re- search direction, driven rapid proliferation large- scale egocentric video datasets. unlike traditional third- person videos, egocentric videos capture everyday activi- ties directly wearers perspective, inherently em- bedding actors attention, actions, subjective inten- tions unique viewpoint closely aligns egocen- tric research goals embodied ai, un- derstanding predicting human behavior ac- tors perspective essential. however, due frequent head body movements limited environmental con- text, egocentric videos present distinct significant ana- lytical challenges, inspiring various complex research prob- lems representative task area long-term ac- tion anticipation lta egod benchmark, aims predict actors future behavior ob- served video sequences, typically represented sequences verbnoun pairs. compared third-person lta tasks, egocentric lta greater potential directly facilitate natural effective humancomputer interaction pre- dicting future actions actors viewpoint. recently, notable breakthroughs visual perception visionlanguage understanding coupled advancements temporal sensitivity large language models llms, significantly enhanced mod- els capabilities video content understanding repre- sentation. research egocentric lta gradually shifted small-scale predictive models methods based llms. palm initially demonstrated feasibility us- ing llama-b action anticipation, antgpt improved upon palm fine-tuning llama-b, achieving notably better performance. however, highlighted palm antgpt, accuracy initial action recognition models significantly affects final prediction quality, existing recognition models still consider- able room improvement. address issue, propose efficient three- stage framework comprising feature extraction, ac- tion recognition, long-term action anticipation. leveraging high-performing visual encoder feature ex- traction, incorporating handobject interaction cues, integrating verbnoun co-occurrence matrix, frame- work improves accuracy action recognition stage. enhancements collectively strengthen reliability precision subsequent long-term action anticipa- tion. empirical validation demonstrates effectiveness method. specifically, egod long-term action anticipation challenge, approach surpasses competitors public leaderboard, achieving first place challenge. transformer top pnoun top pverb co-occurrence matrix verb noun frame feature visual encoder language model prediction feature extraction action recognition mlp long-term action anticipation action prediction prompt prompt predict next possible actions format verb noun pair chronological order match given observed actions common sense most. figure illustration two-stage pipeline. methodology proposed method, illustrated figure com- posed three main stages feature extraction, action recog- nition, long-term action anticipation. feature extraction feature extraction stage, adopt general frame- work introduced antgpt uniformly sampling four frames video segment corresponds ac- tion. evaluating two visual encoders, egovlp egovideo-v select egovideo-v due superior performance extracting visual features. enhance quality input features, incorporate segmentation- based method extract visual information handobject interaction regions. handobject in- teraction regions original video frames fed visual encoder obtain respective features, fused using lightweight mlp. enriched input helps improve downstream action recognition accuracy. action recognition action recognition stage, visual features fed transformer model predict candidate verbs nouns, obtain top five predictions verbs nouns. inspired verb-noun co-occurrence ma- trix proposed querymamba integrate similar co-occurrence matrix recognition model enhance prediction accuracy. specifically, multi- ply probability distributions top-five verbs nouns output transformer model normal- ized co-occurrence matrix. top-five verb generates five noun candidates highest co-occurrence prob- abilities, forming top-five verbnoun pairs. similarly, top-five noun generates five verb candidates, forming another set top-five nounverb pairs. ten can- didate verbnoun pairs, select pair high- est overall probability final prediction action recognition stage. long-term action anticipation action anticipation stage, existing studies typically convert visual observation histories textual form us- ing action recognition model, utilize llms inference, demonstrated methods palm, antgpt, egovideo. notably, antgpt egovideo achieved improved results fine-tuning llama-b vicuna-b respectively. following similar approach, feed textual representations visual observation histories obtained action recognition stage fine-tuned llama-b model, predicting sequence future actions. experiment performance comparison performance lta task evaluated using edit distance metric, measures average num- ber insertions, deletions, substitutions, transpositions predicted sequences ground truth se- quences. table summarizes results existing llm- based methods public leaderboard egod test set. shown table, proposed framework achieves best ranked first scores nouns overall actions. terms verb ed, method ranks sec- ond, slight difference compared table performance comparison existing work top six teams public leaderboard. best results shown bold. method rank lta verb noun action videollm palm challenge antgpt palm paper charlies antgpt hai-pui doggeee finetuning bigmac mtp abcd newdictegovideo ilearn.o top-ranked method. results clearly demonstrate superior performance framework compared exist- ing approaches. ablation study table shows ablation results different visual encoders inference llms. results show approach consistently outperforms baseline methods recognition accuracy, regardless whether egovlp egovideo-v used backbone encoder. furthermore, observe advanced egovideo-v achieves highest action recognition accuracy, reaching long-term action anticipation, utilize fine- tuned llama-b anticipation model. despite action recognition accuracy achieved model slightly lower highest accuracy fine-tuned egovideo-v, overall lta predictions clearly outper- form baselines. combining improvements across three stages, achieve superior overall performance lta task, demonstrating positive contributions component final prediction accuracy. case study figure illustrates successful failed examples action recognition model. failed action recognition case, notice model struggles differ- entiate semantically related nouns verbs varying levels granularity. example, model in- correctly identifies screw metal although screw indeed type metal, difference granularity leads incorrect predictions. addressing limitation future work may involve methods enhance models ability recognize actions different semantic granularities. figure illustrates example action anticipa- tion model. failed example, model demonstrates primarily insufficient predictive capability future scenar- ios, resulting repetitive sequences closely related model output take metal ground truth take screw model output take wrench ground truth take wrench figure successful failed cases action recognition model. prompt predict next possible actions format verb noun pair chronological order match given observed actions common sense most. observed actions. observed actions take screw, put screw, take screw, put screw, attach screw, repair rod, put screwdriver, take wrench prediction model output take wrench, put wrench, take wrench, put wrench, take wrench, put wrench, take wrench, put wrench, take wrench, put wrench, take wrench, put wrench, take wrench, put wrench, take wrench, put wrench, take wrench, put wrench, take wrench, put wrench ground truth put wrench, put drill, take wrench, repair energy, put wrench, repair energy, take wrench, put wrench, take screw, repair energy, take wrench, repair energy, repair energy, take phone, put cement, apply cement, apply cement, wipe floor, dip sponge, dip sponge figure example action anticipation model. final observed actions. instance, future actions could in- volve take phone, put cement, apply cement, model failed anticipate tasks, leading repet- table performance comparison method baselines. indicates using sliding-window strategy indicates model finetuned egod dataset. table, bolded results indicate performance using zero-shot visual encoder without applying sliding window strategy. lta table, bolded results represent best-performing scores. method llm visual encoder acc lta verb noun action verb noun action palm llama-b egovlp egovlp egovideo vicuna-b egovideo-v egovlp llama-b egovlp egovideo-v llama-b egovideo-v itive incorrect predictions. mitigate this, plan incorporate task-intent recognition modules future work, providing model clearer intent-driven guidance accurate predictions. conclusion introduce three-stage enhanced framework lta task, substantially improves performance action recognition long-term action anticipa- tion. experimental results demonstrate method achieves outstanding performance, securing first place cvpr egod long-term action anticipa- tion challenge surpassing previous state-of-the-art ap- proaches. references guo chen, yin-dong zheng, jiahao wang, jilan xu, yifei huang, junting pan, wang, yali wang, qiao, tong lu, al. videollm modeling video sequence large language models. arxiv preprint yisen feng, haoyu zhang, yuquan xie, zaijing li, meng liu, liqiang nie. objectnlq egod episodic memory challenge arxiv preprint yisen feng, haoyu zhang, meng liu, weili guan, liqiang nie. object-shot enhanced grounding network egocentric video. arxiv preprint kristen grauman, andrew westbury, eugene byrne, zachary chavis, antonino furnari, rohit girdhar, jackson hamburger, hao jiang, miao liu, xingyu liu, al. egod around world hours egocentric video. proceedings ieeecvf conference computer vi- sion pattern recognition, pages weili guan, xuemeng song, haoyu zhang, meng liu, chung-hsing yeh, xiaojun chang. bi-directional het- erogeneous graph hashing towards efficient outfit recom- mendation. proceedings acm international conference multimedia, pages daoji huang, otmar hilliges, luc van gool, wang. palm predicting actions language models egod long-term action anticipation challenge arxiv preprint sanghwan kim, daoji huang, yongqin xian, otmar hilliges, luc van gool, wang. palm predicting actions language models. european conference computer vision, pages springer, kevin qinghong lin, jinpeng wang, mattia soldan, michael wray, rui yan, eric xu, difei gao, rong-cheng tu, wen- zhe zhao, weijie kong, al. egocentric video-language pretraining. advances neural information processing sys- tems, meng liu, xiang wang, liqiang nie, xiangnan he, bao- quan chen, tat-seng chua. attentive moment retrieval videos. international acm sigir conference research development information retrieval, pages meng liu, xiang wang, liqiang nie, tian, baoquan chen, tat-seng chua. cross-modal moment localiza- tion videos. proceedings acm international conference multimedia, pages baoqi pei, guo chen, jilan xu, yuping he, yicheng liu, kanghua pan, yifei huang, yali wang, tong lu, limin wang, al. egovideo exploring egocentric founda- tion model downstream adaptation. arxiv preprint nikhila ravi, valentin gabeur, yuan-ting hu, ronghang hu, chaitanya ryali, tengyu ma, haitham khedr, roman radle, chloe rolland, laura gustafson, al. sam segment anything images videos. arxiv preprint hugo touvron, louis martin, kevin stone, peter albert, amjad almahairi, yasmine babaei, nikolay bashlykov, soumya batra, prajjwal bhargava, shruti bhosale, al. llama open foundation fine-tuned chat models. arxiv preprint yunxiao wang, meng liu, rui shao, haoyu zhang, bin wen, fan yang, tingting gao, zhang, liqiang nie. time temporal-sensitive multi-dimensional instruc- tion tuning benchmarking video-llms. arxiv preprint haoyu zhang, meng liu, zan gao, xiaoqiang lei, yinglong wang, liqiang nie. multimodal dialog system rela- tional graph-based context-aware question understanding. proceedings acm international conference multimedia, pages haoyu zhang, meng liu, yuhong li, ming yan, zan gao, xiaojun chang, liqiang nie. attribute-guided collab- orative learning partial person re-identification. ieee transactions pattern analysis machine intelligence, haoyu zhang, meng liu, yaowei wang, cao, weili guan, liqiang nie. uncovering hidden connections iterative tracking reasoning video-grounded dialog. arxiv preprint haoyu zhang, meng liu, zixin liu, xuemeng song, yaowei wang, liqiang nie. multi-factor adaptive vision selec- tion egocentric video question answering. proceedings international conference machine learning, pages pmlr, haoyu zhang, yuquan xie, yisen feng, zaijing li, meng liu, liqiang nie. hcqa egod egoschema challenge arxiv preprint haoyu zhang, qiaohui chu, meng liu, yunxiao wang, bin wen, fan yang, tingting gao, zhang, yaowei wang, liqiang nie. exoego exocentric knowledge guided mllm egocentric video understanding. arxiv preprint haoyu zhang, yisen feng, qiaohui chu, meng liu, weili guan, yaowei wang, liqiang nie. hcqa- egod egoschema challenge arxiv preprint zhao, shijie wang, zhang, changcheng fu, minh quan do, nakul agarwal, kwonjoon lee, chen sun. antgpt large language models help long- term action anticipation videos? arxiv preprint lianmin zheng, wei-lin chiang, ying sheng, siyuan zhuang, zhanghao wu, yonghao zhuang, lin, zhuohan li, dacheng li, eric xing, al. judging llm-as-a-judge mt-bench chatbot arena. advances neural information processing systems, zeyun zhong, manuel martin, frederik diederichs, juergen beyerer. querymamba mamba-based encoder- decoder architecture statistical verb-noun interac- tion module video action forecasting egod long- term action anticipation challenge arxiv preprint", "published_date": "2025-06-03T07:36:52+00:00"}
{"id": "2505.24404v1", "title": "PCIE_Interaction Solution for Ego4D Social Interaction Challenge", "authors": ["Kanokphan Lertniphonphan", "Feng Chen", "Junda Xu", "Fengbu Lan", "Jun Xie", "Tao Zhang", "Zhepeng Wang"], "summary": "report presents teams pcieinteraction solution egod social interaction challenge cvpr addressing looking lam talking ttm tasks. challenge requires accurate detection social interactions subjects camera wearer, lam relying exclusively face crop sequences ttm combining speaker face crops synchronized audio segments. lam track, employ face quality enhancement ensemble methods. ttm task, extend visual interaction analysis fusing audio visual cues, weighted visual quality score. approach achieved mean average precision map lam ttm challenges leader board. code available", "full_text": "cs.cv may pcie interaction solution egod social interaction challenge kanokphan lertniphonphan lenovo research klertniphonplenovo.com feng chen lenovo research chenfenglenovo.com junda beijing normal university mail.bnu.edu.cn fengbu lan tsinghua university lanfbmails.tsinghua.edu.cn jun xie lenovo research xiejunlenovo.com tao zhang tsinghua university taozhangtsinghua.edu.cn zhepeng wang lenovo research wangzpblenovo.com abstract report presents teams pcie interaction solution egod social interaction challenge cvpr addressing looking lam talking ttm tasks. challenge requires accurate detection social interactions subjects camera wearer, lam relying exclusively face crop sequences ttm combining speaker face crops synchronized au- dio segments. lam track, employ face quality enhancement ensemble methods. ttm task, extend visual interaction analysis fusing audio vi- sual cues, weighted visual quality score. approach achieved mean average precision map lam ttm challenges leader board. code available pcieegodsocialinteraction introduction egocentric video captured wearable cameras gained significant importance computer vision robotics. egod social interaction benchmark highlights interactions camera wearer surrounding individuals two key tasks looking lam talk- ing ttm lam task specifically detects whether social part- ners eye gaze directed toward camera wearer, uti- lizing face bounding boxes cross-frame identity con- sistency. contrast, ttm task identifies conversa- tional engagement synchronized audio-visual anal- ysis tracked individuals. tasks employ frame- level predictions, ttms labeling follows utterance-level paradigm even speaker momentarily looks away dur- ing conversation segment, interaction still consid- ered valid audio content indicates engagement camera wearer. several approaches leverage image sequences detect eye gaze toward camera wearer analyzing vi- sual cues face crops. utilizes facial landmarks head pose estimation enhance spatial feature refinement. incorporates spatial temporal dynamic refine- ment. recently, introduced framework combin- ing internvl image encoder bi-lstm net- work jointly model spatial temporal dependencies. baseline model ttm extends lam ar- chitecture incorporating audio encoder extract em- beddings synchronized audio segments. au- dio embeddings concatenated visual features jointly classified predict ttm interactions. however, demonstrated training separate models audio vi- sion gain better results. method processes audio visual modalities independently, fuses predic- tions using face quality score weight ttm score modality. approach social interaction analysis extends lam ttm task employing separate models visual audio modalities. based dataset charac- teristic, introduce visual audio filter fusing score models, yielding robust final interac- tion score ttm. method proposed social interaction framework egod challenges, illustrated figure addresses looking lam talking ttm tasks figure proposed social interaction detection framework designed multimodal architecture. system pro- cesses visual audio modalities independent, task-optimized branches, stream passing modality-specific filters prior fusion. lam, em- ploy visual branch detect gaze patterns. ttm solution incorporates modalities visual pre- dictions undergo max-score filtering generate utterance- level representations, fused processed audio features quality-weighted fusion module. dual-stream approach allows modality com- pensate others limitations maintaining task- specific optimization. looking lam datasets face crops exhibit significant motion blur due relative movement wearable cameras subjects. analysis gazelstm demonstrated augmenting architecture transformer decoder yielded minimal improvement validation data. draw- ing established methods, developed ensemble framework integrates multiple model outputs ad- dressing image quality applying deblurring techniques enhance input face crops. talking adopt dual-modality approach ttm task, im- plementing separate visual audio models following audio processing, employ whisper encoder evaluating small large-v architectures. large model performs better validation data table improvement generalize test set, sug- gesting potential overfitting validation distribution. ttm annotation scheme presents unique challenges visual modality integration. since labels assigned utterance level based audio segments, speakers may intermittently look away still engaged conversation might look camera wearer all. address mismatch frame-level visual predictions utterance-level labels, implement visual max-score fil- ter. filter aggregates visual predictions across au- dio segment, retaining maximum visual score utterance-level representation. shown table filtering significantly improves visual-only performance. multimodal fusion, compare two approaches basic score averaging quality-weighted fusion weights derived frame-level face alignment scores final predictions undergo median filtering smooth temporal inconsistencies submission. experiments dataset evaluation metric evaluate approach egod social benchmark contains video clips minutes each, split training, validation, test clips. benchmark provides two distinct annotation schemes. lam, frame-level binary labels indicating whether detected face looking camera wearer. ttm, utterance-level labels audio-visual segments, marking whether speaker talking camera wearer. mean average precision map top- accuracy used evaluation tasks. results looking first reproduced baseline approach ex- tending architecture transformer decoder. comprehensive comparison, also implemented two state-of-the-art methods gazepose internlstm see table enhance input quality, test face crops preprocessed using image enhancement module reduce motion blur improve image quality. final submission combines predictions mul- tiple approaches ensemble internlstm, validation test method map acc map acc gazelstm gazelstm transformer head gazepose internlstm table looking experimental results validation test method map acc map acc audio modality whisper small whisper large visual modality gazelstm visual filter audio visual modality averaging quality score weight fusion table talking experimental results team map acc pcie lam ustb lam pku-wict-mipl ydejie dejie host team table egod looking challenge leaderboard gazelstm, gazepose models diverse parameter configurations. applied median filter ensem- ble outputs temporal smoothing, achieving robust per- formance. approach attained top results egod looking benchmark mean average precision map top- accuracy shown ta- ble results lam image sequence shown fig talking whisper-based audio feature extraction demon- strates strong sensitivity conversational speech, produc- ing consistently high confidence scores clear dialogue segments. however, found model shows lower output scores non-standard vocal expressions e.g., ex- clamations laughter. pattern results elevated false positive rate using audio modality. visual modality presents unique challenges com- pared lam, subjects frequently avert gaze conversations figure addition, label conversa- tion group people talking me. handle ttm task, pass prediction score visual max-score filter align visual results ttm protocol. applying filter, testing result vi- sual modality improves significantly comparable audio modality results shown table audio effectively detects verbal engagement, vi- sual cues provide crucial disambiguation non-interactive scenarios. synergy proves particularly valuable fil- tering false positives audio-only modality, conversation detection necessarily indicate social interaction. final submission, ensemble multi- ple fusion strategies apply temporal post-processing optimize consistency. submission result tam leader board shown table conclusion propose unified framework social interaction de- tection egod challenge, addressing looking lam talking ttm tasks op- timized multimodal processing. approach employs in- dependently trained visual audio models maximize modality-specific performance. lam, ensemble models diverse parameter configurations, ttm processing incorporates visual max-score filtering align figure looking results team map acc pcie lam exeg ztest hsichelin egoadapt host team table egod talking challenge leaderboard frame-level gaze predictions utterance-level audio seg- ments. framework demonstrates effective complemen- tary fusion audio reliably detects verbal engagement, visual cues crucially eliminate false positives non- interactive scenarios, indicating better performance unimodal approaches. references adrian bulat georgios tzimiropoulos. far solving face alignment problem? dataset facial landmarks. international con- ference computer vision, zhe chen, jiannan wu, wenhai wang, weijie su, guo chen, sen xing, muyan zhong, qinglong zhang, xizhou zhu, lewei lu, bin li, ping luo, tong lu, qiao, jifeng dai. internvl scaling vision foundation models aligning generic visual-linguistic tasks. arxiv preprint xiaojie chu, liangyu chen, wenqing yu. nafssr stereo image super-resolution using nafnet. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, pages kristen grauman al. egod around world hours egocentric video. ieeecvf conference computer vision pattern recognition cvpr, pages kanokphan lertniphonphan, jun xie, yaqing meng, shijing wang, feng chen, zhepeng wang. pcie lam solution egod looking challenge. arxiv, abs., hsi-che lin, chien-yi wang, min-hung chen, szu-wei fu, wang. quavf quality-aware audio-visual fusion egod talking challenge. arxiv, abs., alec radford, jong wook kim, tao xu, greg brockman, christine mcleavey, ilya sutskever. robust speech recognition via large-scale weak supervision, xiyu wei, dejie yang, yuxin peng, yang liu. team pku- wict-mipl egod look challenge technical report.", "published_date": "2025-05-30T09:35:25+00:00"}
{"id": "2505.23129v1", "title": "HMAD: Advancing E2E Driving with Anchored Offset Proposals and Simulation-Supervised Multi-target Scoring", "authors": ["Bin Wang", "Pingjun Li", "Jinkun Liu", "Jun Cheng", "Hailong Lei", "Yinze Rong", "Huan-ang Gao", "Kangliang Chen", "Xing Pan", "Weihao Gu"], "summary": "end-to-end autonomous driving faces persistent challenges generating diverse, rule-compliant trajectories robustly selecting optimal path options via learned, multi-faceted evaluation. address challenges, introduce hmad, framework integrating distinctive birds-eye-view bev based trajectory proposal mechanism learned multi-criteria scoring. hmad leverages bevformer employs learnable anchored queries, initialized trajectory dictionary refined via iterative offset decoding inspired diffusiondrive, produce numerous diverse stable candidate trajectories. key innovation, simulation-supervised scorer module, evaluates proposals critical metrics including at-fault collisions, drivable area compliance, comfortableness, overall driving quality i.e., extended pdm score. demonstrating efficacy, hmad achieves driving score cvpr private test set. work highlights benefits effectively decoupling robust trajectory generation comprehensive, safety-aware learned scoring advanced autonomous driving.", "full_text": "cs.cv may hmad advancing driving anchored offset proposals simulation-supervised multi-target scoring bin wang, pingjun li, jinkun liu, jun cheng, hailong lei, yinze rong, huan-ang gao, kangliang chen, xing pan, weihao gu, haomo.ai technology co., ltd tsinghua university wangbin, guweihaohaomo.ai abstract end-to-end autonomous driving faces persistent challenges generating diverse, rule-compliant trajectories robustly selecting optimal path options via learned, multi-faceted evaluation. address chal- lenges, introduce hmad, framework integrating dis- tinctive birds-eye-view bev based trajectory proposal mechanism learned multi-criteria scoring. hmad leverages bevformer employs learnable anchored queries, initialized trajectory dictionary refined via iterative offset decoding inspired diffusiondrive, produce numerous diverse stable candidate trajec- tories. key innovation, simulation-supervised scorer module, evaluates proposals critical met- rics including at-fault collisions, drivable area compli- ance, comfortableness, overall driving quality i.e., ex- tended pdm score. demonstrating efficacy, hmad achieves driving score cvpr pri- vate test set. work highlights benefits effec- tively decoupling robust trajectory generation com- prehensive, safety-aware learned scoring advanced au- tonomous driving. introduction end-to-end autonomous driving, aspiring learn di- rect mapping sensor observations driving actions presents compelling vision unified potentially robust alternative tra- ditional modular pipelines. however, initial end-to-end paradigms, often centered direct imitation learning regress single trajectory, fundamentally struggled inherent multimodality real-world driving. critical flaw frequently led mode collapseseverely limiting agents capacity explore diverse, valid driving op- tionsand resulted pronounced brittleness complex scenarios requiring nuanced adherence varied traffic con- straints, far beyond simple behavioral cloning could achieve. recognizing limitations, subsequent research in- creasingly explored multimodal planning, focusing gen- erating diverse set potential trajectories. ad- vancement, crucial step selecting optimal tra- jectory often remained significant bottleneck. many systems reverted employing separate, often non- differentiable, post-processing modules laden prede- fined heuristics fixed-weight cost functions architectural choice frag- ments end-to-end learning process, impeding true joint optimization across perception-to-planning pipeline, also inherently limits systems adaptability, fixed rules struggle generalize across vast spectrum dynamic driving contexts. critical gap highlights urgent need frameworks capable generating truly diverse, high-quality trajectories evaluating integrated, learned, context-aware multi- faceted mechanism, ensuring decisions robustly balance critical aspects driving. work, introduce hmad, novel motion plan- ning framework cvpr end-to-end driving challenge, addressing prior shortcomings uniquely com- bining distinct trajectory proposal strategy learned, simulation-supervised multi-criteria scoring module. first, bevformer constructs rich birds-eye-view bev representations. upon bev context, hmad gener- ates diverse stable candidate trajectoriescombating mode collapseusing learnable anchored queries trajectory dictionary refined iterative offset decod- ing process inspired diffusiondrive critically, contrast methods using inflexible heuristics sepa- rate ranking modules, diverse proposals eval- uated sophisticated, fully differentiable score mod- ule. scorer, inspired principle simulation- supervised multi-target evaluation e.g., hydra-mdp performs cross-attention bev features predict key interpretable driving scores e.g., extended pdm score, on-board sensor data bevformer bev feature map anchor-offset refinement module trajectory anchors hypothesis trajectory encoder attentionq, offset clipping hypothesis feature indexing scorer block trajectory encoder attentionq, post-processing top- sampling score decoder feature indexing simulation supervise perception block final trajectory figure overall architecture hmad. at-fault collisions, drivable area compliance, driving comfort simulator ground-truth, enabling learned, context-dependent trade-offs nuanced selection within end-to-end framework. furthermore, recognizing many planning failures concentrated long-tail scenariossuch unpro- tected turns, occluded junctions, sharp curves, lane departureswe integrate hard case mining train- ing regimen. targeted data augmentation significantly enhances model robustness ability generalize critical edge cases. system achieves driv- ing score cvpr private test set, demonstrating efficacy approach. summary, contributions follows propose bev-based end-to-end driving framework fea- turing distinctive trajectory generation mechanism uses trajectory dictionary-initialized learnable queries anchor-based offset decoding inspired diffusiondrive effectively overcome common issues mode col- lapse proposal instability. design dedicated, simulation-supervised scoring network outputs inter- pretable driving metrics multiple crucial criteria, en- abling adaptive, learned trajectory ranking contrast fixed-heuristic methods. enhance model robustness generalization complex urban scenarios tar- geted hard example mining. method bev representation multi-camera input adopt bevformer perception backbone transform multi-view camera inputs unified birds- eye-view bev representation. given set time- synchronized images surround-view cameras, bev- former first extracts image features using shared back- bone e.g., resnet- fpn, uses deformable attention spatiotemporal transformer fuse multi- camera multi-frame features consistent bev fea- ture map bev representation captures spatial layout temporal dynamics scene serves queryable memory downstream trajectory prediction scoring modules. bev-aware trajectory decoding trajectory generation process relies learnable queries interact birds-eye-view bev features. queries initialize diverse trajectory candidates, refined iterative, bev-aware decoding mechanism. learnable anchored query. designed set learnable queries anchored typical driving ma- neuvers. approach aims enhance training conver- gence improve diversity coverage pro- posed candidate trajectories. queries originate pre-constructed anchor trajectory dictionary denotes number future time steps total number anchor trajectories. dictionary constructed applying unsupervised clustering e.g., means spatial positions driving trajectories. anchor trajectory encoded fixed- dimensional embedding vector using shared tra- jectory encoder., trajenci embeddings later used inputs iterative refinement module described next. iterative anchor-offset refinement bev awareness. instead directly regressing full trajectory coordinates, decoder refines trajectories predicting offsets corresponding anchors ai. ac- complished using multi-layer decoder architecture, layer iteratively improves trajectory estimate based bev features. formally, given trajectory query qi, initialize trajectory hypothesis corresponding anchor ai, refine decoder layers predicting residual offsets trajectory-oriented attention de- coder layer djqi, core decoder module trajectory- oriented attention mechanism makes refinement bev-aware. current trajectory hypothesis layer bev context features relevant gathered using sampling function yielding feature se- quence gf, function typically indexes features multiple points along path sampled feature sequence projected pro- duce attention keys values residual off- set decoded using attention mechanism anchor-derived query attends bev-informed keys values gf, projf attnqi, ensure training stability prevent unrealistic trajec- tory modifications, predicted offsets clipped predefined range clipj max, max, max maximum allowed displacement per layer. anchor-offset formulation offers several key bene- fits leverages prior knowledge common driving behaviors via anchors, iterative, bev-aware atten- tion allows adaptive refinement based observed scene context, mimicking human drivers adjust intentions. naturally supports multi-modality use diverse anchors, offset-based decoding, flexibly refining candidate, helps mitigate mode collapse often seen direct trajectory regression. provides structured bounded prediction target, which, along offset clipping, stabilizes training improves data efficiency, especially dealing complex rare driving scenarios. simulation-supervised trajectory scoring selecting optimal driving trajectory multiple pro- posals requires comprehensive evaluation across various, often conflicting, criteria encompassing safety, rule com- pliance, overall driving quality. single, monolithic score might fail capture diverse aspects. therefore, achieve nuanced interpretable assessment, in- spired hydra-mdp design scorer network learns predict multiple, distinct aspects trajectory per- formance multi-task fashion. specifically, utilize simulator generate detailed, ground-truth evaluations specific metric. scorer takes predicted tra- jectory encodes using trajectory embedding module i.e., positional encoding mlp, performs cross- attention bev features incorporate rich contex- tual information scene. scorer trained pre- dict several key metrics, including overall trajectory quality i.e., extended pdm score, collision avoidance, driv- able area compliance, comfortableness. finally, in- ference, metric overall trajectory quality determines selection top-ranked trajectory. data augmentation post-processing improve performance challenging driving scenarios ensure selection single, optimal trajectory, employ targeted data augmentation rigorous post- selection process submitted solution. firstly, enhance model robustness, perform hard case mining identifying difficult situations like unpro- tected turns, occluded junctions, sharp curves, lane de- partures using human trajectory data annotations openscene identified scenarios upsam- pled threefold training ensure model learns ef- fective strategies critical yet less frequent events. secondly, ensure final chosen trajectory multiple candidates verifiably safe feasible immediate environment, perform detailed post- processing step image space. step aims ground planned trajectories direct visual evi- dence. method begins estimating valid driving dis- tance envelopeboth minimum maximum travel dis- tancesbased ego vehicles current speed accel- eration. proposed trajectory, along projected band representing ego vehicles width, mapped onto camera image space, implicitly using perspec- tive transformation accurate representation. concur- rently, open-source model yolopv analyzes image identify positions critical lane lines obstacles. trajectories subsequently filtered based stringent criteria trajectory whose projected path falls outside pre-calculated valid distance envelope, whose ego-vehicle width band intersects detected obstacles deviates drivable area defined lane lines, dis- table ablation results warmup two stage benchmark. scores number decoding layers full model conducting post-processing. best scores metric bold. epdms column, values red parentheses indicate performance drop compared full model. setup backbone dac ddc tlc ttc epdms full model resnet hard case mining resnet scorer resnet decoder layer resnet decoder layer resnet decoder layer resnet carded. remaining pool valid trajectories, one highest score, assigned trajectory scorer module, selected final output. rigorous filtering yield compliant trajectories, fall- back highest-scoring trajectory original set used ensure continuous operation. experiment dataset metrics dataset. conduct experiments mainly navsim dataset, specifically curated eval- uating end-to-end autonomous driving scenarios requir- ing complex decision-making. navsim built upon openscene dataset, compact filtered version nuplan retaining essential sensor data annota- tions sampled hz. metrics. evaluate planning performance consis- tent safety-critical manner, approach adopts extended predictive driver model score epdms, intro- duced navsim epdms metric extends original pdms navsim incorporating addi- tional sub-metrics introducing mechanisms robust, realistic evaluation. total, epdms includes four multiplicative penalty termsno at-fault collisions nc, drivable area compliance dac, driving direction com- pliance ddc, traffic light compliance tlc, false- positive filteringand five weighted average metrics ego progress ep, time-to-collision ttc, history comfort hc, lane keeping lk, extended comfort ec. formally, epdms computed epdms mnc,dac,ddc,tlc filtermagent, human mttc,ep,hc,lk,ec filtermagent, human mttc,ep,hc,lk,ec filter function defined filtermagent, human mhuman magent otherwise implementation details unless otherwise specified, train models nav- train split using nvidia gpus total batch size epochs. learning rate set weight decay input model consists four rgb camera views front, front-left, front-right, rear. bev feature covers region around ego vehicle, discretized grid. improve model robustness, apply gridmask data augmentation probability training. following con- figuration diffusiondrive initialize trajectory queries using predefined anchor set. results present performance method various ab- lations warmup two stage using extended pre- dictive driver model score epdms sub-metrics. shown tab. full model achieves best over- performance epdms model built upon configuration decoder layers, enhanced post-processing module, leads con- sistent gains across safety ttc com- pliance dac ddc removing key components significantly impacts perfor- mance. scorer removal results epdms drop show learning trajectory scoring ef- fectively promote models understanding scene. similarly, removal hard case mining led decrease models safety metrics complex scenar- ios, nc, ttc, dac. study impact decoder depth. increasing layers improves metrics, especially demonstrating better learning capac- ity. however, increasing decoder layers leads slight drop epdms likely due overfitting. references holger caesar, juraj kabzan, kok seang tan, whye kit fong, eric wolff, alex lang, luke fletcher, oscar beijbom, sammy omari. nuplan closed-loop ml-based plan- ning benchmark autonomous vehicles. arxiv preprint shaoyu chen, jiang, hao gao, bencheng liao, qing xu, qian zhang, chang huang, wenyu liu, xinggang wang. vadv end-to-end vectorized autonomous driving via probabilistic planning. arxiv preprint felipe codevilla, eder santana, antonio lopez, adrien gaidon. exploring limitations behavior cloning autonomous driving. proceedings ieeecvf international conference computer vision iccv, openscene contributors. openscene largest up-to- date occupancy prediction benchmark autonomous driving. openscene, daniel dauner, marcel hallgarten, tianyu li, xinshuo weng, zhiyu huang, zetong yang, hongyang li, igor gilitschenski, boris ivanovic, marco pavone, al. navsim data-driven non-reactive autonomous vehicle simulation benchmarking. advances neural information processing systems, kairui ding, boyuan chen, yuchen su, huan-ang gao, jin, chonghao sima, wuqiang zhang, xiaohui li, paul barsch, hongyang li, hao zhao. hint-ad holistically aligned interpretability end-to-end autonomous driving. conference robot learning corl, yuchao feng, wei hua, yuxiang sun. nle-dm natural- language explanations decision making autonomous driving based semantic scene understanding. ieee trans- actions intelligent transportation systems, huan-ang gao, beiwen tian, pengfei li, xiaoxue chen, hao zhao, guyue zhou, yurong chen, hongbin zha. semi-supervised omni-supervised room layout esti- mation using point clouds. ieee international con- ference robotics automation icra, pages ieee, huan-ang gao, beiwen tian, pengfei li, hao zhao, guyue zhou. dqsd densely-matched quantization- aware semi-supervised detection. proceedings ieeecvf international conference computer vision, pages cheng han, qichao zhao, shuyi zhang, yinzi chen, zhen- lin zhang, jinwei yuan. yolopv better, faster, stronger panoptic driving perception. arxiv preprint kaiming he, xiangyu zhang, shaoqing ren, jian sun. deep residual learning image recognition. proceed- ings ieee conference computer vision pattern recognition, pages yihan hu, jiazhi yang, chen, keyu li, chonghao sima, xizhou zhu, siqi chai, senyao du, tianwei lin, wenhai wang, lewei lu, xiaosong jia, qiang liu, jifeng dai, qiao, hongyang li. planning-oriented autonomous driv- ing. proceedings ieeecvf conference com- puter vision pattern recognition cvpr, pages yihan hu, jiazhi yang, chen, keyu li, chonghao sima, xizhou zhu, siqi chai, senyao du, tianwei lin, wenhai wang, lewei lu, xiaosong jia, qiang liu, jifeng dai, qiao, hongyang li. planning-oriented autonomous driv- ing. proceedings ieeecvf conference com- puter vision pattern recognition, jiang, shaoyu chen, qing xu, bencheng liao, jiajie chen, helong zhou, qian zhang, wenyu liu, chang huang, xinggang wang. vad vectorized scene representation efficient autonomous driving. iccv, zhou jiang, zhenxin zhu, pengfei li, huan-ang gao, tianyuan yuan, yongliang shi, hang zhao, hao zhao. p-mapnet far-seeing map generator enhanced sdmap hdmap priors. arxiv preprint alex kendall, jeffrey hawke, david janz, przemyslaw mazur, daniele reda, john-mark allen, vinh-dieu lam, alex bewley, amar shah. learning drive day. international conference robotics automation icra, wenyi li, huan-ang gao, mingju gao, beiwen tian, rong zhi, hao zhao. training-free model merging multi- target domain adaptation. arxiv preprint zhenxin li, kailin li, shihao wang, shiyi lan, zhiding yu, yishen ji, zhiqi li, ziyue zhu, jan kautz, zuxuan wu, al. hydra-mdp end-to-end multimodal planning multi- target hydra-distillation. arxiv preprint zhiqi li, wenhai wang, hongyang li, enze xie, chong- hao sima, tong lu, qiao yu, jifeng dai. bevformer learning birds-eye-view representation lidar-camera via spatiotemporal transformers. ieee transactions pat- tern analysis machine intelligence, bencheng liao, shaoyu chen, haoran yin, jiang, cheng wang, sixu yan, xinbang zhang, xiangyu li, ying zhang, qian zhang, al. diffusiondrive truncated diffusion model end-to-end autonomous driving. arxiv preprint aditya prakash, kashyap chitta, andreas geiger. multi- modal fusion transformer end-to-end autonomous driv- ing. proceedings ieeecvf conference com- puter vision pattern recognition cvpr, pages chonghao sima, kashyap chitta, zhiding yu, shiyi lan, ping luo, andreas geiger, hongyang li, jose al- varez. centaur robust end-to-end autonomous driving test-time training. arxiv preprint beiwen tian, mingdao liu, huan-ang gao, pengfei li, hao zhao, guyue zhou. unsupervised road anomaly de- tection language anchors. ieee international conference robotics automation icra, pages ieee, wenda xu, qian wang, john dolan. autonomous ve- hicle motion planning via recurrent spline optimization. ieee international conference robotics au- tomation icra, zhejun zhang, alexander liniger, dengxin dai, fisher yu, luc van gool. end-to-end urban driving imitat- ing reinforcement learning coach. proceedings ieeecvf international conference computer vision iccv, pages yupeng zheng, chengliang zhong, pengfei li, huan-ang gao, yuhang zheng, jin, ling wang, hao zhao, guyue zhou, qichao zhang, al. steps joint self-supervised nighttime image enhancement depth estimation. ieee international conference robotics automation icra, pages ieee,", "published_date": "2025-05-29T05:59:24+00:00"}
{"id": "2505.21181v1", "title": "Boosting Adversarial Transferability via High-Frequency Augmentation and Hierarchical-Gradient Fusion", "authors": ["Yayin Zheng", "Chen Wan", "Zihong Guo", "Hailing Kuang", "Xiaohai Lu"], "summary": "adversarial attacks become significant challenge security machine learning models, particularly context black-box defense strategies. existing methods enhancing adversarial transferability primarily focus spatial domain. paper presents frequency-space attack fsa, new adversarial attack framework effectively integrates frequency-domain spatial-domain transformations. fsa combines two key techniques high-frequency augmentation, applies fourier transform frequency-selective amplification diversify inputs emphasize critical role high-frequency components adversarial attacks, hierarchical-gradient fusion, merges multi-scale gradient decomposition fusion capture global structures fine-grained details, resulting smoother perturbations. experiment demonstrates fsa consistently outperforms state-of-the-art methods across various black-box models. notably, proposed fsa achieves average attack success rate increase compared bsr cvpr eight black-box defense models.", "full_text": "boosting adversarial transferability via high-frequency augmentation hierarchical-gradient fusion yayin zheng, chen wan, zihong guo, hailing kuang, xiaohai department computer science technology, shantou university, shantou, china wanchenoutlook.com abstract. adversarial attacks become significant challenge security machine learning models, particularly context black-box defense strategies. existing methods enhancing adversarial transferability primarily focus spatial domain. paper presents frequency-space attack fsa, new adversarial attack framework effectively integrates frequency-domain spatial-domain transformations. fsa combines two key techniques high-frequency augmentation, applies fourier transform frequency- selective amplification diversify inputs emphasize critical role high- frequency components adversarial attacks, hierarchical-gradient fu- sion, merges multi-scale gradient decomposition fusion capture global structures fine-grained details, resulting smoother perturba- tions. experiment demonstrates fsa consistently outperforms state-of- the-art methods across various black-box models. notably, proposed fsa achieves average attack success rate increase compared bsr cvpr eight black-box defense models. keywords adversarial attack, transferability, frequency-space attack. introduction deep neural networks dnns achieved remarkable success across various do- mains. however, vulnerability imperceptible adversarial examples aes re- mains critical challenge .these adversarial examples crafted introducing subtle, human-imperceptible perturbations original images, cause dnns produce incorrect predictions. furthermore, aes exhibit concerning prop- erty known transferability examples designed given model is, white-box model also mislead models. investigating transferability aes reveals vulnerabilities dnns, also provides foundation developing robust defenses. enhance transferability adversarial examples, numerous methods proposed, input transformation-based attacks emerging widely adopted strategies. methods apply series operations, random resizing padding translation scaling input examples gradient computation, aiming prevent generated adversarial examples overfitting zheng, wan, guo, kuang, model. recently, complex input transformations introduced. instance, admix blends input image images different categories, sia applies transformations individual blocks, bsr employs block shuffling rotation boost transferability. parallel, several studies explored vulnerabilities dnns frequency-domain perspective. long al. pro- pose ssa, frequency-domain attack using spectral perturbations dct domain gaussian noise uniform spectral masks. despite advances, exploration frequency-domain transformations re- mains limited. notably, studies revealed two critical insights first, dnns exhibit distinct sensitivities different frequency components second, adversarially trained models i.e., defense models show increased robustness processing low-frequency information observations suggest high-fre- quency components may influential crafting effective adversarial perturba- tions, whereas low-frequency components contribute enhancing model robust- ness. building insights, propose new frequency-space attack fsa, leverages complementary strengths frequency-domain spatial- domain manipulations generate adversarial examples. proposed fsa consists two modules high-frequency augmentation module ham frequency domain hierarchical-gradient fusion module hfm spatial domain. ham trans- forms input images frequency domain via fourier trans-form amplifies high-frequency components, hfm operates spatial domain, employ- ing multi-scale approach transforms gradient information different scales fuses them. integrating frequency-domain feature refinement spatial-do- main multi-scale optimization, fsa establishes unified framework generating ad- versarial examples achieve high attack success rates black-box defense models. main contributions paper summarized follows propose new fsa leverages complementary strengths fre- quency-domain spatial-domain enhance adversarial transferability. proposed ham utilizes fourier transforms selective amplification en- hance diversity input examples, revealing critical role high-frequency in- formation adversarial attacks. proposed hfm employs multi-scale gradient decomposition fusion capture global structures local details, effectively mitigating overfitting adversarial examples white-box models. extensive experiments demonstrate fsa consistently outperforms state-of- the-art attack methods, achieving average improvement attack success rate defense models compared bsr. related works improve transferability aes, two commonly adopted strategies gradient-based attacks input transformation-based attacks. gradient-based attacks refine gradient estimation generate imperceptible yet effective perturbations. boosting adversarial transferability via ham hfm mi-fgsm ni-fgsm extend i-fgsm incorporating momentum nesterov acceleration respectively, pi-fgsm enhances gradient updates employing amplification factor redistributing cut noise project ker- nel. contrast, input transformation-based attacks improve generalization aggre- gating gradients computed various transformed inputs. dim applies random resizing padding, tim employs convolutional smoothing enhance translation invariance, sim utilizes multi-scale resampling. admix blends images increase perturbation diversity, sia applies transformations individual blocks, bsr enhances transferability shuffling rotating image blocks. explore frequency-domain behavior dnn, wang al. yin al. found cnns rely heavily high-frequency components, improving accuracy reducing adversarial robustness. yin al. observed traditional data augmentation techniques, gaussian noise injection, induce robustness trade- high-frequency low-frequency perturbations. long al. proposed ssa, frequency-domain attack enhances adversarial transferability introduc- ing spectral perturbations dct domain gaussian noise uniform spec- tral mask. methodology framework overview fig. flowchart proposed fsa. proposed frequency-space attack fsa framework illustrated fig. integrates two modules high-frequency augmentation module ham hierarchical-gradient fusion module hfm. ham leverages fourier transforms zheng, wan, guo, kuang, selective amplification enhance diversity input examples generating aug- mented examples subsequent hfm stage, gradient augmented example undergoes multi-scale gradient decomposition fusion, resulting combined gradient utilized determine adversarial perturbation craft adversarial examples high-frequency augmentation module illustrated fig. proposed fsa begins ham process, gener- ates augmented example ham, input example dimensions first transformed frequency domain using fourier transform resulting frequency representation simultaneously, gaussian noise generated mapped frequency domain produce here, element follows high-frequency weighting matrix defined represent coordinate positions weighting function increases spatial frequency, amplifying high-frequency components. obtaining high-frequency enhanced noise integrated followed introduction random spectral modulation matrix adjust- ment, resulting formulated random frequency-domain mask, modulates intensity frequency-domain adjustments, weighting matrix dynami- cally regulates augmented example reconstructed inverse fourier transformation denotes inverse fourier transform. hierarchical-gradient fusion module incorporating aforementioned fsa, obtain augmented example gradient loss function respect denoted computed guide adversarial perturbation. hfm fig. gaussian pyramid constructed applying gaussian smoothing down-sampling producing series progressively lower-reso- lution gradient representations, namely denotes number layers pyramid. mathematical form given boosting adversarial transferability via ham hfm represents down-sampling operation, denotes gauss- ian blur operation. index denotes pyramid level, recursively generated previous level via gaussian blur down-sampling. aggregate information across different scales, fuse pyramid layers using weighted summation denotes combined gradient, up-samples level di- mension weighting coefficient defined decay factor lies within interval ensuring lower-resolution gra- dients contribute significantly combined gradient strategy inte- grates coarse-grained structural information deeper pyramid layers preserv- ing fine-grained details shallower layers. multi-scale fusion mechanism adap- tively balances local global gradient information optimization, ensuring effective gradient-guided updates. frequency-space attack according equation derive combined gradient forms core algorithm. iterative attack, step uses current adversarial example input compute corresponding combined gradient gradient guides update adversarial perturbation incor- porating mi-fgsm, develop fsa method. process generating adversarial examples using fsa summarized follows denotes momentum factor, denotes maximum perturbation, de- notes step size, represents number iterations, denotes sign function, ensures generated adversarial example clipped within -ball original image zheng, wan, guo, kuang, experiment settings implement framework tensorflow perform evaluations using nvidia rtx gpu. ensure comparability previous studies, adopt -norm metric distortion utilize cross-entropy loss training. dataset test dataset comprises randomly sampled images imagenet validation set provided lin models select four normally trained modelsinc-v inc-v incres- res-v- white-box models generate aes. adversarial examples evaluated eight defense models, including inc-vens inc-vens incres-vens hgd nips-r, nrp baselines six input transformation-based attacks selected baselines, in- cluding dim tim sim admix ssa bsr fairness, input transformations integrated mi-fgsm addition, compare combination versions methods, std combination mi-fgsm sim tim dim fsa-std. hyper-parameters maximum perturbation number iterations step size mi-fgsm uses momentum factor dim transformation probability tim employs kernel. sim admix use copies admix using examples mixing ratio ssa, set tuning factor standard deviation number spectral transformations bsr divides images blocks, applies transformations. proposed fsa, set decay factor pyramid layers ,spectral factor standard deviation number frequency-space attacks worth noting sec. evaluation combined input transformation, reduce spatial complex- ity balance transformation intensity, set parameter settings remain individual attack methods. evaluation single model section analyzes asr adversarial examples generated fsa four stand- ard trained models inc-v, inc-v, incres-v, res- eight defense models compares mainstream adversarial attack methods dim, tim, sim, admix, ssa, bsr. values table represent asr, i.e., misclas- sification rate target model. column represents attacked model, row represents adversarial examples generated corresponding source model. results indicate fsa achieves significantly higher average asr source models compared methods, improvement ranging furthermore, average asr fsa exceeds bsr specifically, compared traditional methods dim, tim, sim, boosting adversarial transferability via ham hfm fsa improves average success rate verifying effec- tiveness enhancing adversarial transferability. incres-vens ensemble model, using inc-v source model, fsa achieves attack success rate significantly outperforming bsr ssa. additionally, across source models, fsa outperforms bsr ssa results demonstrate superiority fsa generating transferable adversarial examples empha- size importance frequency-space collaborative attacks means enhance transferability. table asr eight models different input transformations single-model setting. model attack inc-vens inc-vens incres-vens hgd nips-r nrp avg. inc-v dim tim sim admix ssa bsr fsa inc-v dim tim sim admix ssa bsr fsa incres-v dim tim sim admix ssa bsr fsa res- dim tim sim admix ssa bsr fsa zheng, wan, guo, kuang, evaluation combined input transformation previous studies shown well-designed input transformation-based attack exhibits better transferability also compatible input transfor- mations, thereby generating transferable adversarial examples. evaluate this, integrate proposed fsa various input transformations, denoted dim- fsa, tim-fsa, sim-fsa, std-fsa, admix-fsa, bsr-fsa. report at- tack success rates adversarial examples generated inc-v table results demonstrate fsa significantly enhances transferability input transfor- mation-based attacks. fsa combined transformations, asr im- proves average. notably, fsa integrated si-di- tim, enhances transferability additionally, combining fsa bsr remarkably boosts attack success rate impressive average. findings highlight strong compatibility fsa existing input transformation strategies, demonstrating combination techniques optimize attack generalization enhance adversarial transferability. table asr eight models different transformations combined fsa. indi- cates asr improvement. attack inc-vens inc-vens incres-vens hgd nips-r nrp avg. dim-fsa tim-fsa sim-fsa std-fsa admix-fsa bsr-fsa ensemble models field adversarial attacks, ensemble attacks, generate adversarial exam- ples leveraging gradient information multiple source models simultaneously, proven effective improving transferability. verify compatibility proposed fsa method ensemble attacks, generate adversarial examples using four standard trained models inc-v, inc-v, incres-v, res- eval- uate asr eight defense models mentioned earlier. table presents asr ensemble models. terms average asr, fsa alone achieves comparable sota bsr method significantly outperforms traditional ap- proaches dim sim. moreover, fsa demonstrates distinct advantage ensemble defense models. instance, incres-vens defense model, fsa achieves asr surpassing even highly effective bsr std methods. results validate fsa-generated adversarial examples, crafted frequency-space collaboration, better adapt decision boundaries ensemble models, enhancing transferability robustness attacking defense mechanisms. boosting adversarial transferability via ham hfm table asr eight models adversarial examples crafted using ensemble inc-v, inc-v, incres-v, res-. attack inc-vens inc-vens incres-vens hgd nips-r nrp avg. dim tim sim admix ssa bsr std fsa dim-fsa tim-fsa di-tim-fsa ablation study investigate performance enhancements fsa, conducted ablation study hyperparameter analysis generating adversarial examples inc-v evaluating four standard models, namely inc-v, inc-v, incres-v, res- well three defense models, including inc-vens, inc-vens, incres-vens. number transformed images shown fig. fsa outperforms bsr transferability across three defense models, demonstrating higher efficiency effectiveness. increases, attack performance progres- sively improves stabilizes therefore, set experi- ments. value decay factor shown fig. attack success rate gen- erally improves decay factor increases. specifically, increases significant rise success rate observed. however, beyond improvement plateaus, increasing yields marginal benefits attack performance. therefore, set optimal hyperparameter balance at- tack effectiveness stability. value pyramid layers order evaluate impact number pyramid layers model robustness, ablation study conducted illustrated fig. attack success rate remains relatively stable across different settings. consequently, selected balanced configuration, offers competitive performance preventing potential redundancy un- necessary computational costs introduced deeper pyramids. size gaussian kernel assess impact gaussian kernel size attack performance, conducted ablation study varying pyramid layer ker- nel illustrated fig. attack success rate remains zheng, wan, guo, kuang, relatively stable across kernel sizes, fluctuations typically within narrow mar- gin less specifically, kernel achieves comparable results larger kernels, maintaining computational efficiency avoiding potential over- smoothing effects introduced excessively large kernels. given marginal differ- ences performance increased computational cost associated larger ker- nels, adopt kernel final setting. fig. asr adversarial examples generated fsa varying numbers images perturbation budget iteration steps gaussian kernel values spectral factor standard deviation ensure perturbation scale consistent maximum perturbation magnitude, conduct variance ablation study using multiples shown fig. remains within lower range attack success rate remains high. however, exceeds excessive gaussian noise may interfere optimal perturbation di- rection, leading decline attack effectiveness. additionally, increases attack success rate generally improves, increasing results slight decline. phenomenon may attributed excessive spectral enhancement, disrupts noise distribution reduces attacks generaliza- tion capability. based experimental results, combination boosting adversarial transferability via ham hfm consistently achieves high attack success rates across target models, making optimal configuration final experimental setup. fig. asr adversarial examples generated fsa different values spectral factor standard deviation contribution ham hfm. order comprehensively evaluate standalone effectiveness component within fsa framework, detailed module-level ablation study conducted. illustrated fig. modules demonstrate marked improvement performance applied independently, sur- passing baseline method mi-fgsm across target models. finding indi- cates component contributes significantly generation adversarial ex- amples. specifically, ham significantly enhances attack transferability introducing frequency-based augmentations increase input diversity, hfm improves attack robustness stability multi-scale gradient decomposition fusion. module demonstrates strong performance isolation, integrated fsa config- uration, incorporates ham hfm, attains highest overall perfor- mance, surpassing standalone configurations. results validate complemen- tary roles spatial- frequency-domain manipulations confirm effective- ness rationality fsas modular design. zheng, wan, guo, kuang, fig. asr fsa modules across three target models inc-vens, inc-vens, in- cres-vens. mi-fgsm serves baseline. visualization adversarial examples randomly select several adversarial examples visualization, obtained utilizing mi, bsr fsa attack inc-v. shown fig. five pairs clean images corresponding adversarial counterparts presented. observed, perturbation intensity introduced method comparable baseline methods, noticeable difference. raw image mi-fgsm bsr fsaours fig. visualization randomly selected raw images corresponding adversarial exam- ples crafted inc-v. boosting adversarial transferability via ham hfm conclusion paper, propose new frequency-space attack fsa framework com- bines frequency-domain spatial-domain transformations enhance effective- ness adversarial attacks, particularly black-box defense mechanisms. uti- lizing high-frequency augmentation hierarchical-gradient fusion, fsa signifi- cantly boosts transferability adversarial attacks emphasizing high-frequency components capturing global fine-grained features. experimental re- sults show fsa outperforms current state-of-the-art methods. findings high- light potential combining frequency-domain techniques traditional spatial- domain approaches develop robust efficient adversarial attacks, advancing field machine learning security. acknowledgments. research funded scientific research foundation shantou university grant ntft. disclosure interests. authors declare known competing financial interests personal relationships could appeared influence work reported paper. references shao, c., li, g., wu, j., zheng, exploring semantic redundancy using backdoor triggers complementary insight challenges facing dnn-based software vulnerability de- tection. acm transactions software engineering methodology ye, m., xu, x., zhang, q., wu, sharpness-aware optimization real-world adversarial attacks diverse compute platforms enhanced transferability. ieeecvf conference computer vision pattern recognition workshops cvprw, pp. xie, c., zhang, z., zhou, y., bai, s., wang, j., ren, z., yuille, a.l. improving transfera- bility adversarial examples input diversity. proc. ieee conf. computer vision pattern recognition cvpr, pp. dong, y., pang, t., su, h., zhu, evading defenses transferable adversarial examples translation-invariant attacks. proc. ieee conf. computer vision pattern recognition cvpr, pp. lin, j., song, c., he, k., wang, l., hopcroft, j.e. nesterov accelerated gradient scale invariance adversarial attacks. wang, x., he, x., wang, j., he, admix enhancing transferability adversarial attacks. proc. ieee int. conf. computer vision iccv, pp. wang, x., zhang, z., zhang, structure invariant transformation better adversarial transferability. proc. ieee int. conf. computer vision iccv, pp. wang, k., he, x., wang, w., wang, boosting adversarial transferability block shuf- fle rotation. proc. ieee conf. computer vision pattern recognition cvpr, pp. zheng, wan, guo, kuang, chen, k.-l., lee, c.-h., rao, b.d., garudadri, dnn based normalized time-fre- quency weighted criterion robust wideband doa estimation. icassp ieee international conference acoustics, speech signal processing icassp, pp. ieee zhang, z., meng, d., zhang, l., xiao, w., tian, range harmful frequency dnn corruption robustness. neurocomputing liu, y., wu, h., zhang, robust imperceptible black-box dnn watermarking based fourier perturbation analysis frequency sensitivity clustering. ieee trans- actions dependable secure computing hammoud, h., bibi, a., torr, p.h.s., ghanem, dont freak frequency-in- spired approach detecting backdoor poisoned samples dnns. ieeecvf conference computer vision pattern recognition workshops cvprw, pp. long, y., zhang, q., zeng, b., gao, l., liu, x., zhang, j., song, frequency domain model augmentation adversarial attack. proc. eur. conf. computer vision eccv, pp. springer wang, h., wu, x., huang, z., xing, e.p. high-frequency component helps explain generalization convolutional neural networks. proc. ieee conf. computer vision pattern recognition cvpr, pp. yin, d., gontijo lopes, r., shlens, j., cubuk, e.d., gilmer, fourier perspective model robustness computer vision. advances neural information processing sys- tems. wang, s., veldhuis, r., brune, c., strisciuglio, neural networks learn image classification? frequency shortcut perspective. proc. ieee int. conf. computer vi- sion iccv, pp. zhang, t., zhu, interpreting adversarially trained convolutional neural networks. proc. int. conf. machine learning icml, pp. dong, y., liao, f., pang, t., su, h., zhu, j., hu, x., li, boosting adversarial attacks momentum. proc. ieee conf. computer vision pattern recognition cvpr, pp. kurakin, a., goodfellow, i., bengio, adversarial machine learning scale. gao, l., zhang, q., song, j., liu, x., shen, h.t. patch-wise attack fooling deep neural network. proc. eur. conf. computer vision eccv, pp. springer russakovsky, o., deng, j., su, h., krause, j., satheesh, s., ma, s., huang, z., karpathy, a., khosla, a., bernstein, imagenet large scale visual recognition challenge. international journal computer vision szegedy, c., vanhoucke, v., ioffe, s., shlens, j., wojna, rethinking inception archi- tecture computer vision. proc. ieee conf. computer vision pattern recog- nition cvpr, pp. szegedy, c., ioffe, s., vanhoucke, v., alemi, inception-v, inception-resnet impact residual connections learning. proc. aaai conf. artificial intelligence aaai. he, k., zhang, x., ren, s., sun, deep residual learning image recognition. proc. ieee conf. computer vision pattern recognition cvpr, pp. tramr, f., kurakin, a., papernot, n., goodfellow, i., boneh, d., mcdaniel, ensemble adversarial training attacks defenses. boosting adversarial transferability via ham hfm liao, f., liang, m., dong, y., pang, t., hu, x., zhu, defense adversarial attacks using high-level representation guided denoiser. proc. ieee conf. computer vision pattern recognition cvpr, pp. xie, c., wang, j., zhang, z., ren, z., yuille, mitigating adversarial effects ran- domization. liu, z., liu, q., liu, t., xu, n., lin, x., wang, y., wen, feature distillation dnn- oriented jpeg compression adversarial examples. proc. ieee conf. computer vision pattern recognition cvpr, pp. ieee naseer, m., khan, s., hayat, m., khan, f.s., porikli, self-supervised approach adversarial robustness. proc. ieee conf. computer vision pattern recognition cvpr, pp.", "published_date": "2025-05-27T13:32:52+00:00"}
{"id": "2505.20981v2", "title": "RefAV: Towards Planning-Centric Scenario Mining", "authors": ["Cainan Davidson", "Deva Ramanan", "Neehar Peri"], "summary": "autonomous vehicles avs collect pseudo-label terabytes multi-modal data localized maps normal fleet testing. however, identifying interesting safety-critical scenarios uncurated driving logs remains significant challenge. traditional scenario mining techniques error-prone prohibitively time-consuming, often relying hand-crafted structured queries. work, revisit spatio-temporal scenario mining lens recent vision-language models vlms detect whether described scenario occurs driving log and, so, precisely localize time space. address problem, introduce refav, large-scale dataset diverse natural language queries describe complex multi-agent interactions relevant motion planning derived driving logs argoverse sensor dataset. evaluate several referential multi-object trackers present empirical analysis baselines. notably, find naively repurposing off-the-shelf vlms yields poor performance, suggesting scenario mining presents unique challenges. lastly, discuss recent cvpr competition share insights community. code dataset available", "full_text": "cs.cv jun refav towards planning-centric scenario mining cainan davidson, deva ramanan, neehar peri carnegie mellon university cainand.github.iorefav abstract autonomous vehicles avs collect pseudo-label terabytes multi-modal data localized maps normal fleet testing. however, identifying interesting safety-critical scenarios uncurated driving logs remains significant challenge. traditional scenario mining techniques error-prone prohibitively time-consuming, often relying hand-crafted structured queries. work, revisit spatio-temporal scenario mining lens recent vision-language models vlms detect whether described scenario occurs driving log and, so, precisely localize time space. address problem, introduce refav, large-scale dataset diverse natural language queries describe complex multi-agent interactions relevant motion planning derived driving logs argoverse sensor dataset. evaluate several referential multi-object trackers present empirical analysis baselines. notably, find naively repurposing off-the-shelf vlms yields poor performance, suggesting scenario mining presents unique challenges. lastly, discuss recent cvpr competition share insights community. code dataset available github argoverse. introduction autonomous vehicle deployment public roads increased significantly recent years, waymo completing million rides per week february despite maturity autonomous ride-hailing services, avs still get accidents often require manual intervention although data-driven simulators integral component establishing safety case validating end-to-end autonomy real-world operational data remains critical part testing. however, identifying interesting safety-critical scenarios uncurated driving logs akin finding needle haystack due scale data collected normal fleet operations. paper, revisit task spatio-temporal scenario mining recent vision-language models vlms identify interesting multi-agent interactions using natural language cf. fig. status quo. although language-based scene understanding extensively studied context referential multi-object tracking rmot multi-modal visual question answering vqa vlm-based motion planning argue scenario mining presents unique challenges. rmot extends referential grounding associating referred objects time. however, unlike rmot, scenario mining guarantee referred objects exist given driving log. next, multi-modal vqa extends vqa additional visual modalities like lidar. although spatio-temporal scenario mining also leverages multi-modal input e.g. rgb, lidar maps, require methods output tracks instead text-based answers. lastly, vlm-based motion planners directly estimate future ego-vehicle waypoints based high-level language instructions past sensor measurements. contrast, scenario mining methods reason full driving log identify interactions non-ego vehicles. concretely, spatio-temporal scenario mining requires identifying described scenario occurs driving log raw sensor measurements e.g. rgb lidar, so, precisely localizing referred preprint. vehicle making left turn ego-vehicles path raining. frame frame scenario figure scenario mining problem setup. given natural language prompt vehicle making left turn ego-vehicles path raining, problem setup re- quires models determine whether described scenario occurs within -second driving log, so, precisely localize referred object space time raw sensor data lidar, ring cameras, maps. based example above, vlm localize start end timestamps red mini cooper executing pittsburgh left ego-vehicles path track. notably, pittsburgh left regional driving practice driver quickly makes left turn oncoming traffic proceeds. although common pittsburgh, maneuver technically illegal. therefore, argue scenario mining critical validating end-to-end autonomy order build comprehensive safety case. note referred objects shown blue, related objects green, objects red. objects time space tracks. support task, propose refav, large-scale dataset diverse natural language queries designed evaluate models ability find visual needle haystack. refav scenario mining dataset. repurpose argoverse sensor dataset contains driving logs synchronized lidar, ring cameras, maps, track annotations categories. curate set natural language prompts describe interesting often rare scenarios cf. fig. fig. using combination manual annotations procedural generation using large language models llms. notably, annotates ground truth tracks hz, allows fine-grained motion understanding. contrast, prior referential multi-object tracking benchmarks established task similar spatio-temporal scenario mining use nuscenes annotates ground truth tracks cf. table argoverses higher temporal resolution uniquely allows refav evaluate dynamic multi-agent interactions unlike prior work primarily evaluates referential expressions based static attributes like vehicle color relative heading. interestingly, find simply repurposing vlms scenario mining yields poor performance. referential tracking program synthesis. although prior referential trackers vlms achieve reasonable performance simple referential prompts e.g. find cars, find methods struggle compositional reasoning motion understanding e.g. find cars accelerating changing lanes. address problem, propose referential tracking program synthesis refprog, modular approach combines off-the-shelf tracks llms. inspired recent work program synthesis method uses llm break complex referential expressions simpler compositional actions. specifically, define api describe hand-crafted atomic functions e.g., turning, accelerating, changing lanes use llm synthesize program composes atomic actions corresponding natural language prompt. execute generated program filter off-the-shelf tracks identify subset tracks best align described scenario cf. fig. contributions. present three major contributions. first, introduce refav, large-scale dataset designed evaluate vlms scene understanding spatio-temporal localization. extensive experiments highlight limitations current methods, demonstrate effectiveness proposed program synthesis-based approach. lastly, highlight results recent cvpr challenge hosted conjunction workshop autonomous driving. related works referential grounding tracking long-standing challenges vision-language understanding. early datasets like refcoco, refcoco, refcocog helped popularize task, later adapted autonomous driving although prior work focused single- frame visual grounding, referkitti formalizes problem referential multi-object tracking rmot, extends referential object detection associating referred objects time. referkitti-v expands referkittis manual annotations using large language models llms improve prompt diversity. further, lamot consolidates sequences four tracking datasets create large-scale unified rmot benchmark. referential tracking methods broadly classified two-stage e.g. separately track filter predictions based natural language cues end-to-end approaches. ikun proposes plug- and-play knowledge unification module facilitate language grounding off-the-shelf trackers. similarly, refergpt presents zero-shot method filtering off-the-shelf tracks using clip scores fuzzy matching. contrast, jointnlt proposes unified visual grounding tracking framework, mmtrack reformulates tracking token generation. ovlm introduces memory-aware model rmot, referformer leverages language- conditioned object queries. recently, nuprompt extends referential tracking rgb input. different nuprompt, refav addresses multi-modal referential tracking dynamic scene understanding. visual question answering vision-language models improved significantly recent years due large-scale multi-modal pre-training. vision-language models vlms like llava blip- qwen.-vl show strong generalization across diverse domains however, state-of-the-art models still struggle spatial reasoning grounding address issue, spatialrgpt spatialvlm generate large-scale monocular depth pseudo-labels improve reasoning. despite effectiveness methods, zero-shot prompting yields poor compositional reasoning performance. instead, visprog vipergpt use llms generate executable code use tools like owlvit clip midas glip spatial understanding. however, existing program synthesis approaches primarily focus single-frame reasoning. contrast, interested understanding dynamic multi-agent interactions video sequences. take inspiration existing methods apply similar program synthesis-based approaches spatio-temporal scenario mining. vision-language models understanding extensively explored context representation learning, open-vocabulary perception, end-to-end driving. slidr distills features point clouds cross-modal learning. seal incorporates sam produce class-agnostic segments, sad leverages sam nerfs object segmentation. similarly, recent work like anything-d d-box-segment-anything integrate vlms detectors e.g., voxelnext interactive reconstruction labeling. vlms used extensively open-vocabulary perception autonomous driving. up-vl distills clip features lidar data generate amodal cuboids. recent work uses vlms generate pseudo-labels open-vocabulary perception, enabling zero-shot lidar panoptic segmentation object detection traditional grounding methods struggle complex instructions, multi-modal llms demonstrate impressive visual understanding. however, methods typically produce scene-level analysis end-to-end driving rather precise instance localization. contrast, framework combines language grounding precise geometric localization offline perception methods accurate vision-language reasoning. step define atomic functions def scenarioand def scenarioor def scenarionot def near def indirection def headingtoward def beingcrossedby def category def onintersection def hasvelocity def accelerating def changinglanes def turning object state relationship logic step compose atomic functions step verify scenarios generate verify code verify code reject modify accept verify video diversity sampling modify generated code execute generated code in-context examples api listing figure refav dataset creation. first, define set atomic functions identify state object track, relationship objects e.g. scene graph, set boolean logical operators support function composition. next, prompt llm permute atomic functions generate program corresponding natural language prompt. prompt separate llm generated program natural language prompt verify code. additionally manually verify code correctness. lastly, execute generated code ground-truth tracks visualize referred object track manually verify program output matches natural language prompt. sample valid programs maximize scenario diversity dataset. note yellow boxes highlight automated processes, blue boxes indicate steps requiring manual intervention. refav scenario mining natural language descriptions section, present approach curating natural language prompts sec describe four zero-shot scenario mining baselines sec creating refav unlike prior benchmarks primarily evaluate referential expressions based static attributes like vehicle color relative heading e.g. find red car left, focus mining interesting safety-critical scenarios relevant motion planning cf. fig. take inspiration recent planning benchmarks like nuplan identify planning-centric scenarios. particular, nuplan introduces set scenarios considered relevant safe motion planning. use template identify similar scenarios within av. use argoverse sensor dataset? sensor dataset contains second logs synchronized sensor measurements seven seven ring cameras two lidar sensors. moreover, dataset includes maps lane markings, crosswalk polygons, various lane types e.g. vehicle, bus, bike lanes. notably, annotates object categories track-level annotations hz. choose build refav top prior datasets like nuscenes annotate ground-truth tracks lower temporal resolution e.g. vs. hz, making difficult extract fine-grained motion. ablate appendix addition, kitti waymo open dataset label limited number categories e.g. car, pedestrian, bicycle maps, making difficult evaluate diverse multi-agent interactions. although annotates objects away ego vehicle, clip object tracks find current perception models struggle long-range detection tracking suggesting community ready address spatio-temporal scenario mining range. llm-based procedural scenario generation. key insight many complex scenarios e.g. find cars accelerating changing lanes broken simpler atomic actions e.g. find cars, accelerating, changing lanes, large set atomic actions composed generate new diverse scenarios. end, first define atomic functions based nuplans list planning scenarios cf. figure left. include full api listing appendix generate new scenario definition, provide llm e.g. claude sonnet full api listing, along in-context examples real compositions. prompt llm generate permutations atomic functions describe generated code natural language description cf. figure middle. execute generated code filter ground-truth tracks logs identify true positive matches. aggregate true positive log-prompt matches sample true positive log-prompt pairs maximize scenario diversity. lastly, randomly sample true negative log-prompt pairs. verifying procedurally generated scenarios. although llm-based procedural generation allows generate diverse scenarios scale, process perfect requires extensive manual validation cf. figure right. first, verify generated code aligns generated natural language description. manually review video clips true positive log-prompt matches ensure generated natural language prompt accurately describes localized tracks. notably, find two common error modes. first, llm often reverses relationship objects code prompt. example, given prompt find bicycles front car, generated code might actually describe scenario car front bicycles. second, find llms often generates underspecified code. example, given prompt bicycles traveling traffic, llm correctly generates code finds bicycles riding opposite direction ego vehicle, precisely specify side road ego vehicle. manual scenario annotation. despite versatility procedural generation approach, many interesting interactions cannot easily identified composing atomic functions. therefore, manually inspect videos validation test sets identify interesting driving behaviors cf. fig further, manually annotate weather e.g. clear, cloudy, rain, snow, fog lighting conditions e.g. daylight, duskdawn, night validation test sets attributes relevant establishing safety case driving conditions. see appendix details annotation workflow. dataset statistics. shown table dataset uniquely addresses task spatio-temporal scenario mining, whereas datasets focus object detection, referring multi-object tracking rmot, visual question answering vqa. further, refav uniquely built argoverse whereas existing datasets rely nuscenes kitti next, refav provides track-level annotations higher frequency hz, compared typical datasets, allowing fine-grained temporal analysis. datasets like nugrounding nuscenes-qa include large numbers expressions, refav emphasizes diversity annotation types, including referring expressions capture dynamic multi-agent interactions, weather, lighting conditions. lastly, refav one two datasets support negative prompts. table comparison benchmarks. language-based scene understanding extensively studied context referential multi-object tracking rmot multi-modal visual question answering vqa. different prior work, address problem spatio-temporal scenario mining. specifically, refav based argoverse provides track-level annotations categories hz. although refav include many referential expressions prior work e.g. omnidrive nugrounding referential annotations focus capturing diverse multi-agent interactions. lastly, refav includes negative prompts, allows accurately measure scenario mining performance. dataset base data task view anno. freq. expressions frames neg. prompts human anno. talkcar nuscenes detection front per video referring expression refer-kitti kitti rmot front referring expression nugrounding nuscenes rmot object attribute nuprompt nuscenes rmot object color nuscenes-qa nuscenes vqa none drivelm nuscenes vqa front pairs omnidrive nuscenes vqa none refav argoverse scenario mining referring expression, weather, lighting rgb lidar ego vehicle following vehicle crossed jaywalking pedestrian infrontvehicleand beingcrossedby oncrosswalk pedestrian prompt llm offline perception execute code filter tracks tracks generated code referred tracks figure method overview. refprog dual-path method independently generates perception outputs python-based programs referential grounding. given raw lidar rgb inputs, refprog runs offline perception model generate high quality tracks. parallel, prompts llm generate code identify referred track. finally, generated code executed filter output offline perception model produce final set referred objects shown green, related objects shown blue, objects shown red. scenario mining baselines present four referential tracking baselines scenario mining, including filtering referred class, referential track inflation, referential track classification, referential tracking program synthesis. repurpose trackers winning teams argoverse end-to-end forecasting challenge. filtering referred class refblind. propose simple blind baseline explicitly reason multi-agent interactions. given natural language prompt, use llm parse referred object class keep tracks class. example, natural language query find cars turning left, remove predicted tracks except cars. interestingly, refblind surprisingly strong baseline, even compared complex approaches, described next. referential track inflation refinflate. take inspiration cmd create zero-shot referential tracker using off-the-shelf foundation models. first, prompt detector referential grounding capabilities e.g., groundingdino natural language query e.g., car turning left generate box proposals. next, prompt sam predicted bounding boxes generate high-quality instance segmentation masks. generate oriented cuboid using set lidar points project instance mask. lastly, associate referred detections time using kalman filter referential track classification refclassify. take inspiration refergpt clas- sify output off-the-shelf trackers using clip referred objects objects. track, project predicted bounding box onto image plane extract clip image features timestep. next, compute cosine similarity per-timestep clip image embeddings clip text embedding referential prompt. consider track referreed object similarity score image embeddings text embedding fixed threshold track length. referential tracking program synthesis refprog. refprog takes similar approach llm-based procedural scenario generator. specifically, prompt llm generate code using api listing atomic actions given referential query. execute generated code off-the-shelf tracks identify referred objects, related objects objects cf. fig despite similarity refprog llm-based procedural scenario generator, highlight three key differences. first, refprog generates code based referential query, llm-based procedural scenario generator synthesizes referential query based generated code. posit role scenario generator considerably easier since many valid natural language prompts describe generated program, significantly fewer valid programs corresponding natural language prompt. second, refprog filters predicted tracks, scenario generator uses ground-truth tracks. lastly, code scenario generator verified correctness, code refprog used without modification. experiments section, briefly describe evaluation metrics sec. provide empirical analysis baselines sec. ablate impact different llms refprogs spatio-temporal scenario mining accuracy sec. metrics evaluate scenario mining methods using hota-temporal hota-track variants hota measure referential tracking performance balanced accuracy measure information retrieval accuracy. hota unified metric explicitly balances detection accuracy, association, localization, making better aligned human visual evaluations tracking performance. hota-temporal extends standard hota metric considering referred timestamps within track true positives. hota-track similar hota-temporal, penalize methods incorrectly predicting start end referred action. example, given prompt car turning right, hota-temporal considers timestamps car turning right full referred track true positives, whereas hota-track considers timestamps referred track true positives. also include hota results contextualize standard tracking performance referential tracking performance benchmarked trackers. additionally, use timestamp balanced accuracy timestamp-level classification metric log balanced accuracy log-level classification metric evaluate well methods identify timestamps logs contain objects match referential prompt, respectively. prompt, categorize objects three groups referred objects, primary objects specified prompt related objects, objects interact referred object objects, neither referred relevant prompt. compute hota-temporal hota-track metrics exclusively referred object class main paper. report performance related objects objects appendix table experimental results. evaluate several zero-shot referential tracking baselines. find refprog significantly outperforms zero-shot baselines. posit attributed refgen refclass effectively leveraging temporal context. method hota hota-temporal hota-track log bal. acc. timestamp bal. acc. ground truth filtering referred class refblind ground truth ledee referential track inflation refinflate groundingdino referential track classification refclassify ground truth ledee revoxeldet transfusion referential tracking program synthesis refprog ground truth ledee revoxeldet transfusion valeocast challenge submissions zeekr umcv ucm zxh empirical analysis results table presents several simple baselines addressing spatio-temporal scenario mining. evaluate ground-truth oracles zero-shot referential tracker understand tracking accuracy language grounding independently influence final performance. first, evaluating ground truth tracks without considering language prompts yields hota-temporal further, evaluating refblind ground truth tracks improves referential tracking accuracy however, refblind ledee tracks results slight performance drop. next, find refinflate performs worst four baselines, inflating detections challenging. contrast, refclassify performs considerably better. interestingly, refclassify oracle outperforms refblind oracle, suggesting clip-based filtering capture richer semantics class names alone. however, refclassify underperforms refblind using predicted tracks e.g., ledee scores vs. hota-temporal. finally, proposed refprog baseline significantly outperforms zero-shot methods. notably, refprog ledee tracks achieves improvement hota-temporal refclassify ledee tracks. analysis failure cases. find three baselines unique failure modes. first, note refinflate often predicts bounding boxes even cases referred object exists. attribute groundingdinos referential grounding pre-training, biases model towards always predicting bounding box since training image always least one true positive grouth-truth box. next, find refclassify performs poorly clip image embedding track lacks context required understand multi-agent interactions. specifically, projected image includes tracked object, lacks temporal context. lastly, find refprog fails cases api listing cannot generate valid program given prompt e.g. prompts involving weather lighting conditions. although one always add new atomic actions, strategy scalable. ablation study impact llms code generation. evaluate impact using different llms code generation quality refprog. use ledees tracks experiments ablation. somewhat unsurprisingly, find claude sonnet performs best cf. table achieving hota-temporal posit explicitly tuned code generation instruction following. gemini flash performs considerably worse despite also tuned coding. interestingly, find gemini qwen instruct struggle generate valid programs, instead try generate full programs. cvpr scenario mining challenge hosted challenge cvpr encourage broad community involvement addressing spatio-temporal scenario mining. competition recieved submissions eight teams submissions private close competition june th, aoe. notably, four teams beat best baseline. present current top three teams bottom table summarize contributions top three teams appendix include link full technical reports code here. table impact different llms code synthesis quality. evaluate impact using different llms program synthesis refprog pipeline. interestingly, find claude sonnet lowest program failure rate e.g. claude sonnets generated programs valid, compared qwen instruct, claude sonnet achieves highest hota-temporal. llm failure rate hota-temporal hota-track log bal. acc. timestamp bal. acc. qwen-.-b-instruct gemini-.-flash gemini-.-flash-preview-- claude-.-sonnet- claude-.-sonnet- pedestrian multiple dogs bicycle passing bus construction barrier ego vehicle merging car near stroller road ego vehicle approaching construction traffic controller pedestrian walking cars dog dog dog pedestrian ego vehicle regular vehicle ego vehicle regular vehicle regular vehicle regular vehicle stroller stroller bus bicycle construction barrel regular vehicle pedestrian regular vehicle ego vehicle official signaler consturction cone construction cone construction cone figure examples multi-agent interactions. visualize representative examples refav highlight diversity dataset. capture interactions vulnerable road users vehicles crowded intersection. scenario presents atypical instance common multi-agent interaction e.g. pedestrian walking dog. show complex ego-vehicle trajectory involves multiple moving vehicles. scenario illustrates example rare multi-object interaction. highlight scenario might require evasive maneuvers ego-vehicle e.g. occluded pedestrian might cross path ego-vehicle. finally, subfigure visualizes scenario multiple-step relationship e.g. official signaler standing inside construction zone. note show referred objects green, related objects blue, objects red. limitations future work noisy ground truth perception labels. quality scenario mining dataset limited quality ground truth perception labels av. example, jittery tracks lead poor motion classification short horizons. address issue significantly post-processing manually verifying generated scenarios mitigate label noise. limited diversity. although includes many interesting scenarios cf. fig. still relatively small compared industry-scale datasets. moreover, sensor dataset primarily created train benchmark perception models, intended scenario mining. result, include many useful annotations like traffic light state, weather information, lighting annotations. although dataset similar size prior work like nuprompt, future datasets explicitly curated address scenario mining. runtime complexity. refprog requires running offline perception algorithms executing llm- based programs, prohibitively slow. although components trivially parallelizable, posit approach sufficiently efficient real-world applications. future work also consider runtime scalability method beyond logs. conclusion paper, introduce refav, large-scale benchmark designed evaluate scenario mining. unlike prior language-based scene understanding tasks, find scenario mining poses unique challenges identifying complex multi-agent interactions. notably prior referential tracking baselines struggle challenging benchmark, demonstrating limitations existing methods. future work develop models capable reasoning complex, multi-modal temporal data. acknowledgments work supported part funding bosch research nsf grfp grant no. dge. would also like thank ishan khatri feedback early drafts references shuai bai, keqin chen, xuejing liu, jialin wang, wenbin ge, sibo song, kai dang, peng wang, shijie wang, jun tang, al. qwen. -vl technical report. arxiv preprint xuyang bai, zeyu hu, xinge zhu, qingqiu huang, yilun chen, hongbo fu, chiew-lan tai. transfusion robust lidar-camera fusion object detection transformers. proceedings ieeecvf conference computer vision pattern recognition. pp. yifan bai, dongming wu, yingfei liu, fan jia, weixin mao, ziheng zhang, yucheng zhao, jianbing shen, xing wei, tiancai wang, al. d-tokenized llm key reliable autonomous driving? arxiv preprint holger caesar, varun bankiti, alex lang, sourabh vora, venice erin liong, qiang xu, anush krishnan, pan, giancarlo baldan, oscar beijbom. nuscenes multimodal dataset autonomous driving. proceedings ieeecvf conference computer vision pattern recognition. pp. jiazhong cen, zanwei zhou, jiemin fang, wei shen, lingxi xie, dongsheng jiang, xiaopeng zhang, tian, al. segment anything nerfs. advances neural information processing systems tzoulio chamiti, leandro bella, adrian munteanu, nikos deligiannis. refergpt towards zero-shot referring multi-object tracking. arxiv preprint boyuan chen, zhuo xu, sean kirmani, brain ichter, dorsa sadigh, leonidas guibas, fei xia. spatialvlm endowing vision-language models spatial reasoning capabilities. proceedings ieeecvf conference computer vision pattern recognition. pp. feng chen, kanokphan lertniphonphan, yaqing meng, ling ding, jun xie, kaer huang, zhepeng wang. ledee solution unified detection, tracking, forecasting challenge. yukang chen, jianhui liu, xiangyu zhang, xiaojuan qi, jiaya jia. voxelnext fully sparse voxelnet object detection tracking. proceedings ieeecvf conference computer vision pattern recognition. pp. an-chieh cheng, hongxu yin, yang fu, qiushan guo, ruihan yang, jan kautz, xiaolong wang, sifei liu. spatialrgpt grounded spatial reasoning vision language models. arxiv preprint jang hyun cho, boris ivanovic, yulong cao, edward schmerling, yue wang, xinshuo weng, boyi li, yurong you, philipp krhenbhl, yan wang, al. language-image models understanding. arxiv preprint thierry deruyttere, simon vandenhende, dusan grujicic, luc van gool, marie- francine moens. talkcar taking control self-driving car. arxiv preprint xinpeng ding, jianhua han, hang xu, xiaodan liang, wei zhang, xiaomeng li. holistic autonomous driving understanding birds-eye-view injected multi-modal large models. proceedings ieeecvf conference computer vision pattern recognition. pp. yunhao du, cheng lei, zhicheng zhao, fei su. ikun speak trackers without re- training. proceedings ieeecvf conference computer vision pattern recognition. pp. andreas geiger, philip lenz, raquel urtasun. ready autonomous driving? kitti vision benchmark suite. ieee conference computer vision pattern recognition. ieee. pp. yash goyal, tejas khot, douglas summers-stay, dhruv batra, devi parikh. making vqa matter elevating role image understanding visual question answering. conference computer vision pattern recognition cvpr. cole gulino, justin fu, wenjie luo, george tucker, eli bronstein, yiren lu, jean harb, xinlei pan, yan wang, xiangyu chen, al. waymax accelerated, data-driven simulator large-scale autonomous driving research. advances neural information processing systems pp. tanmay gupta aniruddha kembhavi. visual programming compositional visual reason- ing without training. proceedings ieeecvf conference computer vision pattern recognition. pp. yuichi inoue, yuki yada, kotaro tanahashi, yamaguchi. nuscenes-mqa integrated evaluation captions autonomous driving datasets using markup annotations. proceedings ieeecvf winter conference applications computer vision wacv workshops. jan. pp. napat karnchanachari, dimitris geromichalos, kok seang tan, nanxiang li, christopher eriksen, shakiba yaghoubi, noushin mehdipour, gianmarco bernasconi, whye kit fong, yiluan guo, al. towards learning-based planning nuplan benchmark real-world autonomous driving. ieee international conference robotics automation icra. ieee. pp. sahar kazemzadeh, vicente ordonez, mark matten, tamara berg. referitgame refer- ring objects photographs natural scenes. proceedings conference empirical methods natural language processing emnlp. oct. pp. mehar khurana, neehar peri, deva ramanan, james hays. shelf-supervised multi-modal pre-training object detection. arxiv preprint alexander kirillov, eric mintun, nikhila ravi, hanzi mao, chloe rolland, laura gustafson, tete xiao, spencer whitehead, alexander berg, wan-yen lo, al. segment anything. proceedings ieeecvf international conference computer vision. pp. kirsten korosec. waymo robotaxi got trapped chick-fil-a drive-through. accessed apr. url got-trapped-in-chick-fil-a-drive-through. kirsten korosec. waymo doubled weekly robotaxi rides less year. techcrunch url techcrunch com waymo doubled-its-weekly-robotaxi-rides-in-less-than-a-year. lee, kang, hwang, yoon. typical accident scenarios urban area obtained clustering association rule mining real-world accident reports. heliyon jan. doi .j.heliyon..e. jae-keun lee, jin-hee lee, joohyun lee, soon kwon, heechul jung. re-voxeldet rethinking neck head architectures high-performance voxel-based detection. proceedings ieeecvf winter conference applications computer vision. pp. fuhao li, huan jin, bin gao, liaoyuan fan, lihui jiang, long zeng. nugrounding multi-view visual grounding framework autonomous driving. arxiv preprint junnan li, dongxu li, silvio savarese, steven hoi. blip- bootstrapping language- image pre-training frozen image encoders large language models. proceedings international conference machine learning. liunian harold li, pengchuan zhang, haotian zhang, jianwei yang, chunyuan li, yiwu zhong, lijuan wang, yuan, lei zhang, jenq-neng hwang, al. grounded language- image pre-training. proceedings ieeecvf conference computer vision pattern recognition. pp. yunhao li, xiaoqiong liu, luke liu, heng fan, libo zhang. lamot language-guided multi-object tracking. arxiv preprint haotian liu, chunyuan li, yuheng li, yong jae lee. improved baselines visual instruction tuning. haotian liu, chunyuan li, qingyang wu, yong jae lee. visual instruction tuning. neurips. shilong liu, zhaoyang zeng, tianhe ren, feng li, hao zhang, jie yang, qing jiang, chun- yuan li, jianwei yang, hang su, al. grounding dino marrying dino grounded pre-training open-set object detection. european conference computer vision. springer. pp. youquan liu, lingdong kong, jun cen, runnan chen, wenwei zhang, liang pan, kai chen, ziwei liu. segment point cloud sequences distilling vision foundation models. advances neural information processing systems pp. zhijian liu, haotian tang, alexander amini, xingyu yang, huizi mao, daniela rus, song han. bevfusion multi-task multi-sensor fusion unified birds-eye view representation. ieee international conference robotics automation icra. jonathon luiten, aljosa osep, patrick dendorfer, philip torr, andreas geiger, laura leal- taix, bastian leibe. hota higher order metric evaluating multi-object tracking. international journal computer vision pp. yechi ma, neehar peri, shuoquan wei, wei hua, deva ramanan, yanan li, shu kong. long-tailed detection via late fusion. arxiv preprint anish madan, neehar peri, shu kong, deva ramanan. revisiting few-shot object detection vision-language models. advances neural information processing systems pp. jiageng mao, yuxi qian, junjie ye, hang zhao, yue wang. gpt-driver learning drive gpt. arxiv preprint junhua mao, jonathan huang, alexander toshev, oana camburu, alan yuille, kevin murphy. generation comprehension unambiguous object descriptions. proceed- ings ieee conference computer vision pattern recognition. pp. damiano marsili, rohun agrawal, yisong yue, georgia gkioxari. visual agentic spatial reasoning dynamic api. arxiv preprint dan mihalascu. cruise robotaxi gets stuck wet concrete san francisco. accessed --. aug. url robotaxi-gets-stuck-wet-concrete-san-francisco. matthias minderer, alexey gritsenko, austin stone, maxim neumann, dirk weissenborn, alexey dosovitskiy, aravindh mahendran, anurag arnab, mostafa dehghani, zhuoran shen, al. simple open-vocabulary object detection. european conference computer vision. springer. pp. mahyar najibi, jingwei ji, yin zhou, charles qi, xinchen yan, scott ettinger, dragomir anguelov. unsupervised perception vision-language distillation autonomous driving. proceedings ieeecvf international conference computer vision iccv. oct. pp. aljosa osep, tim meinhardt, francesco ferroni, neehar peri, deva ramanan, laura leal-taixe. better call sal towards learning segment anything lidar. eccv. bastian ptzold, jan nogga, sven behnke. leveraging vision-language models open-vocabulary instance segmentation tracking. arxiv preprint neehar peri, achal dave, deva ramanan, shu kong. towards long tailed detection. corl neehar peri, mengtian li, benjamin wilson, yu-xiong wang, james hays, deva ra- manan. empirical analysis range object detection. arxiv preprint tianwen qian, jingjing chen, linhai zhuo, yang jiao, yu-gang jiang. nuscenes-qa multi-modal visual question answering benchmark autonomous driving scenario. proceedings aaai conference artificial intelligence. vol. pp. alec radford, jong wook kim, chris hallacy, aditya ramesh, gabriel goh, sandhini agarwal, girish sastry, amanda askell, pamela mishkin, jack clark, gretchen krueger, ilya sutskever. learning transferable visual models natural language supervision. proceedings international conference machine learning. ren ranftl, katrin lasinger, david hafner, konrad schindler, vladlen koltun. towards robust monocular depth estimation mixing datasets zero-shot cross-dataset transfer. ieee transactions pattern analysis machine intelligence pp. peter robicheaux, matvei popov, anish madan, isaac robinson, joseph nelson, deva ra- manan, neehar peri. roboflow-vl multi-domain object detection benchmark vision-language models. corentin sautier, gilles puy, spyros gidaris, alexandre boulch, andrei bursuc, renaud marlet. image-to-lidar self-supervised distillation autonomous driving data. pro- ceedings ieeecvf conference computer vision pattern recognition. pp. hao shao, yuxuan hu, letian wang, guanglu song, steven waslander, liu, hong- sheng li. lmdrive closed-loop end-to-end driving large language models. pro- ceedings ieeecvf conference computer vision pattern recognition. pp. qiuhong shen, xingyi yang, xinchao wang. anything-d towards single-view anything reconstruction wild. arxiv preprint chonghao sima, katrin renz, kashyap chitta, chen, hanxue zhang, chengen xie, jens beiwenger, ping luo, andreas geiger, hongyang li. drivelm driving graph visual question answering. european conference computer vision. springer. pp. pei sun, henrik kretzschmar, xerxes dotiwalla, aurelien chouard, vijaysai patnaik, paul tsui, james guo, yin zhou, yuning chai, benjamin caine, al. scalability perception autonomous driving waymo open dataset. proceedings ieeecvf conference computer vision pattern recognition. pp. didac suris, sachit menon, carl vondrick. vipergpt visual inference via python execution reasoning. proceedings ieee international conference computer vision iccv ayca takmaz, cristiano saltori, neehar peri, tim meinhardt, riccardo lutio, laura leal-taixe, aljosa osep. towards learning complete anything lidar. arxiv abs. trisha thadani. woman stuck cruise self-driving car getting hit driver. ac- cessed --. oct. url cruise-self-driving-car-san-francisco. kexin tian, jingrui mao, yunlong zhang, jiwan jiang, yang zhou, zhengzhong tu. nuscenes-spatialqa spatial understanding reasoning benchmark vision- language models autonomous driving. arxiv preprint arun balajee vasudevan, dengxin dai, luc van gool. object referring videos language human gaze. proceedings ieee conference computer vision pattern recognition cvpr. june arun balajee vasudevan, neehar peri, jeff schneider, deva ramanan. planning adaptive world models autonomous driving. arxiv preprint shihao wang, zhiding yu, xiaohui jiang, shiyi lan, min shi, nadine chang, jan kautz, ying li, jose alvarez. omnidrive holistic llm-agent framework autonomous driving perception, reasoning planning. xinshuo weng, jianren wang, david held, kris kitani. multi-object tracking base- line new evaluation metrics. ieeersj international conference intelligent robots systems iros. ieee. pp. benjamin wilson, william qi, tanmay agarwal, john lambert, jagjeet singh, siddhesh khandelwal, bowen pan, ratnesh kumar, andrew hartnett, jhony kaesemodel pontes, al. argoverse next generation datasets self-driving perception forecasting. arxiv preprint dongming wu, wencheng han, tiancai wang, xingping dong, xiangyu zhang, jianbing shen. referring multi-object tracking. proceedings ieeecvf conference computer vision pattern recognition. pp. dongming wu, wencheng han, tiancai wang, yingfei liu, xiangyu zhang, jianbing shen. language prompt autonomous driving. aaai jiannan wu, jiang, peize sun, zehuan yuan, ping luo. language queries refer- ring video object segmentation. proceedings ieeecvf conference computer vision pattern recognition. pp. yihong xu, loi zablocki, alexandre boulch, gilles puy, mickael chen, florent bartoccioni, nermin samet, oriane simoni, spyros gidaris, tuan-hung vu, al. valeocast modular approach end-to-end forecasting. arxiv preprint yang, yun chen, jingkang wang, sivabalan manivasagam, wei-chiu ma, anqi joyce yang, raquel urtasun. unisim neural closed-loop sensor simulator. cvpr. licheng yu, patrick poirson, yang, alexander berg, tamara berg. modeling context referring expressions. computer visioneccv european conference, amsterdam, netherlands, october proceedings, part springer. pp. yani zhang, dongming wu, wencheng han, xingping dong. bootstrapping referring multi-object tracking. arxiv preprint yaozong zheng, bineng zhong, qihua liang, guorong li, rongrong ji, xianxian li. toward unified token learning vision-language tracking. ieee transactions circuits systems video technology pp. doi .tcsvt. zhou, zikun zhou, kaige mao, zhenyu he. joint visual grounding tracking natural language specification. arxiv cs.cv. implementation details present additional implementation details reproduce baseline experiments below. code available github. primarily consider two-stage baselines first track objects, classify referred objects, related objects, objects. repurpose trackers winning teams argoverse end-to-end forecasting challenge, include link base tracker outputs google drive. models trained output tracks annotated categories within argoverse include example prediction timestampns int trackid int confidence float classname str translationm np.array size np.array yaw np.array append track ego vehicle consistent trackid, confidence class name egovehicle, yaw radians, size length, width height meters translation meters translation represents offset centroid ego-vehicle ego-vehicle reference coordinate located near center rear axle. tracks oracle. naive baseline labels ground truth tracks referred objects. means logs timestamps labeled positive, giving balanced accuracy filtering referred class refblind. refblind filters tracks except corresponding referred class. get referred class, prompt claude sonnet follows categories categories please select one category corresponds object focus descriptionnatural language description. example, description vehicle turning intersection nearby pedestrians output would vehicle. description ego vehicle near construction barrel output would egovehicle. output category name, capital letters including underscores necessary. define two super-categories vehicle anyobject. vehicle category man- ually defined contain classes articulatedbus, boxtruck, bus, egovehicle, largevehicle, motorcycle, railedvehicle, regularvehicle, school bus, truck, truckcab. referential track classification refclassify. refclassify filters tracks computing prompt- track clip similarity time. timestep, project eight vertices objects bounding box onto image plane seven cameras. cull objects partially fall within frustum camera. projected bounding box, three vertices fall within image plane, take crop using minimum maximum image coordinates. pad crop pixels side feed resulting image clip vit-l model obtain clip image embeddings. also use clip vit-l compute text embeddings prompts dataset. next, calculate cosine similarity scores tracks image crops text prompt. since individual similarity scores tend noisy, follow refergpt applying dynamic threshold majority voting strategy identify referred object tracks. since dataset includes positive negative examples prompt, group similarity scores prompt threshold top scores. half scores track fall within top threshold, label entire track referred object. reduce number length one tracks i.e., tracks spanning one timestamp, apply similarity score modifier smodified scosine lentrack. referential track inflation refinflate. refinflate leverages off-the-shelf foundational models enable zero-shot referential tracking. implement groundingsam-like pipeline groundingdino generates bounding boxes based given prompt, sam segments objects within bounding boxes. extend process predict bounding boxes using multi-view video lidar data camera, prompt groundingdino input image referential prompt, extract bounding boxes use prompt sam object segmentation. perform operations native image resolution. next, project lidar points within cameras frustum onto corresponding image plane retain points fall within sam segmentation mask. retained points, compute medoid. apply dbscan cluster lidar points, using parameters minsamples filter points include cluster medoid. dbscan identifies medoid noise, instead select largest cluster. draw tight-fitting oriented bounding box around filtered set lidar points. simplify orientation estimation, align front bounding box front ego vehicle. associate bounding boxes across time, use greedy tracker applies hungarian matching match boxes timestep tracks using world-coordinate distance. set maximum matching distance meters. referential tracking program synthesis refprog. refprog filters off-the-shelf tracks using python programs synthesized large language model llm, without explicitly relying information lidar cameras. reduce time required run synthesized programs, first filter input tracks based confidence. compute tracks confidence score summing confidence values across timestamps. classes regularvehicle, pedestrian, bollard, constructioncone, constructionbarrel, retain top tracks per class. classes, retain top tracks per category. prompt llm following prompt please use following functions find instances referred object autonomous driving dataset. precise description, try avoid returning false positives. api listing api listing categories categories define single scenario descriptionnatural language description list examples prediction examples. output code comments part python block. feel free use liberal amount comments. define additional functions, filepaths. include imports. assume logdir outputdir variables given. wrap code one python block provide alternatives. output code even given functions expressive enough find scenario. include api listing appendix found prompt consistently produces reasonable code across tested llms. running synthesized programs, post-process output tracks accordance dataset practices. remove length one tracks noise discard object relationships spatial distance exceeds meters. remaining tracks, apply timestamp dilation reduce flickering referred object labels. symmetrically dilate time segments minimum seconds. example, base tracker tracks object seconds hz, synthesized code marks referred timestamps update final output label track referred seconds. finally, downsample output tracks evaluation. scenario mining annotation tool order manually annotate interesting scenarios, build custom web app claude cf. fig construct scenario, user selects object clicking point within image frame, writes natural language description, selects start end frames referred objects correspond prompt. project ray originating point find ground truth bounding box centroid closest ray. tool allows quickly generate multi-object multi-camera referential tracks. figure manual annotation tool. create annotation tool assist labeling manually defined scenarios. tool allows quickly annotate multi-object referential tracks av. referential tracking evaluation related objects refprog creates scene graph natural language prompt identifies referred objects, also classifies tracks related objects objects. example, given prompt vehicle near pedestrian, vehicle would referred object, pedes- trian would related object, everything else scene would considered objects. benchmark base tracker hota evaluate hota-temporal related objects objects table since objects related prompt, hota hota-temporal similar. table referential tracking accuracy related objects. present refprogs referential tracking accuracy related objects objects. find hota base tracker similar hota-temporal objects relevant referential prompt. method hota hota-temporal related hota-temporal referential tracking program synthesis ground truth ledee revoxeldet transfusion valeocast impact sampling rate refprog evaluate impact sampling rate refprog. first, subsample three detectors ledee valeocast bevfusion hz. next, associate detections abdmot generate tracks respectively. lastly, evaluate refprog trackers hz. find standard tracking performance drops subsampling due difficulty associating sparse detections. see similar drop hota-track, log balanced accuracy timestamp balanced accuracy. interestingly, see similar drop performance hota-temporal. suggests refprog might robust temporally sparse inputs switches. table impact sampling rate. evaluate refprogs performance subsampled inputs measure relative impact referential tracking accuracy. find standard tracking performance drops subsampling due difficulty associating sparse detections. method hota hota-temporal hota-track log bal. acc. timestamp bal. acc. ground truth ground truth ledee det. abdmot ledee det. abdmot valeocast det. abdmot valeocast det. abdmot bevfusion det. abdmot bevfusion det. abdmot dataset statistics present additional dataset statistics figure expression type expressions positive prompts negative prompts procedurally defined manually defined includes related objects ego referred ego related number referred objects number expressions figure refav expression statistics. compute number referring expressions categorized expression type, count number expressions positive negative, whether procedurally generated manually constructed. also highlight number positive expressions include related object annotations explicit references ego vehicle. bar chart right shows number objects refered expression. expressions refav refer zero e.g. negative prompt one object. average, expression refers objects. y-position meters x-position meters refav referred object distribution number bounding boxes y-position meters x-position meters refav sensor object distribution change sensor distribution figure refav object distribution. refav includes referred objects directions away ego vehicle. refav places special focus ego vehicle objects interact ego vehicle. therefore, referred objects disproportionately located road front ego vehicle. referred object heatmaps ego vehicle coordinate frame. summary cvpr competition top performers summarize contributions top teams below. present full technical reports code here. zeekr umcv extends refprog global context-aware generation processes queries collectively improved reasoning consistency. also propose multi-agent refinement system secondary refiner agent iteratively debugs enhances generated code. ucm proposes two key improvements refprog fault-tolerant iterative code generation ft-icg, refines faulty code re-prompting llm error feedback successful execution, enhanced prompting spatial relational functions ep-srf, clarifies function semantics prevent misinterpretations spatial relationships objects. zxh refines refprog prompting llms batch queries in-context examples instead prompting query one time. importantly, zxhs approach relies in-context learning batch prompting improve consistency efficiency. api listing present full python api listing below. def hasobjectsinrelativedirection trackcandidates dict relatedcandidates dict logdirpath directionliteralforward, backward, left, right, minnumberint, maxnumberintnp.inf withindistancefloat lateralthresh floatnp.inf dict identifies tracked objects least minimum number related candidates specified direction. minimum number met create relationships equal maxnumber closest objects. args trackcandidates tracks analyze scenario dictionary. relatedcandidates candidates check direction scenario dictionary. logdir path scenario logs. direction direction analyze track point view forward backward left right minnumber minimum number objects identify direction per timestamp. defaults maxnumber maximum number objects identify direction per timestamp. defaults infinity. withindistance maximum distance considering object direction. defaults infinity. lateralthresh maximum lateral distance related object sides tracked object. defaults infinity. returns dict scenario dictionary keys track uuids values dictionaries containing related candidate uuids lists timestamps condition met relative direction. example vehicleswithpedsinfront hasobjectsinrelativedirection vehicles pedestrians logdir directionforward minnumber def getobjectsinrelativedirection trackcandidates dict relatedcandidates dict logdirpath directionliteralforward, backward, left, right, minnumberint, maxnumberintnp.inf withindistancefloat lateralthresh floatnp.inf-dict returns scenario dictionary related candidates relative direction track candidates. args trackcandidates tracks scenario dictionary. relatedcandidates candidates check direction scenario dictionary. logdir path scenario logs. direction direction analyze track point view forward backward left right minnumber minimum number objects identify direction per timestamp. defaults maxnumber maximum number objects identify direction per timestamp. defaults infinity. withindistance maximum distance considering object direction. defaults infinity. lateralthresh maximum lateral distance related object sides tracked object. lateral distance distance distance sides object parallel specified direction. defaults infinity. returns dict scenario dictionary keys track uuids values dictionaries containing related candidate uuids lists timestamps condition met relative direction. example pedsinfrontofvehicles getobjectsinrelativedirection vehicles pedestrians logdir directionforward minnumber def getobjectsofcategory logdir category-dict returns objects given category log annotations. method accepts super -categories vehicle args logdir path directory containing scenario logs data. category category objects return returns dict scenario dict keys unique uuid object values list timestamps object view ego -vehicle. example trucks getobjectsofcategory logdir categorytruck def iscategory trackcandidates dict logdirpath categorystr returns objects given category trackcandidates dict. method accepts super -categories vehicle args trackcandidates scenario dict containing objects filter logdir path directory containing scenario logs data. category category objects return returns dict scenario dict keys unique object given category values list timestamps object view ego -vehicle. example boxtrucks iscategoryvehicles logdir categoryboxtruck def turning trackcandidates dict logdirpath directionliteralleft, right, none none-dict returns objects turning given direction. args trackcandidates objects want filter scenario dictionary. logdir path scenario logs. direction direction turn track point view left right none. returns dict filtered scenario dictionary keys track uuids meet turning criteria. values nested dictionaries containing timestamps. example turningleft turningvehicles logdir directionleft def changinglanes trackcandidates dict logdirpath directionliteralleft, right, none none dict identifies lane change events tracked objects scenario. args trackcandidates tracks analyze scenario dictionary. logdir path scenario logs. direction direction lane change. none indicates tracking either left right lane changes left right none. returns dict filtered scenario dictionary keys track uuids meet lane change criteria. values nested dictionaries containing timestamps related data. example leftlanechanges changinglanes vehicles logdir directionleft def haslateralacceleration trackcandidates dict logdirpath minaccel-np.inf maxaccelnp.inf dict objects lateral acceleartion minimum maximum thresholds. objects high lateral acceleration turning. postive values indicate accelaration left negative values indicate acceleration right. args trackcandidates tracks analyze scenario dictionary. logdir path scenario logs. direction direction lane change. none indicates tracking either left right lane changes left right none. returns dict filtered scenario dictionary keys track uuids meet lane change criteria. values nested dictionaries containing timestamps related data. example jerkingleft haslateralacceleration nonturningvehicles logdir minaccel def facingtoward trackcandidates dict relatedcandidates dict logdirpath withinanglefloat maxdistancefloat -dict identifies objects trackcandidates facing toward objects related candidates. related candidate must lie within region lying within withinangle degrees either side track -candidate forward axis. args trackcandidates tracks could heading toward another tracks relatedcandidates objects analyze see trackcandidates heading toward logdir path directory containing scenario logs data. fov field view trackcandidates related candidate must lie within region lying within fov degrees either side track -candidate forward axis. maxdistance maximum distance relatedcandidate away considered returns filtered scenario dict contains subset track candidates heading toward least one related candidates. example pedestrianfacingaway scenarionot facingtoward pedestrian egovehicle logdir withinangle def headingtoward trackcandidates dict relatedcandidates dict logdirpath anglethresholdfloat minimumspeedfloat maxdistancefloatnp.inf-dict identifies objects trackcandidates heading toward objects related candidates. track candidates acceleartion vector must within given angle threshold relative position vector. track candidates must component velocity toward related candidate greater minimumaccel. args trackcandidates tracks could heading toward another tracks relatedcandidates objects analyze see trackcandidates heading toward logdir path directory containing scenario logs data. anglethreshold maximum angular difference velocity vector relative position vector track candidate related candidate. minvel minimum magnitude component velocity toward related candidate maxdistance distance meters related candidates away track candidate considered returns filted scenario dict contains subset track candidates heading toward least one related candidates. example headingtowardtrafficcone headingtoward vehicles trafficcone logdir def accelerating trackcandidates dict logdirpath minaccelfloat maxaccelfloatnp.inf-dict identifies objects trackcandidates forward acceleration threshold. values reliably indicates braking. values reliably indiciates accelerating. args trackcandidates tracks analyze acceleration scenario dictionary logdir path directory containing scenario logs data. minaccel lower bound acceleration considered maxaccel upper bound acceleration considered returns filtered scenario dictionary containing objects acceleration lower upper bounds. example acceleratingmotorcycles accelerating motorcycles logdir def hasvelocity trackcandidates dict logdirpath minvelocityfloat maxvelocityfloatnp.inf-dict identifies objects velocity given maximum minimum velocities ms. stationary objects may velocity due annotation jitter. args trackcandidates tracks analyze scenario dictionary. logdir path scenario logs. minvelocity minimum velocity ms. defaults maxvelocity maximum velocity returns filtered scenario dictionary objects meeting velocity criteria. example fastvehicles hasminvelocity vehicles logdir minvelocity def atpedestriancrossing trackcandidates dict logdirpath withindistancefloat -dict identifies objects within certain distance pedestrian crossing. distance zero indicates object within boundaries pedestrian crossing. args trackcandidates tracks analyze scenario dictionary. logdir path scenario logs. withindistance distance meters track candidate must pedestrian crossing. distance zero means object must within boundaries pedestrian crossing. returns filtered scenario dictionary keys track uuids values lists timestamps. example vehiclesatpedcrossing atpedestriancrossing vehicles logdir def onlanetype trackuuiddict logdir lanetypeliteralbus, vehicle, bike -dict identifies objects specific lane type. args trackcandidates tracks analyze scenario dictionary. logdir path scenario logs. lanetype type lane check bus vehicle bike returns filtered scenario dictionary keys track uuids values lists timestamps. example vehiclesonbuslane onlanetype vehicles logdir lanetype bus def nearintersection trackuuiddict logdirpath thresholdfloat -dict identifies objects within specified threshold intersection meters. args trackcandidates tracks analyze scenario dictionary. logdir path scenario logs. threshold distance threshold meters define near intersection. returns filtered scenario dictionary keys track uuids values lists timestamps. example bicyclesnearintersection nearintersection bicycles logdir threshold def onintersection trackcandidates dict logdirpath identifies objects located top road intersection. args trackcandidates tracks analyze scenario dictionary. logdir path scenario logs. returns filtered scenario dictionary keys track uuids values lists timestamps. example strollersonintersection onintersection strollers logdir def beingcrossedby trackcandidates dict relatedcandidates dict logdirpath directionliteralforward, backward, left, rightforward, indirectionliteralclockwise ,counterclockwise either either forwardthresh float lateralthresh float -dict identifies objects crossed one related candidate objects. crossing defined related candidate centroid crossing half -midplane tracked candidate. direction half -midplane specified direction. args trackcandidates tracks analyze relatedcandidates candidates e.g., pedestrians vehicles check crossings. logdir path scenario logs. direction specifies axis direction half midplane extends indirection direction related candidate cross midplane considered crossing forwardthresh far midplane extends edge tracked object lateralthresh two planes offset midplane. related candidate crosses midplane continue considered crossing goes past lateralthresh returns filtered scenario dictionary containing track candidates crossed related candidates given specified constraints. example overtakingonleft beingcrossedby movingcars movingcars logdir direction left, indirection clockwise, forwardthresh vehiclescrossedbypeds beingcrossedby vehicles pedestrians logdir def nearobjects trackuuiddict candidateuuidsdict logdirpath distancethreshfloat minobjectsint, includeselfboolfalse-dict identifies timestamps tracked object near specified set related objects. args trackcandidates tracks analyze scenario dictionary. relatedcandidates candidates check proximity scenario dictionary. logdir path scenario logs. distancethresh maximum distance meters related candidate away considered near minobjects minimum number related objects required near tracked object. returns dict scenario dictionary keys timestamps tracked object near required number related objects. values lists related candidate uuids present timestamps. example vehiclesnearpedgroup nearobjects vehicles pedestrians logdir minobjects def following trackuuiddict candidateuuidsdict logdirpath dict returns timestamps tracked object following lead object following defined simultaneously moving direction lane. def headinginrelativedirectionto trackcandidates relatedcandidates logdir directionliteralsame opposite perpendicular returns subset track candidates traveling given direction compared related candidates. arguements trackcandidates set objects could traveling given direction relatedcandidates set objects direction relative logdir path log data direction direction positive tracks traveling relative related candidates opposite indicates track candidates traveling direction degrees direction related candidates heading toward. indicates track candidates traveling direction degrees direction related candiates heading toward. indicates track candidates traveling direction degrees direction related candiates heading toward. returns subset track candidates traveling given direction compared related candidates. example oncomingtraffic headinginrelativedirectionto vehicles egovehicle logdir directionopposite def stationary trackcandidates dict logdirpath returns objects moved less length observation scneario. object intended separate parked active vehicles. use hasvelocity thresholding want indicate vehicles temporarily stopped. args trackcandidates tracks analyze scenario dictionary. logdir path scenario logs. returns dict filtered scenario dictionary keys track uuids values lists timestamps object stationary. example parkedvehicles stationaryvehicles logdir def atstopsign trackcandidates dict logdirpath forwardthresh float identifies timestamps tracked object lane corresponding stop sign. tracked object must within stop sign. may highlight vehicles using street parking near stopped sign. args trackcandidates tracks analyze scenario dictionary. logdir path scenario logs. forwardthresh distance meters vehcile stop sign stop sign front direction returns dict filtered scenario dictionary keys track uuids values lists timestamps object stop sign. example vehiclesatstopsign atstopsignvehicles logdir def indrivablearea trackcandidates dict logdirpath-dict identifies objects within trackcandidates within drivable area. args trackcandidates tracks analyze scenario dictionary. logdir path scenario logs. returns dict filtered scenario dictionary keys track uuids values lists timestamps object drivable area. example busesindrivablearea indrivablearea buses logdir def onroad trackcandidates dict logdirpath-dict identifies objects road bike lane. function used place indriveablearea referencing objects road. road include parking lots driveable areas connecting road parking lots. args trackcandidates tracks filter scenario dictionary. logdir path scenario logs. returns subset track candidates currently road. example animalsonroad onroadanimals logdir def insamelane trackcandidates dict relatedcandidates dict logdirpath dict identifies tracks road lane related candidate. args trackcandidates tracks filter scenario dictionary relatedcandidates potential objects could lane track scenario dictionary logdir path scenario logs. returns dict filtered scenario dictionary keys track uuids values lists timestamps object road lane. example bicycleinsamelaneasvehicle insamelanebicycle regularvehicle logdir def onrelativesideofroad trackcandidates dict relatedcandidates dict logdirpath sideliteralsame opposite dict identifies tracks road lane related candidate. args trackcandidates tracks filter scenario dictionary relatedcandidates potential objects could lane track scenario dictionary logdir path scenario logs. returns dict filtered scenario dictionary keys track uuids values lists timestamps object road lane. example bicycleinsamelaneasvehicle insamelanebicycle regularvehicle logdir def scenarioandscenariodicts listdict -dict returns composed scenario track objects intersection track objects uuid timestamps. args scenariodicts scenarios combine returns dict filtered scenario dictionary contains tracked objects found given scenario dictionaries example jaywalkingpeds scenarioand pedsonroad pedsnotonpedestriancrossing def scenarioorscenariodicts listdict returns composed scenario tracks objects relationships input scenario dicts. args scenariodicts scenarios combine returns dict expanded scenario dictionary contains every tracked object given scenario dictionaries example becautiousaround scenarioor animalonroad strolleronroad def reverserelationship func wraps relational functions switch top level tracked objects relationships formed function. args relationalfunc function takes trackcandidates relatedcandidates first second arguements returns dict scenario dict swapped top -level tracks related candidates example groupofpedsnearvehicle reverserelationship nearobjectsvehicles peds logdir minobjects def scenarionotfunc wraps composable functions return difference input track dict output scenario dict. using scenarionot composable relational function return relationships args composablefunc function takes trackcandidates first input returns example activevehicles scenarionotstationaryvehicles logdir def outputscenario scenariodict descriptionstr logdirpath outputdirpath visualizeboolfalse visualizationkwargs outputs file containing predictions evaluation -ready format. provide visualization kwargs.", "published_date": "2025-05-27T10:14:35+00:00"}
{"id": "2505.20644v1", "title": "HCQA-1.5 @ Ego4D EgoSchema Challenge 2025", "authors": ["Haoyu Zhang", "Yisen Feng", "Qiaohui Chu", "Meng Liu", "Weili Guan", "Yaowei Wang", "Liqiang Nie"], "summary": "report, present method achieves third place egod egoschema challenge cvpr improve reliability answer prediction egocentric video question answering, propose effective extension previously proposed hcqa framework. approach introduces multi-source aggregation strategy generate diverse predictions, followed confidence-based filtering mechanism selects high-confidence answers directly. low-confidence cases, incorporate fine-grained reasoning module performs additional visual contextual analysis refine predictions. evaluated egoschema blind test set, method achieves accuracy human-curated multiple-choice questions, outperforming last years winning solution majority participating teams. code added", "full_text": "hcqa-. egod egoschema challenge haoyu zhang yisen feng, qiaohui chu meng liu, weili guan, yaowei wang liqiang nie harbin institute technology shenzhen pengcheng laboratory shandong jianzhu university zhang.hy., yisenfeng.hit, qiaohuichu, mengliu.sdu, honeyguan, nieliqianggmail.com wangywpcl.ac.cn abstract report, present method achieves third place egod egoschema challenge cvpr improve reliability answer prediction egocentric video question answering, propose effective exten- sion previously proposed hcqa framework. approach introduces multi-source aggregation strategy generate diverse predictions, followed confidence- based filtering mechanism selects high-confidence an- swers directly. low-confidence cases, incorporate fine-grained reasoning module performs additional vi- sual contextual analysis refine predictions. eval- uated egoschema blind test set, method achieves accuracy human-curated multiple- choice questions, outperforming last years winning solu- tion majority participating teams. code added introduction egocentric video understanding become key research focus fields embodied video-language modeling offering valuable insights people interact world first-person perspec- tive. compared third-person video, egocentric footage captures fine-grained, context-rich visual signals, also poses significant challenges include rapid camera motion, limited field view, frequent oc- clusions, complicate downstream reasoning tasks one representative task egoschema chal- lenge egod benchmark focuses long-form video question answering. goal se- lect correct answer five multiple-choice options, given three-minute-long egocentric video associ- ated question. evaluation conducted egoschema dataset includes human-curated questionanswer pairs spanning hours real-world egocentric video. dataset covers wide range natural human activities presents particularly challenging benchmark due long temporal context semantic ambiguity. recent progress large language models llms vision-language models vlms advanced perfor- mance video question answering existing meth- ods hcqa often rely single llm final decision stage. lead overconfidence in- correct predictions, especially visual cues sparse ambiguous. address this, propose simple yet ef- fective extension hcqa framework improves an- swer reliability two key enhancements multi- source aggregation, multiple advanced llms used produce diverse candidate answers fine- grained reasoning, low-confidence outputs selec- tively reprocessed using visual textual reasoning mod- ules. two-stage strategy enables robust decision- making challenging scenarios without introducing signif- icant architectural complexity. adopting enhanced framework, approach out- performs last years winning solution well majority participating teams current challenge, ultimately securing third place final leaderboard. specifically, pipeline achieves notable improvement hcqa, increasing accuracy shown table methodology illustrated figure given long egocentric video, first apply previously proposed hcqa framework obtain multiple diverse predictions. initial outputs serve basis two-stage decision-making process aimed improving reliability final answer. first stage, perform multi-source aggregation con- solidate results. high-confidence answers directly cs.cv may reason actions described captions involve concrete answer painting picture desk. confidence hcqa multi-source aggregation reason actions described captions involve concrete answer painting picture desk. confidence answer fine-grained reasoning high conf. vision reasoning think reasoning high conf. low conf. figure illustration two-stage decision-making process. selected final outputs, low-confidence ones for- warded fine-grained reasoning module re- finement. multi-source aggregation enhance prediction diversity, replace third-stage llm used original hcqa pipeline, gpt-o, several state-of-the-art models, including gemini-.- pro, gpt-., qwen. use multiple llms introduces complementary perspectives, im- proves robustness coverage across different input cases. effectively integrate outputs, adopt confidence- based filtering mechanism. sample, predictions confidence score higher scale considered reliable retained final answers. remaining low-confidence predictions passed next stage additional processing. fine-grained reasoning samples low-confidence predictions, introduce fine-grained reasoning module includes two com- plementary strategies vision-based reasoning thought- based reasoning. vision-based approach focuses re-analyzing visual evidence. specifically, extract frames uniformly original video input qwen.-vl-b model default settings generate refined predictions based visual content. contrast, thought-based approach emphasizes tex- tual contextual reasoning. aggregates available textual information, including captions summaries ex- tracted hcqa, well predictions generated multi-source aggregation stage. information fed deepseek-r performs deeper thinking rea- soning produce updated answer. table performance comparison existing work top five teams public leaderboard. method rank accuracy mplug-owl longvivit internvideo llovi videoagent proviq lifelongmemory gemini-.-pro gpt-o qwen.-vl-b noahs ark lab ccego pcie pcie reality distortion hcqa-. ilearn. finally, compare outputs reasoning strategies select one higher confidence score final result. combined approach allows system recover reliable answers even initially uncertain predictions, improving overall robustness accuracy framework. experiment performance comparison table presents performance comparison existing methods top five teams public leaderboard egod egoschema challenge. method, hcqa- ilearn., achieves accuracy securing third place among submissions. outperforms several strong summary puts buttons dices table, arranges picks multiple cards. looks around focuses table cards picking placing cards continually. man interact cards, man picking placing cards dices periodically. consistently counts arranges cards dices table. moves around slightly, shuffling selecting cards repeatedly, occasionally looking table cards. continues engage cards small items table, pellets, bottle tops, balls, cloth pieces. throughout activity, man continue picking, placing, interacting cards playing catan game involving dices. process involves routine picking, shuffling, arranging cards, occasional interactions individuals like man person based putting looking cards. finally, continues arrange, pick, put cards, well engage playing activities organizing items table. question describe main objective overall process depicted video, mentioning primary actions performed person. option practicing magic tricks. option playing card game person. option currently, actively engaged playing strategic board game. option currently, diligently sorting numerous playing cards. option currently, diligently constructing delicate house cards. reason throughout video, individuals repeatedly pick, arrange, shuffle, place cards dice table. consistent interactions least one person man including handing cards dice, looking cards, actions specifically referencing playing catan game, well-known strategic board game involves cards, dice, game pieces. clear organization handling cards, references playing catan interactions dice people indicate main objective simply sorting cards, performing magic, building house cards, rather playing strategic board game, specifically settlers catan. therefore, best option actively engaged playing strategic board game. answer option ground truth option picks card gives card arranges cards caption figure one failed example framework egoschema subset. baselines, including gpt-o gemini-.-pro well top teams noahs ark lab ccego results demonstrate effec- tiveness multi-source aggregation fine-grained reasoning handling complex, long-form egocentric video question answering. ablation study table presents ablation study assessing contribu- tions different models stage hcqa-. framework. stage individual models gemini-.- pro, qwen., gpt-. yield varying levels accu- racy, gpt-. achieving highest integrat- ing outputs selecting answer highest confidence leads performance gains, demonstrat- ing advantage model ensemble. stage qwen.-vl-b deepseek-r per- form similarly vs. applied low- confidence samples, confirming value fine-grained reasoning uncertain cases. combining stages full pipeline results highest overall accuracy indicating stage makes positive contribution final performance. case study figure shows failed example framework. model incorrectly selects option actively engaged playing strategic board game instead correct option playing card game person. mistake mainly due models overreliance surface-level visual cuessuch presence dice cardsthat commonly associated strategic board games like settlers catan. however, question emphasizes objects overall objective interaction individuals. table ablation study different models framework. stage model accuracy gemini-.-pro gpt-. qwen. integration qwen.-vl-b deepseek-r hcqa-. summary clearly describes another per- son repeatedly picking, handing, arranging cards to- gether, model fails capture collaborative aspect instead focuses categorizing scene based game components. suggests weakness understand- ing social dynamics intent, particularly multiple plausible interpretations share similar visual elements. conclusion propose effective enhancement hcqa frame- work egocentric video question answering, integrat- ing multi-source aggregation fine-grained reasoning. selectively reasoning low-confidence cases, method improves reliability final answers. achieving accuracy egoschema benchmark ranking third cvpr challenge, approach demon- strates strong performance design. references shuai bai, keqin chen, xuejing liu, jialin wang, wenbin ge, sibo song, kai dang, peng wang, shijie wang, jun tang, al. qwen. -vl technical report. arxiv preprint rohan choudhury, koichiro niinuma, kris kitani, laszlo jeni. zero-shot video question answering pro- cedural programs. arxiv preprint yisen feng, haoyu zhang, yuquan xie, zaijing li, meng liu, liqiang nie. objectnlq egod episodic memory challenge arxiv preprint yisen feng, haoyu zhang, meng liu, weili guan, liqiang nie. object-shot enhanced grounding network egocentric video. arxiv preprint kristen grauman, andrew westbury, eugene byrne, zachary chavis, antonino furnari, rohit girdhar, jackson hamburger, hao jiang, miao liu, xingyu liu, al. egod around world hours egocentric video. pro- ceedings ieeecvf conference computer vision pattern recognition, pages weili guan, xuemeng song, haoyu zhang, meng liu, chung-hsing yeh, xiaojun chang. bi-directional het- erogeneous graph hashing towards efficient outfit recom- mendation. proceedings acm international conference multimedia, pages daya guo, dejian yang, haowei zhang, junxiao song, ruoyu zhang, runxin xu, qihao zhu, shirong ma, peiyi wang, xiao bi, al. deepseek-r incentivizing reasoning capability llms via reinforcement learning. arxiv preprint aaron hurst, adam lerer, adam goucher, adam perel- man, aditya ramesh, aidan clark, ostrow, akila weli- hinda, alan hayes, alec radford, al. gpt-o system card. arxiv preprint karttikeya mangalam, raiymbek akshulakov, jitendra malik. egoschema diagnostic benchmark long- form video language understanding. advances neural in- formation processing systems, karttikeya mangalam, raiymbek akshulakov, jitendra malik. egoschema diagnostic benchmark long- form video language understanding. advances neural in- formation processing systems, pinelopi papalampidi, skanda koppula, shreya pathak, justin chiu, joe heyward, viorica patraucean, jiajun shen, antoine miech, andrew zisserman, aida nematzdeh. simple recipe contrastively pre-training video-first en- coders beyond frames. arxiv preprint machel reid, nikolay savinov, denis teplyashin, dmitry lepikhin, timothy lillicrap, jean-baptiste alayrac, radu soricut, angeliki lazaridou, orhan firat, julian schrit- twieser, al. gemini unlocking multimodal under- standing across millions tokens context. arxiv preprint xiaohan wang, yuhui zhang, orr zohar, serena yeung-levy. videoagent long-form video understand- ing large language model agent. arxiv preprint ying wang, yanlai yang, mengye ren. lifelongmem- ory leveraging llms answering queries egocentric videos. arxiv preprint wang, kunchang li, xinhao li, jiashuo yu, yinan he, guo chen, baoqi pei, rongkun zheng, jilan xu, zun wang, al. internvideo scaling video foundation mod- els multimodal video understanding. arxiv preprint yunxiao wang, meng liu, rui shao, haoyu zhang, bin wen, fan yang, tingting gao, zhang, liqiang nie. time temporal-sensitive multi-dimensional instruc- tion tuning benchmarking video-llms. arxiv preprint yang, anfeng li, baosong yang, beichen zhang, binyuan hui, zheng, bowen yu, chang gao, chengen huang, chenxu lv, al. qwen technical report. arxiv preprint qinghao ye, haiyang xu, guohai xu, jiabo ye, ming yan, yiyang zhou, junyang wang, anwen hu, pengcheng shi, yaya shi, al. mplug-owl modularization empowers large language models multimodality. arxiv preprint zhang, taixi lu, mohaiminul islam, ziyang wang, shoubin yu, mohit bansal, gedas bertasius. sim- ple llm framework long-range video question-answering. arxiv preprint haoyu zhang, meng liu, zan gao, xiaoqiang lei, yinglong wang, liqiang nie. multimodal dialog system rela- tional graph-based context-aware question understanding. proceedings acm international conference multimedia, pages haoyu zhang, meng liu, yuhong li, ming yan, zan gao, xiaojun chang, liqiang nie. attribute-guided collab- orative learning partial person re-identification. ieee transactions pattern analysis machine intelligence, haoyu zhang, meng liu, yaowei wang, cao, weili guan, liqiang nie. uncovering hidden connections iterative tracking reasoning video-grounded dialog. arxiv preprint haoyu zhang, meng liu, zixin liu, xuemeng song, yaowei wang, liqiang nie. multi-factor adaptive vision selec- tion egocentric video question answering. proceedings international conference machine learning, pages pmlr, haoyu zhang, yuquan xie, yisen feng, zaijing li, meng liu, liqiang nie. hcqa egod egoschema challenge arxiv preprint haoyu zhang, qiaohui chu, meng liu, yunxiao wang, bin wen, fan yang, tingting gao, zhang, yaowei wang, liqiang nie. exoego exocentric knowledge guided mllm egocentric video understanding. arxiv preprint", "published_date": "2025-05-27T02:45:14+00:00"}
{"id": "2505.20612v2", "title": "Roboflow100-VL: A Multi-Domain Object Detection Benchmark for Vision-Language Models", "authors": ["Peter Robicheaux", "Matvei Popov", "Anish Madan", "Isaac Robinson", "Joseph Nelson", "Deva Ramanan", "Neehar Peri"], "summary": "vision-language models vlms trained internet-scale data achieve remarkable zero-shot detection performance common objects like car, truck, pedestrian. however, state-of-the-art models still struggle generalize out-of-distribution classes, tasks imaging modalities typically found pre-training. rather simply re-training vlms visual data, argue one align vlms new concepts annotation instructions containing visual examples rich textual descriptions. end, introduce roboflow-vl, large-scale collection multi-modal object detection datasets diverse concepts commonly found vlm pre-training. evaluate state-of-the-art models benchmark zero-shot, few-shot, semi-supervised, fully-supervised settings, allowing comparison across data regimes. notably, find vlms like groundingdino qwen.-vl achieve less zero-shot accuracy challenging medical imaging datasets within roboflow-vl, demonstrating need few-shot concept alignment. lastly, discuss recent cvpr foundational fsod competition share insights community. notably, winning team significantly outperforms baseline map! code dataset available", "full_text": "cs.cv jun roboflow-vl multi-domain object detection benchmark vision-language models peter robicheaux,, matvei popov,, anish madan, isaac robinson, joseph nelson, deva ramanan, neehar peri roboflow, carnegie mellon university rf-vl.org abstract vision-language models vlms trained internet-scale data achieve remark- able zero-shot detection performance common objects like car, truck, pedestrian. however, state-of-the-art models still struggle generalize out- of-distribution classes, tasks imaging modalities typically found pre-training. rather simply re-training vlms visual data, argue one align vlms new concepts annotation instructions contain- ing visual examples rich textual descriptions. end, introduce roboflow-vl, large-scale collection multi-modal object detection datasets diverse concepts commonly found vlm pre-training. evaluate state-of-the-art models benchmark zero-shot, few-shot, semi- supervised, fully-supervised settings, allowing comparison across data regimes. notably, find vlms like groundingdino qwen.-vl achieve less zero-shot accuracy challenging medical imaging datasets within roboflow-vl, demonstrating need few-shot concept alignment. lastly, discuss recent cvpr foundational fsod competition share insights community. notably, winning team significantly outper- forms baseline map! code dataset available github roboflow. introduction vision-language models vlms trained web-scale datasets achieve remarkable zero-shot per- formance many popular academic benchmarks however, performance foundation models varies greatly evaluated in-the-wild, particularly out-of-distribution classes, tasks e.g. material property estimation, defect detection, contextual action recogni- tion imaging modalities e.g. x-rays, thermal spectrum data, aerial imagery. paper, introduce roboflow-vl rf-vl, large-scale multi-domain dataset benchmark state-of-the-art vlms hundreds diverse concepts typically found internet pre-training. status quo. foundation models often trained large-scale datasets curated diverse sources around web. however, despite scale diversity, pre-training datasets still follow long-tail distribution causing foundation models generalize poorly rare concepts common approach improving performance vlms scale training data model size however, argue data always remain out-of-distribution, whether due sequestered internet created models training cutoff motivating need learn new concepts examples. evaluating out-of-distribution generalization. existing benchmarks primarily assess spatial understanding visual question answering vqa common sense reasoning equal contribution preprint. figure roboflow-vl dataset. identify set challenging datasets roboflow universe contain concepts typically found internet-scale pre-training. simplify analysis, cluster datasets using per-dataset clip embeddings seven categories. visualize examples categories above. furthermore, also generate multi-modal instructions dataset visual examples rich textual descriptions per class facilitate few-shot concept alignment. however, argue evaluating model performance compositional reasoning benchmarks alone effectively measure generalization out-of-distribution tasks. moreover, current spatial understanding grounding benchmarks e.g. refcoco odinw typically evaluate performance classes commonly found internet pre-training. demonstrate benchmarks artificially inflate model performance representative many real-world applications cf. table address limitation, introduce rf-vl, large-scale detection benchmark comprised multi-modal datasets diverse domains cf. fig. importantly, carefully curate rf-vl cannot solved simply prompting state-of-the-art models class names. specifically, include datasets classes labeled using scientific names e.g. liver fibrosis steatosis, acronyms e.g. dip mcp, context-dependent names e.g. detecting block vs. set context volleyball, material properties e.g. paper vs. soft plastic, diverse imaging modalities cf. fig. posit models must leverage multi-modal contextual information presented form multi-modal annotator instructions effectively align target concepts rf-vl. multi-modal annotator instructions. annotating large-scale datasets iterative process often requires extensive discussions data curators annotators clarify class definitions ensure label consistency. often multi-modal labeling instructions provide rich contextual information provided class names alone. argue aligning foundation models target concepts principally addressed lens few-shot learning presenting vision-language models visual examples rich textual descriptions per class cf. fig. importantly, approach mirrors align human annotators concepts interest few-shot multi-modal examples block thunderbolt context-dependent class-names mcp fibrosis scientific acronyms concepts normal paper soft plastic catch-all categories bluetooth insulator multi-modal imaging modalities figure hard examples roboflow-vl. dataset particularly challenging difficult detect objects rf-vl using class-names alone. specifically, select datasets classes labeled using scientific names, acronyms, context-dependent names, material properties. posit models must leverage multi-modal contextual annotations address hard examples. contributions. present three major contributions. first, introduce rf-vl, large-scale, multi-domain benchmark designed evaluate vision-language models vlms challenging real-world use cases. evaluate state-of-the-art models benchmark zero-shot, few-shot, semi-supervised, fully-supervised settings, allowing comparison across data regimes. extensive experiments highlight difficulty adapting vlms out-of-distribution tasks reveal limitations current state-of-the-art methods. lastly, highlight results recent cvpr challenge hosted conjunction workshop visual perception via learning open world. related works vision language models trained using large-scale, weakly supervised image-text pairs sourced web. although many vlms primarily focus classification image understanding, recent methods address spatial understanding open-vocabulary detectors. early approaches adapted vlms object detection classifying specific image regions integrating detection components frozen fine-tuned encoders. contrast, regionclip employs multi-stage training strategy involves generating pseudo-labels captioning data, performing region-text contrastive pre-training, fine-tuning detection tasks. glip treats detection phrase grounding problem using single text query entire image. detic improves long-tail detection performance utilizing image-level supervision imagenet notably, recent vlms achieve remarkable zero-shot performance widely used black box models diverse downstream applications recently, multi-modal large language models mllms qwen.-vl gemini pro frame spatial understanding text generation task. interestingly, generalist mllms perform worse object detection task-specific models like groundingdino fine-tuning vision-language models crucial adapting foundation models downstream tasks traditional fine-tuning methods, linear probing full fine-tuning computationally expensive. instead, parameter-efficient approaches like clip-adapter tip-adapter optimize lightweight mlps keeping encoders frozen. although prior few-shot learners commonly used meta-learning recent approaches show transfer learning generalizes better novel categories particular, pan et. al. demonstrates transfer learning effectively used fine-tune foundation models using multi-modal soft plastic soft plastic soft plastic soft plastic soft plastic soft plastic metal metal metal metal metal metal metal metal metal metal figure multi-modal few-shot examples. present example few-shot visual examples rich text descriptions used in-context prompting fine-tuning. notably, image examples used class may overlap guaranteed exhaustive annotations one class. multi-modal examples help clarify ambiguous concepts like soft plastic metal. examples. recently, in-context learning demonstrates promising results test-time few- shot adaptation without gradient-based fine-tuning. explore test-time fine-tuning strategies context mllms learn multi-modal annotator instructions. benchmarking vision-language models significant interest community. state-of-the-art vlms typically evaluated using benchmarks mmstar mmmu mme scienceqa mmbench mm-vet seed-bench mmvp bench- marks evaluate broad set vision-language tasks, including fine-grained perception, reasoning, common sense knowledge, problem solving various domains. however, existing evaluations primarily focus multi-modal understanding context visual question answering vqa. contrast, rf-vl evaluates vlm detection accuracy given visual examples rich textual descriptions. prior vlm grounding benchmarks like refcoco often focus referential grounding common object categories. recent efforts like odinw consider challenging scenarios sourcing real-world data roboflow however, find state-of-the-art methods achieve high zero-shot accuracy refcoco odinw suggesting datasets may well suited evaluating foundational few-shot object detection roboflow-vl benchmark shown fig. rf-vl consists diverse datasets typically found internet-scale pre-training. highlight data curation procedure section present several baselines evaluate state-of-the-art models zero-shot few-shot settings section also evaluate models semi-supervised fully-supervised settings appendix creating roboflow-vl source datasets roboflow universe, community-driven platform hosts diverse open-source datasets created solve real-world computer vision tasks. public datasets spanning medical imaging, agriculture, robotics, manufacturing, focus selecting high-quality datasets commonly found internet-scale pre-training e.g. coco objects goldg ccm better assess vlm generalization rare concepts. selecting candidates rf-vl, prioritized datasets images contained multiple objects, ensuring realistic evaluation beyond classification. addition, sought datasets semantically ambiguous names e.g. button refer clothing electronics encourage algorithms leverage multi-modal annotator instructions rather simply relying class names. manually validate labeling quality dataset ensure exhaustive annotations. cases without exhaustive annotations, manually re-annotate dataset best ability cf. fig total, spent hours labeling, reviewing, preparing dataset. figure dataset curation. begin sorting object detection datasets roboflow universe stars proxy quality usefulness community. next, manually filter datasets common classes, datasets images single focal object, datasets watermarks. generate -shot splits following protocol defined wang et.al. find subset images total instances per class. use -shot splits generate visually grounded annotator instructions, manually update instructions add salient details missed gpt-o. finally, human labelers verify images within dataset follow consistent annotation policies e.g. bounding-box fit, semantic legibility class names, completeness annotation instructions. multi-modal annotation generation. annotator instructions offer precise class definitions visual examples help clarify annotation policies e.g. highlighting typical cases, corner cases, negative examples improve labeling accuracy. despite providing significant value labeling process, datasets publicly release annotator instructions. recognizing importance instructions aligning humans target concepts interest, generate multi-modal annotator instructions datasets within rf-vl cf. fig prompt gpt-o generate initial set annotator instructions, providing in-context examples based nuimages annotator guidelines. prompt includes structured output template, along dataset metadata, class names, few-shot visual examples per class. practice, find gpt-o often overlooks few-shot images instead relies heavily class names generate class descriptions. notably, gpt-o struggles class names uninformative sometimes produces overly vague instructions that, correct, lack useful detail. address this, manually verify generated annotator instructions mitigate hallucinations incorporate additional informative visual details missed model. include annotation generation prompt appendix dataset statistics. figure left presents overview different types datasets within rf-vl, detailing number classes, images annotations per category. rf-vl contains total classes images, million annotations. category highest number classes followed industrial flora fauna despite fewer classes, flora fauna category highest number images annotations indicating higher density annotations per image. figure right provides visual representation class distribution, reinforcing dominance other, industrial, flora fauna categories. contrast, sports fewest classes least representation rf-vl. despite consisting datasets, rf-vl half number images coco making approachable benchmark academic community. state-of-the-art baselines train evaluate models dataset within rf-vl independently. importantly, tune parameters modify zero-shot prompts per-dataset. models, compute metrics using pycocotools maxdets set instead usual many images objects. discuss evaluation protocol appendix zero-shot baselines prompt models class names expressive descriptions detect target concepts. however, effectiveness zero-shot prompting depends pre-training data target class name semantically meaningful aligns well models foundational pre-training, performance strong otherwise, model fails catastrophically. benchmark dataset type classes images anno. aerial document flora fauna industrial medical sports aerial, document, flora fauna, industrial, medical, sports, other, figure dataset statistics. table left provides details number classes, images, annotations across different dataset types within rf-vl. figure right illustrates distribution dataset types count. notably, despite containing datasets, rf-vl size coco number images feasibly benchmarked academic-level compute. zero-shot performance detic owlv groundingdino mq-glip qwenv.-vl gemini pro few-shot baselines. evaluate three types few-shot baselines visual prompting, multi-modal prompting, federated fine-tuning. visual prompting uses images target concepts difficult describe text prompts help models learn novel concepts in-context. example, hard plastic broad ambiguous category hard define text, providing image examples improves concept alignment. typically, visual prompts tokenized fed inputs frozen vlm. here, apply mq-glip image prompting. multi-modal prompting combines language visual prompts leverage multi-modal features. intuitively, using text images yields better alignment using either modality alone. case soft plastic, ambiguous concepts clarified textual descriptions e.g., thin plastic film plastic bag alongside visual examples. visual language prompts tokenized separately fed frozen vlm. evaluate mq-glip gemini pro prompting models class names, few-shot images, annotator instructions. lastly, federated fine-tuning modifies standard cross-entropy classification treat exhaustively annotated classes true negatives image. follow implementation madan et. al. fine-tuning detic slightly modify federated loss fine-tuning yolo avoid using madan et. als frequency prior, opting instead determine hard negatives using per-image annotations. experiments conduct extensive experiments evaluate performance state-of-the-art models rf- vl. present zero-shot few-shot results below. see appendix additional implemen- tation details appendix semi-supervised fully supervised results. metrics dataset within rf-vl independently evaluated using ap. report average accuracy per super-category simplify analysis. rf-vl includes datasets out-of-distribution typical internet-scale pre-training data, making particularly challenging even vlms. construct few-shot split, follow k-shot dataset creation process established see appendix discussion few-shot split selection. importantly, methods across data regimes evaluated fully annotated test set. table highlight prior methods report different results coco odinw reproduced results. yolov yolov achieve slightly different performance coco original results reported using ultralytics, whereas results computed using pycocotools. importantly, discrepancy tooling yields larger disparity rf-vl, discussed appendix further, find qwen.-vl evaluates odinw using referential grounding protocol reported, see github issue instead traditional object detection protocol ours. specifically, referential grounding prompts model true positive classes test image, object detectors prompt model classes. former dramatically reduces number false positives. evaluate gemini pro using protocols completeness. empirical analysis results state-of-the-art zero-shot few-shot models struggle roboflow-vl. rf-vl much harder dataset prior open-vocabulary object detection benchmarks. specifically, groundingdino achieves map odinw-, reaches map rf-vl. similar trends seen qwen.-vl gemini pro cf. table notably, rf-vl odinw- sourced roboflow universe, dataset carefully curated evaluate performance target concepts typically found internet-scale pre-training. open-vocabulary object detectors outperform mllms. find open-vocabulary object detectors like detic, groundingdino, owlv, mq-glip consistently outperform mllms like qwen vl, gemini pro, despite mllms pre-training orders magnitude data. posit poor performance attributed mllms reporting per-box confidence scores ensuring predictions dont overlap e.g. non-maximal suppression. highlights advantage task-specific architectures generalist models. multi-modal annotator instructions provide limited benefit. somewhat surprisingly, state-of- the-art mllms struggle benefit multi-modal annotator instructions. fact, prompting instructions provides inconsistent benefit compared prompting class names e.g. qwen.vl improves gemini pro degrades considerably. intuitively, expect annotator instructions improve object detection performance resolving semantic ambiguity class names providing rich contextual information. however, posit performance decline attributed fact mllms instruction-tuned open vocabulary detection rigid prompt structures, making difficult effectively leverage additional contextual information. large-scale pre-training improves fine-tuned few-shot performance specialists. find fine-tuning groundingdino achieves best few-shot performance, significantly outperforming yolo variants notably, gradient-based fine-tuning baselines outperform in-context visual prompting multi-modal prompting methods, suggesting in- context prompting provides limited benefit rare classes seen pre-training. posit groundingdinos large-scale task-specific pre-training makes easier learn new concepts fine-tuning. coco detectors generalize beyond coco? real-time object detectors often optimized coco, assuming better performance coco translates real-world improvements. however, table comparison benchmarks. find state-of-the-art mllms achieve consid- erably lower performance rf-vl compared odinw-, highlighting difficulty proposed dataset. further, models performed better coco consistently perform better rf-vl, indicating newer yolo models might overfitting coco. lastly, highlight discrepancy reported reproduced numbers coco odinw. discrepancies coco evaluation attributed differences evaluation toolkits, discrepancies odinw evaluation attributed prior work evaluating models using referential grounding evaluation protocols, use standard object detection evaluation protocols. discuss section method coco val odinw- roboflow-vl reported reported zero-shot qwen .-vl class names gemini pro class names fully-supervised yolovn yolovn yolovs yolovs yolovm yolovm table roboflow-vl benchmark. evaluate zero-shot, few-shot, semi-supervised, fully-supervised performance state-of-the-art methods rf-vl benchmark. find rf-vl particularly challenging zero-shot few-shot approaches, methods struggling achieves map averaged datasets. notably, find groundingdino achieves best zero-shot few-shot accuracy. use double horizontal bar separate specialist models generalist mllms. method aerial document flora fauna industrial medical sports zero-shot detic groundingdino owlv class names mq-glip-text class-names qwen class names qwen instructions gemini pro class names gemini pro instructions few-shot shots detic federated loss mq-glip-image images mq-glip class names images groundingdino yolovn yolovn federated loss yolovs yolovs federated loss yolovm yolovm federated loss qwen instructions images gemini pro images gemini pro instructions images real-world datasets rf-vl often much smaller diverse coco, challenging assumption. specifically, although rf-vl half many images coco, seven times many classes cf. fig. interestingly, find models achieved higher performance coco necessarily improve real-world performance rf-vl. example, yolov outperforms yolov coco performs similarly yolov across three tested sizes nano, small, medium rf-vl. suggests newer yolo models may overfitting coco, gains dataset dont transfer real-world datasets. lastly, find increasing model size leads smaller performance improvements rf-vl compared coco. performance difference smallest largest models within model family map, suggesting simply increasing model capacity may lead significant performance gains rf-vl. cvpr foundational fsod chalenge hosted challenge cvpr encourage broad community involvement addressing problem aligning foundation models target concepts few-shot visual examples rich textual descriptions. importantly, use subset datasets rf-vl challenge lower barrier entry. competition received submissions teams submissions private close competition june th, aoe. notably, ten teams beat best baseline. present current top three teams table summarize contributions top three teams appendix include link full technical reports code here. limitations future work reliance crowdsourced annotations. datasets sourced roboflow universe, community platform anyone upload dataset annotations. although allows source diverse datasets, introduces uncertainty regarding overall annotation quality. manually inspect re-annotate datasets ensure quality best ability, verifying annotations specialized domains like medical imaging remains significant challenge. generated annotator instructions may reflect real instructions. annotator instructions automatically generated gpt-o manually verified correctness. however, may fully reflect nuances real-world instructions typically developed alongside dataset table cvpr foundational fsod challenge roboflow-vl method aerial document flora fauna industrial medical sports zero-shot detic groundingdino owlv class names mq-glip-text class-names qwen class names qwen instructions gemini pro class names gemini pro instructions few-shot shots detic federated loss groundingdino mq-glip-image images mq-glip class names images qwen instructions images gemini pro images gemini pro instructions images challenge submissions beaton fduroilab njust-kmg collection. encourage community release real annotator instructions generated iterative discussions annotators stakeholders. furthermore, although annotator instructions provide high-level class descriptions, often directly incorporate image evidence identify typical cases, edge cases, negative examples. future work explore create better automatic annotator instructions. generalist specialist models complementary strengths. although specialist models like groundingdino outperform generalist models like qwen.-vl mllms easily process few-shot visual examples rich textual descriptions. future work combine versatility mllms precision specialist models. conclusion paper, introduce roboflow-vl, large-scale benchmark evaluate state-of-the-art vlms concepts typically found internet-scale pre-training. rf-vl curated evaluate detection performance out-of-distribution tasks e.g. material property estimation, defect detection, contextual action recognition imaging modalities e.g. x-rays, thermal spectrum data, aerial imagery using visual examples rich textual descriptions. find state-of-the-art models struggle challenging benchmark, demonstrating limitations existing methods, highlighting opportunities develop better algorithms effectively use multi-modal annotator instructions. hope rf-vl rigorous test-bench future vlms mllms. acknowledgments work supported part compute provided nvidia, nsf grfp grant no. dge. references josh achiam, steven adler, sandhini agarwal, lama ahmad, ilge akkaya, florencia leoni aleman, diogo almeida, janko altenschmidt, sam altman, shyamal anadkat, al. gpt- technical report. arxiv preprint shuai bai, keqin chen, xuejing liu, jialin wang, wenbin ge, sibo song, kai dang, peng wang, shijie wang, jun tang, al. qwen. -vl technical report. arxiv preprint holger caesar, varun bankiti, alex lang, sourabh vora, venice erin liong, qiang xu, anush krishnan, pan, giancarlo baldan, oscar beijbom. nuscenes multimodal dataset autonomous driving. proceedings ieeecvf conference computer vision pattern recognition. nadine chang, francesco ferroni, michael tarr, martial hebert, deva ramanan. think- ing like annotator generation dataset labeling instructions. arxiv preprint lin chen, jinsong li, xiaoyi dong, pan zhang, yuhang zang, zehui chen, haodong duan, jiaqi wang, qiao, dahua lin, al. right way evaluating large vision- language models? arxiv preprint qiang chen, xiangbo su, xinyu zhang, jian wang, jiahui chen, yunpeng shen, chuchu han, ziliang chen, weixiang xu, fanrong li, al. lw-detr transformer replacement yolo real-time detection. arxiv preprint ting chen, simon kornblith, mohammad norouzi, geoffrey hinton. simple framework contrastive learning visual representations. international conference machine learning. pmlr. pp. floriana ciaglia, francesco saverio zuppichini, paul guerrie, mark mcquade, jacob solawetz. roboflow rich, multi-domain object detection benchmark. arxiv preprint google deepmind. introducing gemini new model agentic era. dec. url update-december-. du, fangyun wei, zihe zhang, miaojing shi, yue gao, guoqi li. learning prompt open-vocabulary object detection vision-language model. proceedings ieeecvf conference computer vision pattern recognition. pp. peng gao, shijie geng, renrui zhang, teli ma, rongyao fang, yongfeng zhang, hongsheng li, qiao. clip-adapter better vision-language models feature adapters. international journal computer vision pp. xiuye gu, tsung-yi lin, weicheng kuo, yin cui. open-vocabulary object detection via vision language knowledge distillation. arxiv preprint xiuye gu, tsung-yi lin, weicheng kuo, yin cui. open-vocabulary object detection via vision language knowledge distillation. arxiv preprint kaiming he, xinlei chen, saining xie, yanghao li, piotr dollr, ross girshick. masked autoencoders scalable vision learners. proceedings ieeecvf conference computer vision pattern recognition. pp. edward hu, yelong shen, phillip wallis, zeyuan allen-zhu, yuanzhi li, shean wang, wang, weizhu chen. lora low-rank adaptation large language models. arxiv preprint glenn jocher, jing qiu, ayush chaurasia. ultralytics yolo. version ... jan. url gaoussou youssouf kebe, padraig higgins, patrick jenkins, kasra darvish, rishabh sachdeva, ryan barron, john winder, donald engel, edward raff, francis ferraro, cynthia ma- tuszek. spoken language dataset descriptions speech-based grounded language learning. thirty-fifth conference neural information processing systems datasets benchmarks track round rahima khanam muhammad hussain. yolov overview key architectural enhancements. arxiv preprint mehar khurana, neehar peri, deva ramanan, james hays. shelf-supervised multi-modal pre-training object detection. arxiv preprint weicheng kuo, yin cui, xiuye gu, piergiovanni, anelia angelova. f-vlm open- vocabulary object detection upon frozen vision language models. arxiv preprint bohao li, rui wang, guangzhi wang, yuying ge, yixiao ge, ying shan. seed- bench benchmarking multimodal llms generative comprehension. arxiv preprint chunyuan li, haotian liu, liunian harold li, pengchuan zhang, jyoti aneja, jianwei yang, ping jin, houdong hu, zicheng liu, yong jae lee, jianfeng gao. elevater benchmark toolkit evaluating language-augmented visual models. neural information processing systems liunian harold li, pengchuan zhang, haotian zhang, jianwei yang, chunyuan li, yiwu zhong, lijuan wang, yuan, lei zhang, jenq-neng hwang, al. grounded language- image pre-training. proceedings ieeecvf conference computer vision pattern recognition. pp. zijing liang, yanjie xu, yifan hong, penghui shang, wang, qiang fu, liu. survey multimodel large language models. proceedings international conference computer, artificial intelligence control engineering. pp. tsung-yi lin, michael maire, serge belongie, james hays, pietro perona, deva ramanan, piotr dollr, lawrence zitnick. microsoft coco common objects context. computer visioneccv european conference, zurich, switzerland, september proceedings, part springer. pp. shilong liu, zhaoyang zeng, tianhe ren, feng li, hao zhang, jie yang, chunyuan li, jianwei yang, hang su, jun zhu, al. grounding dino marrying dino grounded pre-training open-set object detection. arxiv preprint yuan liu, haodong duan, yuanhan zhang, li, songyang zhang, wangbo zhao, yike yuan, jiaqi wang, conghui he, ziwei liu, al. mmbench multi-modal model all-around player? european conference computer vision. springer. pp. pan lu, swaroop mishra, tanglin xia, liang qiu, kai-wei chang, song-chun zhu, oyvind tafjord, peter clark, ashwin kalyan. learn explain multimodal reasoning via thought chains science question answering. advances neural information processing systems pp. yechi ma, neehar peri, shuoquan wei, wei hua, deva ramanan, yanan li, shu kong. long-tailed detection via late fusion. arxiv preprint anish madan, neehar peri, shu kong, deva ramanan. revisiting few-shot object detection vision-language models. advances neural information processing systems pp. sachit menon carl vondrick. visual classification via description large language models. eleventh international conference learning representations iclr. matthias minderer, alexey gritsenko, neil houlsby. scaling open-vocabulary object detection. arxiv preprint matthias minderer, alexey gritsenko, austin stone, maxim neumann, dirk weissenborn, alexey dosovitskiy, aravindh mahendran, anurag arnab, mostafa dehghani, zhuoran shen, al. simple open-vocabulary object detection. european conference computer vision. springer. pp. aljosa osep, tim meinhardt, francesco ferroni, neehar peri, deva ramanan, laura leal-taixe. better call sal towards learning segment anything lidar. eccv. hongpeng pan, shifeng yi, shouwei yang, lei qi, bing hu, xu, yang yang. solution cvpr foundational few-shot object detection challenge. arxiv preprint shubham parashar, zhiqiu lin, tian liu, xiangjue dong, yanan li, deva ramanan, james caverlee, shu kong. neglected tails vision-language models. proceedings ieeecvf conference computer vision pattern recognition. pp. neehar peri, achal dave, deva ramanan, shu kong. towards long-tailed detection. alec radford, jong wook kim, chris hallacy, aditya ramesh, gabriel goh, sandhini agarwal, girish sastry, amanda askell, pamela mishkin, jack clark, al. learning transferable visual models natural language supervision. international conference machine learning. pmlr. pp. roboflow. roboflow inference. url olga russakovsky, jia deng, hao su, jonathan krause, sanjeev satheesh, sean ma, zhiheng huang, andrej karpathy, aditya khosla, michael bernstein, al. imagenet large scale visual recognition challenge. international journal computer vision pp. christoph schuhmann, romain beaumont, richard vencu, cade gordon, ross wightman, mehdi cherti, theo coombes, aarush katta, clayton mullis, mitchell wortsman, al. laion- open large-scale dataset training next generation image-text models. advances neural information processing systems pp. shuai shao, zeming li, tianyuan zhang, chao peng, gang yu, xiangyu zhang, jing li, jian sun. objects large-scale, high-quality dataset object detection. ieeecvf international conference computer vision iccv. pp. doi .iccv... piyush sharma, nan ding, sebastian goodman, radu soricut. conceptual captions cleaned, hypernymed, image alt-text dataset automatic image captioning. proceedings annual meeting association computational linguistics volume long papers. melbourne, australia association computational linguistics, pp. kihyuk sohn, zizhao zhang, chun-liang li, han zhang, chen-yu lee, tomas pfis- ter. simple semi-supervised learning framework object detection. arxiv preprint ayca takmaz, cristiano saltori, neehar peri, tim meinhardt, riccardo lutio, laura leal- taixe, aljosa osep. towards learning complete anything lidar. international conference machine learning icml. tristan thrush, ryan jiang, max bartolo, amanpreet singh, adina williams, douwe kiela, candace ross. winoground probing vision language models visio-linguistic compositionality. proceedings ieeecvf conference computer vision pattern recognition. pp. shengbang tong, zhuang liu, yuexiang zhai, ma, yann lecun, saining xie. eyes wide shut? exploring visual shortcomings multimodal llms. proceedings ieeecvf conference computer vision pattern recognition. pp. liyuan wang, xingxing zhang, hang su, jun zhu. comprehensive survey continual learning theory, method application. ieee transactions pattern analysis machine intelligence xin wang, thomas huang, trevor darrell, joseph gonzalez, fisher yu. frustratingly simple few-shot object detection. international conference machine learning icml. yu-xiong wang, deva ramanan, martial hebert. growing brain fine-tuning increasing model capacity. proceedings ieee conference computer vision pattern recognition. pp. wenhao wu, zhun sun, wanli ouyang. revisiting classifier transferring vision-language models video recognition. proceedings aaai conference artificial intelligence. vol. pp. yifan xu, mengdan zhang, chaoyou fu, peixian chen, xiaoshan yang, li, changsheng xu. multi-modal queried object detection wild. advances neural information processing systems xiaopeng yan, ziliang chen, anni xu, xiaoxi wang, xiaodan liang, liang lin. meta r-cnn towards general solver instance-level low-shot learning. proceedings ieeecvf international conference computer vision. pp. licheng yu, patrick poirson, yang, alexander berg, tamara berg. modeling context referring expressions. computer visioneccv european conference, amsterdam, netherlands, october proceedings, part springer. pp. weihao yu, zhengyuan yang, linjie li, jianfeng wang, kevin lin, zicheng liu, xinchao wang, lijuan wang. mm-vet evaluating large multimodal models integrated capabil- ities. arxiv preprint xiang yue, yuansheng ni, kai zhang, tianyu zheng, ruoqi liu, zhang, samuel stevens, dongfu jiang, weiming ren, yuxuan sun, al. mmmu massive multi-discipline mul- timodal understanding reasoning benchmark expert agi. proceedings ieeecvf conference computer vision pattern recognition. pp. lvmin zhang, anyi rao, maneesh agrawala. adding conditional control text-to-image diffusion models. proceedings ieeecvf international conference computer vision. pp. renrui zhang, rongyao fang, wei zhang, peng gao, kunchang li, jifeng dai, qiao, hongsheng li. tip-adapter training-free clip-adapter better vision-language modeling. arxiv preprint yiwu zhong, jianwei yang, pengchuan zhang, chunyuan li, noel codella, liunian harold li, luowei zhou, xiyang dai, yuan, yin li, al. regionclip region-based language-image pretraining. proceedings ieeecvf conference computer vision pattern recognition. pp. xingyi zhou, rohit girdhar, armand joulin, philipp krhenbhl, ishan misra. detect- ing twenty-thousand classes using image-level supervision. european conference computer vision. springer. pp. implementation details present additional implementation details reproduce baseline experiments below. code available github. detic. use detic swin-l backbone zero-shot experiments. additionally, use model checkpoint trained lvis, coco imagenet-k. use class names provided text prompts detics clip classifier. groundingdino. use groundingdino pretrained weights mmdetection mm- groundingdino-l. prompt model class names combined single prompt. fine-tune groundingdino few-shot dataset epochs batch size learning rate e-. mq-glip. mq-det proposes learnable module enables multi-modal prompting. choose glip swin-l backbone underlying detection model experiments. use model checkpoint trained objects, fourods, goldg, capm. laslty, use class names text prompts few-shot visual examples visual prompts. owlv. use owlv implemented roboflow inference package prompt model class name independently post-process predictions non-maximal suppression. release benchmarking code adapted rf-vl github. qwen-.vl. conduct experiments using qwen.-vl-b-instruct model via api. prompt model based guidelines qwens official documentation. also improve base prompt small-scale validation multiple datasets select best prompt system prompt helpful assistant capable object detection. multi-class detection prompt locate following objects class names image output coordinates json format. single-class detection prompt locate every class name image output coordinates json format. gemini pro. conduct experiments using gemini api gemini-.-pro-preview- model. prompt model based guidelines geminis official documentation, also improve base prompt small-scale validation multiple datasets select best prompt system prompt return bounding boxes json array labels. never return masks code fencing. multi-class detection prompt detect bounding boxes following objects class names single-class detection prompt detect bounding boxes class name. prompting rich textual descriptions evaluate qwen gemini dataset-specific annotator instructions, appended following prompt main prompt use following annotator instructions improve detection accuracy annotator instructions include rich textual description classes using multi-class detection prompt. contrast, append relevant class description extracted using gpt-o using single-class detection prompt. prompting few-shot visual examples provide one image time qwen gemini mimic turn-based pre-training. use few-shot images prompting gemini. however, use three images prompting qwen due api limitations. prompt gemini native resolution images, limit qwens few-shot visual examples minimum pixels maximum pixels due api limitations. manage costs, limit gemini output tokens per request. set token limits qwen. lastly, implement robust parser handle minor json formatting errors. cases many few-shot image examples, api fails return valid response requests excessive size. cases, simply assign score images. due gemini qwen always predicting confidence score bounding boxes, set default. yolov yolov. train yolov yolov family models using ultralytics package default parameters. models, follow established protocol ciaglia et. al. train epochs batch size however, evaluate yolo models using pycocotools instead ultralytics cf. appendix additional evaluation details find metrics reported pycocotools maxdets differs significantly reported ultralytics rf-vl cf. table notably, yolo models report metrics using ultralytics implementation map default. preliminary investigation, supported similar observations github, suggest disparity largely attributed differences integration method precision-recall curve. ultralytics uses trapezoidal sum, inflates model performance much compared pycocotools. choose report results yolo models using pycocotools main paper standardize results baselines. table impact evaluation toolkit rf-vl performance. find ultralytics map calculation significantly over-estimates map compared pycocotools. fair comparison baselines, choose report metrics using pycocotools. method pycocotools map ultralytics map yolovn yolovn yolovs yolovs yolovm yolovm ablation prompting mllms evaluate gemini pro qwen .-vl performance rf-vl using two prompting strategies single-class prompting multi-class prompting. single-class prompting strategy separately performs forward pass class merges results per image. multi-class prompting strategy performs single forward pass classes. gemini pro qwen .-vl recommend single-class prompting strategy. importantly, perform non-maximal suppression either strategy mllms report confidence scores per box. interestingly, observe qwen.vl performs better single-class prompts, gemini performs better multi-class prompts. posit attributed qwens extensive referential object detection pre-training, typically requires detecting single class. con- trast, gemini achieves better performance multi-class prompting, aligned traditional object detection setups. argue multi-class prompting default assessing mllms object detection capabilities since closely mirrors standard object detection protocols. table analysis prompting strategy. mllms typically evaluate detection performance single-class prompts. find qwen.vl achieves better performance single-class prompts, gemini pro achieves better performance multi-class prompts. advocate multi- class prompting since closely matches object detection evaluation. method single-class prompt multi-class prompt gemini pro qwen .-vl comparing different model sizes figure evaluate performance gemini model family time e.g. gemini flash released gemini flash although gemini explicitly fine-tuned rf-vl, see significant increase performance. suggests gemini making real progress towards zero-shot open-vocabulary object detection wild. unsurprisingly, base mllms outperform faster distilled models e.g. gemini pro achieves better performance gemini flash distilled models provide considerably better performance per dollar. importantly, models prompted multi-class prompts cf. appendix figure gemini improves rf-vl time. de- spite explicitly fine-tuning rf-vl, find newer gemini models con- sistently improve older models benchmark. suggests gemini making real progress to- wards improving zero-shot open-vocabulary detection in- the-wild. ablation few-shot split selection prior work typically selects few-shot training examples random. however, madan et. al. demonstrates specific few-shot examples used fine-tuning greatly affects target class performance. specifically, madan et. al. selects informative k-shot examples class nuimages evaluating detic federated fine-tunings class-wise performance held-out validation set. instance, -shot task three random splits, may select five-shot car examples split five-shot bicycles split five-shot debris split based split highest per-class accuracy. shown table best split approach consistently outperforms random selection. despite effectiveness approach, two primary limitations. first, uses validation performance specific model inform few-shot selection. inherently biases few-shot images towards particular model. next, madan et. al.s proposed algorithm computationally expensive since requires fine-tuning model many candidate few-shot splits. approach computationally infeasible rf-vls datasets. address two issues, propose learning-free approach leverages key insight madan et. als analysis best examples typically large unoccluded. concretely, generate random candidate few-shot splits class pick split largest average bounding box area. similar madan et. al., -shot task three random splits, may select five-shot car examples split five-shot bicycles split five-shot debris split evaluate proposed sampling strategy nuimages find approach performs better random, underperforms madan et. al.s approach. future work consider effective strategies selecting best few-shot examples concept alignment. table best split construction. evaluate quality few-shot example selection using random baseline, madan al.s best split approach, chooses per-class few-shot examples based detic federated fine-tunings validation accuracy, proposed learning-free method selects splits largest average bounding box area. madan al.s method performs best, biased towards detic computationally expensive. approach offers tractable alternative improves random baseline. approach average precision many medium detic zero-shot detic federated fine-tuning -shots, random split detic federated fine-tuning -shots, best split detic federated fine-tuning -shots, best split, detic federated fine-tuning -shots, random split detic federated fine-tuning -shots, best split detic federated fine-tuning -shots, best split, semi-supervised fully supervised results present results semi-supervised fully-supervised baselines table importantly, models evaluated data splits zero-shot few-shot baselines. construct semi-supervised split, randomly sample training set. semi-supervised baselines. evaluate variants yolo yolo stac trained dataset rf-vl. stac generates high-confidence pseudo-labels localized objects unlabeled images updates model enforcing consistency strong augmentations. follow training protocol defined sohn et. al. first, train teacher model labeled subset data. then, use teacher model pseudo-label remaining unlabeled subset data. keep detections confidence confidence tuned maximize score teacher model validation set. finally, combine subset data true ground truth labels subset pseudo-labels form training set student model architecture. train student model convergence heavy augmentations. use hyperparameters supervised yolov yolov implementation. yolo models already train significant augmentation, dont add new augmentations student training. fully-supervised baselines. benchmark yolov yolov lw-detr datasets within rf-vl. yolov, developed ultralytics, builds yolov architecture improvements model scaling architectural refinements. yolov adds architecture improvements, primarily validated coco. lw-detr lightweight detection transformer outperforms yolo models real-time object detection, sota original roboflow dataset, predecessor rf-vl. architecture consists vit encoder, projector, shallow detr decoder. baseline serves upper bound performance, though rare cases, few-shot foundation models may surpass target dataset examples. semi-supervised learners data efficient. find leveraging simple semi-supervised learning algorithms like stac significantly improves model performance learning limited labels. half combinations model size data domain, semi-supervised learners improved map least much stepping model size. example, yolovs small trained labeled data stac psuedo-labels achieves better performance overall yolovm medium trained labeled data. table roboflow-vl semi-supervised fully-supervised benchmark. find semi-supervised learners able reach nearly performance fully supervised models using labeled data. method aerial document flora fauna industrial medical sports semi-supervised labels yolovn yolovn stac yolovs yolovs stac yolovm yolovm stac fully-supervised yolovn yolovn yolovs yolovs lw-detrs yolovm yolovm lw-detrm analysis accuracy vs. parameter count figure observe counter-intuitive trend larger models perform worse evaluations. likely due mismatch general-purpose mmlms specialized object detectors. despite largest model pre-trained data, qwen.-vl underperforms groundingdino zero-shot setting also considerably slower. interstingly, find groundingdino fine-tuned few-shot examples surpasses yolo models fine-tuned few-shot examples, indicating large pre-trained backbones enable efficient fine-tuning specialist models. figure accuracy vs. pa- rameter count. somewhat counterintuitively, find model pa- rameters qwen.-vl performs worse signif- icantly smaller models pre- trained less data ground- ingdino zero-shot set- ting. suggests gener- alist mllms parameter in- efficient specialized tasks. correlation model type per-dataset performance figure presents four scatterplots comparing map across different model pairs rf-vl, axis representing one models map point labeled dataset index sorted alphabetically. plots help identify whether certain datasets universally easy, medium, hard across models. compare gemini vs. groundingdino, qwen vs. groundingdino, gemini vs. qwen, groundingdino vs. yolo. gemini qwen, well groundingdino yolo, show stronger linear correlations per-dataset scores, suggesting alignment perceived difficulty. contrast, comparisons generalists gemini qwen specialists groundingdino yolo show weaker correlation. suggests large-scale mllms, likely trained similar web data, align closely other, specialist models like groundingdino yolo show stronger consistency. results imply dataset difficulty levels easy, medium, hard may generalize across model classes, may better defined within model types. additionally, among top datasets gemini outperforms qwen groundingdino, seven overlap. suggests gemini may excel datasets similar found pretraining, struggles generalize novel domains. figure correlation models type performance. see stronger linear trends gemini qwen, groundingdino yolo, indicating aligned perceptions dataset difficulty within model groups. performance variance few-shot models tables measure variance yolov rf-vl. use model proxy understanding few-shot learning variance statistical significance results. train yolovn yolovs federated loss ten times dataset, using ten different random seeds determine model initialization augmentation selection. report mean standard deviation two ways. table take average map across datasets given category e.g. industrial, sports, all, etc., report mean map standard deviation across ten different runs. table measure mean standard deviation dataset across different runs, report average mean standard deviation category. result higher standard deviation. table conveys variance single dataset rf-vl motivates averaging map across multiple datasets stable metric. table roboflow-vl overall variance. evaluate mean map standard deviation ten runs yolov federated loss different subsets roboflow-vl. results used proxy calculate whether new entry table statistically significant. unsurprisingly, averaging datasets yields less noisy estimate model performance method aerial document flora fauna industrial medical sports yolovn yolovs table roboflow-vl dataset variance. evaluate mean map standard deviation runs yolov federated loss datasets roboflow- vl. result helps quantify much model improve single dataset statistically significant. approach quantifying statistical significance shows much higher variance. method aerial document flora fauna industrial medical sports yolovn yolovs summary cvpr competition top performers summarize contributions top teams below. present full technical reports code here. beaton uses nebula-cv base detector, unpublished model built dino architecture swin-b visual backbone bert text encoder, enabling open-set detection cross-modal fusion. model pre-trained two stages first five million curated web-scale images, fine-tuned one million high-quality grounding examples distilled qwen.-vl. address few-shot setting, introduce strategies including optimized text prompts generated qwen.-vl, carefully tuned combination data augmentations e.g., flip, crop, hsv augmentation, copy-paste, pseudo-labeling supplement sparse annotations, dataset-specific inference resolution selection. also tested found minimal benefit federated fine-tuning llm-based post-processing. fduroilab proposes structured fine-tuning strategy enhanced aggressive data augmentation techniques cachedmosaic, yoloxhsvrandomaug, cachedmixup, randomcrop, increase data diversity model robustness. team employs mm-groundingdino swin-l backbone base detector uses qwen.-vl-b post-processing refine classification results correcting errors made primary detector. training conducted across datasets, undergoing independent runs ensure robust optimization. ablation study shows stepwise improvement performance, significant gains fine-tuning, additional augmentations, multiple training runs, mllm-based post-processing. njust-kmg integrates dynamic data augmentation, feature consistency regularization, dynamic freezing mechanism, grid search optimization, inference enhancements via test-time augmenta- tion tta weighted boxes fusion wbf. augmentation pipeline dynamically adjusts probabilities cachedmosaic, mixup, hsv jitter, randomcrop based training progression, freezing strategy customizes parameter updates depending dataset size domain similarity. njust-kmg also uses grid search process tune hyperparameters configurations dataset maximize validation map. inference, predictions refined combining outputs top models using wbf confidence calibration. annotation generation instructions present prompt generating multi-modal annotator gpt-o below. pay attention following example annotation instructions nu-images, object detection dataset nuimages annotator instructions example object detection annotation instructions. using instructions rough inspiration, come annotation instructions dataset. annotation instructions markdown format, follow following outline markdown overview table contents introduction introduction dataset. introduce task dataset trying solve. list classes provide brief description class. object classes class description provide description class, paying attention visually distinctive elements class. instructions provide detailed instructions annotate class. give specific references class, pay attention example labeled images provided. provide specific descriptions label, applicable. ... class ... class ... please pay specific attention provided visual example images ground response examples. brief concise, comprehensive. make sure instructions class provides visual descriptions exactly annotate. visual descriptions make specific reference object looks image. object something everyone knows, describe distinctive shape, color, texture, etc. look example pictures coming instructions. respond markdown content, text backticks. describe color bounding box, describe find spatial extent object image. final markdown file make specific reference provided example images. simply help come instructions. annotator able recreate annotations example images using generated instructions. classes similar, make sure instructions specify disambiguate visually, specific visual features look for. visual content image used clarify description class. feel free generalize present dataset example images. general metadata dataset metadata class names class names example images few-shot example images sample annotation instructions present sample annotator instructions below. use dataset metadata, class names few-shot visual examples prompt gpt-o generate annotator instructions cf. appendix manually verify instructions accurately describe few-shot examples. annotator instructions recode-waste-czvmg-fsod-yxsw. overview introductionintroduction object classesobject-classes aggregateaggregate cardboardcardboard hard plastichard-plastic metalmetal soft plasticsoft-plastic timbertimber introduction dataset designed waste classification within different material classes. goal accurately identify annotate different types waste materials sorting recycling purposes. classes represented aggregate, cardboard, hard plastic, metal, soft plastic, timber. object classes aggregate description aggregate refers small, granular materials, often irregular shape rough surfaces. generally appear pieces stone concrete. instructions annotate visible portions aggregate items. ensure include entire objects even occluded materials, estimating boundaries necessary. exclude dust fine particles form distinct objects. cardboard description cardboard objects typically flat layered texture. may appear boxes sheets. instructions annotate distinguishable pieces cardboard, focusing flat surfaces visible layering. annotate cardboard part another object soiled beyond recognition. hard plastic description hard plastics rigid maintain shape. cylindrical, tubular, robust objects often found industrial contexts. instructions annotate entire visible area hard plastic objects, ensuring capture solid structure. avoid labeling small, indistinct pieces plastic appears flexible. metal description metal objects robust, often shiny reflective. appear rods, sheets, distinct shapes. instructions label distinct metal parts, taking care capture complete form. avoid labeling rust marks indistinct metallic fragments lacking shape. soft plastic description soft plastics flexible often transparent translucent. may appear form bags wrappers. instructions focus full pieces soft plastic material, ensuring include areas visible creases folds indicating flexibility. label pieces smaller recognizable package mixed materials. timber description timber objects wooden, either rough smooth, often elongated rectangular. instructions annotate entire visible portion timber, focusing grain wood texture. label splinters fragments exhibit clear wooden structure. roboflow-vl datasets present table links datasets within roboflow-vl fully-supervised fsod datasets below. flora fauna link aquarium-combined fsod, fully supervised bees fsod, fully supervised deepfruits fsod, fully supervised exploratorium-daphnia fsod, fully supervised grapes- fsod, fully supervised grass-weeds fsod, fully supervised gwhd fsod, fully supervised into-the-vale fsod, fully supervised jellyfish fsod, fully supervised marine-sharks fsod, fully supervised orgharvest fsod, fully supervised peixos-fish fsod, fully supervised penguin-finder-seg fsod, fully supervised pig-detection fsod, fully supervised roboflow-trained-dataset fsod, fully supervised sea-cucumbers-new-tiles fsod, fully supervised thermal-cheetah fsod, fully supervised tomatoes- fsod, fully supervised trail-camera fsod, fully supervised underwater-objects fsod, fully supervised varroa-mites-detectiontest-set fsod, fully supervised wb-prova fsod, fully supervised weeds fsod, fully supervised industrial link -grccs fsod, fully supervised -lkc fsod, fully supervised -frc fsod, fully supervised aircraft-turnaround-dataset fsod, fully supervised asphaltdistressdetection fsod, fully supervised cable-damage fsod, fully supervised conveyor-t-shirts fsod, fully supervised dataconvert fsod, fully supervised deeppcb fsod, fully supervised defect-detection fsod, fully supervised fruitjes fsod, fully supervised infraredimageofpowerequipment fsod, fully supervised ism-band-packet-detection fsod, fully supervised lul fsod, fully supervised needle-base-tip-min-max fsod, fully supervised recode-waste fsod, fully supervised screwdetectclassification fsod, fully supervised smd-components fsod, fully supervised truck-movement fsod, fully supervised tube fsod, fully supervised water-meter fsod, fully supervised wheel-defect-detection fsod, fully supervised document link activity-diagrams fsod, fully supervised all-elements fsod, fully supervised circuit-voltages fsod, fully supervised invoice-processing fsod, fully supervised label-printing-defect-version- fsod, fully supervised macro-segmentation fsod, fully supervised paper-parts fsod, fully supervised signatures fsod, fully supervised speech-bubbles-detection fsod, fully supervised wine-labels fsod, fully supervised medical link canalstenosis fsod, fully supervised crystal-clean-brain-tumors-mri-dataset fsod, fully supervised dentalai fsod, fully supervised inbreast fsod, fully supervised liver-disease fsod, fully supervised nih-xray fsod, fully supervised spinefrxnormalvindr fsod, fully supervised stomata-cells fsod, fully supervised train fsod, fully supervised ufba- fsod, fully supervised urine-analysis fsod, fully supervised x-ray-id fsod, fully supervised xray fsod, fully supervised aerial link aerial-airport fsod, fully supervised aerial-cows fsod, fully supervised aerial-sheep fsod, fully supervised apoce-aerial-photographs-for-object- detection-of-construction-equipment fsod, fully supervised electric-pylon-detection-in-rsi fsod, fully supervised floating-waste fsod, fully supervised human-detection-in-floods fsod, fully supervised sssod fsod, fully supervised uavdet-small fsod, fully supervised wildfire-smoke fsod, fully supervised zebrasatasturias fsod, fully supervised sports link actions fsod, fully supervised aerial-pool fsod, fully supervised ball fsod, fully supervised bibdetection fsod, fully supervised football-player-detection fsod, fully supervised lacrosse-object-detection fsod, fully supervised link buoy-onboarding fsod, fully supervised car-logo-detection fsod, fully supervised clashroyalechardetector fsod, fully supervised cod-mw-warzone fsod, fully supervised countingpills fsod, fully supervised everdaynew fsod, fully supervised flir-camera-objects fsod, fully supervised halo-infinite-angel-videogame fsod, fully supervised mahjong fsod, fully supervised new-defects-in-wood fsod, fully supervised orionproducts fsod, fully supervised pill fsod, fully supervised soda-bottles fsod, fully supervised taco-trash-annotations-in-context fsod, fully supervised the-dreidel-project fsod, fully supervised", "published_date": "2025-05-27T01:24:29+00:00"}
{"id": "2505.19750v2", "title": "SuperAD: A Training-free Anomaly Classification and Segmentation Method for CVPR 2025 VAND 3.0 Workshop Challenge Track 1: Adapt & Detect", "authors": ["Huaiyuan Zhang", "Hang Chen", "Yu Cheng", "Shunyi Wu", "Linghao Sun", "Linao Han", "Zeyu Shi", "Lei Qi"], "summary": "technical report, present solution cvpr visual anomaly novelty detection vand workshop challenge track adapt detect robust anomaly detection real-world applications. real-world industrial anomaly detection, crucial accurately identify anomalies physical complexity, transparent reflective surfaces, occlusions, low-contrast contaminations. recently proposed mvtec dataset significantly narrows gap publicly available benchmarks anomalies found real-world industrial environments. address challenges posed dataset--such complex varying lighting conditions real anomalies large scale differences--we propose fully training-free anomaly detection segmentation method based feature extraction using dinov model named superad. method carefully selects small number normal reference images constructs memory bank leveraging strong representational power dinov. anomalies segmented performing nearest neighbor matching test image features memory bank. method achieves competitive results test sets mvtec dataset.", "full_text": "superad training-free anomaly classification segmentation method cvpr vand workshop challenge track adapt detect huaiyuan zhang hang chen cheng shunyi linghao sun linao han zeyu shi lei school computer science engineering, southeast university, china school computer science engineering, nanjing university science technology, china zhang hy, hangchen, chengyu, shunyiwu, linghaosun, linaohan, qileiseu.edu.cn shizeyunjust.edu.cn abstract technical report, present solution cvpr visual anomaly novelty detection vand workshop challenge track adapt detect robust anomaly detection real-world applications. real- world industrial anomaly detection, crucial accu- rately identify anomalies physical complexity, transparent reflective surfaces, occlusions, low- contrast contaminations. recently proposed mvtec dataset significantly narrows gap publicly available benchmarks anomalies found real-world industrial environments. address challenges posed datasetsuch complex varying lighting con- ditions real anomalies large scale differenceswe propose fully training-free anomaly detection seg- mentation method based feature extraction using di- nov model named superad. method carefully selects small number normal reference images constructs memory bank leveraging strong representational power dinov. anomalies segmented per- forming nearest neighbor matching test image fea- tures memory bank. method achieves compet- itive results test sets mvtec dataset. introduction background unsupervised anomaly detection localization emerged key technology computer vision, wide- ranging applications real-world scenarios indus- trial quality inspection autonomous driving. cen- tral challenge task lies training models solely corresponding author normal samples, enabling accurate identification precise localization previously unseen defects dur- ing testing. recent years, fueled advancement deep learning, numerous methods achieved remark- able progress mainstream benchmark datasets mvtec visa. however, model performance datasets approaches saturationfor instance, seg- mentation au-pro scores algorithms mvtec surpassed limitations become in- creasingly apparent limited scene diversity existing datasets primarily focus objects clear textures simple struc- tures, lacking coverage complex industrial scenarios involving transparent reflective surfaces e.g., glass- ware, metal products, well bulk, overlapping items e.g., granular materials. idealized defect types defects current datasets large, centrally located anomalies, overlooking real-world industrial defects edge anomalies, subpixel-level scratches e.g., hairline cracks, low- contrast contaminations e.g., transparent foreign ob- jects. insufficient environmental robustness datasets often ignore variations lighting conditions e.g., dark-field, backlighting, overexposure, resulting models limited generalization ability deployed across different devices varying environments. challenge description next-generation benchmark industrial anomaly de- tection, mvtec dataset systematically ad- dresses aforementioned limitations introducing eight complex diverse scenarios. core features include simulation physical complexity transparent reflective surfaces categories cs.cv may vial feature liquid refraction artifacts, sheet metal includes mirror-like reflections, challenge models ability reason light propagation. bulk overlapping objects examples like wallplugs walnuts involve random occlu- sions truncated boundaries objects, re- quiring semantic-level understanding. high intra-class variability normal samples cat- egories fabric diverse textures geometric pattern deformations demand models learn tight boundaries normal data manifold. verification detection limits tiny objects boundary anomalies plastic contam- inants occupying less image area rice category, missing regions image boundaries wallplugs pose significant challenges models perceptual capability high resolution. implicit structural consistency fruit jelly re- quire model assess plausibility ingredi- ent distributions, despite absence explicit logical constraints. cross-domain generalization evaluation scene includes least four lighting conditions regular, under- exposed, overexposed, additional light sources, sim- ulating distribution shifts caused device variations real-world production environments. enables sys- tematic evaluation model robustness. mvtec dataset, mainstream methods ex- hibit clear performance bottlenecks limited localization capability efficientad patchcore achieve average au-pro. scores respectively. highly complex scenarios rice, performance drops poor robustness msflow shows significant performance degradation au-pro. mixed lighting test set testpriv,mix compared standard test set testpriv, highlighting sensitivity environmen- tal variations. summary, paper targets key challenges high- lighted mvtec dataset aims develop novel highly robust model capable accu- rately detecting subtle defects within highly variant nor- mal samples enhance localization performance small targets structurally complex objects, transparent over- lapping instances strengthen models generalization ability vary- ing illumination conditions, thereby improving practi- cal applicability industrial deployment. methodology model design ... approach recent years, memory bank-based methods demon- strated remarkable performance various anomaly detec- tion segmentation benchmarks. patchcore intro- duces greedy coreset selection mechanism construct compact memory bank, significantly reducing number features preserving overall distribution original feature space. design effectively balances de- tection accuracy retrieval efficiency. dmad uni- fies two commonly encountered scenarios industrial set- tings one normal samples available limited number labeled anomalies. achieves constructing normal memory bank expandable anomaly memory bank, store features normal observed anomalous patterns, respectively, thereby improv- ing adaptability real-world complexities. worth noting success memory-based approaches heavily depends quality feature extrac- tion. result, methods rely powerful pre-trained visual backbones wideresnet vision trans- former vit prior studies reveal different lay- ers deep models capture distinct types information shallow layers tend focus local high-frequency details e.g., textures, edges, whereas deeper layers encode abstract semantic information. therefore, combining shallow deep features enables model capture global structure local details, crucial accu- rate anomaly detection. instance, padim employs pre-trained resnet extract patch-wise features four different layers models distribution normal features spatial location using multivariate gaussian distribution, param- eterized mean covariance. april-gan hand, leverages clips powerful multimodal alignment capabilities. extracts four-layer features test image corresponding normal reference image, performs layer-wise matching, averages re- sults localize anomalous regions. fortunately, recent advances self-supervised learning, dinov shown strong capability cap- turing rich semantic information visual tasks. build- ing upon this, method constructs class-specific normal feature memory bank category mvtec dataset. inference, features various regions input image matched stored memory bank detect anomalies. adopt powerful dinov backbone extract multi-level features, aiming achieve accurate fine-grained anomaly segmentation. dinov-large dinov-large search search search search reference images test image anomaly map frozen parameters element-wise addition nearest neighbor search figure overall architecture proposed method. ... architecture overall architecture proposed method illus- trated figure category dataset, construct memory bank consisting normal reference images. selec- tion reference images follows two-step procedure. first, extract cls token representations training images using dinov model. then, greedy coreset selection method used patchcore applied group feature vectors clusters. strategy maximizes coverage diverse normal patterns within category without changing distribution features, thereby enhancing representativeness reference set reducing false positive rates. test image, first extract multi-level features using dinov model. level features, compute similarity test image features stored memory bank retrieve nearest neighbor information. method based assump- tion normal regions test image tend find sim- ilar regions among reference images, anomalous regions lack matches. evaluating similarity spatial location, obtain anomaly map level. finally, anomaly maps averaged upsam- pled original resolution generate final anomaly segmentation map. ... training proposed method requires training. cat- egory mvtec dataset, construct few- shot feature memory bank normal reference images us- ing samples training set. number reference samples fixed adopt pretrained dinov- vit-l- model feature extractor, consists transformer layers approximately million pa- rameters. features extracted four specific layers i.e., layer generate final anomaly segmentation map. ensure memory efficiency, input image resolutions adjusted categories except sheet metal, shorter side resized pixels preserving original aspect ratio. sheet metal, due elon- gated image size, shorter side resized pixels, also preserving original aspect ratio. configuration allows method run entirely single gpu e.g., nvidia geforce rtx vial wallplugs categories, fur- ther enhance segmentation performance extracting fore- ground features input images, detailed sec- tion ... fabric walnuts categories, cases central pattern entirely normal anomalies exist edges e.g., fabric cat- egory, piece fabric may placed background fabric. initial detection, apply post-processing two categories filling interiors closed re- gions, improving prediction accuracy. dataset evaluation ... dataset utilization since proposed method training-free, cate- gory, utilize images training set con- struct few-shot memory bank. method, inno- vatively propose adaptive background mask generation technique based principal component analysis pca morphological optimization image preprocess- ing stage. enhances visual feature representation suppressing redundant background information. first, apply pca reduce dimensionality fea- tures extracted input image extract first principal component. technique captures direc- tion maximum variance feature space perform- ing singular value decomposition covariance matrix, mathematically expressed arg max varxv, denotes normalized feature matrix, projection vector. first principal component reflects primary mode variation feature distribution. subsequently, binarize projection values based predefined threshold generate initial mask miniti pci otherwise since initially unclear part corresponds foreground background, propose adaptive deci- sion strategy based variance analysis. specifically, initially designate one region foreground background, compute feature variance within region compare medians. variance foreground region lower background region, suggests potential misclassification, in- vert mask accordingly. strategy effectively mit- igates foreground-background misclassification caused improper thresholding. minit minit mdvarfmsk mdvarfn-msk minit otherwise minit denote initial binary mask masked region, unmasked region, fmsk rnmskd fn-msk rnn-mskd represent feature matrices masked unmasked regions, respectively. var com- putes per-channel variance dimension returns median value variance vector suppress influence outliers. operator denotes boolean mask inversion. address discrete noise initial mask, apply morphological operations post-processing. specifically, use square kernel dilate mask, enhancing region connectivity. combining dilation erosion, eliminate holes smooth boundaries. process formally defined mfinal closing dil. minit morphological kernel spa- tial dimension feature grid. finally, optimized mask reshaped back feature vector dimension, yielding boolean mask matrix used element-wise fil- tering original features. effectively suppresses interference low-information background regions. object au-roc. score fabric fruit jelly rice vial wallplugs walnuts sheet metal mean table au-roc. segmentation score bina- rized images testpublic set. experiments, set number pca compo- nents threshold morphological kernel size parameters generalize well across various datasets require dataset-specific tuning. ex- perimental results demonstrate proposed prepro- cessing method preserves integrity main object features significantly reducing background noise in- terference, thereby improving model performance. ... evaluation criteria evaluate models performance primarily using pixel-level score. metric combines precision recall computing harmonic mean, thereby provid- ing balanced measure models ability detect anomalies pixel level. specifically, score calculated precision recall precision recall, precision denotes proportion predicted anoma- lous pixels truly anomalous, recall represents proportion actual anomalous pixels correctly identified model. evaluation, optimize score adjust- ing decision threshold testpublic dataset determine optimal boundary anomaly segmentation. pro- cess ensures model achieves balanced trade-off precision recall. use pixel-level score evaluation metric enables precise assessment models capabil- ity identify anomalous regions complex image data. notably, mvtec dataset emphasizes detec- tion small defects, may occupy pixels image. cases, conventional metrics like area receiver operating characteristic curve au-roc dominated larger defects, thus fail- ing accurately reflect models performance smaller anomalies. contrast, pixel-level score places equal object patchcore efficientad msflow simplenet dsr fabric fruit jelly rice sheet metal vial wallplugs walnuts mean table performance comparison segmentation score binarized images testpriv testpriv,mix set. best results highlighted bold. emphasis detecting anomalous regions, regardless size. makes particularly well-suited chal- lenges posed mvtec dataset, reliably evaluate models ability correctly identify even smallest defects. results performance metrics proposed method demonstrates excellent performance pixel-level anomaly detection tasks. specifically, testpublic dataset, models performance terms au- roc. segmentation score summarized ta- ble moreover, based official test results vand challenge server, model achieves score testpriv dataset testpriv,mix dataset. results indicate model achieves good balance precision recall, effectively detect anomalous pixels images. notably, model maintains relatively stable performance even un- der distributional shifts data. comprehensively evaluate models perfor- mance, also consider key metrics. testpriv dataset, model achieves aucpro. score testpriv,mix dataset, score results suggest model strong detection ca- pabilities anomalous regions across different thresholds remains accurate even variations illumination environmental factors. addition, image-level classification tasks, model achieved classf scores testpriv testpriv,mix datasets, respectively, demonstrat- ing strong capability distinguishing normal anomalous samples image level. comparison due recent release mvtec dataset unavailability ground truth testpriv testpriv,mix test sets, compare proposed method approaches listed mvtec dataset paper, shown table test sets, method, superad, consistently outperforms previous methods. moreover, method requires training, demonstrating superior gener- alization capabilities compared approaches. discussion challenges solutions experiments, observe certain categories fruit jelly, vial wallplugs exhibit high intra-class variability. constructing reference feature memory bank using randomly selected images categories, many normal regions incorrectly iden- tified anomalies due lack sufficiently similar pat- terns memory bank. address issue, propose selecting reference images using greedy coreset selection strategy rather random sampling. approach in- creases diversity patterns within memory bank helps reduce false positives. detailed explanation method provided section ... analyze anomaly segmentation maps gen- erated similarity scores extracted fea- tures four different layers reference images category. observe that, due relatively clear separation foreground background cat- egories mvtec dataset, false positives back- ground regions generally limited. however, case wallplugs category, false detections still oc- cur background owing highly complex di- verse patterns. gain deeper insights, evaluate effectiveness pca-based binary classification features extracted dinov. results indicate applying pca shallow layers dinov, threshold set effec- tively separates foreground background regions. details found section ... based observation, apply foreground feature ground truth anomaly map original img. figure examples typical failure cases false positives air bubbles missed detection blurred objects false posi- tives due specular highlights missed detections missing- type anomalies. extraction preprocessing strategy wallplugs cate- gory, leads improved segmentation performance particularly challenging class. model robustness adaptability worth emphasizing proposed method entirely training-free, thus require fine-tuning dinov specific category. result, generalization ability model remains fully preserved. approach relies heavily powerful feature ex- traction capabilities dinov, enables extrac- tion semantically rich representations suitable com- parison across diverse categories. construction memory bank, select reference images also employ greedy coreset selection strategy ensure selected images diverse possible. strategy effectively allevi- ates issue limited pattern diversity within memory bank, might otherwise fail represent full range intra-class variability. consequently, design enhances robustness model. future work although method achieves competitive performance, several prominent issues remain unresolved warrant investigation. false positives air bubbles illustrated fig- ure fruit jelly category contains wide variety air bubbles high uncertainty. false positives primarily caused diversity bubble appearances, often match bubble patterns fixed memory bank normal features. future, robust- ness model complex pattern variations could enhanced reduce misclassifications. missed detections reflections blurry objects shown figure fruit jelly category, light- colored reflections light scattering caused optical properties jelly diminish perceived abnormality dark objects. model may erroneously match regions normal patterns. future work could improve models ability recognize objects within transparent semi-transparent media reduce missed detections. false positives specular highlights depicted figure vial categories, specular high- lights caused illumination often lead false predictions. incorporating advanced illumination compensation high- light removal techniques may enhance models robust- ness lighting artifacts reduce false positives. missed detections missing-type anomalies shown figure sheet metal vial cate- gories, missing-type anomalies sometimes difficult detect due high visual similarity back- ground. future research may explore ways enable model better capture object completeness features, thereby im- proving ability detect missing-type anomalies. conclusion report, propose fully training-free anomaly de- tection segmentation method achieves robust per- formance complex varying lighting conditions. method, first employ greedy coreset selection strategy select small number diverse normal ref- erence images. then, leveraging powerful represen- tational capacity dinov model, extract image features selected references construct mem- ory bank. test image, extract multi-scale features perform nearest neighbor matching memory bank scale generate anomaly segmentation maps. maps averaged produce final result. method demonstrates large pre-trained model dinov possesses excellent image representation capabilities. relying solely capabilities without fine-tuning enables effective performance dense pre- diction tasks anomaly segmentation. references kilian batzner, lars heckler, rebecca konig. efficien- tad accurate visual anomaly detection millisecond-level latencies. proceedings ieeecvf winter confer- ence applications computer vision, pages xuhai chen, yue han, jiangning zhang. april-gan zero-few-shot anomaly classification segmentation method cvpr vand workshop challenge tracks place zero-shot place few-shot ad. arxiv preprint thomas defard, aleksandr setkov, angelique loesch, romaric audigier. padim patch distribution modeling framework anomaly detection localization. inter- national conference pattern recognition, pages springer, hanqiu deng xingyu li. anomaly detection via reverse distillation one-class embedding. proceedings ieeecvf conference computer vision pattern recognition, pages alexey dosovitskiy, lucas beyer, alexander kolesnikov, dirk weissenborn, xiaohua zhai, thomas unterthiner, mostafa dehghani, matthias minderer, georg heigold, syl- vain gelly, al. image worth words trans- formers image recognition scale. arxiv preprint lars heckler-kram, jan-hendrik neudeck, ulla scheler, re- becca konig, carsten steger. mvtec dataset advanced scenarios unsupervised anomaly detection. arxiv preprint jianlong hu, chen, zhenye gan, jinlong peng, shengchuan zhang, jiangning zhang, yabiao wang, chengjie wang, liujuan cao, rongrong ji. dmad dual memory bank real-world anomaly detection. arxiv preprint zhikang liu, yiming zhou, yuansheng xu, zilei wang. simplenet simple network image anomaly detection localization. proceedings ieeecvf con- ference computer vision pattern recognition, pages maxime oquab, timothee darcet, theo moutakanni, huy vo, marc szafraniec, vasil khalidov, pierre fernandez, daniel haziza, francisco massa, alaaeldin el-nouby, al. dinov learning robust visual features without supervision. arxiv preprint karsten roth, latha pemula, joaquin zepeda, bernhard scholkopf, thomas brox, peter gehler. towards to- tal recall industrial anomaly detection. proceedings ieeecvf conference computer vision pattern recognition, pages tran dinh tien, anh tuan nguyen, nguyen hoang tran, duc huy, soan duong, chanh nguyen, steven truong. revisiting reverse distillation anomaly detection. proceedings ieeecvf con- ference computer vision pattern recognition, pages sergey zagoruyko nikos komodakis. wide residual net- works. arxiv preprint vitjan zavrtanik, matej kristan, danijel skocaj. dsr dual subspace re-projection network surface anomaly detection. european conference computer vision, pages springer, yixuan zhou, xing xu, jingkuan song, fumin shen, heng tao shen. msflow multiscale flow-based framework unsupervised anomaly detection. ieee transactions neural networks learning systems,", "published_date": "2025-05-26T09:29:27+00:00"}
{"id": "2505.18988v1", "title": "NTIRE 2025 Challenge on Video Quality Enhancement for Video Conferencing: Datasets, Methods and Results", "authors": ["Varun Jain", "Zongwei Wu", "Quan Zou", "Louis Florentin", "Henrik Turbell", "Sandeep Siddhartha", "Radu Timofte", "others"], "summary": "paper presents comprehensive review challenge video quality enhancement video conferencing held ntire workshop cvpr highlights problem statement, datasets, proposed solutions, results. aim challenge design video quality enhancement vqe model enhance video quality video conferencing scenarios improving lighting, enhancing colors, reducing noise, enhancing sharpness giving professional studio-like effect. participants given differentiable video quality assessment vqa model, training, test videos. total participants registered challenge. received valid submissions evaluated crowdsourced framework.", "full_text": "cs.cv may ntire challenge video quality enhancement video conferencing datasets, methods results varun jain, zongwei wu, quan zou, louis florentin, henrik turbell, sandeep siddhartha, radu timofte, qifan gao, linyan jiang, qing luo, jack song, yaqing li, summer luo, mae chen, stefan liu, danie song, huimin zeng, chen, ajeet verma, shweta tripathi, vinit jakhetiya, badri subhdhi, sunil jaiswal microsoft university wurzburg jain.varunmicrosoft.com, zongwei.wuuni-wuerzburg.de, quan.zou, lflorentin, heturbel, ssiddharthamicrosoft.com, radu.timofteuni-wuerzburg.de abstract paper presents comprehensive review challenge video quality enhancement video con- ferencing held ntire workshop cvpr highlights problem statement, datasets, proposed solu- tions, results. aim challenge design video quality enhancement vqe model enhance video quality video conferencing scenarios improving lighting, enhancing colors, reducing noise, enhancing sharpness giving professional studio-like ef- fect. participants given differentiable video quality assessment vqa model, training, test videos. to- tal participants registered challenge. re- ceived valid submissions evaluated crowd- sourced framework. additional materials found project website introduction light crucial component visual expression key controlling texture, appearance, composi- tion. professional photographers often sophisticated studio lights reflectors illuminate subjects true visual cues expressed captured. sim- ilarly, tech-savvy users modern desk setups employ members participants co-authored report de- tailing methodologies, challenge organizers. please refer appendix correspondence details. www microsoft com research academic-programntire--vqe sophisticated combination key fill lights give control illumination shadow char- acteristics. however, many users constrained physical environment, may lead poor positioning ambient lighting lack thereof. also commonplace encounter flares, scattering, specular reflections may come windows mirror-like surfaces. problems compounded poor-quality cameras may in- troduce sensor noise. leads poor visual experience video calls negative impact down- stream tasks denoising, super-resolution, segmenta- tion, face detection. current light correction solution microsoft teams, called autoadjust, finds global mapping input output colors updated sporadically. since mapping global, gives importance foreground colors, may lead improper exposure color shifts background. hand, popular single-image portrait relighting methods estimate local correction foreground preserve background im- plicit in-network matte layer. possible side effect local correction reduction local contrast, of- ten serves proxy convey depth images, making people appear dull cases. conducted studies totaling pair- wise comparisons measured peoples preference autoadjust portrait relighting effect im- ages manually edited experts adobe lightroom. used bradleyterry model estimate scores method observed people preferred autoad- method. take next step towards achieving studio-grade video quality, one would need understand people figure ground truth top synthetics framework, bottom autoadjust solution. top row shows input suboptimal foreground illumination fixed adding studio light setup front subject simulated synthetics predicted via global changes real data. prefer construct differentiable video quality assess- ment vqa metric, able train video quality enhancement vqe model optimizes metric. solve first problem, trained vqa model that, given pair videos gives probability better described sec. given stan- dard test set, information used construct ranking order given set methods. invited researchers participate challenge aimed developing neural processing unit npu friendly vqe models leverage trained vqa model im- prove video quality. challenge one ntire work- shop associated challenges ambient lighting normaliza- tion reflection removal wild shadow re- moval event-based image deblurring image de- noising xgc quality assessment ugc video en- hancement night photography rendering image super-resolution real-world face restoration ef- ficient super-resolution depth estimation ef- ficient burst hdr restoration cross-domain few- shot object detection short-form ugc video quality assessment enhancement text image gen- eration model quality assessment day night rain- drop removal dual-focused images video quality enhancement video conferencing, low light image en- hancement light field super-resolution restore image model raim wild raw restoration super-resolution raw reconstruction rgb smartphones challenge problem statement task enhance video quality video conferencing scenarios. looked following properties video judge studio-grade quality foreground illumination person body parts clothing optimally lit. natural colors correction may make local global color changes make videos pleasing. temporal noise correct image video encoding artefacts sensor noise. sharpness ensure correction algorithms introduce softness, final image least sharp input. understand may many aspects good video. simplicity, discounted except ones mentioned above. specifically, measure following egocentric motion unstable camera may introduce sweeping motion small vibrations aim correct. makeup beautification commonplace users apply beautification filters alter skin tone facial features found instagram snapchat. aim aesthetic. removal reflection glasses lens flare although common occurrence video teleconference sce- narios, aim remove reflections may come screens light sources onto users glasses due risk associated altering eye ap- pearance gaze direction. avatars solution synthesizes photorealistic avatar subject drives based input video would score highest terms noise, illumi- nation, color. indeed minimizes total cost function takes account factors, would acceptable. baseline solution since autoadjust model ranked higher expert- edited images portrait relighting methods, provided participants baseline solution could reproduce autoadjust feature currently shipped microsoft teams. provided python script calls au- toadjust executable, includes code post-processing. compute constraints goal computationally efficient solution offloaded npu coreml inference. established qualifying criterion coreml uint models macs per frame input resolution estimate model per frame processing time ultra powered mac studio pro powered mac mini given input resolution. submissions meet criterion considered evaluation. dataset ... unpaired real data host web service reaches users world prompts sit front laptop pc. record minute-long videos users perform hand gestures body movements. sampled videos dataset training, validation, testing vqe methods. videos long, encoded fps average, amount total frames. kept videos testing ranking submis- sions make available teams. could choose split training validation sets desire. teams also free use publicly available datasets, mindful data drift. videos, selected high quality videos raters voted strongly favor au- figure comparison lighting setup synthetic portrait relighting dataset. left lighting hdri, center key light hdri lighting turned off, right key fill lights hdri lighting turned off. note hdri used background using studio lighting con- tribute illumination subject. grayscale pixel color intensity probability density histogram face color intensity values source target figure color intensity source target images syn- thetic portrait relighting dataset. source images dark intensity centered around target fixes boosting il- lumination making uniform span larger range. toadjust result, shown bottom half figure assumed ground truth. done videos shows mean opinion score mos favor target. ... paired synthetic portrait relighting data addition data, also provided paired data fully supervised learning shown top half fig- ure note possible learn correction different ground truth labels achieve higher mos. hence, labels treated suggestive improvements, global optima. use physically-based path tracer photorealistic assets blender render videos training videos testing. video long encoded fps. source image lighting high dynamic range image hdri environment. target, teamname input resolution inference resolution training time epochs ensemble lut attention macsframe latencyframe gpunpu tmobilerestore day yes yes summer days yes yes xteam hrs yes yes velta hrs yes yes deepview days yes yes auv day yes yes meeting hrs yes yes maqic days yes lut days yes wizard days yes yes table final results ntire challenge video quality enhancement video conferencing held cvpr added diffuse light sources simulate studio light- ing setup. refer figure visualize effect light sources figure statistics color inten- sity values face. images used finetune portrait relighting method. ensure data generalize well wild, refer image-level degradations used real- esrgan applied source image. simulate out-of-focus blur, applied generalized gaus- sian blur kernels ramp edges flat top areas better modeling combined effects lens defocusing light diffraction. color noise, used channel- independent additive gaussian noise, gray noise added applying gaussian noise chan- nels. finally, sensor noise modeled sampling poisson distribution. lastly, applied random resizing jpeg compression. done videos shows mos fa- vor target indicating make better target compared baseline autoadjust solution. exam- ples pairs shown figure details rendering framework found vqa model vqatheta label eqvqa provided teams pre-trained siamese video quality assessment model trained videos enhancement models. ground- truth collected prompting human raters side-by-side video comparisons. high-level se- mantic understanding, used models pre-trained collection real synthetic images tasks person segmentation, face quality image aesthetics. low-level features noise, flicker video coding artifacts used dover model. took penultimate feature maps models, per- formed average pooling across temporal spatial dimen- sions concatenated them. used set projec- tions predict final logits. given pair images videos model predicts probability preferred study. also provides auxiliary scores input ...a correspond factors image aesthetic, color harmonization, color liveliness, key- lighting, noise, image composition, face capture quality etc. supervised metrics obtained publicly available apple vision apis. metrics evaluating submissions final goal rank submissions according scores. asked teams submit predic- tions real-video test set. compared submissions given input, baseline, other. shown figure comparison using bradleyterry model gives score submission maximizes likelihood observed voting. framework throughput votes per week. case two methods statistically insignifi- cant difference subjective scores, used objective score shown equation break ties. rea hat theta sum phat yi, xi, ahat vqatheta hat yi, label eqmetricreal ynt frac frac sum sqrt hat label eqmetricsynth y,y ,x, eal jhat y,x,theta times ssynthobjhat y,y label eqmetric due infeasibility getting scores real- time, teams could use objective score sobj contin- uous independent evaluation. unsuper- vised videos, teams required submit per-video vqa score yi,xi along auxiliary scores predicted vqa model shown equation synthetic test set, teams reported root mean squared error rmse per video. scores also published leaderboard participants could track progress relative teams. however, rank teams based objective metrics since possible learn correction different sub- jectively better ground truth provided. bradley-terry score overall vote baseline auv meeting tmobilerestore velta xteam inputs maqic lut wizard summer deepview bradley-terry score color baseline auv meeting tmobilerestore velta xteam inputs maqic lut wizard summer deepview bradley-terry score brightness baseline auv meeting tmobilerestore velta xteam inputs maqic lut wizard summer deepview bradley-terry score skintone baseline auv meeting tmobilerestore velta xteam inputs maqic lut wizard summer deepview figure interval plots illustrating mean bradley-terry scores corresponding confidence intervals submissions, input videos, provided baseline. top overall preference, bottom factors influencing preference. results received complete submissions mid-point final evaluations. teams submission, uti- lized crowd-sourced framework evaluate video test set. involved presenting human raters side-by-side video comparisons. raters asked provide preference scale represent strong preference left right video respectively, represent weak preference. rating indicates preference. furthermore, raters prompted specify decision primarily in- fluenced image colors, image brightness, skin tone. bradleyterry scores team maximize likelihood observed voting shown figure challenge methods section outlines methodologies datasets used highest-ranking submissions. observe look- table lut based solutions tmobilerestore deep- view scored highest. attributed effi- cient, yet temporally stable nature correction compared methods predict dense pixel-to-pixel map- ping input output image pairs. tmobilerestore figure two stage video conferencing enhancement framework proposed team tmobilerestore. ... description propose video enhancement algorithm designed tackle common issues found video conferencing videos, noise, compression artifacts, pathological illumina- tion, visual inconsistencies. algorithm employs two-stage training process achieve optimal results first stage uses lut brightness color correction, second stage focuses removing compression noise, sensor noise, enhancing overall video qual- ity, shown figure first stage video enhancement framework designed tackle color distortions typically found video conferencing footage, inconsistent lighting color shifts, considerably lowers visual quality. effectively rectify distortions, use combi- nation clookup table clut based methods convolutional neural network structures. phase, network processes input frames extracting features multiple levels, allowing concurrently extract image features. implement clut, neural net- work predicts content-dependent weights downsam- pled input merge basic cluts image-adaptive one, thereby enhancing original input image. second stage dedicated rectifying low-level dis- tortions noise compression artifacts. address problems, utilize lightweight u-net architec- ture skip connections, specifically engineered ef- fective robust restoration. network extracts fea- tures various scales, enabling concurrently address local artifacts like blocky compression noise global degradations. skip connections encoder decoder ensure preservation fine-grained details throughout restoration process. first phase includes convolutional layers ability broaden re- ceptive fields carry global local refinement image distortions. allows network restore natural visually pleasant context throughout video. ... datasets train two-stage network, used combination public datasets, including ldv reds datasets provided competition. realistic degra- dation simulation, model mixed distortions create training data closely resembled real-world scenarios. training data first stage incorporated color dis- tortions, random saturation shifts contrast ad- justments. second stage, training data included randomized degradations poisson-gaussian noise, motion blur, h.h. compression. degrada- tion parameters dynamically sampled per batch im- prove robustness. stages, applied spatial aug- mentations rotation, flipping, chromatic aber- ration, well temporal jitter techniques frame dropping shuffling prevent overfitting. ... experiments results stage clut trained hybrid loss func- tion combining loss cosine color shift loss, iterations batch size patch size initial learning rate set halved every iterations, using adam optimizer second stage, sub-network optimized us- ing combination loss, perceptual loss, lpips gan loss enhance textures without over-smoothing, iterations batch size patch size pretraining stages independently, jointly fine-tuned network additional iterations reduced learning rate details listed table xteam meeting similar method early training termination iterations respectively. summer method also consists color enhancement sub- network video restoration sub-network. color enhancement network uses set five pre-trained luts dynamically adjust color tone video frames real-time luts trained using provided supervised vqe dataset, ensures lut represents distinct style color tone transforma- tion tailored video content. predict optimal com- bination luts frame, method employs convolutional neural network cnn seven convolu- tional blocks. cnn extracts global features downsampled video frames, capturing essential characteris- tics influence color-enhancement process. ana- lyzing features, network predicts weights blending five luts, resulting final lut adapted specific content video frame. video restoration sub-network utilizes seven resid- ual blocks progressively refine video frames, reducing noise, correcting blurriness, restoring details. deep learning approach effectively learns map degraded images high-quality images, ensuring clear detailed video frames. velta similar method, training ending early iterations. deepview propose video conference enhancement network addresses degradation distortion repair color en- hancement ensure high-quality video communication. distortion repair, tmobilerestore. color enhancement, adopt hvi-cidnet ap- proach includes hvi color space cidnet architecture. hvi color space minimizes noise compresses low-light regions, cidnets dual- branch network handles chromatic denoising brightness enhancement. processing images hvi color space applying cross-attention mechanisms, approach re- stores natural colors details, providing vibrant ac- curate color representation video conferencing. auv similar method, training ending early iterations. maqic ... description online video streams suffer physical environ- ment, including poor positioning ambient lighting lack thereof, leading poor visual experience video calls may perturb downstream tasks. considering figure typical dlut-based retouching pipeline. human-region focus video calls, interpreted challenge task video por- trait retouching, aims improve aesthetic qual- ity input portrait photos especially requires human- region priority deep learning-based meth- ods largely elevate retouching efficiency provide promising retouched results, con- centrate image tasks, leads efficiency bottleneck translating video task. therefore, consider efficient solution, i.e., look-up table lut based retouching, performs fast inverse tone mapping according trained look-up table pixel value. video portrait retouching task, improve temporal consistency, existing video-based enhancement methods typically include additional optical flow estimator e.g., spynet propagate informa- tion adjacent frames. however, suitable highly efficient lut-based solution, online spynet inferencing inevitably slows whole re- touching pipeline. therefore, choose image-based icelut shown figure retouching back- bone. contributions summarized choosing efficient backbone video portrait retouching stage-wise training strategy achieve perceptual satisfying retouched results. typical lut solution shown figure pro- vides lut-based pixel value transfer accelerate retouching process. portrait scenario, re- touching two-fold retouching background foreground, focus highlight- ing human region instead background. there- fore adopting solution region-wise adaptation e.g., icelut necessary filter background focus portrait region. shown figure given low-quality input frame, adopted method adaptively performs fusion multiple luts composes lut, performs tone mapping obtain visu- ally satisfying result frame. ... datasets notice given dataset contains limited image resolution, degradations noise, motion blur, flicker. training data complexes tar- get goal, requiring model simultaneously perform retouching video restoration. therefore, adopt stage-wise training strategy separate aforementioned goals. first stage, train mit-adobe fivek high-quality tone mapping dataset without im- age degradations. enables luts retouch input videos, adaptively changes tone frames adjusts light condition. based pre-trained luts, conduct fine-tuning supervised subset given challenge equip luts restoration ability. figure adopted icelut used maqic constructs weighted lut multiple lookup table candidates per- form real-time video retouching. ... experiments results model implemented pytorch framework, conduct experiments single nvidia tesla gpu. include additional details table lut similar method early training termination. wizard ... description approach integrates technical aesthetic quality assessment algorithms video enhancement task. building upon hvi-cidnet framework, introduce perceptual quality-aware color inten- sity decoupling network leverages lighten cross- attention lca mechanism. addition, incorporate quality loss function based clip-iqa metrics en- hance perceptual quality output video, ensuring alignment human visual preferences. accelerate convergence enhance performance, initialize model pretrained hvi-cidnet weights, leveraging prior knowledge effective spatial chromatic feature handling video frames. major limitation traditional image enhancement models often prioritize technical fidelity perceptual qual- ity. however, video conferencing applications, visual ap- peal important technical accuracy. address this, introduce perceptual quality loss function incorporates metrics clip-iqa combination video quality assessment metrics equation figure overview quality-aware cidnet proposed wizard. training, use extracted frames input. inputs first passed hvi transform network, obtained hvi features processed cidnet, lastly perceptual-inverse hvi transform phvit applied get srgb-enhanced image. outputs vqe model evaluated using clip-iqa perceptual quality assessment, resulting scores utilized quality loss. lcolorspace constructed combination loss edge loss le, perceptual loss lp. quality loss component encourages model prior- itize aesthetic factors, ensuring output video frames align closely human visual preferences. given block diagram figure training, let input frames protect hat enhanced frames generated enhancement network. enhanced frames fed clip-iqa model, computes qual- ity scores qhat xi. scores typically range higher score indicates better perceptual quality, objective maximize score. incorporate loss function ensuring optimization, mean quality score qxi across batch normalized follows thcal qualityclip bar qchat xi, qcxi given as, bar frac sum qchat xi, where, represents batch size. here, value cor- responds highest quality, manner, another term vqa model equation constructed athcal qualityvqa bar qvhat finally, quality loss given mathc quali mathcal lqualityclip mathcal lqualityvqa term lquality integrated overall loss function guide training process. total loss func- tion used training weighted sum standard colorspace loss perceptual quality loss mat cal lto mathcal lcolorspace lambda cdot mathcal lquality lcolorspace loss term used base- model hyperparameter controls contribution perceptual quality term, ensuring balance technical accuracy visual quality. figure inference pipeline proposed quality-aware cidnet. first frames extracted input video, extracted frames fed q-cidnet enhancement. ... datasets develop evaluate video quality enhancement vqe model, selectively utilized subsets real synthetic datasets provided challenge. total real videos synthetic videos, opted focused approach training leveraging rep- resentative samples dataset real dataset utilization real videos provided training validation, selected videos training. subset chosen capture diverse lighting conditions, variations ambient reflec- tions, noise characteristics maintaining bal- ance complexity model training efficiency. synthetic dataset utilization synthetic videos provided training, selected videos training. synthetic samples curated include variety lighting configurations generated adding diffuse light sources simulate studio setup. data instrumental fine-tuning models ability han- dle lighting corrections improve visual appeal. ... experiments results initializing model pretrained weights hvi-cidnet, fine-tuned using adam optimizer hyperparameters epochs. parameter set quality loss in- clusion. learning rate initially set gradually decreased using cosine an- nealing schedule training process. input model requires gmacs, equivalent gflops, parameters. measured inference latency per frame. de- tails listed table inference detailed figure acknowledgments authors thank human understanding toolkit team, led tadas baltrusaitis, support using synthetic data generation framework adapting needs. especially thank lohit petikam criti- cal help designing studio lighting setup blender collaborating writing rendering scripts. work partially supported humboldt foundation. thank ntire sponsors bytedance, meituan, kuaishou, university wurzburg computer vision lab. teams affiliations ntire team title ntire challenge video quality enhance- ment video conferencing members varun jain jain.varunmicrosoft.com, zongwei zongwei.wuuni-wuerzburg.de, quan zou quan.zoumicrosoft.com, louis florentin lflorentinmicrosoft.com, henrik turbell heturbelmicrosoft.com, sandeep siddhartha ssiddharthamicrosoft.com, radu timofte radu.timofteuni-wuerzburg.de affiliations microsoft, redmond wa, usa computer vision lab, university wurzburg, germany tmobilerestore members qifan gao gaooutlook.com, linyan jiang jlygmail.com, qing luo luoqing.qq.com, jie song sjgmail.com, yaqing lyqstudyqq.com affiliations shannonlab, tencent, china xidian university, china summer members summer luo qq.com, mae chen chenxm mnankai.edu.cn affiliations independent researcher, china nankai uninversity, china deepview members stefan liu stefan.com, danie song hypoxgmail.com affiliations shanghai jiao tong university, china shenzhen university, china maqic members huimin zeng zeng.huimnortheastern.edu, chen qchenjh.edu affiliations northeastern university, usa johns hopkins university, usa wizard title q-cidnet perceptual quality aware color intensity decoupling network video quality enhance- ment members ajeet kumar verma ajeet.vermaiitjammu.ac.in, shweta tripathi pcsiitjammu.ac.in, vinit jakhetiya vinit.jakhetiyaiitjammu.ac.in, badri subhdhi subudhi.badriiitjammu.ac.in, sunil jaiswal sunil.jaiswalk-lens.de affiliations department computer science engineering, iit jammu, india department electrical engineering, iit jammu, india klens, gmbh, germany references ralph allan bradley milton terry. rank analysis incomplete block designs method paired compar- isons. biometrika, vladimir bychkovsky, sylvain paris, eric chan, fredo durand. learning photographic global tonal adjustment database input output image pairs. twenty- fourth ieee conference computer vision pattern recognition, kelvin chan, shangchen zhou, xiangyu xu, chen change loy. basicvsr improving video super- resolution enhanced propagation alignment. pro- ceedings ieeecvf conference computer vision pattern recognition, pages yu-sheng chen, yu-ching wang, man-hsin kao, yung- chuang. deep photo enhancer unpaired learning image enhancement photographs gans. cvpr, pages zheng chen, kai liu, jue gong, jingkai wang, lei sun, zongwei wu, radu timofte, yulun zhang, al. ntire challenge image super-resolution methods results. proceedings ieeecvf conference computer vision pattern recognition cvpr work- shops, zheng chen, jingkai wang, kai liu, jue gong, lei sun, zongwei wu, radu timofte, yulun zhang, al. ntire challenge real-world face restoration methods results. proceedings ieeecvf conference computer vision pattern recognition cvpr work- shops, marcos conde, radu timofte, al. ntire challenge raw image restoration super-resolution. proceed- ings ieeecvf conference computer vision pattern recognition cvpr workshops, marcos conde, radu timofte, al. raw image reconstruc- tion rgb smartphones. ntire challenge re- port. proceedings ieeecvf conference com- puter vision pattern recognition cvpr workshops, yubin deng, chen change loy, xiaoou tang. aesthetic- driven image enhancement adversarial learning. acm mm, pages egor ershov, sergey korchagin, alexei khalin, artyom pan- shin, arseniy terekhin, ekaterina zaychenkova, georgiy lobarev, vsevolod plokhotnyuk, denis abramov, elisey zhdanov, sofia dorogova, yasin mamedov, nikola banic, georgii perevozchikov, radu timofte, al. ntire challenge night photography rendering. proceedings ieeecvf conference computer vision pat- tern recognition cvpr workshops, yuqian fu, xingyu qiu, bin ren yanwei fu, radu timofte, nicu sebe, ming-hsuan yang, luc van gool, al. ntire challenge cross-domain few-shot object detection methods results. proceedings ieeecvf con- ference computer vision pattern recognition cvpr workshops, shuhao han, haotian fan, fangyuan kong, wenjie liao, chunle guo, chongyi li, radu timofte, al. ntire challenge text image generation model quality assess- ment. proceedings ieeecvf conference com- puter vision pattern recognition cvpr workshops, charlie hewitt, fatemeh saleh, sadegh aliakbarian, lohit petikam, shideh rezaeifar, louis florentin, zafiirah hose- nie, thomas cashman, julien valentin, darren cosker, al. look ma, markers holistic performance capture without hassle. acm transactions graphics tog, andrey ignatov, nikolay kobyshev, radu timofte, kenneth vanhoey, luc van gool. dslr-quality photos mobile devices deep convolutional networks. iccv, pages gregory koch, richard zemel, ruslan salakhutdinov, al. siamese neural networks one-shot image recognition. icml deep learning workshop, pages lille, sangmin lee, eunpil park, angel canelo, hyunhee park, youngjo kim, hyungju chun, xin jin, chongyi li, chun-le guo, radu timofte, al. ntire challenge efficient burst hdr restoration datasets, methods, results. proceedings ieeecvf conference computer vi- sion pattern recognition cvpr workshops, xin li, yeying jin, xin jin, zongwei wu, bingchen li, yufei wang, wenhan yang, li, zhibo chen, bihan wen, robby tan, radu timofte, al. ntire challenge day night raindrop removal dual-focused images methods results. proceedings ieeecvf conference computer vision pattern recognition cvpr work- shops, xin li, xijun wang, bingchen li, kun yuan, yizhen shao, suhang yao, ming sun, chao zhou, radu timofte, zhibo chen. ntire challenge short-form ugc video quality assessment enhancement kwaisr dataset study. proceedings ieeecvf conference com- puter vision pattern recognition cvpr workshops, xin li, kun yuan, bingchen li, fengbin guan, yizhen shao, zihao yu, xijun wang, yiting lu, wei luo, suhang yao, ming sun, chao zhou, zhibo chen, radu timofte, al. ntire challenge short-form ugc video quality assessment enhancement methods results. pro- ceedings ieeecvf conference computer vision pattern recognition cvpr workshops, jie liang, radu timofte, qiaosi yi, zhengqiang zhang, shuaizheng liu, lingchen sun, rongyuan wu, xindong zhang, hui zeng, lei zhang, al. ntire restore image model raim wild challenge. proceedings ieeecvf conference computer vi- sion pattern recognition cvpr workshops, jing lin, xiaowan hu, yuanhao cai, haoqian wang, you- liang yan, xueyi zou, yulun zhang, luc van gool. un- supervised flow-aligned sequence-to-sequence learning video restoration. international conference machine learning, pages pmlr, hongying liu, zhubo ruan, peng zhao, chao dong, fan- hua shang, yuanyuan liu, linlin yang, radu timofte. video super-resolution based deep learning compre- hensive survey. artificial intelligence review, xiaohong liu, xiongkuo min, qiang hu, xiaoyun zhang, jie guo, al. ntire xgc quality assessment chal- lenge methods results. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, xiaoning liu, zongwei wu, florin-alexandru vasluianu, hailong yan, bin ren, yulun zhang, shuhang gu, zhang, zhu, radu timofte, al. ntire challenge low light image enhancement methods results. proceedings ieeecvf conference computer vi- sion pattern recognition cvpr workshops, yu-qi liu, xin du, hui-liang shen, shu-jie chen. es- timating generalized gaussian blur kernels out-of-focus image deblurring. ieee transactions circuits sys- tems video technology, babak naderi ross cutler. crowdsourcing approach video quality assessment. icassp ieee international conference acoustics, speech signal processing icassp, pages ieee, anurag ranjan michael black. optical flow estima- tion using spatial pyramid network. proceedings ieee conference computer vision pattern recogni- tion, pages bin ren, hang guo, lei sun, zongwei wu, radu timo- fte, yawei li, al. tenth ntire efficient super- resolution challenge report. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, nickolay safonov, alexey bryntsev, andrey moskalenko, dmitry kulikov, dmitriy vatolin, radu timofte, al. ntire challenge ugc video enhancement meth- ods results. proceedings ieeecvf conference computer vision pattern recognition cvpr work- shops, lei sun, andrea alfarano, peiqi duan, shaolin su, kaiwei wang, boxin shi, radu timofte, danda pani paudel, luc van gool, al. ntire challenge event-based image deblurring methods results. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, lei sun, hang guo, bin ren, luc van gool, radu timo- fte, yawei li, al. tenth ntire image denoising challenge report. proceedings ieeecvf confer- ence computer vision pattern recognition cvpr workshops, florin-alexandru vasluianu, tim seizinger, zhuyun zhou, cailian chen, zongwei wu, radu timofte, al. ntire image shadow removal challenge report. proceed- ings ieeecvf conference computer vision pattern recognition cvpr workshops, florin-alexandru vasluianu, tim seizinger, zhuyun zhou, zongwei wu, radu timofte, al. ntire ambi- ent lighting normalization challenge. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, ajeet kumar verma, shweta tripathi, vinit jakhetiya, badri subudhi, sunil jaiswal. q-cidnet perceptual quality aware color intensity decoupling network video quality enhancement. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, jianyi wang, kelvin chan, chen change loy. ex- ploring clip assessing look feel images. pro- ceedings aaai conference artificial intelligence, pages xintao wang, liangbin xie, chao dong, ying shan. real-esrgan training real-world blind super-resolution pure synthetic data. proceedings ieeecvf inter- national conference computer vision, pages yingqian wang, zhengyu liang, fengyuan zhang, lvli tian, longguang wang, juncheng li, jungang yang, radu timofte, yulan guo, al. ntire challenge light field image super-resolution methods results. pro- ceedings ieeecvf conference computer vision pattern recognition cvpr workshops, haoning wu, erli zhang, liang liao, chaofeng chen, jing- wen hou, annan wang, wenxiu sun, qiong yan, weisi lin. exploring video quality assessment user gener- ated contents aesthetic technical perspectives. proceedings ieeecvf international conference computer vision, pages qingsen yan, yixu feng, cheng zhang, guansong pang, kangbiao shi, peng wu, wei dong, jinqiu sun, yan- ning zhang. hvi new color space low-light image enhancement. arxiv preprint kangning yang, jie cai, ling ouyang, florin-alexandru vasluianu, radu timofte, jiaming ding, huiming sun, lan fu, jinlong li, chiu man ho, zibo meng, al. ntire challenge single image reflection removal wild datasets, methods results. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, ren yang, radu timofte, al. aim challenge super-resolution compressed image video dataset, methods results. european conference computer vision workshops, sidi yang, binxiao huang, mingdeng cao, yatai ji, hanzhong guo, ngai wong, yujiu yang. taming lookup tables efficient image retouching. european confer- ence computer vision, pages springer, pierluigi zama ramirez, fabio tosi, luigi stefano, radu timofte, alex costanzino, matteo poggi, samuele salti, ste- fano mattoccia, al. ntire challenge depth images specular transparent surfaces. pro- ceedings ieeecvf conference computer vision pattern recognition cvpr workshops, hui zeng, jianrui cai, lida li, zisheng cao, lei zhang. learning image-adaptive lookup tables high perfor- mance photo enhancement real-time. ieee transactions pattern analysis machine intelligence, huimin zeng, jie huang, jiacheng li, zhiwei xiong. region-aware portrait retouching sparse interactive guidance. ieee transactions multimedia, fengyi zhang, hui zeng, tianjun zhang, lin zhang. clut-net learning adaptively compressed representations dluts lightweight image enhancement. proceedings acm international conference multimedia, pages yongbing zhang, siyuan liu, chao dong, xinfeng zhang, yuan yuan. multiple cycle-in-cycle generative adversar- ial networks unsupervised image super-resolution. tip, hao zhou, sunil hadap, kalyan sunkavalli, david jacobs. deep single-image portrait relighting. proceed- ings ieeecvf international conference computer vision, pages", "published_date": "2025-05-25T05:53:24+00:00"}
{"id": "2505.16784v2", "title": "Four Eyes Are Better Than Two: Harnessing the Collaborative Potential of Large Models via Differentiated Thinking and Complementary Ensembles", "authors": ["Jun Xie", "Xiongjun Guan", "Yingjian Zhu", "Zhaoran Zhao", "Xinming Wang", "Hongzhu Yi", "Feng Chen", "Zhepeng Wang"], "summary": "paper, present runner-up solution egod egoschema challenge cvpr confirmed may inspired success large models, evaluate leverage leading accessible multimodal large models adapt video understanding tasks via few-shot learning model ensemble strategies. specifically, diversified prompt styles process paradigms systematically explored evaluated effectively guide attention large models, fully unleashing powerful generalization adaptability abilities. experimental results demonstrate that, carefully designed approach, directly utilizing individual multimodal model already outperforms previous state-of-the-art sota method includes several additional processes. besides, additional stage introduced facilitates cooperation ensemble periodic results, achieves impressive performance improvements. hope work serves valuable reference practical application large models inspires future research field. code available", "full_text": "cs.cv jun four eyes better two harnessing collaborative potential large models via differentiated thinking complementary ensembles jun xie,, xiongjun guan,,, yingjian zhu, zhaoran zhao, xinming wang,, hongzhu yi, feng chen, zhepeng wang, lenovo research tsinghua university school artificial intelligence, university chinese academy sciences ucas institute automation, chinese academy sciencescas zhongguancun academy university chinese academy sciences xiejun, zhaozr, chenfeng, wangzpblenovo.com, zhuyingjian,yihongzhumails.ucas.ac.cn, wangxinmingia.ac.cn, gxjmails.tsinghua.edu.cn abstract paper, present runner-up solution egod egoschema challenge cvpr confirmed may inspired success large mod- els, evaluate leverage leading accessible multi- modal large models adapt video understand- ing tasks via few-shot learning model ensemble strate- gies. specifically, diversified prompt styles process paradigms systematically explored evaluated ef- fectively guide attention large models, fully unleash- ing powerful generalization adaptability abilities. experimental results demonstrate that, carefully designed approach, directly utilizing individual mul- timodal model already outperforms previous state-of- the-art sota method includes several additional processes. besides, additional stage intro- duced facilitates cooperation ensemble pe- riodic results, achieves impressive performance im- provements. hope work serves valuable refer- ence practical application large models in- spires future research field. code available introduction egoschema long-form video question- answering dataset derived egod comprises manually curated multiple-choice question- answer pairs, covering hours real-world authors contributed equally. corresponding author. video data. question, egoschema challenge cvpr requires selecting correct answer five given options based three-minute video clip. dataset spans wide range natural human activities behaviors, characterized long temporal certificate minimum video duration human needs answer ques- tion accurately diverse complexities. therefore, serve diagnostic benchmark evaluating long- form video-language comprehension abilities contempo- rary multimodal systems. according training requirements, existing methods roughly divided three categories directly train powerful model organizing appro- priate datasets refining reward functions, methods flexibly align models expected behavior de- sired trends, enabling better capture comprehend specific details within certain domains. nevertheless, data scale training costs usually significant limitation. fine tune large models adapt specific tasks focusing training specific modules, over- workload greatly reduced, efficiently leverag- ing foundational knowledge integrated pre- training stage. however, still certain requirements data scale, diversity, quality. perform prompt tuning function assemble accessible large models approaches enable low-cost alignment large models user intent. given impressive per- formance current large models, paradigm rapid de- ployment without additional training presents highly ap- pealing prospect. table presents performance comparison existing works participation methods. experimental re- sults first group, seen methods based video question option input content vlm prompt caption cot summary answer confidence reason caption cot summary reason cot answer confidence vlm vlm prompt stage pipeline stage pipeline prompt differentiated thinking complementary ensemble ... uid ... formatted result figure framework proposed algorithm. specific text content omitted flowchart reported main text. prompt tuning model assembling demonstrates remarkable superiority. inspired this, considering limited availability labeled data samples, also adopt training-free approach conduct exploration. unlike previous serial approaches involve multi- stage processes conducted systematic eval- uation prompts chains thought, achieving com- parable even superior performance directly adopting end-to-end large models. approach eliminates need cumbersome functional integration, significantly sim- plifying complexity processing workflow. fur- thermore, performed in-depth analysis result complementarity brought differentiated thinking patterns paradigms. leveraging collaborative advantage model ensemble, achieved substantial improvement accuracy. method overview proposed algorithm shown figure given set input content, includes spe- cific video along corresponding question options, construct various prompts, data streams, chain thoughts cot. fed video-language models vlms generate several responses multiple focal points. next, complementary cot results summarized ensembled determine final answer, presented structured formatted man- ner. following subsections provide detailed introduc- tions module, organized according sequence data flow. model selection first determine candidate large vlms adapt task. fundamental principle video descrip- tion function demonstrate strong multimodal vi- sual comprehension, response function requires robust logical reasoning language proficiency. accord- ing relevant evaluations reports, following mod- els selected tested lavila gpt-o gemini flash pro deepseek design pipeline previous methods mostly adopted -stage pro- cess. shown red part figure first stage, vlm used generate description video. second stage, intermediate results input content input another vlm obtain final result. approach considered trade-off, earlier vlms lacked adequate multimodal understanding ability process long-form content effectively. advancements large-scale modeling technology largely addressed limitations. leveraging progress, develope -stage process, illustrated green part figure experimental results reveal scheme outperforms stage pipeline. one reasonable explanation -stage paradigm inherently captures information, minimizing omissions cumulative errors phases. more- over, end-to-end input-output design significantly sim- plifies complex multi-stage process. adjustment prompt words three styles prompts carefully designed vali- dated, including minimalist style text suc- cinctly explains requirements detailed style text provides examples lengthy thought processes guiding style text provides rule guidance moderate examples. specific text content mentioned provided supplementary materials section construction cot part, implicitly construct cot process large vlms specifying items output, thereby guid- ing analysis inference. specifically, five compo- nents taken account captions video clips paper, summary entire video, reason be- hind decision, answer selected option number, confidence level. -stage paradigm, first two last three items serve outputs first second stages, respectively. hand, content needs provided sequentially -stage paradigm. model ensemble arranging combining aforementioned strate- gies, variety differentiated thinking outcomes achieved. based foundation, highly effective distinctive modes selected integrated maximize complementary strengths. let represent ground truth predicted result, respectively, confi- dence level certain mode calculated iyi indicator function, total number sam- ples evaluated. similarity two modes represented sima, iya similarity used modulate weight mode avoid influence excessively amplifying similar opinions. final expression model ensemble expressed argmaxc sima, indexes different modes, option number, final decision. experiments table lists primary results existing publicly avail- able methods participating teams leaderboard. table performance comparison existing works upper group top five teams lower group public leaderboard. method indicated gray shading. method rank accuracy mplug-owl longvivit internvideo llovi videoagent proviq gemini pro lifelongmemory ilearn noahs ark lab ccego ilearn. pcie pcie reality distortion reported results demonstrate algorithm outperforms almost previously proposed methods, showcasing significant improvements compared past works proposed related papers time, method ranks among non-public competition proposals, ex- hibiting certain level competitiveness. comprehen- sive roadmap technological evolution presented figure due limited resources, initial exploration conducted validation samples marked gray gradually transitioned entire set marked purple. section supplementary materials, briefly explain ablation studies proposed so- lution, offering potential insights inspiration future works. conclusion present proposed runner-up solution egod egoschema challenge cvpr experimental re- sults demonstrate superiority approach. es- pecially, paper details in-depth exploration prompts chain thought methodologies, specifi- cally highlighting advantages collaboration among multiple large vision-language models. hope design philosophy evolution work provide valuable insights inspiration future re- searches. references josh achiam, steven adler, sandhini agarwal, lama ah- mad, ilge akkaya, florencia leoni aleman, diogo almeida, janko altenschmidt, sam altman, shyamal anadkat, al. -stage -stage -stage -stage prompt prompt cotcap,rs,ans,conf cotsum,ans cotrs, ans,conf -stage focus -stage focus -stage g-exp -stage g-preview macro design few-shot learning micro design vlm model model ensemble acc ilearn figure comprehensive roadmap technological evo- lution. diagonal line indicates corresponding item adopted final solution. green dashed line represents publicly available sota method gpt- technical report. arxiv preprint ivana balazevic, yuge shi, pinelopi papalampidi, rahma chaabouni, skanda koppula, olivier henaff. mem- ory consolidation enables long-context video understanding. arxiv preprint zebang cheng, zhi-qi cheng, jun-yan he, kai wang, yuxi- ang lin, zheng lian, xiaojiang peng, alexander haupt- mann. emotion-llama multimodal emotion recognition reasoning instruction tuning. advances neural infor- mation processing systems, rohan choudhury, koichiro niinuma, kris kitani, laszlo jeni. zero-shot video question answering pro- cedural programs. arxiv preprint egovis. second joint egocentric vision egovis workshop, accessed may kristen grauman, andrew westbury, eugene byrne, zachary chavis, antonino furnari, rohit girdhar, jackson hamburger, hao jiang, miao liu, xingyu liu, al. egod around world hours egocentric video. proceedings ieeecvf conference computer vi- sion pattern recognition, pages neil houlsby, andrei giurgiu, stanislaw jastrzebski, bruna morrone, quentin laroussilhe, andrea gesmundo, mona attariyan, sylvain gelly. parameter-efficient transfer learning nlp. international conference machine learning, pages pmlr, edward hu, yelong shen, phillip wallis, zeyuan allen- zhu, yuanzhi li, shean wang, wang, weizhu chen, al. lora low-rank adaptation large language models. iclr, karttikeya mangalam, raiymbek akshulakov, jitendra malik. egoschema diagnostic benchmark long- form video language understanding. advances neural in- formation processing systems, pinelopi papalampidi, skanda koppula, shreya pathak, justin chiu, joe heyward, viorica patraucean, jiajun shen, antoine miech, andrew zisserman, aida nematzdeh. simple recipe contrastively pre-training video-first en- coders beyond frames. proceedings ieeecvf conference computer vision pattern recognition, pages gemini team, petko georgiev, ving ian lei, ryan burnell, libin bai, anmol gulati, garrett tanzer, damien vincent, zhufeng pan, shibo wang, al. gemini unlocking multimodal understanding across millions tokens con- text. arxiv preprint xiaohan wang, yuhui zhang, orr zohar, serena yeung-levy. videoagent long-form video understand- ing large language model agent. arxiv preprint ying wang, yanlai yang, mengye ren. lifelongmem- ory leveraging llms answering queries long-form egocentric videos. arxiv preprint wang, kunchang li, xinhao li, jiashuo yu, yinan he, guo chen, baoqi pei, rongkun zheng, zun wang, yansong shi, al. internvideo scaling foundation models mul- timodal video understanding. european conference computer vision, pages springer, qinghao ye, haiyang xu, guohai xu, jiabo ye, ming yan, yiyang zhou, junyang wang, anwen hu, pengcheng shi, yaya shi, al. mplug-owl modularization empowers large language models multimodality. arxiv preprint zhang, taixi lu, mohaiminul islam, ziyang wang, shoubin yu, mohit bansal, gedas bertasius. sim- ple llm framework long-range video question-answering. arxiv preprint haoyu zhang, yuquan xie, yisen feng, zaijing li, meng liu, liqiang nie. hcqaegod egoschema challenge arxiv preprint yue zhao, ishan misra, philipp krahenbuhl, rohit girdhar. learning video representations large lan- guage models. proceedings ieeecvf conference computer vision pattern recognition, pages four eyes better two harnessing collaborative potential large models via differentiated thinking complementary ensembles supplementary material prompt word example prompt words three style introduced section show figure among them, underline indicates content needs filled based sample. italics indicate corresponding guiding content, promote large model pay attention task related information may gains. symbol ... indicates redundant text omitted location. noted work, require vlm output json format easy access corresponding output section. order constrain standardization format, prompt explicitly provides required out- put format. additionally, provided example also positive impact formatting extent. ablation study macro design technological evolution starts sota solution, -stage framework ilearn selec- tion vlms data flow paradigm thor- oughly evaluated. top three rows, gemini flash lavila used extract captions, re- spectively. subsequently, deepseek-r deepseek- used generate summary caption perform inference final stage. seen em- ploying large models stronger reasoning abilities dur- ing thinking phase offers significant improvement accuracy, consistent intuition group line hand, lavila, fine- tuned dataset, able generate appropriate descriptions compared gemini, resulting notable ad- vancements group line also suggests, extent, -stage paradigm, initial process extracting descriptions videos susceptible content omission misinterpretation. correspondingly ar- gument, -stage paradigm demonstrates significant su- periority group line noted since test results already better gpt-o based method re- ported use subsequent experiments due expensive api cost. few-shot learning phase, adjustment prompt words con- struction cot comprehensively evaluated. line group, interpreted excessively long text prompts reasoning tasks might overwhelm distract large models, leading suboptimal performance. hand, concise appropriately structured prompts, combined clear rule-based guidance, ef- fectively enhance models task-specific focus, resulting improved outcomes. subsequent attempts cot ex- periments demonstrate varying modes thinking significant impact final accuracy. strongly sug- visual question answering expert. choose correct answer five options based video content. watch video carefully answer following question selecting appropriate option. qustion question option option give examples follow example question ... option ... json caption person throws wrapper butter stick trash can.n-, person cuts butter stick puts yellow bowl.n- person continues put butter yellow bowl. ... answer confidence analyze video content choose correct option number must output json format. answer option number, confidence confidence level figure example prompt words style visual question answering expert. choose correct answer five options based video content. watch video carefully answer following question selecting appropriate option. question question option option rules based provided video content question below, evaluate plausibility option step-by-step. select option best suits question video content. please detail reasoning process results. must output json format. output reasoning process, results, confidence sequence. format json think chain thoughts answer option number, confiden confidence level example question ... option ... output human expert json think understanding problem nbased content question, may need focus repetitive behavior pay attention details actions, ... analyze option conjunction video naccording option pay attention walking status. video, spends time walking dog occasionally stops, without repeatedly, valid. confidence level ... comprehensive analysis nbased analysis, first exclude options content clearly match video. ... therefore, option suitable., answer confidence figure example prompt words style visual question answering expert. choose correct answer five options based video content. watch video carefully answer following question selecting appropriate option. question question option option rules please first analyze video generate -second text descriptions caption help answer question. please provide summary entire video content summary section. descriptions must explicitly state subject. subject pronouns use represent primary agent. indicates image seen point view, indicates people image seen. focus descriptions elements relevant answering potential questions. based description, summary, questions, options video clips, provide analysis process results. please think step step provide brief reason reason. please provide results confidence level answer confidence. must output json format. output caption, summary, reasoning process, results, confidence sequence. format json caption drops card table left, ..., man picks card stack cards table left, ..., writes book pen right hand summary summary video content, reason chain thoughts, answer option number, confidence confidence level example json caption washes tray., ..., man rinses plate., ..., washes board. summary begins scrubbing washing tray using sponge scouring pad. proceeds wash tin, various plates, grater, consistently using tap rinsing scrubbing utensils ... reason based consistent actions washing scrubbing various kitchen items, primary objective appears cleaning dishes kitchen utensils. indications cooking, laundry, bathroom cleaning activities given captions., answer confidence figure example prompt words style gests structure content output terms heavily influence inference performance vlms. finding underscores importance carefully designing reasoning process output format tasks involving vlms achieve optimal performance. another interest- ing phenomenon stark difference trends observed validation set test set, indicating significant data bias. highlights advantages large models terms generality adaptability, require training process thus avoid affected small number outlier samples. comparisons cot constructions provided table table ablation experiment results cot via gemini flash validation dataset. abbreviations first line, left right, caption, summary, reason, answer, confidence accuracy respectively. cap sum ans conf acc micro design thoroughly evaluating -stage paradigm, gained valuable experience understanding. based this, attempted improve refine second stage paradigm. first, aim utilize vlm generate content may require attention based questions options. focus content focal, along video original prompt words, enter first stage process. hand, directly input video large model identify output parts considers important, without provid- ing questions options. approach aims enhance focus video focus. set results indicates guided generation improve targeting efficiency, free-form attention enhance focus comprehensiveness content itself. choosing ap- propriate approach based specific needs effec- tively utilize analytical capabilities large models. vlm model experiment conducted basic exploration several key aspects large models. phase, fix configuration previous optimal -stage solution replace gemini flash powerful vlms, including gemini exp g-exp gemini preview g-preview. naturally, upgrade tools brings significant increase accuracy. worth noting -stage paradigm explored manner due significantly higher cost invoking apis twice, table accuracy different modes test dataset. mode acc figure similarity matrix results different modes test dataset. makes less practical. additionally, coordination stages introduces uncontrollable interference factors, complicating implementation relia- bility. therefore, inclined towards one-stage solution practical applications. model ensemble final phase, select integrate prominent modes explored above. total sets results adopted, high accuracy low similarity. ta- ble shows accuracy weights different modes cal- culated equation visualization result similar- ity matrix calculated equation shown figure reported main results assembly modes last group figure among them, represents ac- tivation, represents non activation. several answers activated modes voted according equation excit- ingly, lightweight post-processing strategy deliv- ered remarkable improvement performance. further- more, important emphasize although multiple large models assembled, calls executed parallel, assembly process model consumed virtually time. say, final solution still equivalent -stage end-to-end model, maintaining simplicity efficiency.", "published_date": "2025-05-22T15:27:31+00:00"}
{"id": "2505.16314v1", "title": "NTIRE 2025 challenge on Text to Image Generation Model Quality Assessment", "authors": ["Shuhao Han", "Haotian Fan", "Fangyuan Kong", "Wenjie Liao", "Chunle Guo", "Chongyi Li", "Radu Timofte", "Liang Li", "Tao Li", "Junhui Cui", "Yunqiu Wang", "Yang Tai", "Jingwei Sun", "Jianhui Sun", "Xinli Yue", "Tianyi Wang", "Huan Hou", "Junda Lu", "Xinyang Huang", "Zitang Zhou", "Zijian Zhang", "Xuhui Zheng", "Xuecheng Wu", "Chong Peng", "Xuezhi Cao", "Trong-Hieu Nguyen-Mau", "Minh-Hoang Le", "Minh-Khoa Le-Phan", "Duy-Nam Ly", "Hai-Dang Nguyen", "Minh-Triet Tran", "Yukang Lin", "Yan Hong", "Chuanbiao Song", "Siyuan Li", "Jun Lan", "Zhichao Zhang", "Xinyue Li", "Wei Sun", "Zicheng Zhang", "Yunhao Li", "Xiaohong Liu", "Guangtao Zhai", "Zitong Xu", "Huiyu Duan", "Jiarui Wang", "Guangji Ma", "Liu Yang", "Lu Liu", "Qiang Hu", "Xiongkuo Min", "Zichuan Wang", "Zhenchen Tang", "Bo Peng", "Jing Dong", "Fengbin Guan", "Zihao Yu", "Yiting Lu", "Wei Luo", "Xin Li", "Minhao Lin", "Haofeng Chen", "Xuanxuan He", "Kele Xu", "Qisheng Xu", "Zijian Gao", "Tianjiao Wan", "Bo-Cheng Qiu", "Chih-Chung Hsu", "Chia-ming Lee", "Yu-Fan Lin", "Bo Yu", "Zehao Wang", "Da Mu", "Mingxiu Chen", "Junkang Fang", "Huamei Sun", "Wending Zhao", "Zhiyu Wang", "Wang Liu", "Weikang Yu", "Puhong Duan", "Bin Sun", "Xudong Kang", "Shutao Li", "Shuai He", "Lingzhi Fu", "Heng Cong", "Rongyu Zhang", "Jiarong He", "Zhishan Qiao", "Yongqing Huang", "Zewen Chen", "Zhe Pang", "Juan Wang", "Jian Guo", "Zhizhuo Shao", "Ziyu Feng", "Bing Li", "Weiming Hu", "Hesong Li", "Dehua Liu", "Zeming Liu", "Qingsong Xie", "Ruichen Wang", "Zhihao Li", "Yuqi Liang", "Jianqi Bi", "Jun Luo", "Junfeng Yang", "Can Li", "Jing Fu", "Hongwei Xu", "Mingrui Long", "Lulin Tang"], "summary": "paper reports ntire challenge text image generation model quality assessment, held conjunction new trends image restoration enhancement workshop ntire cvpr aim challenge address fine-grained quality assessment text-to-image generation models. challenge evaluates text-to-image models two aspects image-text alignment image structural distortion detection, divided alignment track structural track. alignment track uses evalmuse-k, contains around ai-generated images aigis generated popular generative models. alignment track total registered participants. total submissions received development phase, submissions received test phase. finally, participating teams submitted models fact sheets. structure track uses evalmuse-structure, contains ai-generated images aigis corresponding structural distortion mask. total participants registered structure track. total submissions received development phase, submissions received test phase. finally, participating teams submitted models fact sheets. almost methods achieved better results baseline methods, winning methods tracks demonstrated superior prediction performance model quality assessment.", "full_text": "cs.cv may ntire challenge text image generation model quality assessment shuhao han haotian fan fangyuan kong wenjie liao chunle guo chongyi radu timofte liang tao junhui cui yunqiu wang yang tai jingwei sun jianhui sun xinli yue tianyi wang huan hou junda xinyang huang zitang zhou zijian zhang xuhui zheng xuecheng chong peng xuezhi cao trong-hieu nguyen-mau minh-hoang minh-khoa le-phan duy-nam hai-dang nguyen minh-triet tran yukang lin yan hong chuanbiao song siyuan jun lan zhichao zhang xinyue wei sun zicheng zhang yunhao xiaohong liu guangtao zhai zitong huiyu duan jiarui wang guangji liu yang liu qiang xiongkuo min zichuan wang zhenchen tang peng jing dong fengbin guan zihao yiting wei luo xin minhao lin haofeng chen xuanxuan kele qisheng zijian gao tianjiao wan bo-cheng, qiu chih-chung hsu chia-ming lee yu-fan lin zehao wang mingxiu chen junkangfang huamei sun wending zhao zhiyu wang wang liu weikang puhong duan bin sun xudong kang shutao shuai lingzhi heng cong rongyu zhang jiarong zhishan qiao yongqing huang zewen chen zhe pang juan wang jian guo zhizhuo shao ziyu feng bing weiming hesong dehua liu zeming liu qingsong xie ruichen wang zhihao yuqi liang jianqi jun luo junfeng yang jing hongwei mingrui long lulin tang abstract paper reports ntire challenge text image generation model quality assessment, held conjunction new trends im- age restoration enhancement workshop ntire cvpr aim challenge address fine-grained quality assessment text-to-image generation models. challenge evaluates text-to-image models two aspects image-text alignment image structural dis- tortion detection, divided alignment track structural track. alignment track uses evalmuse-k, contains around ai-generated images aigis generated popular generative mod- els. alignment track total registered par- ticipants. total submissions received development phase, submissions received organizers ntire challenge text image gener- ation model quality assessment. ntire website test phase. finally, participating teams submitted models fact sheets. structure track uses evalmuse-structure, contains ai-generated images aigis corresponding structural distortion mask. total participants registered structure track. total submissions received development phase, submissions received test phase. finally, participating teams submitted models fact sheets. almost methods achieved better results baseline methods, winning meth- ods tracks demonstrated superior prediction performance model quality assessment. introduction rapid development generative models, ad- vanced text-to-image models capable gener- ating many impressive images. however, generated images still face challenges terms alignment text structural fidelity. currently, widely used bench- marks methods eval- uating quality generated images primarily focus annotating predicting image quality using mean opin- ion scores mos. provide comprehensive evalu- ation, certain benchmarks, aigciaq as- sess images across multiple dimensions, including quality, authenticity correspondence. images low text alignment scores, often difficult identify ele- ments text reflected generated images. similarly, images low authenticity scores, pinpoint- ing specific locations structural distortion within im- age remains challenging. thus, developing effective methods assessing image-text alignment detecting structural degradation generated images important. ntire text image generation model quality assessment challenge aims promote de- velopment methods predicting quality scores generated images enabling fine-grained evaluation, thereby guiding performance improvement gener- ative models. challenge divided align- ment track structure track. alignment track, use evalmuse-k consists around image-text pairs fine-grained alignment annotations. structure track, construct new dataset called evalmuse-structure, includes generated images. image provided structure score image authenticity well corresponding structural annota- tion mask indicates whether structural distortions occur specific locations within image. first time fine-grained generated im- age quality assessment challenge held ntire workshop. challenge total registered partic- ipants, alignment track structure track. total submissions received development phase, prediction results sub- mitted final test phase. finally, valid partic- ipating teams image track valid participating teams video track submitted final models fact sheets. provided detailed introductions iqa methods fine-grained alignment structural evaluation. provide detailed results challenge section describe specific methods used participating teams section hope chal- lenge promote development fine-grained quality evaluation generated images guide improvements models, particularly terms image-text alignment structural fidelity. challenge one ntire work- shop associated challenges ambient lighting normaliza- tion reflection removal wild shadow re- moval event-based image deblurring image de- noising xgc quality assessment ugc video en- hancement night photography rendering image super-resolution real-world face restoration efficient super-resolution depth estimation efficient burst hdr restoration cross-domain few- shot object detection short-form ugc video quality assessment enhancement text image gen- eration model quality assessment day night rain- drop removal dual-focused images video quality assessment video conferencing low light image enhancement light field super-resolution restore image model raim wild raw restoration super-resolution raw reconstruction rgb smartphones related work aigi dataset recent years, several ai-generated images aigi datasets proposed. benefiting suc- cess stable diffusion diffusiondb col- lected million images generated stable diffusion based prompts hyperparameters provided real users. hps pick-a-pic collect large num- ber side-by-side image comparisons evaluate qual- ity generated images. agiqa-k agiqa- aigciqa aigiqa-k anno- tate quality generated images scores get moss assess quality generated images. genai- bench provides score-based annotations evaluate alignment generated images text overall image-text alignment. gecko richhf en- able fine-grained evaluation annotating inconsistent words text relative generated images. addi- tionally, richhf employs point annotations gener- ated images highlight regions distortion. chal- lenge, utilize evalmuse-k image-text align- ment evaluation, splits text image-text pair performs element-level annotations. meanwhile, use evalmuse-structure assess structural fidelity generated images, annotates structural distortions images using bounding boxes. iqa method traditional iqa methods focus various image distortions noise, blur semantic con- tent. generated images, consistency used text considered important part generated im- ages quality evaluation. hps pickscore leverage clip model simulate human prefer- ences generated images. imagereward fga- blip employ blip-based architectures predict scores. advancement multi-modal large language models mllms, vqascore align evaluate image-text alignment quality generated images employing visual question answering mllms. fine-grained image-text alignment eval- uation, tifa gecko generate specific ques- tions targeting different elements text evaluate fine-grained alignment capabilities generated im- ages. rahf model performs fine-grained alignment evaluation identifying inconsistent words predicts heatmaps assess structural distortions. ntire challenge text image generation model quality assessment ntire challenge text image genera- tion model quality assessment organized improve fined-grained quality assessment generated images. main goal challenge achieve fine-grained alignment evaluation predicting alignment scores elements fine-grained structural evaluation predict- ing structural distortion masks. accomplish goals, challenge divided alignment track structure track. details whole challenge described following parts, including datasets, evaluation protocol challenge phases. datasets alignment track, use evalmuse-k dataset training validation. dataset consists around image-text pairs fine-grained align- ment annotations, including real prompts syn- thetic prompts. real prompts selected using mixed integer linear programming milp sampling strategy ensure diversity category balance. synthetic prompts generated gpt- based predefined templates tar- geting specific aspects quantity spatial rela- tionships. furthermore, prompts split elements achieve fine-grained annotations. annotation, anno- tators provide overall alignment score image- text pair verify whether split elements repre- sented generated images. pair annotated three annotators, pairs significant disagreements alignment scores re-annotated. testing phase, additional image-text pairs introduced annotated us- ing process, combined por- tion validation set test dataset. structure track, create new dataset, evalmuse-structure, training, validation, testing. dataset includes generated images fine- grained structural annotations, used training, validation, testing. generated image, annotators provide structural scores use bounding boxes annotate regions structural dis- tortions. image annotated three annotators, fine-grained annotations, structural distortion masks derived overlapping distorted regions identified least two annotators. evaluation protocol tracks, main scores utilized determine rankings participating teams. evaluate alignment scores structure scores predicted model tracks using spearman rank-order correlation coefficient srcc person linear correlation coefficient plcc. srcc measures prediction monotonicity, plcc measures prediction accuracy. better iqa methods larger srcc plcc values. calcu- lating plcc index, perform third-order polynomial nonlinear regression. alignment track, use accuracy acc model determining whether elements prompt presented generated image measure models fine-grained alignment evaluation capability. primary score computation method alignment track fol- lows athrm core mathrm acc mathrm srcc mathrm plcc structure track, evaluate models fine- grained structural assessment capability using score computed model-predicted structural distortion mask human-annotated structural distortion mask. primary score computation method structure track follows athrm scor mathrm mathrm srcc mathrm plcc challenge phases tracks consist two phases development phase testing phase. development phase phase, participants ac- cess training dataset, contains generated im- ages, corresponding prompts, relevant annotations. par- ticipants learn structure dataset develop methods. also release validation dataset includes generated images correspond- ing prompts lacks annotations. participants use methods predict annotations validation dataset upload results server. immediate feedback pro- vided, enabling participants analyze effectiveness methods validation set. validation leader- board available. testing phase phase, participants access test dataset contains generated images cor- responding prompts, without annotations. participants need upload final predicted annotations test set challenge deadline. leaderboard avail- able stage participants see scores. time, team required submit table quantitative results ntire challenge text image generation model quality assessment track alignment. rank team leader main score srcc plcc acc ih-vqa jianhui sun evalthon zijian zhang hcmus trong-hieu nguyen-mau micv jun lan sjtu-mmlab zhichao zhang sjtumm zitong zichuan wang yag fengbin guan sprank minhao lin aiig xuanxuan joe bo-cheng, qiu icost baseline fga-blip table quantitative results ntire challenge text image generation model quality assessment track structure. rank team leader main score score srcc plcc hnu-vpai zhiyu wang opdai shuai micv jun lan memory qiao zhishan group zewen chen brute force wins zeming liu wecan evalaig jun luo tenryu babu junfeng yang baseline rahf model source codeexecutable files fact sheet containing detailed description proposed methodology cor- responding team information. final results rankings sent participants. challenge results challenge, teams alignment track teams structure track submitted final codeexecutables fact sheets. tab. summarize key results important information valid teams. methods briefly described section team members listed appendix alignment track, fga-blip used base- line, rahf model serves baseline structure track. tab. reveal submit- ted results participating teams achieve better perfor- mance baseline. alignment track, top- performing team, ih-vqa, achieves higher main score baseline, six teams main scores higher structure track, leading team, hnu-vpai, exceeds baseline main score. methods achieve better fine-grained evalua- tions image-text alignment structure distortion detection, significantly advancing development fine-grained evaluation generated images facili- tate better improvement generative models. challenge methods image-text alignment track ... ih-vqa team ih-vqa wins championship image- text alignment track proposed method, imatch instruction-augmented multimodal alignment image- text element matching, illustrated figure method aims precisely assess alignment generated images textual descriptions several key innovations. fine-tune mllms using fine-grained image-text matching annotations evalmuse-k dataset guide model learning nuanced correspon- figure overview team ih-vqa proposed imatch. figure overview team evalthon proposed method. dences. enhance performance, propose four augmentation strategies qalign strategy, maps textual rating levels numerical scores applies soft mapping prediction probabilities accurate score conversion validation set augmentation strat- egy, model generates high-quality pseudo-labels validation set training, merged back training set improve generalization element augmentation strategy, incorporates element labels user query, enabling chain-of-thought-style reasoning derive better overall matching scores image augmentation strategy, introducing three augmen- tation techniques increase diversity training images enhance robustness visual variations. element matching task, propose two augmentation techniques prompt type augmentation, embeds prompt type real synthetic query help model distinguish different source char- acteristics score perturbation augmentation, adds slight random noise target labels prevent overfit- ting improve models generalization. finally, adopt model ensemble strategy. image-text matching task, average results several fine-tuned mllms qwen.-vl-b-instruct, ovis-b, ovis-b, ovis-b. ele- ment matching task, ensemble combines ovis-b ovis-b. figure overview team hcmus proposed method. ... evalthon team evalthon win second place image- text alignment track. train multiple large vi- sion language modelslvlms integrate us- ing xgboost model. baselines qwen.-vl- b,internvl.-b internvl.-b. depicted figure training procedure consists three stages. initially, partition dataset five folds,allocating training validation within fold. guiding principle eliminate du- plicate prompts, allowing image generation models overlap across folds, aligning testing data distri- bution. subsequently, utilize combination image text, integrating certain statistical features directly textual inputs. training phase, low rank training technique employed. training step, directly extract logits corresponding token positions hidden state, proceeding apply weighted sum- mation leveraging mean squared errormse loss fit model label. testing, utilize various models predict test set deploying checkpoints derived training across different folds. finally, employ xgboost integrate predicted scores selected statistical fea- tures, consolidating final score. ... hcmus team hcmus wins third place image-text alignment track. propose approach focusing fine-tuning qwen-vl qwen.-vl models using low-rank adaptation lora techniques. also apply ensemble approaches, post-processing, utilize external datasets generalize achieve better performance, aiming offer refined, human-aligned evaluation frame- work assess image-text alignment greater pre- cision sensitivity fine-grained details. figure illustrates comprehensive pipeline approach. baseline models qwen-vl- b-instruct, qwen.-vl-b-instruct, qwen-vl-b- instruct qwen.-vl-b-instruct. training datasets, average element-wise scores origi- nal dataset produce single score per field. pro- cessed dataset formatted instruction-based input- output pairs suitable vlm training. also incor- porate external datasets, tifa genaibench, enhance models performance text-to-image generation quality assessment. first train qwen.-vl-b-instruct formatted original dataset directly output results json structure. tifa genaibench, filter incomplete invalid samples two extra datasets ensure high-quality data. items non-json-parsable null-valued outputs discarded. use trained model generate pseudo labels merge figure overview team micv proposed method. pseudo-labeled datasets original evalmuse dataset training qwen.-vl-b-instruct. denote dataset external, also construct external private dataset removing type attribute. specifically, train models simply average results obtain final scores. trained models following qwen-vl-b-instructoriginal, qwen.-vl-b- instructoriginal, qwen-vl-b-instructoriginal, qwen.-vl-b-instructoriginal, qwen.-vl-b- instructexternal, qwen.-vl-b-instructexternal private. ... micv team micv propose prior information-guided multi- modal approach. main focus learn generation characteristics different generative models train- ing data, alignment level different elements various prompts, consider multi-annotator process quality assessment task. utilize qwen.-vl model predict global alignment score local element alignment score. leveraging structure original data, con- struct global local element question-answer templates. based generative model information train- ing data relevant information human annota- tions, model predicts data distribution. figure shows overview methods. ... sjtu-mmlab team sjtu-mmlab propose method similar align. overall alignment score, fine- tune four powerful multimodal large language mod- els mllmsinternvl-b, internvl-b, qwenvl-b, qwenvl-b leveraging supervised fine-tuning sft low-rank adaptation lora optimize vision- language components models. figure shows overview methods. model outputs qual- ity score five categories excellent, good, fair, poor, bad. corresponding probabilities cate- gory weighted according predefined criteria, final overall alignment score calculated averaging weighted probabilities across models. train mod- els, use combination cross-entropy loss loss mean squared error mse loss, ensuring mod- els classify alignment correctly predict qual- ity scores high accuracy. fine-grained align- ment score, fine-tune three mllmsinternvl-b, qwenvl-b, qwenvl-busing sft lora approach. models perform binary classification, outputting either yes indicate presence element image. final fine-grained alignment score derived averaging probabilities yesno decisions models. novelty lies combina- tion mse loss loss, allows precise figure overview team sjtu-mmlab proposed method. classification alignment quality fine-tuning align- ment score values, leading accurate reliable measurements. additionally, designing custom prompts element category enables model focus unique characteristics element, enhancing ability reflect specific features generated image. ... sjtumm team sjtumm propose two-stage training stage mul- timodal large language models. figure overview training method. first training stage, use cross-entropy loss adapt mllms fixed output for- mat like alignment score floating number. second training stage, last hidden state represent- ing token score decoded quality score decodernot head, score num- ber mean squared error loss used training. change llm backbone internvl., qwen.vl deepseekvl average total scores. element scores obtained one three mod- els determines element, element score one. ... team adopt methods proposed q-align deqa-score. choose internvl.-b baseline model. design tailored instructions, responses image-text alignment categorized bad, poor, fair, good, excellent, element presence evaluated yesno response. ensure robust accurate scor- ing, employ dual-method strategy weighted probability-based scoring. image-text alignment scores computed weighting prob- abilities five response categories. element pres- ence scqres derived probability yes response. score distribution alignment via divergence. recognizing discrete textual responses nat- urally translate continuous scores, introduce score distribution modeling technique. treat mean opinion score mos gaussian distribution, computing probabilities across five response levels. applying divergence, align predicted probability distribution true distribution, ensur- ing precise reliable scoring. divergence also applied element presence evaluation, aligning probabilities yes true probabilities. model mos gaussian distribution use divergence align predicted distribution actual one, improving accuracy score prediction interval probability divided mos inter- vals used mean variance mos label estimate probability belonging interval. probability calculation formula follows int dx, quad gaussian probability density function mos. probability adjustment sum five interval probabilities modeled way equal leading mos shift. therefore, apply linear trans- formation ensure accuracy mos. formula follows new alpha beta solving following equations, obtain values alpha beta beg pitext new sum pitext new mos. end cases figure overview team sjtumm proposed method. loss function use divergence align pre- dicted probabilities five tokens llm interval probabilities itext new log left frac pitext predpitext newright testing phase, fused results three checkpoints. specifically, training, saved checkpoint every steps selected three best- performing checkpoints validation set steps, steps, steps testing test set. final results obtained averaging outputs. ... yag propose multi-granularity quality assessment frame- work ai-generated content aigc leveraging robust multimodal capabilities foundation models qwen.-vl llava-onevision. figure shows overview methods. evaluate text-image alignment quality, design model-specific question answering templates guide models assess element-level details overall coherence. qwen, prompt instructs model evaluate presence pre- defined elements image binary responses judge alignment generation prompt image scale. llava, following unified reward inspired structure, model evaluates alignment caption, visual quality, extracts el- ement presence scores using structured output for- mats. enhance cross-modal interactions, integrate qformer architecture fga blip frame- work, enabling joint analysis element-level holistic scores. critically, address potential omissions ele- ment detection, adopt ensemble strategy element marked present identified model, ensuring comprehensive coverage mitigating individ- ual model biases. approach combines structured scor- ing robust element detection achieve reliable aigc quality assessment. additionally, employ ensemble three distinct multi-modal models fga-blip, qwen.-vl variants, unified reward version. specifi- cally, qwen variants untrained base qwen serv- ing control reference, lora-adapted qwen parameter-efficient fine-tuning fully fine-tuned qwen full parameter optimization. outputs qwen variants aggregated inference re- sults unified reward trained evalmuse-k dataset, trained fga-blip outputs, form final detection ensemble. ... sprank team sprank propose qwen-assisted image-text align- ment scoring. leverages finetuned qwen.-vl-b- instruct model enhance alignment element scor- ing image-text assessment. first, finetune given data question-answer pairs generate custom element questions specialized fine-tuning. alignment scoring, use specifically designed prompts guide models evaluation. finally, combine fine-tuned model scores baseline scores weighted summation, optimizing element existence judgment image-text alignment assessment. testing, ensemble results fine-tuned model baseline method using weighted summation produce final results beg aligned ime sate aseline times satext qwen times setext baseline times setext qwen end aligned denotes alignment score, represents score generated fine-tuned qwen. model. ... aiig team aiig selected model fga-blip baseline. fga-blip enables fine-grained alignment evaluation figure overview team yag proposed method. figure overview team aiig proposed method. combining training overall element alignment scores. use itm setup blip concatenate query embedded text, cross-focus im- age. final alignment score obtained two-class linear classifier, query sections averaged produce overall alignment score, text sections corresponding position provide alignment score specific element. text prompts first fed self-attention layer extract text features, passed mlp. mlp module captures local con- text pattern sequence convd layer, maintains stable gradient flow deepening network depth residual connections. multi-scale features output fused fully connected layer. final mlp predicts mask represents validity text tag. show figure length query text prompt, respectively. respec- tively represent overall split element scores image-text alignment. represents loss predicted overall alignment score manual annota- tion, represents loss predicted element alignment score manual fine-grained annotation, represents loss predicted valid text annotation real element. addition, use vari- ance weight loss function. greater variance image-text pair, higher loss. final loss objec- tive function shown formula sigma vari- ance overall alignment score different images gen- erated text prompt. weight parameters alpha beta set method makes model pay attention samples large alignment score difference training process, thus improving accu- racy robustness evaluation. bel esigma times alpha beta ... joe team joe enhance fga-blip using multi- component loss function combines original itm image-text matching loss three specialized com- ponents. training, model processes image-text pairs visual encoder qformer architec- ture, generating embeddings scores over- match individual elements within prompt. training process utilizes base loss derived original fga-blip model score difference, token score, mask prediction distribution matching loss guides model toward generating predictions similar distribution character- istics validation set element type adaptive weighting based statistical properties different element types contrastive learning ensure elements type similar feature representations training uses linear warmup followed cosine learn- ing rate scheduling, image augmentation techniques like random resizing cropping. model leverages pre-trained weights blip fine-tunes fine- grained evaluation dataset keeping vision encoder frozen prevent overfitting. ... icost team icost utilizes training policy used deepseek- challenge scenario. choose two base- line models qwen.-vl-b qwen.-vl-b. first, transform given dataset image-text conversa- tion. second, pick according attribute confidence, higher confidence may reflect higher quality. let qwen.-vl-b output evaluation scores fine-tune make output con- sistent ground truth sft. pick high-quality image-text continue fine-tune model grpo. structure distortion detection ... hnu-vpai team hnu-vpai wins championship structure distortion detection track.this team used separate models two subtasks. use image quality assessment iqa model predict quality scores instance seg- mentation model identify structural issue regions image. image quality assessment sub-task, team adopt cnn-transformer hybrid architecture leverage strengths models detecting structural distor- tions. convolutional neural networks cnns excel cap- turing fine-grained, local features, making particularly effective identifying localized distortions image, misalignments local texture anomalies. hand, transformers highly capable capturing long-range dependencies global context, allows understand overall structure generated image detect distortions might immediately apparent local regions affect images global co- herence. combining cnns transformers, ap- proach benefits detailed local feature extraction cnns global context modeling transform- ers. hybrid architecture particularly effective structure distortion detection, enables precise identifi- cation local global distortions, leading accurate robust quality assessments generated im- ages. inspired loda team first employ cnn extract local distortion features input images, inject cnn-extracted multi-scale local distortion features vit using cross-attention mechanism. allows vit focus distortion-related fea- tures maintaining strength capturing global con- text. querying relevant multi-scale features cnn fusing vits image tokens, ensure local global distortions captured simultaneously. optimize efficiency reduce computational overhead, follow loda down-project high-dimensional vit tokens multi-scale features smaller dimension performing cross-attention. ensures model remains computationally effi- cient leveraging rich information cnn vit. important note cnn vit pretrained models, parameters remain frozen training. update parameters lightweight cross-attention module. approach re- tains rich knowledge embedded pretrained mod- els focusing training task-specific compo- nents, ensuring efficient adaptation minimal compu- tational overhead. conducted extensive experiments various variants pretrained cnn vit models found simple resnet architecture performed well cnn, clip dinov pretrained mod- els yielded best results vit. structural distortion detection sub-task, team utilize state-of-the-art instance segmentation model e.g., co-detr identify regions structural distortions. even though baseline method frames task semantic segmentation problem, found treating instance segmentation task leads better results. co-detr advanced instance segmentation model enhances performance detr-based de- tectors incorporating collaborative hybrid assignments figure overview team memory proposed method. training scheme. approach utilizes versatile one-to- many label assignments, atss faster rcnn, enrich supervision provided encoders output, thereby improving discriminative capabilities. addition- ally, co-detr introduces customized positive queries extracting positive coordinates auxiliary heads, enhances training efficiency decoder. in- ference, auxiliary heads discarded, ensuring method introduce extra parameters compu- tational costs original detector. context struc- tural distortion detection, co-detrs architecture partic- ularly effective. encoder-decoder structure, combined collaborative hybrid assignments training scheme, allows precise identification regions struc- tural distortions. treating task instance seg- mentation problem, co-detr distinguish individual distorted regions, leading accurate detection com- pared semantic segmentation approaches. ... opdai team opdai propose multi-task architecture leveraging florences foundation model capabilities simul- taneous structural distortion localization heatmap predic- tion severity estimation implausibility scoring. network employs florence-large encoder frozen weights initialized fld-b pretraining u-net style decoder heatmap prediction, opti- mized using combination mean squared error mse dice-iou losses. multi-layer perceptron head regression scoring, trained composite loss comprising mse,plcc loss srcc loss. use two-stage training strategy, heads-only training epochs lre-, full fine-tuning training another epochs lre- backbone heads. different losses two tasks heatmap ath lhm text mse text dice-iou score mat cal core text mse .-text plcc -text srcc ... micv team micv propose sgl-sdd, clip-based model tai- lored detecting structure distortion images generated models. reduce label noise intro- duced annotators, adopt study group learning strategy detection learning. team observe would better divide struc- ture distortion detection task two models learning, design two models, one model score learning one model heatmap learning. learning, adopt strategy study learning groupslg reduce label noise training set. specifically, learning, randomly averagely split whole training set subsets xk, yk, train totally models mk. model mk, train xk, yk. training model set, infer estimated score heatmap label figure overview team group proposed method. pseudo label based model mk. fi- nally, take average true label pseudo label label model training. besides, also adopt strategy prompt rewriting make text data augmentation based gpt-o. adopt pre- trained model siglip model. training time model gpus hours. parame- ters models millon. adopt strategy test-time augmentation testing. test sample, conduct model pre- diction six times different prompts generated prompt rewriting, take average results final prediction result. ... memory team memory optimize baseline architecture structure distortion detection track several key mod- ifications optimizations improve performance. model designed jointly predict heatmaps scores images text. employ siglip-som-patch- backbone model architecture. last hidden states visual textual encoders concatenated along token dimension, yielding unified multimodal representation seamlessly integrates information modalities. perform two key optimizations self-attention module, one using xformers library memory-efficient attention computation, another replac- ing traditional layernorm rmsnorm normal- ization layer stabilizes training normalizing activa- tions. heatmap predictor also replaces layernorm rmsnorm, along adjustments normalization di- mension improved efficiency. simplify score prediction branch replacing baselines convolutional mlp-based approach straightforward de- sign. new architecture computes global score refined multimodal embeddings mean pooling fully connected layers. streamlined approach effec- tively summarizes multimodal features single confi- dence score reducing complexity. utilize adamw legacy optimizer initial learning rate combined cosine an- nealing learning rate scheduler ensure stable conver- gence. accelerate training optimize memory usage, employ automatic mixed precision amp. addition- ally, optimizer configured weight decay caution flag set true. caution mecha- nism skips updates certain parameters update direction conflicts current gradient direction, mit- igating potential instability caused noisy gradients ensuring robust training noisy conditions. loss function based mean squared error mse components objective. specifically, total loss computed oss cdo text lossheatmap cdot text lossscore loss heatmap corresponds mse loss heatmap prediction task, loss score corresponds mse loss score prediction task. training process involves iterations, weight decay batch size gradient ac- cumulation steps. experiments performed single nvidia gpu equipped memory. training dataset comprises data provided competition organizers, overall training pro- cess takes approximately hours complete. ... group team group propose mfm-iqa framework, leverages multi-level multi-scale image prompt fea- tures achieve accurate quality assessment structure distortion detection aigc images. mfm-iqa mainly consists three components im- age text encoders extract multi-level visual textual features multi-scale features fusion module mfm designed extract multi-scale fusion features integrating multi-level visual textual features decoders tailored downstream tasksspecifically, up- sampling convtransposed decoders predicting dis- torted region masks, regression modules composed score prediction pipeline heatmap prediction pipeline figure overview team brute force wins proposed method. cnns fully connected layers estimate corresponding mos scores. next, introduce three components details. specifically, adopt altclip pre-trained large-scale aigc images, image text encoder. in- spired prior studies demonstrating multi-level image features enhance image assessment per- formance, given vit extracts detailed information shallow layers semantic information deeper layers, select features layers multi- level image representations. then, propose multi-scale features fusion mod- ule mfm effectively fuse visual textual features. specifically, mfm employs self-attention mechanisms spatial channel dimensions model relation- ships images texts. resulting features integrated efficient swin transformers patch merging blocks capture multi-scale semantic, structural, detailed information. multi-scale fu- sion features enhance models capability distortion detection quality assessment. lastly, design task-specific decoders structure distortion detection quality assessment. structure distortion detection, four conv transposed layers employed upsample multi-scale fusion features tar- get mask resolution. quality assessment, inspired maniqa adopt dual-branch approach pre- dict pixel-wise quality scores corresponding weights. final quality score computed weighted sum predicted pixel scores. model trained four rtx gpus, keeping image text encoders frozen. due gpu devices limitation, separately train model struc- ture distortion detection quality assessment tasks. repeatably train mfm-iqa task several times, finally choose four models masks prediction three models score prediction. score prediction, ad- ditionally train one model replacing mfm figure overview team wecan evalaig proposed method. maniqa decoder part. final results average predictions. ... brute force wins brute force wins team use separate models two sub- tasks. heatmap prediction task, employ samunet fine-grained segmentation distortion regions. subsequently, utilize qwen.vl filter refine distortion heatmaps predicted samunet. detail, trained specialized segmentation model predict segment distortion regions. sam model provides fine-grained region predictions, lacks ad- vanced semantic understanding, making unable deter- mine whether region distorted not. example, model detect human hand image cannot distinguish normal hand, hand incor- rect number fingers, hand correctly numbered fingers misplaced incorrect position. address limitation, semantic understanding capabilities re- quired. overcome issue, team cropped re- gions predicted sam sequentially evaluated using mllm. quantitative perspective, stan- dalone sam baseline models suffer high recall low precision. introducing mllm discrimi- nator, achieved significant improvement precision maintaining relatively high recall, thereby increas- image text prompt vision transformer man practicing guitar the.... text embedding image token text token conv-relu conv-relu slam slam conv mlp conv-deconv score heatmap self-attention chw chw flatten mlpexpand figure overview team tenryu badu proposed method. ing f-score. scoring task, brute force wins team adopt training strategy inspired deqa-score align fine-tune mplug-owl-llama evaluat- ing structral score images. prompting model would rate quality image? al- lowing output level token. final score computed weighted sum token probabilities corresponding weights. new method, training evaluation times significantly shorter baseline, final scores marginally outperformed baseline. ... wecan evalaig team wecan evalaig designed two-tier parallel training framework. first tier, pre-trained eva- clip model employed foundational classification semantic segmentation tasks, high-frequency fea- tures extracted resnet network dis- crete wavelet transform dwt. features subsequently fused eva-clip. sec- ond tier implements instance segmentation tasks based maskformer architecture. ultimately, segmentation results tiers integrated form comprehensive outcomes. model trained official training set provided competition, without using additional data. training phase completed epochs batch size color jitter data augmentation applied. testing, use test time augumentation mmsegmen- tation. ... tenryu babu team tenryu badu proposed unified assessment frame- work integrates cross-modal semantic understanding spatial structural reasoning enable fine-grained structural quality assessment. specifically, framework leverages clips dual encoders extract visual tex- tual embeddings generated image cor- responding prompt. embeddings subsequently fused cross-modal self-attention module iden- tify semantic discrepancies related structural content. fused representations passed two task- specific branches distortion scoring branch, imple- mented mlp-based regression module predicts scalar scores reflecting structural quality defect lo- calization branch, designed u-net-style decoder skip connections pixel-level mask prediction struc- tural distortions. key component architecture spatial layout attention module slam, adap- tively reweights channel-wise feature responses based spatial activation patterns. flattening feature maps processing shared mlp, slam learns identify enhance sensitivity geometric anomalies like hand distortions facial collapse. joint optimization structural score regression defect mask prediction, proposed framework enables human-aligned evaluation structural realism providing explicit lo- calization distortions. experimental results demonstrate effectiveness proposed framework accurately assessing structural realism ti-generated images. model training process divided two phases, totaling epochs, optimize performance prevent overfitting. first phase initial training, epochs, pre-trained vit text embedding modules frozen, remaining parts model image text token branches trained focus learning task-related features keeping pre-trained compo- nents stable. second phase fine-tuning, epochs, modules unfrozen, entire model jointly optimized better perform structural distortion detection structural quality assessment. two-stage training strategy leverages pre-trained knowledge effectively, pro- gressively refines model improves performance ef- ficiently. testing phase, model receives unseen gen- erated images corresponding text prompts in- put. image passed vit extract image tokens, text prompt passed text em- bedding module generate text tokens. tokens concatenated, sent image token branch text token branch processing respectively. image token branch generates heat map accurately lo- cating areas structural distortion image incorrect number limbs facial collapse text token branch calculates score reflects structural quality image. finally, model outputs mask image locating structural distortions structural score, compared annotations evalmuse part dataset evaluate accuracy effectiveness model practical applications. acknowledgments work partially supported humboldt foun- dation. thank ntire sponsors bytedance, meituan, kuaishou, university wurzburg computer vision lab. ntire organizers title ntire challenge text image generation model quality assessment members shuhao han, hanshmail.nankai.edu.cn, haotian fan, fangyuan kong, wenjie liao,, chunle guo, chongyi li, radu timofte, liang li, tao li, junhui cui, yunqiu wang, yang tai, jingwei sun affiliations nankai university, china bytedance inc, china computer vision lab, university wurzburg, germany teams affiliations alignment track ih-vqa title instruction-augmented multimodal alignment text- image element matching members jianhui sun nimosuntencent.com, xinli yue, tianyi wang, huan hou, junda lu, xinyang huang, zitang zhou affiliations wechat wuhan university evalthon title enhancing capabilities large vision lan-guage models integration statistical features members zijian zhang zhangzijianmeituan.com, xuhui zheng,xuecheng wu, chong peng, xuezhi cao affiliations meituan hcmus title fine-tuning qwen-vl family models lora text-to-image qual-ity assessment members trong-hieu nguyen-mau nmthieuselab.hcmus.edu.vn, minh-hoang le, minh-khoa le-phan, duy-nam ly, hai-dang nguyen, minh-triet tran affiliations university science, vnu-hcm micv title prior information-guided multi-modal approach members jun lan lanjun yelan.com, yukang lin, yan hong, chuanbiao song, siyuan affiliations micv sjtu-mmlab title evaluation image-text alignment multimodal large languagemodel members zhichao zhang liquortectsjtu.edu.cn, xinyue li, wei sun, zicheng zhang, yunhao li,xiaohong liu, guangtao zhai affiliations shanghai jiao tong university sjtumm title multi-encoder framework quality assessment via cross-modal alignment llm adaptation members zitong qq.com, huiyuduan, jiaruiwang, guangjima, liuyang, luliu, qianghu, xiongkuo min, guangtao zhai affiliations shanghai jiao tong university title using mllm evaluate image-text alignment sft score distribution members zichuan wang wangzichuania.ac.cn, zhenchen tang, peng, jing dong affiliations institute automation, chinese academy sciences yag title multi-level granularity aigc quality assessment method based multi-modal understanding members fengbin guan guanfbmail.ustc.edu.cn, zihao yu, yiting lu, wei luo, xin affiliations university science technology china sprank title qwen-assisted image-text alignment scoring members minhao lin hdu.edu.cn, haofeng chen affiliations hangzhou dianzi university, aiig title fine-grained image-text alignment evaluation members xuanxuan qq.com, kele xu, qisheng xu, zijian gao, tianjiao wan affiliations national university defense technology joe title enhanced fga-blip model contrastive learning members bo-cheng, qiu egs.ncku.edu.tw, chih-chung hsu, chia-ming lee, yu-fan lin affiliations national cheng kung university icost title vlm-r utilization task matching score evalua- tion members abupt.edu.cn, zehao wang, mu, mingxiu chen, junkang fang, huamei sun, wending zhao affiliations beijing university posts telecommunications teams affiliations structure track hnu-vpai title learning local distortion features quality assessment text-to-image generation models members zhiyu wang wangzhiyu.wzygmail.com, wang liu, weikang yu, puhong duan, bin sun, xudong kang, shutao affiliations hunan university technical university munich opdai title florence-based multi-task architecture structural distortion detection aigc imagery members shuai heshuai.secgmail.com, lingzhi heng cong rongyu zhang, jiarong affiliations opdai,netease game,guangzhou china micv title study group learning structure distortiondetection generation models members jun lan lanjun yelan.com, yukanglin, yanhong, chuanbiaosong, siyuan affiliations micv memory title memory members zhishan qiao .com, yongqing huang affiliations group title aigc image quality assessment structure distortion detection via image-prompt members zewen chen chenzewenia.ac.cn, zhe pang,, juan wang, jian guo, zhizhuo shao, ziyu feng, bing li,, weiming hu,,, hesong li, dehua liu affiliations state key laboratory multimodal artificial intelligence systems,institute automation, chinese academy sci- ences school artificial intelligence, university chinese academy sciences beijing union university beijing jiaotong university peopleai inc.beijing, china school information science technology, shang- haitech university transsion inc. brute force wins title structure distortion detection scoring model based sam mllm members zeming liu buuugmaker.com, qingsong xie, ruichen wang, zhihao affiliations oppo center wecanevalaigc title vlm-r utilization task matching score evalua- tion members jun luo wuguo.ljantgroup.com, yuqi liang, jianqi affiliations antgroup tenryu babu title attention-driven structure quality assessment text- to-image gener-ation members junfeng yang bhnu.edu.cn, jing hongwei mingrui long, lulin tang affiliations xiangjiang laboratory, hunan university technology business references shuai bai, keqin chen, xuejing liu, jialin wang, wenbin ge, sibo song, kai dang, peng wang, shijie wang, jun tang, al. qwen. -vl technical report. arxiv preprint shuai bai, keqin chen, xuejing liu, jialin wang, wenbin ge, sibo song, kai dang, peng wang, shijie wang, jun tang, al. qwen. -vl technical report. arxiv preprint tianqi chen carlos guestrin. xgboost scalable tree boosting system. proceedings acm sigkdd international conference knowledge discovery data mining, pages zhongzhi chen, guang liu, bo-wen zhang, fulong ye, qinghong yang, ledell wu. altclip altering lan- guage encoder clip extended language capabilities. arxiv preprint zewen chen, juan wang, bing li, chunfeng yuan, wei- hua xiong, rui cheng, weiming hu. teacher-guided learning blind image quality assessment. proceedings asian conference computer vision, pages zewen chen, juan wang, wen wang, sunhan xu, hang xiong, yun zeng, jian guo, shuxun wang, chunfeng yuan, bing li, al. seagull no-reference image quality assess- ment regions interest via vision-language instruction tuning. arxiv preprint zhe chen, weiyun wang, yue cao, yangzhou liu, zhang- wei gao, erfei cui, jinguo zhu, shenglong ye, hao tian, zhaoyang liu, al. expanding performance boundaries open-source multimodal models model, data, test- time scaling. arxiv preprint zheng chen, kai liu, jue gong, jingkai wang, lei sun, zongwei wu, radu timofte, yulun zhang, al. ntire challenge image super-resolution methods results. proceedings ieeecvf conference computer vision pattern recognition cvpr work- shops, zewen chen, haina qin, juan wang, chunfeng yuan, bing li, weiming hu, liang wang. promptiqa boosting performance generalization no-reference image quality assessment via prompts. european conference computer vision, pages springer, zheng chen, jingkai wang, kai liu, jue gong, lei sun, zongwei wu, radu timofte, yulun zhang, al. ntire challenge real-world face restoration methods results. proceedings ieeecvf conference computer vision pattern recognition cvpr work- shops, marcos conde, radu timofte, al. ntire challenge raw image restoration super-resolution. proceed- ings ieeecvf conference computer vision pattern recognition cvpr workshops, marcos conde, radu timofte, al. raw image reconstruc- tion rgb smartphones. ntire challenge re- port. proceedings ieeecvf conference com- puter vision pattern recognition cvpr workshops, egor ershov, sergey korchagin, alexei khalin, artyom pan- shin, arseniy terekhin, ekaterina zaychenkova, georgiy lobarev, vsevolod plokhotnyuk, denis abramov, elisey zhdanov, sofia dorogova, yasin mamedov, nikola banic, georgii perevozchikov, radu timofte, al. ntire challenge night photography rendering. proceedings ieeecvf conference computer vision pat- tern recognition cvpr workshops, yuxin fang, wen wang, binhui xie, quan sun, ledell wu, xinggang wang, tiejun huang, xinlong wang, yue cao. eva exploring limits masked visual represen- tation learning scale. arxiv preprint yuqian fu, xingyu qiu, bin ren yanwei fu, radu timofte, nicu sebe, ming-hsuan yang, luc van gool, al. ntire challenge cross-domain few-shot object detection methods results. proceedings ieeecvf con- ference computer vision pattern recognition cvpr workshops, daya guo, dejian yang, haowei zhang, junxiao song, ruoyu zhang, runxin xu, qihao zhu, shirong ma, peiyi wang, xiao bi, al. deepseek-r incentivizing reasoning capability llms via reinforcement learning. arxiv preprint shuhao han, haotian fan, jiachen fu, liang li, tao li, jun- hui cui, yunqiu wang, yang tai, jingwei sun, chunle guo, chongyi li. evalmuse-k reliable fine-grained benchmark comprehensive human annotations text- to-image generation model evaluation, shuhao han, haotian fan, fangyuan kong, wenjie liao, chunle guo, chongyi li, radu timofte, al. ntire challenge text image generation model quality assess- ment. proceedings ieeecvf conference com- puter vision pattern recognition cvpr workshops, yingqing he, tianyu yang, yong zhang, ying shan, qifeng chen. latent video diffusion models high-fidelity video generation arbitrary lengths. arxiv preprint edward hu, yelong shen, phillip wallis, zeyuan allen- zhu, yuanzhi li, shean wang, wang, weizhu chen. lora low-rank adaptation large language models. iclr, yushi hu, benlin liu, jungo kasai, yizhong wang, mari ostendorf, ranjay krishna, noah smith. tifa accu- rate interpretable text-to-image faithfulness evaluation question answering. proceedings ieeecvf international conference computer vision, pages varun jain, zongwei wu, quan zou, louis florentin, hen- rik turbell, sandeep siddhartha, radu timofte, al. ntire challenge video quality enhancement video con- ferencing datasets, methods results. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, yuval kirstain, adam polyak, uriel singer, shahbuland ma- tiana, joe penna, omer levy. pick-a-pic open dataset user preferences text-to-image generation. ad- vances neural information processing systems, sangmin lee, eunpil park, angel canelo, hyunhee park, youngjo kim, hyungju chun, xin jin, chongyi li, chun-le guo, radu timofte, al. ntire challenge efficient burst hdr restoration datasets, methods, results. proceedings ieeecvf conference computer vi- sion pattern recognition cvpr workshops, baiqi li, zhiqiu lin, deepak pathak, jiayao li, yixin fei, kewen wu, tiffany ling, xide xia, pengchuan zhang, gra- ham neubig, al. genai-bench evaluating improv- ing compositional text-to-visual generation. arxiv preprint li, yuanhan zhang, dong guo, renrui zhang, feng li, hao zhang, kaichen zhang, peiyuan zhang, yanwei li, zi- wei liu, al. llava-onevision easy visual task transfer. arxiv preprint chunyi li, zicheng zhang, haoning wu, wei sun, xiongkuo min, xiaohong liu, guangtao zhai, weisi lin. agiqa-k open database ai-generated image quality assessment. ieee transactions circuits sys- tems video technology, chunyi li, tengchuan kou, yixuan gao, yuqin cao, wei sun, zicheng zhang, yingjie zhou, zhichao zhang, haon- ing wu, weixia zhang, xiaohong liu, xiongkuo min, guangtao zhai. aigiqa-k large database ai- generated image quality assessment. proceedings ieeecvf conference computer vision pattern recognition workshops, junnan li, dongxu li, caiming xiong, steven hoi. blip bootstrapping language-image pre-training unified vision-language understanding generation. interna- tional conference machine learning, pages pmlr, junnan li, dongxu li, silvio savarese, steven hoi. blip- bootstrapping language-image pre-training frozen image encoders large language models. in- ternational conference machine learning, pages pmlr, xin li, yeying jin, xin jin, zongwei wu, bingchen li, yufei wang, wenhan yang, li, zhibo chen, bihan wen, robby tan, radu timofte, al. ntire challenge day night raindrop removal dual-focused images methods results. proceedings ieeecvf conference computer vision pattern recognition cvpr work- shops, xin li, xijun wang, bingchen li, kun yuan, yizhen shao, suhang yao, ming sun, chao zhou, radu timofte, zhibo chen. ntire challenge short-form ugc video quality assessment enhancement kwaisr dataset study. proceedings ieeecvf conference com- puter vision pattern recognition cvpr workshops, xin li, kun yuan, bingchen li, fengbin guan, yizhen shao, zihao yu, xijun wang, yiting lu, wei luo, suhang yao, ming sun, chao zhou, zhibo chen, radu timofte, al. ntire challenge short-form ugc video quality assessment enhancement methods results. pro- ceedings ieeecvf conference computer vision pattern recognition cvpr workshops, jie liang, radu timofte, qiaosi yi, zhengqiang zhang, shuaizheng liu, lingchen sun, rongyuan wu, xindong zhang, hui zeng, lei zhang, al. ntire restore image model raim wild challenge. proceedings ieeecvf conference computer vi- sion pattern recognition cvpr workshops, youwei liang, junfeng he, gang li, peizhao li, arseniy klimovskiy, nicholas carolan, jiao sun, jordi pont-tuset, sarah young, feng yang, al. rich human feedback text-to-image generation. proceedings ieeecvf conference computer vision pattern recognition, pages zhiqiu lin, deepak pathak, baiqi li, jiayao li, xide xia, graham neubig, pengchuan zhang, deva ramanan. evaluating text-to-visual generation image-to-text gen- eration. european conference computer vision, pages springer, xiaohong liu, xiongkuo min, qiang hu, xiaoyun zhang, jie guo, al. ntire xgc quality assessment chal- lenge methods results. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, xiaoning liu, zongwei wu, florin-alexandru vasluianu, hailong yan, bin ren, yulun zhang, shuhang gu, zhang, zhu, radu timofte, al. ntire challenge low light image enhancement methods results. proceedings ieeecvf conference computer vi- sion pattern recognition cvpr workshops, liu, yutong lin, yue cao, han hu, yixuan wei, zheng zhang, stephen lin, baining guo. swin transformer hierarchical vision transformer using shifted windows. proceedings ieeecvf international conference computer vision, pages shiyin lu, yang li, qing-guo chen, zhao xu, weihua luo, kaifu zhang, han-jia ye. ovis structural em- bedding alignment multimodal large language model. alec radford, jong wook kim, chris hallacy, aditya ramesh, gabriel goh, sandhini agarwal, girish sastry, amanda askell, pamela mishkin, jack clark, al. learning transferable visual models natural language supervi- sion. international conference machine learning, pages pmlr, bin ren, hang guo, lei sun, zongwei wu, radu timo- fte, yawei li, al. tenth ntire efficient super- resolution challenge report. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, olaf ronneberger, philipp fischer, thomas brox. net convolutional networks biomedical image segmen- tation. medical image computing computer-assisted interventionmiccai international conference, munich, germany, october proceedings, part iii pages springer, nickolay safonov, alexey bryntsev, andrey moskalenko, dmitry kulikov, dmitriy vatolin, radu timofte, al. ntire challenge ugc video enhancement meth- ods results. proceedings ieeecvf conference computer vision pattern recognition cvpr work- shops, lei sun, andrea alfarano, peiqi duan, shaolin su, kaiwei wang, boxin shi, radu timofte, danda pani paudel, luc van gool, al. ntire challenge event-based image deblurring methods results. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, lei sun, hang guo, bin ren, luc van gool, radu timo- fte, yawei li, al. tenth ntire image denoising challenge report. proceedings ieeecvf confer- ence computer vision pattern recognition cvpr workshops, wei sun, xiongkuo min, danyang tu, siwei ma, guangtao zhai. blind quality assessment in-the-wild images via hierarchical feature fusion iterative mixed database training. ieee journal selected topics sig- nal processing, michael tschannen, alexey gritsenko, xiao wang, muham- mad ferjad naeem, ibrahim alabdulmohsin, nikhil parthasarathy, talfan evans, lucas beyer, xia, basil mustafa, al. siglip multilingual vision-language en- coders improved semantic understanding, localization, dense features. arxiv preprint florin-alexandru vasluianu, tim seizinger, zhuyun zhou, cailian chen, zongwei wu, radu timofte, al. ntire image shadow removal challenge report. proceed- ings ieeecvf conference computer vision pattern recognition cvpr workshops, florin-alexandru vasluianu, tim seizinger, zhuyun zhou, zongwei wu, radu timofte, al. ntire ambi- ent lighting normalization challenge. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, juan wang, zewen chen, chunfeng yuan, bing li, wentao ma, weiming hu. hierarchical curriculum learning no-reference image quality assessment. international jour- nal computer vision, pages jiarui wang, huiyu duan, jing liu, shi chen, xiongkuo min, guangtao zhai. aigciqa large-scale image quality assessment database generated images perspectives quality, authenticity correspondence. caai international conference artificial intelligence, pages springer, yingqian wang, zhengyu liang, fengyuan zhang, lvli tian, longguang wang, juncheng li, jungang yang, radu timofte, yulan guo, al. ntire challenge light field image super-resolution methods results. pro- ceedings ieeecvf conference computer vision pattern recognition cvpr workshops, yibin wang, yuhang zang, hao li, cheng jin, jiaqi wang. unified reward model multimodal understanding generation. arxiv preprint zijie wang, evan montoya, david munechika, haoyang yang, benjamin hoover, duen horng chau. diffu- siondb large-scale prompt gallery dataset text-to- image generative models. arxiv preprint olivia wiles, chuhan zhang, isabela albuquerque, ivana kajic, wang, emanuele bugliarello, yasumasa onoe, pinelopi papalampidi, ira ktena, chris knutsen, al. revisiting text-to-image evaluation gecko met- rics, prompts, human ratings. arxiv preprint haoning wu, zicheng zhang, weixia zhang, chaofeng chen, liang liao, chunyi li, yixuan gao, annan wang, erli zhang, wenxiu sun, al. q-align teaching lmms visual scoring via discrete text-defined levels. arxiv preprint haoning wu, zicheng zhang, weixia zhang, chaofeng chen, liang liao, chunyi li, yixuan gao, annan wang, erli zhang, wenxiu sun, al. q-align teaching lmms visual scoring via discrete text-defined levels. arxiv preprint xiaoshi wu, keqiang sun, feng zhu, rui zhao, hong- sheng li. human preference score better aligning text- to-image models human preference. proceedings ieeecvf international conference computer vi- sion, pages zhiyu wu, xiaokang chen, zizheng pan, xingchao liu, wen liu, damai dai, huazuo gao, yiyang ma, chengyue wu, bingxuan wang, al. deepseek-vl mixture-of- experts vision-language models advanced multimodal understanding. arxiv preprint bin xiao, haiping wu, weijian xu, xiyang dai, houdong hu, yumao lu, michael zeng, liu, yuan. florence- advancing unified representation variety vision tasks. proceedings ieeecvf conference computer vision pattern recognition, pages xinyu xiong, zihuang wu, shuangyi tan, wenxue li, fei- long tang, ying chen, siying li, jie ma, guanbin li. sam-unet segment anything makes strong encoder natural medical image segmentation. arxiv preprint jiazheng xu, xiao liu, yuchen wu, yuxuan tong, qinkai li, ming ding, jie tang, yuxiao dong. imagere- ward learning evaluating human preferences text- to-image generation. advances neural information pro- cessing systems, kangmin xu, liang liao, jing xiao, chaofeng chen, haon- ing wu, qiong yan, weisi lin. boosting image quality assessment efficient transformer adaptation lo- cal feature enhancement. proceedings ieeecvf conference computer vision pattern recognition, pages shilin yan, ouxiang li, jiayin cai, yanbin hao, xi- aolong jiang, yao hu, weidi xie. sanity check ai-generated image detection. arxiv preprint kangning yang, jie cai, ling ouyang, florin-alexandru vasluianu, radu timofte, jiaming ding, huiming sun, lan fu, jinlong li, chiu man ho, zibo meng, al. ntire challenge single image reflection removal wild datasets, methods results. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, sidi yang, tianhe wu, shuwei shi, shanshan lao, yuan gong, mingdeng cao, jiahao wang, yujiu yang. maniqa multi-dimension attention network no-reference image quality assessment. proceedings ieeecvf conference computer vision pattern recognition, pages qinghao ye, haiyang xu, jiabo ye, ming yan, anwen hu, haowei liu, qian, zhang, fei huang. mplug- owl revolutionizing multi-modal large language model modality collaboration. proceedings ieeecvf conference computer vision pattern recognition, pages zhiyuan you, xin cai, jinjin gu, tianfan xue, chao dong. teaching large language models regress accurate image quality scores using score distribution. arxiv preprint zhiyuan you, xin cai, jinjin gu, tianfan xue, chao dong. teaching large language models regress accurate image quality scores using score distribution. arxiv preprint xinli yue, jianhui sun, junda lu, liangchao yao, fan xia, tianyi wang, fengyun rao, jing lyu, yuetang deng. instruction-augmented multimodal alignment image-text element matching. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, pierluigi zama ramirez, fabio tosi, luigi stefano, radu timofte, alex costanzino, matteo poggi, samuele salti, ste- fano mattoccia, al. ntire challenge depth images specular transparent surfaces. pro- ceedings ieeecvf conference computer vision pattern recognition cvpr workshops, weixia zhang, kede ma, jia yan, dexiang deng, zhou wang. blind image quality assessment using deep bilinear convolutional neural network. ieee transactions cir- cuits systems video technology, weixia zhang, guangtao zhai, ying wei, xiaokang yang, kede ma. blind image quality assessment via vision- language correspondence multitask learning perspective. ieee conference computer vision pattern recog- nition, pages zicheng zhang, chunyi li, wei sun, xiaohong liu, xiongkuo min, guangtao zhai. perceptual quality assessment exploration aigc images. ieee in- ternational conference multimedia expo workshops icmew, pages ieee, zijian zhang, xuhui zheng, xuecheng wu, chong peng, xuezhi cao. tokenfocus-vqa enhancing text-to-image alignment position-aware focus multi-perspective aggregations lvlms, yuqian zhou, hanchao yu, humphrey shi. study group learning improving retinal vessel segmentation trained noisy labels. medical image computing computer assisted interventionmiccai international conference, strasbourg, france, september october proceedings, part pages springer, zhuofan zong, guanglu song, liu. detrs col- laborative hybrid assignments training. proceedings ieeecvf international conference computer vision, pages", "published_date": "2025-05-22T07:12:36+00:00"}
{"id": "2505.04965v1", "title": "DenseGrounding: Improving Dense Language-Vision Semantics for Ego-Centric 3D Visual Grounding", "authors": ["Henry Zheng", "Hao Shi", "Qihang Peng", "Yong Xien Chng", "Rui Huang", "Yepeng Weng", "Zhongchao Shi", "Gao Huang"], "summary": "enabling intelligent agents comprehend interact environments natural language crucial advancing robotics human-computer interaction. fundamental task field ego-centric visual grounding, agents locate target objects real-world spaces based verbal descriptions. however, task faces two significant challenges loss fine-grained visual semantics due sparse fusion point clouds ego-centric multi-view images, limited textual semantic context due arbitrary language descriptions. propose densegrounding, novel approach designed address issues enhancing visual textual semantics. visual features, introduce hierarchical scene semantic enhancer, retains dense semantics capturing fine-grained global scene features facilitating cross-modal alignment. text descriptions, propose language semantic enhancer leverages large language models provide rich context diverse language descriptions additional context model training. extensive experiments show densegrounding significantly outperforms existing methods overall accuracy, improvements trained comprehensive full dataset smaller mini subset, respectively, advancing sota egocentric visual grounding. method also achieves place receives innovation award cvpr autonomous grand challenge multi-view visual grounding track, validating effectiveness robustness.", "full_text": "cs.cv may published conference paper iclr densegrounding improving dense language- vision semantics ego-centric visual grounding henry zheng, hao shi, qihang peng, yong xien chng, rui huang, yepeng weng, zhongchao shi, gao huangb department automation, bnrist, tsinghua university lab, lenovo research jh-zheng,shi-h,pqh,chngyx,hrmails.tsinghua.edu.cn wengyp,shizclenovo.com gaohuangtsinghua.edu.cn abstract enabling intelligent agents comprehend interact environments natural language crucial advancing robotics human-computer interaction. fundamental task field ego-centric visual grounding, agents locate target objects real-world spaces based verbal descrip- tions. however, task faces two significant challenges loss fine-grained visual semantics due sparse fusion point clouds ego-centric multi-view images, limited textual semantic context due arbitrary language descriptions. propose densegrounding, novel approach designed address issues enhancing visual textual semantics. visual features, introduce hierarchical scene semantic enhancer, retains dense semantics cap- turing fine-grained global scene features facilitating cross-modal alignment. text descriptions, propose language semantic enhancer leverages large language models provide rich context diverse language descriptions additional context model training. extensive experiments show densegrounding significantly outperforms existing methods overall accuracy, improvements trained comprehensive full dataset smaller mini subset, respectively, advancing sota ego- centric visual grounding. method also achieves place receives innovation award cvpr autonomous grand challenge multi-view visual grounding track, validating effectiveness robustness. code introduction recent years seen increasing attention field embodied ai, introduction visual grounding benchmarks achlioptas al., chen al., wang al., prompting surge research works guo al., al., zhao al., jain al., yang al., huang al., wang al., task visual grounding, aims locate target objects real-world environments based natural language descriptions, fundamental perception task embodied agents. capability crucial enabling agents interact understand surroundings language, facilitating applications robotics human-computer interaction. instance, accurate visual grounding empowers robots perform tasks object retrieval manipulation based verbal instructions, enhancing functionality service assistive scenarios. despite advances, significant challenges continue hinder performance perception systems. one major challenge lies embodied agents perceive environment, typ- ically rely ego-centric observations multiple views moving around, lacking holistic, scene-level perception. methods attempt enhance scene-level understanding using recon- structed scene-level point clouds al., zhao al., jain al., yang equal contributions. bcorresponding author. published conference paper iclr intended target additional anchors distractor box printer oven box anchors view view view view sparse fusion description box near printer augmented description select box proximity printer, situated near oven limited points small objects sampled point cloud insufficient language semantics due limited context. sparse geometrics dense semantics semantics loss due sparse fusion. figure illustrates limited context due arbitrary descriptions leads insufficient lan- guage semantics. highlights issue losing fine-grained semantics sparse fusion. al., guo al., huang al., following previous perception methods al., jiang al., approach impractical real-world appli- cations comprehensive scene-level information readily available. context, embodiedscan wang al., emerged, utilizing multi-view ego-centric rgb-d scans address need models understand scenes directly sparse views. decouples encoding rgb images depth-reconstructed point clouds ego-centric views extract semantic geometric information. semantic features projected onto space using intrinsic extrinsic matrices create semantically enriched sparse point cloud bounding box regression. however, due high number points reconstructed point cloud computational limitations, sparse subset around sampled. sampling results significant loss visual semantics, limiting models ability retain fine-grained object details, especially smaller objects, ultimately hindering grounding performance. another challenge ambiguity natural language descriptions found existing datasets chen al., achlioptas al., wang al., humans naturally provide concise instruc- tions, often lacking critical reference points anchor objects, leads inherent ambiguities. example, command like please get water bottle top table specify table provide additional context environments multiple similar objects. conciseness makes difficult models disambiguate similar objects within complex scenes wang al., furthermore, increased complexity scenes new datasetssuch similar objects instances single sceneexacerbates issue. result, training data may cause model suboptimal performance. previous attempts made enrich descriptions using large language models predefined templates guo al., concatenating related descriptions wang al., methods fully mitigated ambiguities present annotations utilize available informations dataset. addressing challenges crucial advancing field embodied perception en- hancing practical utility visual grounding real-world applications. response challenges, propose densegrounding, novel method multi-view visual grounding alleviates sparsity visual textual features. specifically, address loss fine- grained visual semantics, introduce hierarchical scene semantic enhancer hsse, enriches visual representations global scene-level semantics. hsse operates hierarchically, starting view-level semantic aggregation integrates multi-scale features rgb images within view capture detailed view-level semantics. then, hsse performs scene-level se- mantic interaction combine aggregated multi-view semantics language features descriptions, promoting cross-modal fusion holistic understanding scene. finally, hsse conducts semantic broadcast infuses enriched global scene-level semantics multi- scale feature maps depth-reconstructed point cloud, improving fine-grained semantics. published conference paper iclr mitigate ambiguity natural language descriptions used training, text modality, propose language semantic enhancer lse constructs scene information database based existing dataset, embodiedscan. database captures crucial information object relationships locations within scene. training, lse leverages llm enriches input descriptions additional anchors context, reducing confusion among similar objects providing robust representations incorporating external knowledge. enrichment increases semantic richness language descriptions reduces inherent ambiguities. addressing ambiguities annotated text enhancing models capacity capture scene- level global visual semantics ego-centric multi-view inputs, densegrounding, advances state-of-the-art visual grounding. extensive experiments demonstrate effectiveness approach, highlighting importance enriched textual descriptions bidirectional interaction text visual modalities overcoming limitations previous methods. main contributions follows develop novel language semantic enhancer lse employs llm-assisted de- scription augmentation increase semantic richness significantly reduce ambiguities natural language descriptions. leveraging llm grounded scene information database, approach enriches diversity contextual clarity textual features. introduce hierarchical scene semantic enhancer hsse effectively aggregates ego-centric multi-view features captures fine-grained scene-level global semantics. hsse enables interaction textual visual features, ensuring model captures global context detailed object semantics ego-centric inputs. method achieves state-of-the-art performance embodiedscan benchmark, sub- stantially outperforming existing approaches. secured first place cvpr autonomous driving grand challenge track multi-view visual grounding zheng al., demonstrating practical effectiveness impact contributions. related works visual grounding. integration language perception scene understanding crucial embodied ai, enabling agents navigate, understand, interact complex real- world environments. key tasks chen al., achlioptas al., azuma al., chen al., domain, visual grounding dvg al., jain al., zhu al., huang al., cai al., captioning huang al., chen al., cai al., question answering dqa huang al., combine natural language understanding spatial reasoning advance multimodal intelligence spaces. work focuses ego-centric visual grounding tasks integrate multimodal data localize objects space. major visual grounding methods divided one-stage two-stage architectures. one- stage approaches chen al., liao al., luo al., geng yin, ding, fuse textual visual features single step, enabling end-to-end optimization faster inference. recent works optimized single stage methods various ways sps luo al., progressively selects keypoints guided language viewinferd geng yin, uses large language models llms infer embodied viewpoints refmaskd ding, integrates language geometrically coherent sub-clouds via cross-modal group- word attention, producing semantic primitives enhance vision-language understanding. two-stage approaches yang al., achlioptas al., huang al., guo al., al., chang al., first use pre-trained object detectors jiang al., al., generate object proposals utilize ground truth bounding boxes, matched linguistic input. advances framework include multi-view trans- former huang al., projecting scene holistic multi-view space viewre- fer guo al., leveraging llms expand grounding texts enhancing cross-view in- teractions inter-view attention mikasa transformer chang al., introducing scene-aware object encoder multi-key-anchor technique improve object recognition spatial understanding. paper, target challenging setting single-stage methods. published conference paper iclr llms data augmentation. recent advances large language models llms achiam al., touvron al., al., demonstrated remarkable capabilities. fully lever- aging power llms beyond language processing, recent studies successfully in- tegrated modalities, leading development highly effective multi-modal methods moon al., guo al., feng al., vision-language tasks. however, directly utilizing llm vlms robotics perception tasks remains research ques- tion kim al., zhen al., phan al., therefore, leveraging llms directly implement data augmentation maximizing potential enhance text data diversity become new trend peng al., khan al., dunlap al., guo al., ding al., among them, alia dunlap al., leverages llms generate image descriptions guide language-based image modifications, augmenting training data. approach surpasses traditional data augmentation techniques fine-grained classification, underscoring value llm-driven augmentation visual tasks. visual grounding, although viewrefer guo al., employs llms augment textual data rephrasing view-dependent descriptions varying perspectives, however data samples augmented individually suffer limited textual semantic information. obtain high-quality description augmentations disambiguate among similar items robust representations, utilize llms enrich input descriptions utilizing readily available information dataset additional context. preliminaries ego-centric visual grounding real-world scenarios, intelligent agents perceive environment without prior scene knowl- edge. often rely ego-centric observations, multi-view rgb-d images, rather pre-established scene-level priors like pre-reconstructed point clouds entire scene, com- monly used previous studies al., huang al., chng al., following wang al. formalize ego-centric visual grounding task follows given language description together views rgb-d images iv, dvv rhw represents rgb image rhw denotes depth im- age v-th view along corresponding sensor intrinsics ex- trinsics objective output -degree-of-freedom dof bounding box here, coordinates objects center, dimensions, orientation angles. task determine accurately corresponds object described within scene represented iv, dvv overview architecture adopt improve upon previous sota ego-centric visual grounding wang al., prevent geometric information interfering semantic extraction fully lever- age modalitys strengths, wang al. decouple encoding input rgb depth signals ego-centric views, following liu al. specifically, depth data view transformed partial point cloud, integrated holistic point cloud using global alignment matrices, thereby preserving precise geometric details. next, semantic encoder geometric encoder extract multi-scale semantic geomet- ric features ivv respectively, denoted sems hswscs geos rncs. here, total number points sampled perspective views, number scale encoded feature maps. semantic features rgb data lifted space using intrinsic extrinsic matrices concatenated geometric features multiple scales, followed fusion feature pyramid network fpn. however, due large number points depth-reconstructed point cloud computational limitations, sparse subset sampled, semantic features unsampled locations discarded, resulting significant semantic information loss. address issue, introduce hierarchical scene semantic en- hancer module detailed sec. improve scene-level semantics. enhanced features published conference paper iclr geometric encoder semantic encoder text encoder multi-scale fusion self attention vision-guided cross attention multi-modal decoder text-guided cross attention ffn bbox head hierarchical scene semantic enhancer language semantic enhancer lse view-level semantic aggregate ...... scene-level semantic interaction pool pool ...... view hierarchical scene semantic enhancer hsse view query selection facing front guitar, choose curtain right back guitar, select curtain left it, near bed, picture clock, far away bicycle text feature sematic broadcast view view sematic broadcast view view view-level semantic aggregate find chair beside table find chair beside table, nearest television, furthest windows language semantic enhancer view ...... view view ...... view overview densegrounding detection model bbox labels object location branch object relation branch language desc select scene info database input text augmented text predicted bbox scene-level semantic interaction scene-level semantic interaction figure shows overall framework, details language semantic enhancer lse module, describes hierarchical scene semantic enhancer hsse module. fed decoder, remains unchanged previous work, visual tokens similar text features selected queries. method section, present proposed method, densegrounding, ego-centric visual ground- ing. shown figure method consists three key components hierarchical scene se- mantic enhancer sec. llm-based language semantic enhancer sec. enhanced baseline model sec. describe component detail following subsections. hierarchical scene semantic enhancer mitigate semantic loss inherent sparse point clouds, introduce hierarchical scene semantic enhancer hsse module, extracts individual view semantics enriches visual representation global scene-level semantics. enriched information unprojected depth reconstructed point cloud fusion, minimizing semantic loss. proposed hsse module works hierarchical manner, starting view-level semantic ag- gregation capture view-level semantics individual egocentric perspective views. then, fuses aggregated view semantics language semantics, facilitating scene-level multi-view semantic interaction cross-modal feature fusion. moreover, broadcast module integrates scene-level semantics multi-scale semantic feature maps, enriching fine-grained semantics sparse point cloud. finally, hsse employs aggregation-broadcast mechanism filter irrelevant information, ensuring efficient streamlined semantic transfer point cloud enhanced performance. published conference paper iclr view-level semantic aggregation. encoded features view, hsse performs view-level semantic aggregation capture view-level global semantics within view. v-th view, given multi-scale semantic features v,s sems rhswscs extracted rgb image, aim capture salient semantic cues view. first, semantic features processed feature pyramid network fpn integrate information across different scales. specific scale feature map selected reference feature ref rhswscs. ref conv upsamplef v,i sem rhswscs next, adaptive average pooling applied reference feature reduce spatial dimensions, yielding initial query features rhswscs. poolf ref rhswscs subsequently, compute cross-attention pooled queries reference feature aggregate semantic information view level. crossattnq ref, ref rhswscs encapsulates global semantics view. apply self attention layer, refine features model intra-view relationships. selfattnq scene-level semantic interaction. moreover, hsse conducts scene-level semantic interaction integrating aggregated multi-view semantics language semantics flang derived language description. promote cross-modal fusion scene-level interaction among multi-view semantics, concatenate flang apply lscene layers self-attention. lang selfattnlscene flang, captures scene-level global semantics, lang represents refined language semantics interaction. semantic broadcast. finally, hsse broadcasts scene-level global semantics back multi-scale semantic feature maps s,v sems rhswscs view. specifically, com- pute cross-attention spatial features s,v sems scene-level semantics treat- ing spatial features queries. s,v sems crossattnq s,v sems s,v sems represents enhanced feature maps enriched scene-level global semantics. llm-based language semantic enhancer human interactions intelligent agents often involve casual vague language descriptions, resulting input lacks clear anchors, limited textual context, ambiguities. address this, propose language semantic enhancement lse pipeline based large language models llms enhance training data. pipeline enriches input language descriptions lever- aging llms, grounded scene information database sidb containing object locations relationships, providing contextual details corresponding scene. construct scene information database sidb. visual grounding datasets typically built indoor detection datasets, collected rgb-d images point clouds used represent scene. object within scene annotated bounding boxes, visual grounding datasets extend adding language descriptions detailing objects relationship surround- ings. ensure accurate contextualization llms augmenting language descriptions, published conference paper iclr propose construct scene information database sidb based annotated training set also used baseline methods model training. scene, follow design datasets assume every language description targets specific bounding box. descriptions grouped according corresponding scenes, allowing sidb store detailed object relationships within scene. approach enables llms better understand spatial relational dynamics objects given scene, leading holistic comprehension environment. moreover, provide location information object instances scene, enrich descriptions sidb position information original dataset annotations pseudo- labels detection model, associating described object spatial location. descriptions, combining spatial semantic information, form backbone sidb. providing structured detailed knowledge, sidb allows llms generate contextually informed reliable text descriptions grounded specific dynamics scene. prompt llm-based enhancement. effectively enrich input descriptions, design prompt llm leverages sidb generate detailed contextually rich de- scriptions. instructing llm utilize positional relationships among objects scene, prompt ensures sufficient reliable anchors, encouraging description target object using multiple reference objects minimize ambiguity distractions. instance, draws spatial context database accurately reflect objects relationships surrounding items. moreover, prompt guides llm maintain consistency original augmented text preserving original content adding additional information, reduces risk generated text deviating initial prompt. enhance description llm sidb. description augmented raw descrip- tion, select related context descriptions corresponding scene based object names sidb provide llm. details context description selection found appendix a... llm leverages database incorporate spatial semantic details, aiming enrich text additional contextual anchors help prevent confusion similar-looking objects. enhanced baseline work, introduce strong baseline building upon state-of-the-art method estab- lished embodiedscan ego-centric multiview visual grounding boosting performance. details proposed improvements found appendix a... experiments dataset benchmark. embodiedscan dataset wang al., used experiments, large-scale, multi-modal, ego-centric dataset comprehensive scene understanding. consists scene scans sourced well-known datasets scannet dai al., rscan wald al., matterportd chang al., extensive dataset pro- vides diverse rich foundation visual grounding tasks, offering broader scene coverage compared prior datasets. makes dataset significantly larger challenging previous ones, providing rigorous benchmark visual grounding tasks. benchmarking, official dataset maintains non-public test set test leaderboard divides original training set new subsets training validation. paper, refer training validation sets, non-public test set called testing set. experimental settings. due resource limitations, reserve full training dataset baseline comparisons test set leaderboard submissions ensure fair comprehensive evalua- tion. mini data data column table analysis experiments sec. use smaller subset data proxy task performing experiments. subset referred mini sets, available official release wang al. published conference paper iclr table validation result accuracy performance models official full validation set. follow experimental setting proposed wang al. use rgb-d visual input. data column indicates data split used. denotes improved baseline discussed sec. results reported embodiedscan obtained re-evaluating validation set using officially provided weights. method data easy hard indep dep overall acc acc acc acc acc scanrefer chen al., full butd-detr jain al., full ldet zhu al., full embodiedscan wang al., full densegrounding full embodiedscan wang al., mini embodiedscan mini densegrounding mini report accuracy using official metric, considering instances iou exceeds addi- tionally, present results easy hard scenes, classifying scene hard contains objects class. dep indep metrics challenge spatial under- standing ability assessing performance without perspective-specific descriptions. implementation details. follow embodiedscan wang al., using feature encoders, sparse fusion modules, detr-based carion al., decoder. specifically, use resnet al., minknet choy al., vision encoders, respectively. moreover, mentioned sec. replace roberta clip radford al., text encoder language feature encoders. first pre-train image point cloud en- coders object detection task, adhering embodiedscans training settings integrating cbgs zhu al., lse-augmented text generated using gpt-o mini utilized exclusively training. inference, model processes descriptions directly, without enhancement, aligning baseline methods fair comparison. details app. a... main results evaluation. evaluate visual grounding performance proposed method, densegrounding, report results table compare established sota methods dataset benchmark. table divided two sections upper half presents models trained full training set, lower half showcases performance using mini training set, approximately full dataset. full training set, densegrounding achieves significant improvement pre- vious strongest baseline, embodiedscan. substantial gain highlights effectiveness approach leveraging extensive training data capture complex visual-linguistic correlations environments. demonstrates methods superior ability understand ground linguistic expressions rich visual scenes. trained manageable mini-training set, intro- duce enhanced baseline, denoted embodiedscan, achieves performance gain original embodiedscan. stronger baseline ensures rigorous fair compar- ison method. remarkably, even enhanced baseline, densegrounding attains substantial improvement overall accuracy, culminating total performance gain previous state-of-the-art. specifically, hard samples, scenes contain three objects class, observe increase accuracy. demonstrates methods proficiency accurately identifying correct target among similar-looking objects. published conference paper iclr table ablation lse. refers object relationship object location infor- mation sidb, respectively. method easy hard overall concat samples llm llmdbr llmdbrl table ablation proposed methods. reported values accuracies predic- tions greater iou groundtruth. lse hsse easy hard overall detailed table densegrounding consistently outperforms baselines across various evaluation categories, including easy hard samples, well view-independent view-dependent tasks. consistent gains across different metrics underscore robustness generalizability approach visual grounding tasks. moreover, model achieves highest performance among methods also secured place cvpr autonomous driving grand challenge track multi-view visual grounding. analysis experiments effectiveness component. conduct ablation analysis assess effectiveness component, shown tab. baseline enhanced model excludes text augmentations semantic enhancements. introducing augmented data led remarkable accuracy improvement highlighting significant impact. integration hsse module provided additional performance boost furthermore, combined use lse hsse resulted accuracy improvement hard samples compared baseline, underscoring models enhanced capacity visual language understanding, particularly disambiguation challenging cases. ablation lse. evaluate effectiveness proposed lse, conducted experiments comparing several text augmentation methods. concat samples method embodied- scan disambiguate descriptions concatenating multiple annotations. llm refers re- implementation template-based llm augmentation used viewrefer guo al., method, llmdbr, employs llm sidb incorporating object relationships only, llmdbrl extends approach adding location information bounding boxes. worth noting since concat samples method data, limit meth- ods use data fair comparisons. results demonstrate llmdbrl achieves notable improvement naive baseline, confirming effectiveness incorporating object relationships location data augmentation process. ablation hsse components. explore design hsse module comprehensive experiments, presented tab. tab. conduct ablation study determine optimal number self-attention layers needed effective learning scene- feature representation. additionally, tab. examines impact varying feature map sizes pooled feature. results indicate small feature map size may insufficient capture necessary information, excessively large feature map may include redundant details. based findings, adopted best configuration final model. qualitative analysis present qualitative results illustrate effectiveness densegrounding improving ego- centric visual grounding performance. figure shows comparison model baseline model, embodiedscan. clearly seen method outperforms baseline correctly identifying target objects based ambiguous descriptions. cases baseline model struggles disambiguate multiple similar objects, densegrounding successfully de- tects correct target leveraging enriched textual descriptions robust cross-modal interac- tions. instance, environments descriptions select keyboard close published conference paper iclr table ablation number self at- tention layers hsse. lscene easy hard overall table ablation view feature map size pooling hsse. pooled size easy hard overall choose chair far copier select keyboard close fan select lamp front door text input ground truth baseline figure qualitative analysis. comparison ground truth, baseline, densegrounding. ground truth boxes shown green, baseline red, densegroundings predictions blue. fan provided, model accurately localizes keyboard, baseline misidentifies nearby objects. improvement attributed hsse, captures fine-grained details provides better alignment text, enabling better object identification. qualitative re- sults demonstrate enhanced performance densegrounding, especially complex scenes multiple distractors, solidifying robustness precision real-world applications. limitations densegrounding significantly improves ego-centric visual grounding task perfor- mance, limitations. real-life applications, vague ambiguous descriptions human instructions pose challenges, model struggles without necessary information resolve ambiguities. lack clear instructions limits effectiveness embodied agent. future research explore integrating human-agent interaction, allowing model query users clarification, improving adaptability robustness real-world scenarios. conclusion work, propose densegrounding, novel approach address challenges ambiguity natural language descriptions loss fine-grained semantic features multi-view visual grounding. leveraging llms description enhancement introducing hsse enhance fine-grained visual semantics, method significantly improves accuracy robustness visual grounding ego-centric environments. extensive experiments embodiedscan bench- mark demonstrate densegrounding outperforms existing state-of-the-art models, securing first place cvpr autonomous grand challenge. results highlight importance enriched language descriptions effective cross-modal feature interactions advancing embod- ied perception enabling real-world applications robotics human-computer interaction. published conference paper iclr acknowledgments work supported part national key program china grant yfb national natural science foundation china grant ub. references josh achiam, steven adler, sandhini agarwal, lama ahmad, ilge akkaya, florencia leoni ale- man, diogo almeida, janko altenschmidt, sam altman, shyamal anadkat, al. gpt- technical report. arxiv preprint panos achlioptas, ahmed abdelreheem, fei xia, mohamed elhoseiny, leonidas guibas. referitd neural listeners fine-grained object identification real-world scenes. eccv, daichi azuma, taiki miyanishi, shuhei kurita, motoaki kawanabe. scanqa question an- swering spatial scene understanding. cvpr, daigang cai, lichen zhao, jing zhang, sheng, dong xu. djcg unified framework joint dense captioning visual grounding point clouds. cvpr, nicolas carion, francisco massa, gabriel synnaeve, nicolas usunier, alexander kirillov, sergey zagoruyko. end-to-end object detection transformers. eccv. springer, angel chang, angela dai, thomas funkhouser, maciej halber, matthias niessner, manolis savva, shuran song, andy zeng, yinda zhang. matterportd learning rgb-d data indoor environments. arxiv preprint chun-peng chang, shaoxiang wang, alain pagani, didier stricker. mikasa multi-key-anchor scene-aware transformer visual grounding. cvpr, dave zhenyu chen, angel chang, matthias niener. scanrefer object localization rgb-d scans using natural language. eccv. springer, sijin chen, hongyuan zhu, xin chen, yinjie lei, gang yu, tao chen. end-to-end dense captioning votecap-detr. cvpr, xinpeng chen, lin ma, jingyuan chen, zequn jie, wei liu, jiebo luo. real-time referring expression comprehension single-stage grounding network. arxiv preprint zhenyu chen, ali gholami, matthias niener, angel chang. scancap context-aware dense captioning rgb-d scans. cvpr, yong xien chng, xuchong qiu, yizeng han, yifan pu, jiewei cao, gao huang. explor- ing contextual modeling linear complexity point cloud segmentation. arxiv preprint christopher choy, junyoung gwak, silvio savarese. spatio-temporal convnets minkowski convolutional neural networks. cvpr, angela dai, angel chang, manolis savva, maciej halber, thomas funkhouser, matthias niener. scannet richly-annotated reconstructions indoor scenes. cvpr, bosheng ding, chengwei qin, ruochen zhao, tianze luo, xinze li, guizhen chen, wenhan xia, junjie hu, anh tuan luu, shafiq joty. data augmentation using llms data perspectives, learning paradigms challenges. arxiv preprint zhengxiao du, yujie qian, xiao liu, ming ding, jiezhong qiu, zhilin yang, jie tang. glm general language model pretraining autoregressive blank infilling. arxiv preprint lisa dunlap, alyssa umino, han zhang, jiezhi yang, joseph gonzalez, trevor darrell. di- versify vision datasets automatic diffusion-based augmentation. neurips, published conference paper iclr chun feng, joy hsu, weiyu liu, jiajun wu. naturally supervised visual grounding language-regularized concept learners. cvpr, liang geng jianqin yin. viewinferd visual grounding based embodied viewpoint inference. ieee ra-l, zoey guo, yiwen tang, ray zhang, dong wang, zhigang wang, bin zhao, xuelong li. viewrefer grasp multi-view knowledge visual grounding. iccv, kaiming he, xiangyu zhang, shaoqing ren, jian sun. deep residual learning image recog- nition. cvpr, shuting henghui ding. refmaskd language-guided transformer referring segmen- tation. arxiv preprint haifeng huang, yilun chen, zehan wang, rongjie huang, runsen xu, tai wang, luping liu, xize cheng, yang zhao, jiangmiao pang, al. chat-scene bridging scene large language models object identifiers. neurips, rui huang, songyou peng, ayca takmaz, federico tombari, marc pollefeys, shiji song, gao huang, francis engelmann. segmentd learning fine-grained class-agnostic segmen- tation without manual labels. eccv, rui huang, henry zheng, yan wang, zhuofan xia, marco pavone, gao huang. training open-vocabulary monocular detection model without data. neurips, shijia huang, yilun chen, jiaya jia, liwei wang. multi-view transformer visual ground- ing. cvpr, ayush jain, nikolaos gkanatsios, ishita mediratta, katerina fragkiadaki. bottom top detection transformers language grounding images point clouds. eccv. springer, jiang, hengshuang zhao, shaoshuai shi, shu liu, chi-wing fu, jiaya jia. pointgroup dual-set point grouping instance segmentation. cvpr, zaid khan, vijay kumar bg, samuel schulter, xiang yu, yun fu, manmohan chandraker. specialize large vision-language models data-scarce vqa tasks? self-train unlabeled images! cvpr, moo jin kim, karl pertsch, siddharth karamcheti, ted xiao, ashwin balakrishna, suraj nair, rafael rafailov, ethan foster, grace lam, pannag sanketi, al. openvla open-source vision-language-action model. arxiv preprint yue liao, liu, guanbin li, fei wang, yanjie chen, chen qian, li. real-time cross- modality correlation filtering method referring expression comprehension. cvpr, yinhan liu, myle ott, naman goyal, jingfei du, mandar joshi, danqi chen, omer levy, mike lewis, luke zettlemoyer, veselin stoyanov. roberta robustly optimized bert pretraining approach, zhijian liu, haotian tang, alexander amini, xinyu yang, huizi mao, daniela rus, song han. bevfusion multi-task multi-sensor fusion unified birds-eye view representation. icra. ieee, junyu luo, jiahui fu, xianghao kong, chen gao, haibing ren, hao shen, huaxia xia, liu. d-sps single-stage visual grounding via referred point progressive selection. cvpr, jong hak moon, hyungyung lee, woncheol shin, young-hak kim, edward choi. multi-modal understanding generation medical images text via vision-language pre-training. ieee journal biomedical health informatics, baolin peng, chunyuan li, pengcheng he, michel galley, jianfeng gao. instruction tuning gpt-. arxiv preprint published conference paper iclr thinh phan, khoa vo, duy le, gianfranco doretto, donald adjeroh, ngan le. zeetad adapt- ing pretrained vision-language model zero-shot end-to-end temporal action detection. wacv, alec radford, jong wook kim, chris hallacy, aditya ramesh, gabriel goh, sandhini agarwal, girish sastry, amanda askell, pamela mishkin, jack clark, al. learning transferable visual models natural language supervision. icml, hugo touvron, thibaut lavril, gautier izacard, xavier martinet, marie-anne lachaux, timothee lacroix, baptiste roziere, naman goyal, eric hambro, faisal azhar, al. llama open efficient foundation language models. arxiv preprint johanna wald, armen avetisyan, nassir navab, federico tombari, matthias niener. rio object instance re-localization changing indoor environments. iccv, tai wang, xiaohan mao, chenming zhu, runsen xu, ruiyuan lyu, peisen li, xiao chen, wenwei zhang, kai chen, tianfan xue, xihui liu, cewu lu, dahua lin, jiangmiao pang. embod- iedscan holistic multi-modal perception suite towards embodied ai. cvpr, tung-yu wu, sheng-yu huang, yu-chiang frank wang. dora visual grounding order- aware referring. arxiv preprint xiaoyang wu, jiang, peng-shuai wang, zhijian liu, xihui liu, qiao, wanli ouyang, tong he, hengshuang zhao. point transformer simpler faster stronger. cvpr, yanmin wu, xinhua cheng, renrui zhang, zesen cheng, jian zhang. eda explicit text- decoupling dense alignment visual grounding. cvpr, yang, ziqi zhang, zhongang qi, yan xu, wei liu, ying shan, bing li, weiping yang, peng li, yan wang, al. exploiting contextual objects relations visual grounding. neurips, sibei yang, guanbin li, yizhou yu. dynamic graph attention referring expression compre- hension. iccv, lichen zhao, daigang cai, sheng, dong xu. dvg-transformer relation modeling visual grounding point clouds. iccv, haoyu zhen, xiaowen qiu, peihao chen, jincheng yang, xin yan, yilun du, yining hong, chuang gan. d-vla vision-language-action generative world model. arxiv preprint henry zheng, hao shi, yong xien chng, rui huang, zanlin ni, tan tianyi, peng qihang, weng yepeng, zhongchao shi, gao huang. denseg alleviating vision-language feature sparsity multi-view visual grounding. cvprw, benjin zhu, zhengkai jiang, xiangxin zhou, zeming li, gang yu. class-balanced grouping sampling point cloud object detection. arxiv preprint chenming zhu, wenwei zhang, tai wang, xihui liu, kai chen. objectscene putting objects context open-vocabulary detection, ziyu zhu, xiaojian ma, yixin chen, zhidong deng, siyuan huang, qing li. d-vista pre- trained transformer vision text alignment. cvpr, published conference paper iclr appendix contents implementation details a.. enhanced baseline a.. hyperparameters performance individual classes analysis limited data scenario robustness analysis implementation details lse a.. prompt lse module a.. details sidb implementation details a.. enhanced baseline first, replace roberta liu al., language encoder clip radford al., encoder. given visual grounding requires deep understanding linguistic in- structions environmental visual features, strong cross-modal alignment critical success. clip, specifically designed tasks, provides superior alignment language vi- sion, making natural fit application. moreover, visual grounding often involves identify- ing diverse range objects, original embodiedscan model relies detection pipeline pretrain visual feature encoders. however, approach hampered long-tailed distribution objects pretraining dataset, leading suboptimal detection performance. ad- dress this, incorporate class-balanced grouping sampling cbgs method zhu al., pretraining, proven effective mitigating data imbalance enhancing detection accuracy across rare common object categories. a.. hyperparameters multi-view visual grounding model, densegrounding, trained adamw optimizer using learning rate e-, weight decay e-, batch size model trained epochs, learning rate reduced epochs settings align embodiedscan. performance individual classes show performance method individual classes validation set fig. figure performance method class validation set embodiedscan published conference paper iclr analysis limited data scenario figure present comparative analysis performance method baseline varying amounts training data. results clearly demonstrate method consistently outperforms baseline, especially scenarios limited data availability. notably, trained training data, method surpasses baseline model trained data. highlights data efficiency robustness approach, indicating effectiveness even training data scarce. figure comparison densegrounding embodiedscan limited data scenario. robustness analysis table zero-shot cross dataset performance method training scenes testing scenes easy hard overall embodiedscan rscanmatterport scannet densegrounding rscanmatterport scannet cross-dataset evaluation essential testing generalization capabilities robustness egocentric visual grounding methods. process particularly challenging due differences camera settings, scene layouts, object characteristics visual data across various datasets, significantly impact model performance. assess zero-shot generalization approach, reorganized embodiedscan dataset create zero-shot setting. specifically, trained model exclusively scenes rscan matterportd evaluated performance scannet scenes, entirely unseen training. setup tests models ability adapt new environments different camera settings scene characteristics. shown table method significantly outperforms baseline zero-shot setting, demonstrating superior robustness ability generalize effectively new diverse scenes despite inherent cross-dataset challenges. implementation details lse section, provide implementation details language semantic enhancer lse module, focusing llm prompted. delve specific prompts used explain information selected provide context llm. a.. prompt lse module section, present prompts used language semantic enhancer lse module, along example input output, illustrated fig. published conference paper iclr system spatial reasoning assistant specialized rephrasing expressions related indoor scenes. task rephrase given expression, enhancing accuracy spatial relationships based provided context. expressions visual grounding descriptions describe scene one specific expression needs rephrasing. text location info provides position objects centers unit meters lidar coordinate system. enhance specific expressions accuracy diversity using anchor points. note input several visual grounding descriptions provide reliable positional information. description includes coordinates objects, coordinates given units meters. visual grounding description needs rephrasing. output rephrased visual grounding description. ensure target object remains unchanged. use positional relationships objects scene describe it, ensuring higher accuracy without potential ambiguity. describe spatial relationships high confidence avoid adding extra information assumptions. understand spatial relationships objects based coordinates supplement spatial information. provide location info rephrased expression. output rephrased sentence directly, prohibit outputting statements. example maybe scene rephrase select pillow near decoration. location info pillow located decoration located rephrase choose pillow near decoration, also situated beneath mirror positioned fireplace decoration example maybe scene .... user visual grounding descriptions choose clock front towel. location info clock located -., towel located select clock farthest pitcher. location info clock located -., pitcher located -., clock closer chandelier. location info clock located -., chandelier located ........ visual grounding description needs rephrasing clock closer refrigerator. location info clock located -., refrigerator located -., now, rephrase according requirements. assistant choose clock closer refrigerator, positioned beneath chandelier front towel figure sample input output llm language enhancement published conference paper iclr a.. details sidb context description selection. given text description augmented i.e., raw description sidb containing text descriptions i.e., context descriptions, select relevant context descriptions used provide llm scene object relationship information. specifically, selecting descriptions context utilized llm augmentation, find descriptions sidb belong scene raw description contain target anchor class raw description text. descriptions qualify, randomly select descriptions among them. conversely, fewer descriptions qualify, randomly select descriptions scene. strategy simplifies process context selection allows diversity context provided llm.", "published_date": "2025-05-08T05:49:06+00:00"}
{"id": "2504.18249v1", "title": "Event-Based Eye Tracking. 2025 Event-based Vision Workshop", "authors": ["Qinyu Chen", "Chang Gao", "Min Liu", "Daniele Perrone", "Yan Ru Pei", "Zuowen Wang", "Zhuo Zou", "Shihang Tan", "Tao Han", "Guorui Lu", "Zhen Xu", "Junyuan Ding", "Ziteng Wang", "Zongwei Wu", "Han Han", "Yuliang Wu", "Jinze Chen", "Wei Zhai", "Yang Cao", "Zheng-jun Zha", "Nuwan Bandara", "Thivya Kandappu", "Archan Misra", "Xiaopeng Lin", "Hongxiang Huang", "Hongwei Ren", "Bojun Cheng", "Hoang M. Truong", "Vinh-Thuan Ly", "Huy G. Tran", "Thuan-Phat Nguyen", "Tram T. Doan"], "summary": "survey serves review event-based eye tracking challenge organized part cvpr event-based vision workshop. challenge focuses task predicting pupil center processing event camera recorded eye movement. review summarize innovative methods teams rank top challenge advance future event-based eye tracking research. method, accuracy, model size, number operations reported. survey, also discuss event-based eye tracking perspective hardware design.", "full_text": "event-based eye tracking. event-based vision workshop qinyu chenb chang gao min liu daniele perrone yan pei zuowen wang zhuo zou shihang tan tao han guorui zhen junyuan ding ziteng wang zongwei han han yuliang jinze chen wei zhai yang cao zheng-jun zha nuwan bandara thivya kandappu archan misra xiaopeng lin hongxiang huang hongwei ren bojun cheng hoang truong, vinh-thuan ly, huy tran, thuan-phat nguyen, tram doan, leiden university delft university technology dvsense prophesee nvidia institute neuroinformatics, uzheth zurich fudan university university wurzburg university science technology china singapore management university hong kong university science technology guangzhou university science, vnu-hcm, chi minh city, vietnam vietnam national university, chi minh city, vietnam abstract survey serves review event-based eye tracking challenge organized part cvpr event-based vision workshop. challenge focuses task predicting pupil center processing event camera recorded eye movement. review summarize innovative methods teams rank top chal- lenge advance future event-based eye tracking research. method, accuracy, model size, number op- erations reported. survey, also discuss event- based eye tracking perspective hardware design. introduction rapid evolution augmented virtual reality arvr technologies, advanced consumer electron- ics industry particularly, role accurate respon- qinyu chen q.chenliacs.leidenuniv.nl corresponding au- thor. challenge website github.io. demonstration code repository comeetchallengeetchallenge. challenge kag- gle website www kaggle com competitions event-based-eye-tracking-cvpr-overview. dataset based-eye-tracking-cvpr-data. event-based vision workshop host website eventvision. sive eye-tracking systems become increasingly impor- tant. instance, apple vision pro features ad- vanced eye-tracking system uses high-speed infrared cameras led illuminators monitor eye movements remarkable precision. technology aims accu- rate gaze estimation, allowing users interact intuitively simply looking objects confirming selections subtle hand gestures. beyond human-computer interaction applications, eye-tracking technology also emerging valuable tool domain healthcare. tasks gaze estimation, pupil shape tracking, eye movement analysis offer powerful, non-invasive methods detecting monitoring neurological disorders, including parkin- sons alzheimers diseases mobile platforms typically constrained strict power computing budgets, making challenging de- ploy complex software algorithms. addition, eye- tracking tasks demand high-frequency sensory sampling, imposes additional burdens hardware data pipelines. mobile arvr applications, eye-tracking system lightweight integrate seamlessly head-mounted devices supporting high temporal res- olution good task accuracy. particularly critical considering human eye move angular veloci- ties exceeding accelerations necessitating sampling rates kilohertz range ac- curately capture onset dynamics fast eye move- ments. however, achieving high frame rate chal- cs.cv apr figure comparison processing flow estimation patterns frame-based event-based systems eye tracking. adapted lenging wearable devices, must operate low power levels, typically milliwatt range. head- mounted devices rely frame-based eye-tracking systems. recent study indicates many systems ex- perience tracking delays ranging ms, insufficient capturing rapid eye movements re- quire kilohertz-level frame rates. furthermore, frame-based sensors capable operating kilohertz tend consume large amount power. resulting high data throughput also demands considerable bandwidth energy trans- mission computation, making real-time deployment low-power wearable platforms difficult. event cameras, also named dynamic vision sensors dvs unique vision sensors offer several potential advantages eye-tracking mobile de- vices. different traditional cameras, event cameras asynchronously detect log intensity changes brightness exceed certain threshold. unique way sensing induces spatiotemporally sparse camera outputs events. many research works proposed exploit spatiotemporal sparsity, aiming reduce hardware platform requirements computation energy. addition, temporal precision eye-tracking tasks could benefit high temporal resolution data prop- erty event cameras. fig. shows commonly used head- mounted devices corresponding frame-based eye- tracking processing flow systems comparison event-based solution utilizing dvs. event-based ap- proach shows promising potential providing ro- bust tracking performance requiring small bandwidth less power consumption. unique characteris- tics make event cameras highly suitable high-speed, low-power eye tracking produce less data re- duce processing needs fixation still capturing fast subtle eye movements saccades. previous event-based eye-tracking studies shown promising re- sults event-based eye tracking challenge set explore algorithmic potentials event-based eye-tracking. emphasizing efficient methods extract eye- position-relevant information sparse event streams, challenge seeks drive advancements event-based eye- tracking fast efficient suitable wear- able healthcare devices real-time arvr applications. event-based eye tracking challenge introduction dataset dataset offers comprehensive bench- mark event-based eye-tracking research. captured using dvxplorer mini event camera, features recordings participants, contributing ses- sions. session, subjects required perform five distinct eye movement tasks random movements, sac- cades, text reading, smooth pursuits, blinks. ground truth annotations provided hz, including binary label indicating presence absence blink manually labeled pupil center spatial coordinates precise tracking. task description input raw events xi, yi, ti, recording eye movements. xi, spatial coordinate, times- tamp polarity event ei. task predict spatial coordinates xi, pupil center specified timestamps, matching fre- quency ground truth, within input space. metric evaluation metric years challenge differs last year year, primary metric kaggle leaderboard pixel error, defined euclidean distance spatial coordinates predicted label ground truth. contrast, last years leaderboard used p-accuracy main evalua- tion metric. metric, prediction considered correct pixel error within pixels. leader- board used threshold pixels. shift p-accuracy pixel error motivated observa- tion last years models often achieved near-perfect scores close making difficult distin- guish high-performing models limiting potential improvement. data loading training pipeline challenge provided participants convenient data loading training pipeline. data loader designed compatible tonic library allowing users experiment different event-based feature representa- tions. also supports caching generated features either memory disk first training epoch, enabling faster data loading subsequent epochs. training process, participants could easily integrate deep learning models adjust hyperparameters needed. machine learning monitoring library, namely mlflow library also provided challenge pipeline code participants monitor various metrics record hyperparameters, well checkpoints. challenge phases challenge organized three key phases preparation phase feb. finalize challenge dataset, develop code pipeline, set competition website, prepare kaggle platform. competition phase starting feb. kag- gle competition officially launches. teams register, download dataset, begin working solu- tions. submission evaluation phase mar. submission portal closes. private leaderboard scores revealed, top-performing teams invited submit factsheets source code. selected teams also encouraged contribute paper detailing methods associated workshop. related challenge challenge part event-based vision work- shop serves second edition event-based eye tracking challenge, following first iteration pre- sented main improvements years chal- lenge include upgrading label frequency finer temporal resolution, changing eval- uation metric p-accuracy pixel error pre- cise performance differentiation. results demonstrate notable improvement tracking accuracy four participat- ing teams achieved pixel error outperforming best results previous year. challenge results summarize main evaluation results partici- pating teams tab. pixel error described sec. used primary evaluation metric kaggle com- petition ranking. model size reported tab. number operations reported sections team. team rank pixel error param ustceventgroup eyetrackingsmu hkustgz cherrychums table final results top performing teams. details found survey paper. architectures main ideas methods proposed participating teams range novel pre-processing techniques custom model ar- chitecture designs motion-aware postprocessing method. major novelties contributions summarized follows modeling short- long-term temporal depen- dencies effectively capturing short- long-term temporal dynamics important accurate event-based eye tracking. team ustceventgroup modeled short-term motion bi-gru and, following, long-term de- pendencies using self-attention module enhanced bidirectional relative positional attention. team hkustgz adopted cnn capturing implicit short-term temporal dynamics eye movements, long- term dependencies handled cascade gru mamba modules. data augmentation generalization strategies team cherrychums implemented practical augmentation pipeline, including temporal shift, spatial flip, random event deletion, simulating real-world perturbations motion jitter, mirroring, sensor dropout. addi- tion, pretraining external event dataset synthetic dataset used team ustceventgroup im- prove generalization provide stronger initialization un- der limited training data conditions. model-agnostic inference-time post-processing blink artifacts interrupt event streams cause erro- neous gaze predictions, temporal inconsistency may lead unstable, non-smooth gaze trajectories. team eye- trackingsmu proposed two lightweight post-processing techniques motion-aware median filtering ensure temporal smoothness adaptively smoothing gaze trajectories based motion variance. optical flow-based refinement ofe adjusts predictions using local event motion flow correct spatial misalignments. steps require retraining model changes flexibly applied existing model. participants were, total, user accounts registered par- ticipated challenge, teams private pixel error lower submitted factsheets describing meth- ods. inclusiveness fairness ensure inclusiveness fairness, several initiatives implemented challenge. first, dataset task design carefully optimized reduce computa- tional demands, making participation accessible teams limited hardware resources. second, ready-to- use training testing pipeline provided, allowing even minimal experience event-based data quickly get started. finally, submitting source code along- side factsheets mandated guarantee repro- ducibility results. conclusion outlook last two years, series challenges signif- icantly advanced event-based eye tracking field. participating teams demonstrated remarkable innovation various approaches. although teams reported basic metrics parameter counts pixel error, deeper understanding model efficiency computational costs remains essential give useful insights hardware designers edge hardware accelerators arvr wearables more. example, could explore met- rics capture actual computational workload mod- els, arithmetic operations, memory footprint activations feature maps analyze level sparsity neural network optimized induce spa- tial temporal spatio-temporal sparsity using tools like neurobench discussion hardware design seen sec. acknowledgements challenge partially e.g., dataset collection funded swiss national science foundation innosuisse bridge proof concept project nwo dutch research council talent programme veni aes file number dataset collec- tion partially supported telluride neuro- morphic cognition engineering workshop. challenge partially supported dvsense. challenge teams methods following sections present overview top- performing solutions challenge. method description authored submitted respective teams part contribution survey. team ustceventgroup han han, yuliang wu, jinze chen, wei zhai, yang cao, zheng-jun zha university science technology china contact hanhmail.ustc.edu.cn description. ustceventgroup proposed bidi- rectional relative positional attention transformer brat method shown fig. network composed spatial encoder temporal decoder. former utilizes cnn extract geometric structural features event representations, latter combines bi-gru block brat block analyze temporal motion patterns accurately localize pupil positions. spatial fea- ture extraction phase, input binary representation ini- tially processed convolutional layer kernel size resulting channels. subsequently, spatial features extracted three additional convolutional layers use larger kernel sizes first two large-kernel convolutional layer, pooling layers ap- plied. following third convolutional layer, fea- ture maps undergo spatial dropout layer mitigate over- fitting. extracting spatial information, features fed bidirectional gru blocks capture short-term tem- poral patterns. processed within brat architecture model long-range dependencies. brat architecture, standard multi-head self-attention transformer replaced bidirectional variant explicitly encodes relative temporal distances, see fig. given sequence length attention heads, output head computed attentioni softmax qiki vi, relative position bias matrix de- signed modulate attention weights based temporal dis- tance. decomposed forward backward compo- figure brat network team ustceventgroup. nents forward backward denotes sensitivity head relative position, generated via monotonically decreasing linear mapping progressively diminish attention distant steps. furthermore, improve robustness model ustceventgroup adopted multi-time-step data sampling strategy training. specifically, sliding window fixed length applied long event sequence stride generating dense training samples. within window, frames sampled uniform intervals de- fined step size, allowing model observe motion longer temporal spans controlling input density. training supervision, loss calculated finding squared differences predicted values true labels time step, taking square root av- eraging time dimension loss yt,pred yt,label. formulation captures overall error across tem- poral sequence, normalizing sequence length ac- count varying durations. metric value param number macs table model complexity brat. implementation details. experiments con- ducted using pytorch, employing cosine annealing warm restart learning rate scheduler, starting initial rate model initially pretrained simulation dataset followed training evaluation dataset, took approximately hours epochs batch size single rtx gpu. tab. shows model complexity metrics, model require additional stages deployment. results. brat proposed ustceventgroup achieved first place leaderboard. results tab. visualization results demonstrate models high accuracy robust tracking capabilities different cases. visualization results code available xiaohuevent- based-eye-tracking-challenge-solution. pixel error table validation results brat. team eyetrackingsmu nuwan bandara, thivya kandappu, archan misra singapore management university contact thivyaksmu.edu.sg description motivation background. work team specifically addresses several key limi- tations existing event-based eye tracking models. first limitation handling blink artifacts, cause interruptions event data lead erroneous gaze figure bidirectional relative positional attention. predictions another limitation temporal incon- sistency often observed predictions, eye move- ments physiologically continuous models some- times fail enforce temporal smoothness, leading abrupt gaze shifts undermine tracking stability additionally, existing models often fail fully leverage local event distributions, resulting misaligned gaze pre- dictions. challenges, coupled inherent label sparsity event datasets, make difficult develop uni- versally robust event-based tracking system. address challenges, propose model- agnostic inference-time post-processing enhance ac- curacy robustness event-based eye tracking. ap- proach targets shortcomings existing spatio-temporal models introducing lightweight, post-processing tech- niques integrated model without re- quiring retraining architectural changes. makes method flexible easily applicable wide range existing models. post-processing framework consists two key components motion-aware median filtering mf, enforces temporal smoothness taking ad- vantage continuous nature eye movements, optical flow-based local refinement ofe, improves spatial consistency aligning gaze predictions domi- nant motion patterns local event neighborhood. refinements mitigate blinking artifacts also en- sure gaze predictions remain temporally continuous spatially accurate, even presence rapid eye move- ments motion artifacts. algorithm motion-aware median filtering require original predictions xpred, ypred, base win- dow local motion variance estimation wbase, min- imum allowed smoothing window wmin, maximum allowed smoothing window wmax, percentile de- termine adaptive window size method displacement, velocity, acceleration, covariance, fre- quency output filtered predictions xf,pred, yf,pred local motion variance fxpred, ypred, wbase smoothened variance rolling meanwbase, local motion variance median window clippingsmoothened variance, wmin, wmax adaptive windows clippingwmin, wmax, rollingmedian window, wbase, xf,pred, yf,pred rolling medianxpred, ypred, adaptive windows description inference-time post-processing. dis- cussed above, address shortcomings existing meth- ods inference stage, propose add two light- weight post-processing techniques specifically targeting following limitations motion-aware median filtering algorithm ensure temporal consistency predictions since eye movements physiologically bound continuous spatial domain re- duce blinking artifacts optical flow estimation local spatial neighbourhood algorithm smoothly shift original predictions flow vector original prediction unaligned cumulative local neighbour- algorithm rule-based optical flow estimation smooth shifts require continuous event stream number events i,t,x,y,p filtered predic- tions xf,pred, yf,pred, scaling parameter count threshold difference threshold output refined predictions xr,f,pred, yr,f,pred timestep evin,tmaxevi,tmin xf,pred,yf,pred previous timestamp evi tmin i,t,x,y,p roi size f,pred, f,pred xf,pred, yf,pred current timestamp previous timestamp timestep difference absolutexj f,pred meanxjcj f,pred difference absoluteyj f,pred meanyjcj f,pred difference difference else end end events roi evt previous timestamp, cur- rent timestamp, f,pred f,pred f,pred f,pred i,t,x,y,p previous timestamp current timestamp events roi events roix events roix events roiy events roiy absolutedx absolutedy r,f,pred f,pred dx,dy r,f,pred f,pred dx,dy end end end end hood flow direction. specifically inspired empirical observations hint original predic- tions tend occupy negligence towards event motion flow local neighbourhood, suggesting lack atten- tion local event distribution original models. method ofe pixel error pixel error public private table evaluation results team eyetrackingsmus ap- proach. method ofe model size macs size table computational complexity details team eyetrackingsmus approach. descriptively, motion-aware filtering shown algorithm first estimate local motion variance temporal dimension i.e., within set time window using set alternative methods including order ki- netics, covariance frequency subsequently, assign median-based adaptive filter windows set time windows kernel size median filtering adaptive appropriate background pupil movement also ensuring temporal consistency. contrast, optical flow estimation shown algorithm first es- timate appropriate size region interest roi around filtered prediction using first order deriva- tives then, number events within selected roi exceeds set threshold, accumulate determine cumulative vector trajectory events within roi softly shift filtered prediction refine spatial position. implementation details base models. since pro- posed method presented post-processing step works model-agnostic fashion, select two recent models base models cnn-gru big- brains show impact proposed pipeline towards improved pupil coordinate predictions case. specific, simple convolutional gated recurrent unit architecture specifically de- signed efficient spatio-temporal modelling predict pupil coordinates sparse event frames, whereas at- tempts preserve causality learn spatial relationships using lightweight model consisting spatial tempo- ral convolutions. figure architecture tdtracker. tdtracker primarily comprises two components, implicit temporal dynamic itd explicit temporal dynamic etd, structure featuring three itd components ensure effective feature abstraction. employs cascaded architecture three distinct time series models capture temporal information comprehensively. results. shown tab. post processing techniques consistently improved results vanilla pre- dictions method thereby suggest efficacy proposed model-agnostic post-processing methods. addition, since methods executed inference time light-weight post-processing steps, estimate flops ofe per predic- tion instance i.e., per event frame, respectively, whereas learnable parameter space effectively null. shown tab. present detail post-processing steps add negligible overhead base models terms computational complexity, despite consistently improv- ing vanilla prediction results base models. code available githubeyelorin. team hkustgz xiaopeng lin, hongxiang huang, hongwei ren, yue zhou, bojun cheng hong kong university science technology guangzhou contact bochenghkust-gz.edu.cn description. hkustgz team proposes td- tracker framework designed address challenges high-speed, high-precision eye tracking us- ing event-based cameras shown fig. consists two main components convolutional neural network cnn cascaded structure includes frequency- aware module gated recurrent units gru mamba models. cnn responsible capturing implicit short-term temporal dynamics eye movements, cascaded structure focuses ex- tracting explicit long-term temporal dynamics. initial phase, event-based eye tracking data input cnn, effectively captures fine-grained short-term temporal dynamics eye move- ments. frequency-aware module used en- hance models ability focus relevant frequency features contribute accurate eye movement tracking. following this, gru mamba models employed analyze long-term temporal patterns, enabling sys- tem track eye movements across extended periods with- losing accuracy. integration components, tdtracker achieves superior performance eye movement predic- figure visualization heatmap generated tdtracker. metric private public param flops fps tdtracker table tdtrackers performance tion localization, enabling real-time processing minimal computational overhead. additionally, predic- tion heatmap generated precise eye coordinate re- gression, improving tracking accuracy. ap- proach demonstrates state-of-the-art performance event- based eye-tracking challenges, showcasing effectiveness combining temporal dynamics advanced neural net- work architectures. implementation details. server leverages py- torch deep learning framework selects adamw op- timizer initial learning rate set em- ploys cosine decay strategy, accompanied weight de- cay parameter configuration meticulously chosen enhance models convergence perfor- mance adaptive learning rate adjustments. training conducted nvidia geforce rtx gpu memory, enabling batch size results. competition, found using se- quence training, sequence testing highest ac- curacy mse however, since parame- ters frequency-aware module tied sequence length, canceled module competition. addition, since model consider open closed eye cases, simply use ratio number events basis judgment set current ratio smaller value, infer- ence eye coordinate changed sample overwritten inference value closest sample. whats more, differ directly regressing coordinate infor- mation using predicted probability density map, provides additional probability model predicting image shown fig. probability less believe predicted result. tab. shows performance tdtracker private public test sets. post-processing, mse optimized interpolation ground truth team cherrychums hoang truong, vinh-thuan ly, huy tran, thuan-phat nguyen, tram doan university science, vietnam national university chi minh city contact student.hcmus.edu.vn description. cherrychums team presents robust data augmentation strategies within lightweight spatiotemporal network introduced pei al. network archi- tecture illustrated fig. spatiotemporal block fig. approach enhances model resilience real-world perturbations, including abrupt eye movements environmental noise. data augmentation pipeline, depicted fig. com- prises temporal shift, spatial flip, event deletion. augmentations significantly bolster models robustness preserving computational efficiency. specifi- cally temporal shift given asynchronous nature event- based data, temporal augmentation crucial improv- ing model resilience timing variations. apply ran- dom shift event timestamps within range milliseconds ensuring proper alignment ground truth labels. since labels sampled ev- ery ms, recompute label indices shifting timestamps maintain accurate correspondence. spatial flip introduce spatial invariance, hori- zontally vertically flip event coordinates corresponding labels, including pupil center posi- tions, undergo transformation preserve spatial consistency. event deletion simulate real-world sensor noise occlusions, randomly remove events keeping label sequence unchanged. augmen- tation forces model learn incomplete event streams enhances robustness missing data. lightweight spatiotemporal network optimized real-time performance edge devices, leveraging causal spatiotemporal convolutional blocks efficient event bin- ning. combining techniques, method achieves improved accuracy robustness event-based eye track- ing. implementation details. lightweight spatiotem- poral network, adopt original training configuration pei al. specifically, train model us- ing batch size batch contains event frames. network optimized epochs using adamw optimizer base learning rate weight decay employ cosine de- cay learning rate schedule linear warmup, figure compact spatiotemporal model integrating data augmentation spatial temporal processing blocks. convolutional layers extract spatial temporal features efficiently. figure illustration spatiotemporal processing temporal block applies temporal convolution across frames, spa- tial block extracts spatial features using convolutional filters. warmup phase spans total training steps. ad- ditionally, leverage automatic mixed-precision amp, pytorch compilation accelerate training improve efficiency. experiments conducted single nvidia tesla gpu provided kaggle. figure overview data augmentation techniques used cherrychums. results. approach demonstrated significant improve- ments robustness accuracy, achieving euclidean distance error compared original spa- tiotemporal network trained without additional augmen- tation strategy, shown tab. results highlight effectiveness data augmentation strategy enhanc- ing model performance challenging conditions. part challenge report, provide summary year augmentation pixel error table comparison euclidean distance accuracy event-based eye tracking challenges. model size number macs required per frame, shown tab. information crucial understanding efficiency real-time deployment ca- pabilities network. metric value parameters macs per frame table summary model size macs hardware discussion hardware design event-based eye tracking systems necessitates careful consideration power efficiency, la- tency, adaptability ensure robust performance real- world applications. central advantage event-based ar- chitectures lies dvss inherent power efficiency, stems sparse data generation. unlike conventional frame-based systems continuously sample transmit data, event-based systems respond changes reti- nal activity, drastically reducing redundant data throughput. specialized event-driven hardware designs leverage unique property dvs potential achieve high-performance tracking rapid eye movements saccades minimizing redundant processing fixation periods. efficiency enabled optimization strategies event-triggered circuit ar- chitectures, dynamically activate computational re- sources response detected motion, thereby align- ing power consumption real-time demands. addition- ally, approach aligns emerging paradigms near- sensor in-sensor processing, computation local- ized near sensing element minimize data transmission external processors. recent studies showed in- sensor event filtering preprocessing could reduce com- munication bandwidth, thereby lowering latency power consumption critical challenge event-driven hardware design in- volves balancing relationship processing la- tency sensors sampling rate. dvss high sam- pling rate theoretically improves temporal resolution, also brings potential risk overwhelming downstream pro- cessing pipelines, leading data congestion potential data leakage. therefore, optimizing buffering strategies parallel processing architectures becomes important handle sporadic bursts events without introducing bottle- necks. instance, integrating dedicated memory hierar- chies distributed processing units prevent data con- tention high-activity intervals. optimizations ensure hardware maintains low-latency responsive- ness, critical applications like foveated rendering real-time human-computer interaction, avoiding unnecessary computational overhead periods eye fixation. moreover, configurability emerges pivotal design consideration enhance adaptability event-based eye tracking systems across diverse operational environments. preferably, hardware support tunable pa- rameters, event detection thresholds, temporal fil- tering windows, region-of-interest roi prioritization, accommodate variations lighting conditions, user er- gonomics, application-specific requirements. exam- ple, dynamic reconfiguration event thresholds could op- timize sensitivity low-light environments, selective roi processing could conserve resources focusing com- putation critical areas visual field. hardware de- signs flexibility extend systems utility across different use cases also future-proof design evolving algorithmic demands. embed- ding configurability hardware architecture, design- ers strike balance generality efficiency, ensuring event-based eye tracking systems remain practical scalable real-world deployments. references mlflow machine learning lifecycle platform. mlflow.org. accessed --. dvxplorer mini user guide. com content uploads dvxplorer-mini.pdf. anastasios angelopoulos, julien n.p. martel, amit kohli, jorg conradt, gordon wetzstein. event-based near-eye gaze tracking beyond hz. ieee transactions visualization computer graphics, nuwan bandara, thivya kandappu, argha sen, ila gokarn, archan misra. eyegraph modularity-aware spatio tem- poral graph clustering continuous event-based eye track- ing. advances neural information processing systems, nuwan bandara, thivya kandappu, archan misra. model-agnostic inference-time post-processing local re- finement enhanced event-based eye tracking. arxiv, pietro bonazzi, sizhen bian, giovanni lippolis, yawei li, sadique sheik, michele magno. retina low-power eye tracking event camera spiking hardware. pro- ceedings ieeecvf conference computer vision pattern recognition, pages han cai, chuang gan, ligeng zhu massachusetts insti- tute technology, song han massachusetts institute technology. tinytl reduce memory, parameters ef- ficient on-device learning. proceedings inter- national conference neural information processing sys- tems, red hook, ny, usa, curran associates inc. qinyu chen, yan huang, rui sun, wenqing song, zhonghai lu, yuxiang fu, li. efficient accelerator multi- ple convolutions sparsity perspective. ieee trans- actions large scale integration vlsi systems, qinyu chen, chang gao, xinyuan fang, haitao luan. skydiver spiking neural network accelerator exploiting spatio-temporal workload balance. ieee transactions computer-aided design integrated circuits systems, qinyu chen, zuowen wang, shih-chii liu, chang gao. efficient event-based eye tracking using change-based convlstm network. ieee biomedical circuits sys- tems conference biocas, qinyu chen, chang gao, min liu, daniele perrone, al. event-based eye tracking. event-based vision work- shop. proceedings ieeecvf conference com- puter vision pattern recognition workshops, qinyu chen, kwantae kim, chang gao, sheng zhou, taek- wang jang, tobi delbruck, shih-chii liu. deltakws njdecision bio-inspired temporal-sparsity-aware digital keyword spotting near-threshold sram. ieee transactions circuits systems artificial in- telligence, junyoung chung, aglar gulcehre, kyunghyun cho, yoshua bengio. empirical evaluation gated recurrent neu- ral networks sequence modeling. corr, abs., junyuan ding, ziteng wang, chang gao, min liu, qinyu chen. facet fast accurate event-based eye track- ing using ellipse modeling extended reality. huiyu duan, guangtao zhai, xiongkuo min, zhaohui che, fang, xiaokang yang, jesus gutierrez, patrick callet. dataset eye movements children autism spectrum disorder. proceedings acm multimedia systems conference, pages feng, tianrui ma, yuhao zhu, xuan zhang. bliss- cam boosting eye tracking efficiency learned in-sensor sparse sampling. acmieee annual interna- tional symposium computer architecture isca, pages ieee, guillermo gallego, tobi delbruck, garrick orchard, chiara bartolozzi, brian taba, andrea censi, stefan leutenegger, andrew davison, jorg conradt, kostas daniilidis, al. event-based vision survey. ieee transactions pattern analysis machine intelligence, chang gao, antonio rios-navarro, chen, shih-chii liu, tobi delbruck. edgedrnn recurrent neural network accelerator edge inference. ieee journal emerging selected topics circuits systems, chang gao, tobi delbruck, shih-chii liu. spartus tops fpga-based lstm accelerator exploiting spatio- temporal sparsity. ieee transactions neural networks learning systems, albert tri dao. mamba linear-time sequence mod- eling selective state spaces, song han, jeff pool, john tran, william dally. learn- ing weights connections efficient neural net- works. proceedings international conference neural information processing systems volume page cambridge, ma, usa, mit press. song han, xingyu liu, huizi mao, jing pu, ardavan pe- dram, mark horowitz, william dally. eie effi- cient inference engine compressed deep neural network. acm sigarch computer architecture news, song han, junlong kang, huizi mao, yiming hu, xin li, yubin li, dongliang xie, hong luo, song yao, wang, al. ese efficient speech recognition engine sparse lstm fpga. proceedings acmsigda in- ternational symposium field-programmable gate arrays, pages kenneth holmqvist, marcus nystrom, richard andersson, richard dewhurst, halszka jarodzka, joost van wei- jer. eye tracking comprehensive guide methods measures. oup oxford, kevin hunter, lawrence spracklen, subutai ahmad. two sparsities better one unlocking perfor- mance benefits sparsesparse networks. neuromorphic computing engineering, kwang-hyuk lee leanne williams. eye movement dysfunction biological marker risk schizophre- nia. australian new zealand journal psychiatry, supplaa, pmid gregor lenz, kenneth chaney, sumit bam shrestha, omar oubari, serge picaud, guido zarrella. tonic event- based datasets transformations., documentation available chenghan li, christian brandli, raphael berner, hongjie liu, minhao yang, shih-chii liu, tobi delbruck. de- sign rgbw color vga rolling global shutter dynamic active-pixel vision sensor. ieee international symposium circuits systems iscas, pages nealson li, ashwin bhat, arijit raychowdhury. track eye tracking event camera extended reality applications. ieee international confer- ence artificial intelligence circuits systems aicas, pages ieee, nealson li, muya chang, arijit raychowdhury. gaze gaze estimation event camera. ieee transac- tions pattern analysis machine intelligence, patrick lichtsteiner, christoph posch, tobi delbruck. latency asynchronous temporal con- trast vision sensor. ieee journal solid-state circuits, xiaopeng lin, hongwei ren, bojun cheng. fapnet effective frequency adaptive point-based eye tracker. pro- ceedings ieeecvf conference computer vision pattern recognition, pages shih-chii liu, chang gao, kwantae kim, tobi del- bruck. energy-efficient activity-driven computing architec- tures edge intelligence. international electron devices meeting iedm, pages ...., shih-chii liu, sheng zhou, zixiao li, chang gao, kwan- tae kim, tobi delbruck. bringing dynamic sparsity forefront low-power audio edge computing brain- inspired approach sparsifying network updates. ieee solid-state circuits magazine, wenxuan liu, budmonde duinkharjav, sun, sai qian zhang. fovealnet advancing ai-driven gaze tracking so- lutions efficient foveated rendering virtual reality. ieee transactions visualization computer graph- ics, nico messikommer, daniel gehrig, antonio loquercio, davide scaramuzza. event-based asynchronous sparse con- volutional networks. computer vision eccv yan pei, sasskia bruers, sebastien crouzet, douglas mclelland, olivier coenen. lightweight spatiotem- poral network online eye tracking event camera. proceedings ieeecvf conference computer vision pattern recognition workshops, elena pretegiani lance optican. eye movements parkinsons disease inherited parkinsonian syndromes. frontiers neurology, hongwei ren, fei ma, xiaopeng lin, yuetong fang, hongx- iang huang, yulong huang, yue zhou, haotian fu, ziyi yang, fei richard yu, al. frequency-aware event cloud network. arxiv preprint hongwei ren, xiaopeng lin, hongxiang huang, yue zhou, bojun cheng. exploring temporal dynamics event- based eye tracker. arxiv preprint alberto sabater, luis montesano, ana murillo. event transformer. sparse-aware solution efficient event data processing. proceedings ieeecvf conference computer vision pattern recognition cvpr work- shops, pages sekikawa, hara, saito. eventnet asynchronous recursive event processing. ieeecvf conference computer vision pattern recognition cvpr, pages los alamitos, ca, usa, ieee computer society. niklas stein, diederick niehorster, tamara watson, frank steinicke, katharina rifai, siegfried wahl, markus lappe. comparison eye tracking latencies among sev- eral commercial head-mounted displays. i-perception, timo stoffregen, hossein daraei, clare robinson, alexander fix. event-based kilohertz eye tracking using coded differential lighting. proceedings ieeecvf winter conference applications computer vision, pages shihang tan, jinqiao yang, jiayu huang, ziyi yang, qinyu chen, lirong zheng, zhuo zou. toward efficient eye tracking arvr devices near-eye dvs-based processor real-time gaze estimation. ieee transactions circuits systems regular papers, pages zuowen wang, yuhuang hu, shih-chii liu. exploiting spatial sparsity event cameras visual transformers. ieee international conference image processing icip, pages zuowen wang, chang gao, zongwei wu, marcos conde, radu timofte, shih-chii liu, qinyu chen, al. event- based eye tracking. ais challenge survey. pro- ceedings ieeecvf conference computer vision pattern recognition workshops, zhong wang, zengyu wan, han han, bohao liao, yu- liang wu, wei zhai, yang cao, zheng-jun zha. mam- bapupil bidirectional selective recurrent model event- based eye tracking. proceedings ieeecvf con- ference computer vision pattern recognition, pages jason yik, korneel van den berghe, douwe den blanken, younes bouhadjar, maxime fabre, paul hueber, weijie ke, mina khoei, denis kleyko, noah pacik-nelson, al. neurobench framework benchmarking neuromorphic computing algorithms systems. nature communica- tions, tongyu zhang, yiran shen, guangrong zhao, lin wang, xi- aoming chen, bai, yuanfeng zhou. swift-eye to- wards anti-blink pupil tracking precise robust high- frequency near-eye movement analysis event cameras. ieee transactions visualization computer graph- ics, guangrong zhao, yurun yang, jingwei liu, ning chen, yi- ran shen, hongkai wen, guohao lan. ev-eye rethink- ing high-frequency eye tracking lenses event cameras. advances neural information processing sys- tems,", "published_date": "2025-04-25T10:50:14+00:00"}
{"id": "2504.14582v2", "title": "NTIRE 2025 Challenge on Image Super-Resolution ($\\times$4): Methods and Results", "authors": ["Zheng Chen", "Kai Liu", "Jue Gong", "Jingkai Wang", "Lei Sun", "Zongwei Wu", "Radu Timofte", "Yulun Zhang", "Xiangyu Kong", "Xiaoxuan Yu", "Hyunhee Park", "Suejin Han", "Hakjae Jeon", "Dafeng Zhang", "Hyung-Ju Chun", "Donghun Ryou", "Inju Ha", "Bohyung Han", "Lu Zhao", "Yuyi Zhang", "Pengyu Yan", "Jiawei Hu", "Pengwei Liu", "Fengjun Guo", "Hongyuan Yu", "Pufan Xu", "Zhijuan Huang", "Shuyuan Cui", "Peng Guo", "Jiahui Liu", "Dongkai Zhang", "Heng Zhang", "Huiyuan Fu", "Huadong Ma", "Yanhui Guo", "Sisi Tian", "Xin Liu", "Jinwen Liang", "Jie Liu", "Jie Tang", "Gangshan Wu", "Zeyu Xiao", "Zhuoyuan Li", "Yinxiang Zhang", "Wenxuan Cai", "Vijayalaxmi Ashok Aralikatti", "Nikhil Akalwadi", "G Gyaneshwar Rao", "Chaitra Desai", "Ramesh Ashok Tabib", "Uma Mudenagudi", "Marcos V. Conde", "Alejandro Merino", "Bruno Longarela", "Javier Abad", "Weijun Yuan", "Zhan Li", "Zhanglu Chen", "Boyang Yao", "Aagam Jain", "Milan Kumar Singh", "Ankit Kumar", "Shubh Kawa", "Divyavardhan Singh", "Anjali Sarvaiya", "Kishor Upla", "Raghavendra Ramachandra", "Chia-Ming Lee", "Yu-Fan Lin", "Chih-Chung Hsu", "Risheek V Hiremath", "Yashaswini Palani", "Yuxuan Jiang", "Qiang Zhu", "Siyue Teng", "Fan Zhang", "Shuyuan Zhu", "Bing Zeng", "David Bull", "Jingwei Liao", "Yuqing Yang", "Wenda Shao", "Junyi Zhao", "Qisheng Xu", "Kele Xu", "Sunder Ali Khowaja", "Ik Hyun Lee", "Snehal Singh Tomar", "Rajarshi Ray", "Klaus Mueller", "Sachin Chaudhary", "Surya Vashisth", "Akshay Dudhane", "Praful Hambarde", "Satya Naryan Tazi", "Prashant Patil", "Santosh Kumar Vipparthi", "Subrahmanyam Murala", "Bilel Benjdira", "Anas M. Ali", "Wadii Boulila", "Zahra Moammeri", "Ahmad Mahmoudi-Aznaveh", "Ali Karbasi", "Hossein Motamednia", "Liangyan Li", "Guanhua Zhao", "Kevin Le", "Yimo Ning", "Haoxuan Huang", "Jun Chen"], "summary": "paper presents ntire image super-resolution times challenge, one associated competitions ntire workshop cvpr challenge aims recover high-resolution images low-resolution counterparts generated bicubic downsampling times scaling factor. objective develop effective network designs solutions achieve state-of-the-art performance. reflect dual objectives image research, challenge includes two sub-tracks restoration track, emphasizes pixel-wise accuracy ranks submissions based psnr perceptual track, focuses visual realism ranks results perceptual score. total participants registered competition, teams submitting valid entries. report summarizes challenge design, datasets, evaluation protocol, main results, methods team. challenge serves benchmark advance state art foster progress image sr.", "full_text": "ntire challenge image super-resolution methods results zheng chen kai liu jue gong jingkai wang lei sun zongwei radu timofte yulun zhang xiangyu kong xiaoxuan hyunhee park suejin han hakjae jeon dafeng zhang hyung-ju chun donghun ryou inju bohyung han zhao yuyi zhang pengyu yan jiawei pengwei liu fengjun guo hongyuan pufan zhijuan huang shuyuan cui peng guo jiahui liu dongkai zhang heng zhang huiyuan huadong yanhui guo sisi tian xin liu jinwen liang jie liu jie tang gangshan zeyu xiao zhuoyuan yinxiang zhang wenxuan cai vijayalaxmi ashok aralikatti nikhil akalwadi gyaneshwar rao chaitra desai ramesh ashok tabib uma mudenagudi marcos conde alejandro merino bruno longarela javier abad weijun yuan zhan zhanglu chen boyang yao aagam jain milan kumar singh ankit kumar shubh kawa divyavardhan singh anjali sarvaiya kishor upla raghavendra ramachandra chia-ming lee yu-fan lin chih-chung hsu risheek hiremath yashaswini palani yuxuan jiang qiang zhu siyue teng fan zhang shuyuan zhu bing zeng david bull jingwei liao yuqing yang wenda shao junyi zhao qisheng kele sunder ali khowaja hyun lee snehal singh tomar rajarshi ray klaus mueller sachin chaudhary surya vashisth akshay dudhane praful hambarde satya naryan tazi prashant patil santosh kumar vipparthi subrahmanyam murala bilel benjdira anas ali wadii boulila zahra moammeri ahmad mahmoudi-aznaveh ali karbasi hossein motamednia liangyan guanhua zhao kevin yimo ning haoxuan huang jun chen abstract paper presents ntire image super-resolution challenge, one associated competitions ntire workshop cvpr challenge aims recover high-resolution images low-resolution counterparts generated bicubic downsam- pling scaling factor. objective develop effective network designs solutions achieve state-of- the-art performance. reflect dual objectives image research, challenge includes two sub-tracks zheng chen, kai liu, jue gong, jingkai wang, lei sun, zong- wei wu, radu timofte, yulun zhang challenge organizers, authors participated challenge. corresponding au- thor yulun zhang. section supplementary materials contains authors teams affiliations. ntire webpage cvlai.netntire. code zhengchenntireimagesrx. restoration track, emphasizes pixel-wise accuracy ranks submissions based psnr perceptual track, focuses visual realism ranks results perceptual score. total participants registered com- petition, teams submitting valid entries. re- port summarizes challenge design, datasets, evaluation protocol, main results, methods team. challenge serves benchmark advance state art foster progress image sr. introduction single image super-resolution refers process reconstructing high-resolution image low-resolution counterpart, undergone information-losing degradation process. fundamen- tal problem computer vision, underpins numerous practical applications, including video surveillance, digital pathology, satellite imagery. among various cs.cv apr formulations, classical image task one commonly adopted benchmarks. setting, image typically generated predefined downsampling operation, often bicubic interpolation. degradation removes signifi- cant high-frequency content, rendering task detail re- covery inherently ill-posed inverse problem. goal recover missing high-frequency components using learned priors external data. owing well-defined setting, classical task standard benchmark, en- abling fair comparison across methods. moreover, models trained framework often generalize well complex degradations transfer learning. early image super-resolution methods rely in- terpolation reconstruction-based algorithms however, approaches often produce over-smoothed results lack fine textures realistic details. advent deep learning, convolutional neural networks cnns become dominant paradigm beginning pioneering work srcnn later models incorporate deeper architectures residual learning attention mechanisms improve reconstruction accuracy. meanwhile, transformer- based architectures advance modeling long- range dependencies self-attention be- sides, recent state-space models i.e., mamba offer im- proved scalability efficient sequence modeling approaches typically trained using pixel- wise losses mean squared error, primarily focus enhancing restoration quality maximize fidelity. recent years, field increasingly shifted to- ward models emphasize perceptual realism. gener- ative approaches, generative adversarial networks gans introduced produce per- ceptually appealing outputs. recently, diffusion mod- els emerged powerful alternative generating realistic textures models pro- gressively transform random noise high-resolution im- ages denoising process conditioned input. diffusion-based methods show strong potential improving perception quality, better aligns human visual preferences. overall, development methods reflects dual objective achieving high restoration quality faithful reconstruction, high perception qual- ity producing visually pleasing, realistic results, essential practical application. collaborating edition ntire workshop cvpr organize ntire challenge example-based single image super-resolution challenge focuses recovering high-resolution details single low-resolution input, following classical bicubic degradation setting. goal advance development effective models provide uni- fied benchmark comparison. comprehensively evaluate performance, challenge includes two tracks one targeting restoration quality, focuses pixel-wise accuracy, targeting per- ception quality, emphasizes perceptual realism output. dual-track design reflects evolving goals image research encourages solutions balance fidelity perceptual quality. challenge one ntire work- shop associated challenges ambient lighting normaliza- tion reflection removal wild shadow re- moval event-based image deblurring image de- noising xgc quality assessment ugc video en- hancement night photography rendering image super-resolution real-world face restoration ef- ficient super-resolution depth estimation ef- ficient burst hdr restoration cross-domain few- shot object detection short-form ugc video quality assessment enhancement text image gener- ation model quality assessment day night raindrop removal dual-focused images video quality assess- ment video conferencing low light image enhance- ment light field super-resolution restore im- age model raim wild raw restoration super-resolution raw reconstruction rgb smartphones ntire image super-resolution ntire image super-resolution challenge, one associated challenges ntire two primary objectives. firstly, intends offer thorough overview latest advancements emerg- ing tendencies within field image super-resolution sr. secondly, functions platform enables academic researchers industrial professionals con- verge investigate possible collaborative opportunities. following part delves specific aspects. dataset challenge provides three official datasets, including divk flickrk lsdir additionally, supplementary data allowed used. chal- lenge, lr-hr image pairs constructed images via bicubic interpolation downsampling. divk divk dataset contains images, divided three parts train- ing, validation, testing, respectively. ensure fair- ness, participants access high-resolution images divk validation set except testing phase. furthermore, test images remain hidden throughout entire challenge. flickrk flickrk dataset images, cov- ering wide range quality levels contents. challenge, images flickrk dataset available. lsdir lsdir dataset comprises images flickr platform. divided three parts training, validation, testing. track competition year, competition features two tracks restora- tion track perceptual track. restoration track. consistent last years chal- lenge teams ranked based psnr enhanced images compared images divk testing dataset. perceptual track. year, perceptual track introduced. track, seven widely used iqa met- rics collected evaluate restored results thor- oughly. metrics include lpips, dists, clip-iqa, maniqa, musiq, niqe. teams ultimately ranked based overall perceptual score align text score left tex righ left text distsright text clip-iqa text maniqa frac text musiq max left frac text niqeright end aligned challenge phases. development validation phase phase, participants access two datasets pairs lrhr training images valida- tion images divk dataset, lrhr training image pairs validation images lsdir dataset. participants also permitted use ad- ditional data training purposes. restored images generated models submitted codalab server evaluation based eight performance metrics, immediate feedback provided. testing phase final phase, participants receive test images, corresponding ground truth images provided. required upload outputs codalab server submit code detailed report organizers via email. chal- lenge ends, organizers validate code send results participants. evaluation protocol. evaluation process involves eight standard metrics psnr, ssim, lpips, dists, niqe, maniqa, musiq, clip-iqa. evalua- tion, -pixel border around image excluded, calculations performed channel ycbcr color space. results primarily based submissions made codalab server. code sub- mitted participants used reproduction verifica- tion, small discrepancies precision considered acceptable. script calculating metrics found ntireimagesrx, repository also in- cludes source code pre-trained models. challenge results challenge includes two sub-tracks track restoration quality track perception quality. results rankings participating teams provided tab. track restoration quality. samsungaicamera team achieves top performance db. notably, two teams surpass last years best psnr score db, ten teams obtain results db, highlighting clear improvement reconstruction accuracy. track perception quality. snucv team ranks first highest perceptual score two teams achieve score seven teams exceed in- dicating advancements perceptual quality. details evaluation protocol two tracks provided sec. team order sec. follows order presented tab. top teams method details highlighted here. due space limitations, remaining teams listed sec. supplementary materials. team member information also found sec. supplementary materials. architectures main ideas throughout challenge, various innovative techniques introduced boost performance. here, team members summarize principal concepts. transformer-based architectures remain main- stream approach. transformer-based methods, hat swinir dat continue deliver strong reconstruction results capturing long- range dependencies. many teams utilized pre-trained transformer models fine-tuned hybrid at- tention self-attention mechanisms. bbox team, example, used transformer models capture local global information, improving reconstruction quality model ensembles. integrating mamba architectures improved global context modeling. recognizing importance capturing extensive contextual information, par- ticipants employed mamba architecture better model long-range spatial dependencies global con- text, leading significant performance improvements super-resolution tasks. xiaomimm team, ex- ample, integrated mamba hat model, adapting branches based image statistics combining mul- tiple mamba-enhanced networks. dynamic fusion multiple network architectures boosts performance. several teams utilized dy- namic model fusion strategies, combining outputs transformer-based models cnns. leveraging strengths architectures, methods captured local textures global structural information, signif- icantly improving reconstruction quality. winning team name rank rank psnr ssim lpips dists niqe maniqa musiq clip-iqa perceptual score track track track track samsungaicamera snucv bbox microsr xiaomimm nju mcg x-l endeavour cidautai kletech-cevi jnu acvlab svnit hyperpix bvivsr adadat junyi svnit sak dcu vai-gm quantum res psu ivplab-sbu mcmir aimanga ipcv team table results ntire image super-resolution challenge. psnr seven perceptual metric scores measured divk testing dataset. restoration track, team rankings based primarily psnr. perceptual track, team ranking based perceptual score, weighted average seven perceptual metrics. ranking mainly depends higher ranking among two tracks, secondly average value rankings two tracks. team order sec. follows sequence table. teams i.e., aimanga ipcv team submitted late failed submit code ranked. team, samsungaicamera, instance, combined pre- trained hat transformer cnn-based nafnet, achieving top performance restoration track. incorporating frequency-domain loss functions enhanced detail reconstruction. preserve fine details textures, participants employed frequency- domain loss functions, stationary wavelet trans- form swt loss, improve high-frequency detail re- covery reduce artifacts. kletech-cevi team, example, used wavelet-based loss hat frame- work, applying symlet filters maintain texture sharp- ness prevent chroma distortion. advanced multi-stage training strategies progres- sively refine model performance. participants adopted multi-stage training methods, gradually increasing patch sizes training, improved reconstruction quality model stability. example, bbox team used three-stage training strategy, adjusting patch sizes loss functions across stages, followed clip- based semantic filtering fine-tuning, ultimately en- hancing fidelity perceptual quality. participants year, image challenge attracted registered participants, teams submitting valid entries. comparison last years challenge in- crease number valid submissions significant improvement results. submis- sions set new benchmark state-of-the-art image super-resolution fairness set rules established ensure fairness competition. use divk test images training strictly prohibited, although divk test images available. participants allowed train using additional datasets, flickrk lsdir datasets. use data augmentation techniques dur- ing training testing considered fair practice. figure team samsungaicamera conclusions insights gained analyzing results ntire image super-resolution challenge summa- rized follows integration transformer cnn hybrid archi- tectures demonstrated exceptional performance modeling global context reconstructing local details, achieving balanced approach. mamba-based state space models widely adopted, showcasing scalability computational efficiency modeling capabilities, offering robust alternative traditional self-attention mechanisms. advanced training strategies, including multi-stage pipelines, progressive patch training, clip-based semantic filtering, boosted model generalization robustness. frequency-domain losses en- hanced high-frequency texture restoration. innovative use generative priors, particularly pre-trained diffusion models combined clip-based reference-free perceptual losses, achieved superior perceptual quality minimal training. challenge methods teams samsungaicamera description. proposed solution shown fig. im- age super-resolution critical task computer vi- sion, aiming reconstruct high-quality images figure team samsungaicamera low-resolution versions. significant advances made recent years, existing methods often face chal- lenges effectively capturing global context fine local details simultaneously. address limitations, samsungaicameras ap- proach combines two powerful networks transformer- based network hat convolution-based network nafnet hat try leverage global local infor- mation effective combine channel attention overlapping cross-attention, still struggle capturing fine-grained local features. team design super- resolution network detail enhance network refer two method shown fig. achieve supe- rior performance image super-resolution, dynamicly fuse feature extracted transformer-based network convolution-based network design addresses limitations individual component also opens new possibilities hybrid architectures image super- resolution tasks. implementation details. datasets.the team used three datasets total divk dataset, lsdir dataset, self-collected custom dataset consisting million images. specific ways team utilized training sets across different training phases detailed training details section. trainning strategy. model training consists two stages. first stage, team pre-train entire net- work using custom dataset million images ls- dir datasets, initial learning rate training time approximately hours. second stage, fine-tune network module using divk datasets, initial learning rate training duration hours, enhanced models ability restore details. used data augmentation methods channel shuffle mixing perform progressive learning network trained different image patch sizes grad- ually enlarged patch size figure team snucv increases, performance gradually improve. training loss, train model alternately iterating loss, loss, stationary wavelet transformswt loss found adding swt loss training helps model escape local optima. model trained gpu. snucv description. illustrated fig. snucv team first employs classic super-resolution model perform upsampling input image. subsequently, one- step diffusion model applied enhance super- resolution process. upsampling model, team adopts mambairv diffusion model utilizes tsd-sr architecture. fully leverage gen- erative prior diffusion model, refrain ad- ditional training directly use pretrained weights, fo- cusing instead fine-tuning upsampler. let diffusion model upsampler input image ground-truth image igt. en- sure output mui intermediate out- put accurately resemble ground-truth igt, team defines loss function composed lpips loss de- noted protect mathcal ltext lpips, loss, denoted protect mathcal mathca ext curacy thcal lui, igt nonumber quad alpha cdot mathcal ltext lpipsmui, igt, protect mathcal ltext lpips captures perceptual similarity final output ground-truth, protect mathcal enforces pixel- wise accuracy. enhance perceptual quality, team intro- duces no-reference clip-based image quality assessment iqa loss denoted protect mathcal ltext clip. loss utilizes clip models text encoder protect text cliptext text image encoder protect text cliptext image. two text prompts defined good photo bad photo, embeddings computed normalized text pos norm text cliptext tex ttext good photo ttext neg text normtext cliptext texttext bad photo generated images feature representation text pred text normtext cliptext imagemui then, similarities image features text embeddings computed text text red cdot ttext postop stext neg ftext pred cdot ttext negtop clip loss defined increase similarity good photo reduce similarity bad photo text clip stext pos stext neg total training loss formulated mathcal mathcal ltext accuracy beta cdot mathcal ltext clip set lpha eta implementation details. training upsampler mambairv, team utilizes adamw optimizer hyperparameters learn- ing rate times ground-truth images cropped patches size times training conducted iterations batch size training dataset divk lsdir bbox description. solution proposed bbox il- lustrated fig. transformer-based models con- sistently demonstrated remarkable performance field super-resolution, exemplified methods hat dat swinir recently, numer- ous studies shown mamba architectures also achieve impressive results domain, evidenced models mambair mambairv smamba notably, prior research indicates transformer-based models excel modeling sequential relationships, mamba-based approaches partic- ularly adept capturing long-range contextual informa- tion capabilities crucial pixel-dense super- resolution tasks, require models simultaneously capture spatial relationships individual pixels model long-range contextual dependencies. inspired complementary strengths, team members adopt hat-l mambairv-l foundation models, train- ing independently employing model ensemble strategy effectively leverage advantages, thereby achieving optimal performance. divk lsdir flickrk unsplash test data clip vit-h- clip vit-h- filtered dataset cosine similarity dataset filtering pipeline model training pipeline hat-l self ensemble mambairv-l model ensemble ensemble pipeline divklsdir flickrk stage batch patch itersk initial ire- hat-l mambairv-l divklsdir flickrk stage batch patch itersk initial ire- filtered dataset stage batch patch itersk initial ire- hat-l mambairv-l hat-l mambairv-l figure team bbox. swin transformer conv concat input ouput s-layer s-layer s-layer s-layer conv reconstruction dense residual connection figure team microsr implementation details. training dataset comprises divk, lsdir, flickrk, selected unsplash datasets. augment training data, team members implement random flip rotation strategies. hat model, team members initialize network using pre- trained weights hat-l model, previ- ously trained imagenet dataset. contrast, team members train mambairv-l model scratch. specifically, training follows three-stage progres- sive strategy multi-scale loss computes losses resolutions enhance performance across different scales. first stage, team members train divk, lsdir, flickrk datasets. team members set patch size batch size conducting iterations initial learning rate stage, team members minimize loss using adam optimizer sec- ond stage, maintaining training datasets, team members increase patch size reduce batch size team members also switch loss mse loss. stage comprises iterations reduced initial learning rate fi- nal stage, achieve superior model performance, team members implement semantic selection using clip features. team members filter similar images divk, lsdir, flickrk, unsplash datasets fine-tuning. stage, learning rate set model trained additional iterations. multi-step learning rate decay method ap- plied across three training phases. mambairv- hat-l models, team members repeat afore- mentioned three training stages five times ensure optimal convergence. inference stage, team members employ self- ensemble strategy enhance performance hat- mambairv-l models. mambairv-l specifi- cally, team members utilize multiple sliding windows varying sizes inference integrate results. finally, team members implement weighted fusion method generate outputs adaptive combi- nation hat-l mambairv-l models. microsr description. microsr team proposes super- resolution method based architectural principles drct swinfir also building foundation established swinir novel s-layer introduced residual connections improve cap- ture global information efficiently. illustrated fig. architecture comprises shallow feature ex- traction layer, s-layers constructed dense residual connections, final reconstruction layer. implementation details. proposed solution adopts multi-stage greedy training policy. first stage, model trained mse loss epochs. sec- ond stage, adversarial loss introduced fine-tune model better perceptual quality. training, learning rate decays progressively factor stage. within stage, learning rate remains fixed, best-performing checkpoint selected used initialize next stage. figure team xiaomimm enhance performance, team integrates neural degradation algorithm data augmentation. training data includes divk dataset sup- plemented real-world paired images resulting diverse dataset improved robustness un- seen degradations. team re-trains drct using of- ficial training code augmented dataset. training initialized official pretrained model real-drct- gan finetuned. final model contains param- eters, flops. average inference time per image reported seconds. xiaomimm description. based hat model team members propose two variants improve image super- resolution performance. first variant introduces multi-branch structure, designed according statistical feature information brightness, contrast, sharpness, noise, saturation in- put image. five statistical features divided two categories, resulting ten-branch network struc- ture. utilizing fixed statistical information instead al- lowing network learn features autonomously, team members introduce additional prior information improve models generalization ability. second variant integrates mamba structure hat model, thereby fully leveraging advan- tageous characteristics mamba. detailed network structure illustrated fig. finally, team members trained fused origi- nal hat model two variants, achieving best results experiments. implementation details. dataset utilized training comprises divk lsdir. training batch, rgb patches measuring cropped subjected random flipping rotation. learning rate initialized halved every iter- ations. network undergoes training total iterations, minimizing loss function using adam optimizer team members repeated aforemen- tioned training settings four times loading trained weights. subsequently, fine-tuning executed using loss functions, initial learning rate iterations patch size team members conducted fine-tuning four models utilizing losses employed batch sizes finally, team members integrated five models obtain ultimate model also used means based fusion strategy achieve better results. methods remaining teams teams presented innovative ideas thorough ex- periments competition. however, due space lim- itations, in-depth discussion found sec. supplementary materials, contains detailed de- scriptions methods implementation details remaining teams participated challenge. teams discussed main report, approaches still highlighted, offering insight unique strategies technical implementations. acknowledgements work supported shanghai municipal sci- ence technology major project shzdzx fundamental research funds central uni- versities. work partially supported hum- boldt foundation. thank ntire spon- sors bytedance, meituan, kuaishou, university wurzburg computer vision lab. references unsplash full dataset ... data. accessed --. liangyu chen, xiaojie chu, xiangyu zhang, jian sun. simple baselines image restoration. european confer- ence computer vision, pages springer, xiangyu chen, xintao wang, jiantao zhou, qiao, chao dong. activating pixels image super- resolution transformer. cvpr, zheng chen, yulun zhang, jinjin gu, linghe kong, xi- aokang yang, fisher yu. dual aggregation transformer image super-resolution. iccv, zheng chen, zongwei wu, eduard sebastian zamfir, kai zhang, yulun zhang, radu timofte, xiaokang yang, al. ntire challenge image super-resolution methods results. cvprw, zheng chen, kai liu, jue gong, jingkai wang, lei sun, zongwei wu, radu timofte, yulun zhang, al. ntire challenge image super-resolution methods results. proceedings ieeecvf conference computer vision pattern recognition cvpr work- shops, zheng chen, jingkai wang, kai liu, jue gong, lei sun, zongwei wu, radu timofte, yulun zhang, al. ntire challenge real-world face restoration methods results. proceedings ieeecvf conference computer vision pattern recognition cvpr work- shops, marcos conde, radu timofte, al. ntire challenge raw image restoration super-resolution. proceed- ings ieeecvf conference computer vision pattern recognition cvpr workshops, marcos conde, radu timofte, al. raw image reconstruc- tion rgb smartphones. ntire challenge re- port. proceedings ieeecvf conference com- puter vision pattern recognition cvpr workshops, tao dai, jianrui cai, yongbing zhang, shu-tao xia, lei zhang. second-order attention network single image super-resolution. cvpr, chao dong, chen change loy, kaiming he, xiaoou tang. learning deep convolutional network image super-resolution. eccv, linwei dong, qingnan fan, yihong guo, zhonghao wang, zhang, jinwei chen, yawei luo, changqing zou. tsd-sr one-step diffusion target score distillation real-world image super-resolution. arxiv preprint egor ershov, sergey korchagin, alexei khalin, artyom pan- shin, arseniy terekhin, ekaterina zaychenkova, georgiy lobarev, vsevolod plokhotnyuk, denis abramov, elisey zhdanov, sofia dorogova, yasin mamedov, nikola banic, georgii perevozchikov, radu timofte, al. ntire challenge night photography rendering. proceedings ieeecvf conference computer vision pat- tern recognition cvpr workshops, yuqian fu, xingyu qiu, bin ren yanwei fu, radu timofte, nicu sebe, ming-hsuan yang, luc van gool, al. ntire challenge cross-domain few-shot object detection methods results. proceedings ieeecvf con- ference computer vision pattern recognition cvpr workshops, ian goodfellow, jean pouget-abadie, mehdi mirza, bing xu, david warde-farley, sherjil ozair, aaron courville, yoshua bengio. generative adversarial nets. neurips, albert tri dao. mamba linear-time sequence modeling selective state spaces. arxiv preprint hang guo, yong guo, yaohua zha, yulun zhang, wenbo li, tao dai, shu-tao xia, yawei li. mambairv attentive state space restoration. arxiv preprint hang guo, jinmin li, tao dai, zhihao ouyang, xudong ren, shu-tao xia. mambair simple baseline image restoration state-space model. eccv, pages springer, yanhui guo, xiaolin wu, xiao shu. data acquisition preparation dual-reference deep learning image super-resolution. tip, shuhao han, haotian fan, fangyuan kong, wenjie liao, chunle guo, chongyi li, radu timofte, al. ntire challenge text image generation model quality assess- ment. proceedings ieeecvf conference com- puter vision pattern recognition cvpr workshops, jonathan ho, ajay jain, pieter abbeel. denoising diffu- sion probabilistic models. neurips, chih-chung hsu, chia-ming lee, yi-shiuan chou. drct saving image super-resolution away information bottleneck. proceedings ieeecvf conference computer vision pattern recognition, pages varun jain, zongwei wu, quan zou, louis florentin, hen- rik turbell, sandeep siddhartha, radu timofte, al. ntire challenge video quality enhancement video con- ferencing datasets, methods results. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, jiwon kim, jung kwon lee, kyoung lee. accurate image super-resolution using deep convolutional net- works. cvpr, diederik kingma jimmy ba. adam method stochastic optimization. arxiv preprint cansu korkmaz murat tekalp. training transformer models wavelet losses improves quantitative visual performance single image super-resolution. proceed- ings ieeecvf conference computer vision pattern recognition, pages christian ledig, lucas theis, ferenc huszar, jose caballero, andrew cunningham, alejandro acosta, andrew aitken, alykhan tejani, johannes totz, zehan wang, wenzhe shi. photo-realistic single image super-resolution using generative adversarial network. cvpr, sangmin lee, eunpil park, angel canelo, hyunhee park, youngjo kim, hyungju chun, xin jin, chongyi li, chun-le guo, radu timofte, al. ntire challenge efficient burst hdr restoration datasets, methods, results. proceedings ieeecvf conference computer vi- sion pattern recognition cvpr workshops, jianze li, jiezhang cao, zichen zou, xiongfei su, xin yuan, yulun zhang, yong guo, xiaokang yang. distillation-free one-step diffusion real-world image super-resolution. arxiv preprint xin li, yeying jin, xin jin, zongwei wu, bingchen li, yufei wang, wenhan yang, li, zhibo chen, bihan wen, robby tan, radu timofte, al. ntire challenge day night raindrop removal dual-focused images methods results. proceedings ieeecvf conference computer vision pattern recognition cvpr work- shops, xin li, xijun wang, bingchen li, kun yuan, yizhen shao, suhang yao, ming sun, chao zhou, radu timofte, zhibo chen. ntire challenge short-form ugc video quality assessment enhancement kwaisr dataset study. proceedings ieeecvf conference com- puter vision pattern recognition cvpr workshops, xin li, kun yuan, bingchen li, fengbin guan, yizhen shao, zihao yu, xijun wang, yiting lu, wei luo, suhang yao, ming sun, chao zhou, zhibo chen, radu timofte, al. ntire challenge short-form ugc video quality assessment enhancement methods results. pro- ceedings ieeecvf conference computer vision pattern recognition cvpr workshops, yawei li, kai zhang, jingyun liang, jiezhang cao, liu, rui gong, yulun zhang, hao tang, yun liu, denis deman- dolx, rakesh ranjan, radu timofte, luc van gool. ls- dir large scale dataset image restoration. cvprw, jingyun liang, jiezhang cao, guolei sun, kai zhang, luc van gool, radu timofte. swinir image restoration us- ing swin transformer. iccvw, jie liang, radu timofte, qiaosi yi, zhengqiang zhang, shuaizheng liu, lingchen sun, rongyuan wu, xindong zhang, hui zeng, lei zhang, al. ntire restore image model raim wild challenge. proceedings ieeecvf conference computer vi- sion pattern recognition cvpr workshops, bee lim, sanghyun son, heewon kim, seungjun nah, kyoung lee. enhanced deep residual networks single image super-resolution. cvprw, xiaohong liu, xiongkuo min, qiang hu, xiaoyun zhang, jie guo, al. ntire xgc quality assessment chal- lenge methods results. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, xiaoning liu, zongwei wu, florin-alexandru vasluianu, hailong yan, bin ren, yulun zhang, shuhang gu, zhang, zhu, radu timofte, al. ntire challenge low light image enhancement methods results. proceedings ieeecvf conference computer vi- sion pattern recognition cvpr workshops, yue liu, yunjie tian, yuzhong zhao, hongtian yu, lingxi xie, yaowei wang, qixiang ye, yunfan liu. vmamba visual state space model. arxiv preprint liu, yutong lin, yue cao, han hu, yixuan wei, zheng zhang, stephen lin, baining guo. swin transformer hierarchical vision transformer using shifted windows. iccv, ilya loshchilov frank hutter. decoupled weight decay regularization. arxiv preprint fangzhou luo, xiaolin wu, yanhui guo. ad- versarial neural degradation learning blind image super- resolution. neurips, alec radford, jong wook kim, chris hallacy, aditya ramesh, gabriel goh, sandhini agarwal, girish sastry, amanda askell, pamela mishkin, jack clark, al. learn- ing transferable visual models natural language super- vision. icml, bin ren, hang guo, lei sun, zongwei wu, radu timo- fte, yawei li, al. tenth ntire efficient super- resolution challenge report. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, nickolay safonov, alexey bryntsev, andrey moskalenko, dmitry kulikov, dmitriy vatolin, radu timofte, al. ntire challenge ugc video enhancement meth- ods results. proceedings ieeecvf conference computer vision pattern recognition cvpr work- shops, chitwan saharia, jonathan ho, william chan, tim sali- mans, david fleet, mohammad norouzi. image super- resolution via iterative refinement. tpami, jian sun, jiejie zhu, marshall tappen. context- constrained hallucination image super-resolution. cvpr, lei sun, andrea alfarano, peiqi duan, shaolin su, kaiwei wang, boxin shi, radu timofte, danda pani paudel, luc van gool, al. ntire challenge event-based image deblurring methods results. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, lei sun, hang guo, bin ren, luc van gool, radu timo- fte, yawei li, al. tenth ntire image denoising challenge report. proceedings ieeecvf confer- ence computer vision pattern recognition cvpr workshops, radu timofte, eirikur agustsson, luc van gool, ming- hsuan yang, lei zhang, bee lim, sanghyun son, heewon kim, seungjun nah, kyoung lee, al. ntire chal- lenge single image super-resolution methods results. cvprw, florin-alexandru vasluianu, tim seizinger, zhuyun zhou, cailian chen, zongwei wu, radu timofte, al. ntire image shadow removal challenge report. proceed- ings ieeecvf conference computer vision pattern recognition cvpr workshops, florin-alexandru vasluianu, tim seizinger, zhuyun zhou, zongwei wu, radu timofte, al. ntire ambi- ent lighting normalization challenge. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, ashish vaswani, noam shazeer, niki parmar, jakob uszko- reit, llion jones, aidan gomez, ukasz kaiser, illia polosukhin. attention need. neurips, jianyi wang, kelvin chan, chen change loy. ex- ploring clip assessing look feel images. aaai, yingqian wang, zhengyu liang, fengyuan zhang, lvli tian, longguang wang, juncheng li, jungang yang, radu timofte, yulan guo, al. ntire challenge light field image super-resolution methods results. pro- ceedings ieeecvf conference computer vision pattern recognition cvpr workshops, zhaowen wang, ding liu, jianchao yang, wei han, thomas huang. deep networks image super-resolution sparse prior. iccv, rongyuan wu, tao yang, lingchen sun, zhengqiang zhang, shuai li, lei zhang. seesr towards semantics-aware real-world image super-resolution. cvpr, bin xia, yulun zhang, shiyin wang, yitong wang, xing- long wu, yapeng tian, wenming yang, luc van gool. diffir efficient diffusion model image restoration. arxiv preprint peizhe xia, long peng, xin di, renjing pei, yang wang, yang cao, zheng-jun zha. smamba arbitrary-scale super-resolution via scaleable state space model. arxiv preprint kangning yang, jie cai, ling ouyang, florin-alexandru vasluianu, radu timofte, jiaming ding, huiming sun, lan fu, jinlong li, chiu man ho, zibo meng, al. ntire challenge single image reflection removal wild datasets, methods results. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, pierluigi zama ramirez, fabio tosi, luigi stefano, radu timofte, alex costanzino, matteo poggi, samuele salti, ste- fano mattoccia, al. ntire challenge depth images specular transparent surfaces. pro- ceedings ieeecvf conference computer vision pattern recognition cvpr workshops, dafeng zhang, feiyu huang, shizhuo liu, xiaobing wang, zhezhu jin. swinfir revisiting swinir fast fourier convolution improved training image super- resolution. arxiv preprint zhang, vishwanath sindagi, vishal patel. im- age de-raining using conditional generative adversarial net- work. arxiv preprint kaibing zhang, xinbo gao, dacheng tao, xuelong li. single image super-resolution non-local means steering kernel regression. tip, richard zhang, phillip isola, alexei efros, eli shechtman, oliver wang. unreasonable effectiveness deep features perceptual metric. cvpr, yulun zhang, kunpeng li, kai li, lichen wang, bineng zhong, yun fu. image super-resolution using deep residual channel attention networks. eccv, yulun zhang, yapeng tian, kong, bineng zhong, yun fu. residual dense network image super-resolution. cvpr, yulun zhang, kai zhang, zheng chen, yawei li, radu tim- ofte, junpei zhang, kexin zhang, rui peng, yanbiao ma, licheng jia, al. ntire challenge image super- resolution methods results. cvprw, yulun zhang, kai zhang, zheng chen, yawei li, radu tim- ofte, al. ntire challenge image super-resolution methods results. proceedings ieeecvf conference computer vision pattern recognition workshops, marcelo victor wust zibetti joceli mayer. robust computationally efficient simultaneous super-resolution scheme image sequences. tcsvt,", "published_date": "2025-04-20T12:08:22+00:00"}
{"id": "2504.12711v2", "title": "NTIRE 2025 Challenge on Day and Night Raindrop Removal for Dual-Focused Images: Methods and Results", "authors": ["Xin Li", "Yeying Jin", "Xin Jin", "Zongwei Wu", "Bingchen Li", "Yufei Wang", "Wenhan Yang", "Yu Li", "Zhibo Chen", "Bihan Wen", "Robby T. Tan", "Radu Timofte", "Qiyu Rong", "Hongyuan Jing", "Mengmeng Zhang", "Jinglong Li", "Xiangyu Lu", "Yi Ren", "Yuting Liu", "Meng Zhang", "Xiang Chen", "Qiyuan Guan", "Jiangxin Dong", "Jinshan Pan", "Conglin Gou", "Qirui Yang", "Fangpu Zhang", "Yunlong Lin", "Sixiang Chen", "Guoxi Huang", "Ruirui Lin", "Yan Zhang", "Jingyu Yang", "Huanjing Yue", "Jiyuan Chen", "Qiaosi Yi", "Hongjun Wang", "Chenxi Xie", "Shuai Li", "Yuhui Wu", "Kaiyi Ma", "Jiakui Hu", "Juncheng Li", "Liwen Pan", "Guangwei Gao", "Wenjie Li", "Zhenyu Jin", "Heng Guo", "Zhanyu Ma", "Yubo Wang", "Jinghua Wang", "Wangzhi Xing", "Anjusree Karnavar", "Diqi Chen", "Mohammad Aminul Islam", "Hao Yang", "Ruikun Zhang", "Liyuan Pan", "Qianhao Luo", "XinCao", "Han Zhou", "Yan Min", "Wei Dong", "Jun Chen", "Taoyi Wu", "Weijia Dou", "Yu Wang", "Shengjie Zhao", "Yongcheng Huang", "Xingyu Han", "Anyan Huang", "Hongtao Wu", "Hong Wang", "Yefeng Zheng", "Abhijeet Kumar", "Aman Kumar", "Marcos V. Conde", "Paula Garrido", "Daniel Feijoo", "Juan C. Benito", "Guanglu Dong", "Xin Lin", "Siyuan Liu", "Tianheng Zheng", "Jiayu Zhong", "Shouyi Wang", "Xiangtai Li", "Lanqing Guo", "Lu Qi", "Chao Ren", "Shuaibo Wang", "Shilong Zhang", "Wanyu Zhou", "Yunze Wu", "Qinzhong Tan", "Jieyuan Pei", "Zhuoxuan Li", "Jiayu Wang", "Haoyu Bian", "Haoran Sun", "Subhajit Paul", "Ni Tang", "Junhao Huang", "Zihan Cheng", "Hongyun Zhu", "Yuehan Wu", "Kaixin Deng", "Hang Ouyang", "Tianxin Xiao", "Fan Yang", "Zhizun Luo", "Zeyu Xiao", "Zhuoyuan Li", "Nguyen Pham Hoang Le", "An Dinh Thien", "Son T. Luu", "Kiet Van Nguyen", "Ronghua Xu", "Xianmin Tian", "Weijian Zhou", "Jiacheng Zhang", "Yuqian Chen", "Yihang Duan", "Yujie Wu", "Suresh Raikwar", "Arsh Garg", "Kritika", "Jianhua Zheng", "Xiaoshan Ma", "Ruolin Zhao", "Yongyu Yang", "Yongsheng Liang", "Guiming Huang", "Qiang Li", "Hongbin Zhang", "Xiangyu Zheng", "A. N. Rajagopalan"], "summary": "paper reviews ntire challenge day night raindrop removal dual-focused images. challenge received wide range impressive solutions, developed evaluated using collected real-world raindrop clarity dataset. unlike existing deraining datasets, raindrop clarity dataset diverse challenging degradation types contents, includes day raindrop-focused, day background-focused, night raindrop-focused, night background-focused degradations. dataset divided three subsets competition images training, images validation, images testing. primary objective challenge establish new powerful benchmark task removing raindrops varying lighting focus conditions. total participants competition, teams submitting valid solutions fact sheets final testing phase. submissions achieved state-of-the-art sota performance raindrop clarity dataset. project found", "full_text": "ntire challenge day night raindrop removal dual-focused images methods results xin yeying jin xin jin zongwei bingchen yufei wang wenhan yang zhibo chen bihan wen robby tan radu timofte qiyu rong hongyuan jing mengmeng zhang jinglong xiangyu ren yuting liu meng zhang xiang chen qiyuan guan jiangxin dong jinshan pan conglin gou qirui yang fangpu zhang yunlong lin sixiang chen guoxi huang ruirui lin yan zhang jingyu yang huanjing yue jiyuan chen qiaosi hongjun wang chenxi xie shuai yuhui kaiyi jiakui juncheng liwen pan guangwei gao wenjie zhenyu jin heng guo zhanyu yubo wang jinghua wang wangzhi xing anjusree karnavar diqi chen mohammad aminul islam hao yang ruikun zhang liyuan pan qianhao luo xincao han zhou yan min wei dong jun chen taoyi weijia dou wang shengjie zhao yongcheng huang xingyu han anyan huang hongtao hong wang yefeng zheng abhijeet kumar aman kumar marcos conde paula garrido daniel feijoo juan benito guanglu dong xin lin siyuan liu tianheng zheng jiayu zhong shouyi wang xiangtai lanqing guo chao ren shuaibo wang shilong zhang wanyu zhou yunze qinzhong tan jieyuan pei zhuoxuan jiayu wang haoyu bian haoran sun subhajit paul tang junhao huang zihan cheng hongyun zhu yuehan kaixin deng hang ouyang tianxin xiao fan yang zhizun luo zeyu xiao zhuoyuan nguyen pham hoang dinh thien son luu kiet van nguyen ronghua xianmin tian weijian zhou jiacheng zhang yuqian chen yihang duan yujie suresh raikwar arsh garg kritika jianhua zheng xiaoshan ruolin zhao yongyu yang yongsheng liang guiming huang qiang hongbin zhang xiangyu zheng a.n. rajagopalan abstract paper reviews ntire challenge day night raindrop removal dual-focused images. challenge received wide range impressive solutions, developed evaluated using collected real- world raindrop clarity dataset unlike existing de- li, jin, jin, wu, li, wang, yang, li, chen, wen, tan timofte challenge organizers. contact details found section corresponding authors xin li, yeying jin xin jin. authors participants ntire challenge day night raindrop removal dual-focused images. ntire website competition website codalab lisn upsaclay.frcompetitions raindrop clarity database github com jinyeyingraindropclarity raining datasets, raindrop clarity dataset di- verse challenging degradation types contents, includes day raindrop-focused, day background- focused, night raindrop-focused, night background- focused degradations. dataset divided three subsets competition images training, images validation, images testing. pri- mary objective challenge establish new powerful benchmark task removing raindrops un- der varying lighting focus conditions. total participants competition, teams sub- mitting valid solutions fact sheets final testing phase. submissions achieved state-of-the-art sota performance raindrop clarity dataset. project found iocvpr-ntire-raindrop-competition. cs.cv apr github.io. introduction image deraining long-standing research topic low-level image processing aiming remove visual artifacts caused rain streaks raindrops adverse weather conditions. plays crucial role enhancing perceptual quality images also improving performance high-level vision tasks, autonomous driving pedestrian detection. rapid develop- ment diverse backbone architectures low-level image processing several deraining benchmarks intro- duced, incorporating architectures resnet transformer mlp mamba-based frameworks moving beyond single-task benchmarks, recent studies begun explore multi-task deraining methods unified frameworks capable han- dling various adverse weather conditions simultaneously, rain, haze snow removal. multi-task de- raining methods impose new challenge model de- sign, requires higher adaptability generaliza- tion capabilities. furthermore, enhance subjective vi- sual quality diffusion-based deraining meth- ods weatherdiff re- cently emerged, demonstrating promising results open- ing new opportunities advanced research image de- raining. datasets essential evaluating effectiveness deraining algorithms. early stages, rain degradation typically synthesized capturing paired rainy clean images simulta- neously extremely challenging, even professional cameras controlled environments. address lim- itation, recent works proposed methods generate real-world rainy degradation utilizing video tempo- ral priors inserting glass adherent rain- drops sprayed water raindrops, typical rain degradation type, usually appear cam- era lenses windshields, significantly reduces im- age visibility human life, posing challenges appli- cations like surveillance autonomous driving. effec- tive raindrop removal crucial ensuring reliable perfor- mance systems. however, deraining datasets overlooked complex environment real life. par- ticular, raindrop-focused datasets, existing datasets focus capturing background scenes camera focused background. mean- while, datasets related works primarily target day- time scenarios, limited attention nighttime condi- tions address aforementioned challenges, collabo- rate new trends image restoration enhancement ntire workshop organize first challenge day night raindrop removal dual- focused images. challenge based raindrop clarity dataset contains daytime nighttime raindrop image pairs triplets. pri- mary objective promote research real-world rain removal varying lighting conditions simulta- neously restoring fine image textures degraded raindrop- induced defocus. competition attracted total participants, resulting teams submitting methods results, documented challenge report. believe challenge encourage ad- vancements research image deraining. challenge one ntire work- shop associated challenges ambient lighting normaliza- tion reflection removal wild shadow re- moval event-based image deblurring image de- noising xgc quality assessment ugc video en- hancement night photography rendering image super-resolution real-world face restoration efficient super-resolution depth estimation efficient burst hdr restoration cross-domain few- shot object detection short-form ugc video quality assessment enhancement text image gen- eration model quality assessment day night rain- drop removal dual-focused images video quality assessment video conferencing low light image en- hancement light field super-resolution restore image model raim wild raw restoration super-resolution raw reconstruction rgb smartphones challenge ntire challenge day night raindrop removal dual-focused images first competition organized advance development real-world image draining different light conditions focusing degrees. details whole challenge follows datasets dataset used challenge raindrop clarity dataset includes daytime nighttime scenes training, validation, testing. original dataset in- cludes daytime nighttime scenes training testing daytime raindrop dataset contains total paired triplet images, pairstriplets used training remaining pairstriplets test- ing specifically, among daytime training image pairstriplets, background-focused raindrop-focused nighttime raindrop dataset consists paired triplet images, pairstriplets allocated training set, re- maining pairstriplets reserved valida- tion test set. specifically, among nighttime training image pairstriplets, background-focused raindrop-focused. challenge, raindrop clarity dataset di- vided training, validation, testing subsets, training set comprises triplets, totaling images, validation testing sets contain image, respectively. ensure diversity pre- vent distribution bias, validation testing sets reorganized maintain balanced distribution various rainy scenes. particular, validation testing phases, intermediate rain-free blurry images withheld better simulate real-world application scenarios. addition- ally, rigorously evaluate robustness algorithms submitted, explicit distinction made day- night-time scenes validation testing subsets. training set challenge exactly used raindrop clarity dataset paper vali- dation set images consists daytime nighttime images, including raindrop-focused background-focused samples. test set images contains daytime images nighttime images. among daytime images, raindrop-focused background-focused. nighttime im- ages, raindrop-focused background- focused. evaluation protocol challenge utilizes three metrics measure objec- tive subjective quality restored images, i.e., psnr, ssim, lpips, respectively. final score used ranking computed reweighting metrics score psnryssimylpips, denotes psnr ssim measured channel coverting image rgb space ycbcr space. lpips, first normalize image pixel values range utilize alex net- work configuration distance measurement re- stored images ground-truth images. psnr ssim, two widely used metrics deraining restoration tasks challenge phases two phases challenge, i.e., development testing phases. details follows. development phase development phase, release triplets, totaling images raindrop clar- ity dataset, including daytime nighttime rainy images, rain-free blur images, corresponding ground-truth images, support team developing algorithms. moreover, release daytime nighttime rainy im- ages without ground truth validation. partici- pant upload restored images challenge plat- form removing raindrop developed algo- rithm. obtain corresponding final score, psnr, ssim, lpips. development phases, received submissions teams total. testing phases testing phases, release day- time nighttime rainy images without ground truth testing. ensure fairness challenge, hide leaderboard, team cannot find perfor- mance teams. final ranking achieved score eq. test stage, teams submitted final results challenge platform. end competition, received fact sheets source codes teams, utilized final ranking. challenge results summarized challenge results table among submissions, team miracle ranked highest final score achieving top perfor- mance across psnr ssim well competitive lpips score despite using rela- tively moderate parameter count ensemble extra data. place, entrovision, delivered final score achieved high psnr ssim values, along relatively low lpips model benefited use ensemble extra data. place, iirlab, achieved comparable accuracy final score psnr ssim lpips notably, accom- plished maintaining lightweight designusing parameters .gflops, despite employ- ing ensemble strategy, demonstrating excellent trade- accuracy efficiency. overall, table re- veals wide diversity model complexity, ranging lightweight solutions highly complex networks, reflect- ing different trade-offs performance resource consumption. teams methods miracle team proposes strrnet de- veloped based restormer shown fig categorize training images four classes based lighting conditions raindrop types night focus, night raindrop focus, day focus, day raindrop focus. shown fig text embedder first trained labeled training set using four categories. then, design semantic guidance module, table quantitative results ntire day night raindrop removal dual-focused images. best second results red blue, respectively. teams leader final score psnr ssim lpips params. gflops ensemble extra data rank miracle qiyu rong entrovision xiang chen iirlab conglin gou polyrain jiyuan chen hfcz kaiyi iic lab juncheng bupt cat wenjie wirteam yubo wang gurain wangzhi xing bit ssvgg hao yang cisdiinfo-mfdehaznet qianhao luo mcmaster-cv han zhou falconi taoyi dfusion yongcheng huang rainmamba hongtao raindropx abhijeetkumar cidaut marcos conde dgl deraindrop guanglu dong xdu shuaibo wang edgeclear-dnsst team jieyuan pei mplnet jiayu wang singularity subhajit paul viplab tang agent kaixin deng x-l zeyu xiao uit-shanks nguyen pham hoang one ronghua dualbranchderainnet yuqian chen qwe yihang duan visual signal information processing team suresh raikwar zheng family group jianhua zheng rainclear pioneers qiang figure framework strrnet, proposed team miracle figure semantic guidance module strrnet, proposed team miracle figure training strategy strrnet, proposed team mir- acle added end restormer encoder. module utilizes encoded image features restormer guide decoder performing distinct image restora- tion operations four different types images. additionally, introduce background restoration subnetwork output restormer, consists multiple convolutional layers enhance image details. training strategy strrnet illustrated fig first, train pre-trained model original training dataset. model used perform inference frames within scene test set. inferred images may still contain residual raindrops artifacts. since background remains consistent across different time frames raindrop positions vary temporally, motivates perform median fusion across multiple frames scene obtain median-fused image. due dynamic nature raindrop artifacts, inconsistent locations across frames typically prevent appearing median values, whereas stable background information preserved. treat median-fused image pseudo ground truth use semi-supervised fine-tuning phase enhance models raindrop removal capability unlabeled images. training description adam optimizer used training, total iterations. learning rate set first iterations gradually decays remaining iterations. images randomly cropped fixed size network training, geometric image augmentation applied. network optimized using loss function multi-scale ssim loss function, weights respectively. experiments conducted rtx testing description use sliding window move across image, applying model rain removal window. set sliding window size overlap then, obtain median image applying median fusion images scene. finally, perform weighted sum median image original image obtain final output. entrovision figure overview technique proposed team entrovi- sion raindrop removal. team utilizes two-stage approach achieve rain- drop removal challenge. technical overview shown fig. given impact multi-scale fea- tures image deraining pre-train derain- ing model first stage using msdt rain- drop clarity training set. enhance models gen- eralization capability, additional pre-training conducted uav-raink dataset. then, pre-trained de- raining model utilized obtain pseudo-ground-truth images testing images test-time learning second stage. second stage process, performed fine- tuning using test samples generated paired pseudo ground-truth images. approach provides clear direction transferring pretrained knowledge, rather simply relying models generalization ability. subse- quently, process testing inputs using fine-tuned deraining model obtain final output results. notably, designed deraining network specifically according characteristics dataset used. address sce- narios blurry clear backgrounds might coexist, employ median filtering preserve edge information avoid excessive blurring. two-stage processing strategies specially designed median fil- tering technique, obtain clear deraining results. iirlab figure pipeline method proposed team iirlab team observed raindrop dataset used challenge differs significantly existing raindrop removal datasets. traditional datasets primarily focus background clarity image shooting, resulting clean backgrounds blurry raindrops foreground. cases, simply removing raindrops sufficient recover clear background. however, dataset challenge includes notable portion images camera focused raindrops shooting, lead- ing clean raindrops blurred backgrounds. in- troduces new challenge removing raindrops also mitigating background blur caused defocus, ultimately recover clean draining image. based new question, team chosen histoformer network raindrop removal model, shown fig histoformer transformer- based model designed restore images degraded se- vere weather conditions. incorporates histogram self- attention mechanism, sorts segments spatial fea- tures intensity-based bins applies self-attention within bin. enables model focus spa- tial features across dynamic intensity ranges handle long-range dependencies similarly degraded pix- els. built upon restormer histoformer well-suited addressing raindrop artifacts defocus blur, making strong candidate task. based architecture prior performance, team chose histo- former core model challenge. training details. team utilized image pairs pro- vided training set, consisting raindrop-degraded images drop corresponding clean background images clean. this, extracted image pairs validation, including daytime night- time samples. training, employed two-stage training strategy combines regular training sub- sequent fine-tuning. first stage, draining model trained iterations using default histo- former configuration. second stage, model fine-tuned additional iterations using loss function. progressive training approach effec- tively enhanced model performance, leading improved final results. implememtation details. implementation based pytorch conducted nvidia rtx gpu. network trained total iter- ations, initial batch size patch size following progressive learning strategy. team employed adamw optimizer initial learn- ing rate first iterations, gradually reduced using cosine annealing schedule remaining iterations. num- ber blocks stage set li,,, channel dimension fixed channel expansion factor dgff module set number self-attention heads stage configured respectively. data augmen- tation, horizontal vertical flips applied randomly training. polyrain figure pipeline method proposed team polyrain team initialized trained dense x-restormer model given dataset based restormer first training stage, finetuned model larger patch size different loss functions. whole framework team shown fig. training details. training process consists two phases. first phase, patch size training im- age set batch size total training iterations. learning rate set loss used loss function. second phase, model fine-tuned learning rate im- age patch phase, model trained simultaneously using loss, lpips loss, ssim loss weights respectively, iterations. testing details. enhance robustness model, self-ensemble technique employed. implementa- tion references basicsr library implementation details. method implemented based famous basicsr framework written python. utilized adamw optimizer initial learning rate eight gpus used model training, lasting hours iter- ations. addition, cosineannealingrestart-cycliclr scheduler chosen restart learning rate setting use efficient optimiza- tion strategy extra datasets. hfcz team utilizes rddm deraining back- bone trained patch size subse- quently, self-ensemble strategy used edsr improved replacing average ensemble dual kmeans fusion fig. experimental re- sults indicate employing dual kmeans fusion increases score rddm baseline baseline baseline baseline final result augment kmeans fusion kmeans fusion kmeans fusion kmeans fusion augment augment figure dual kmeans fusion raindrop task proposed team hfcz. dual kmeans fusion. given clean image igt, several raindrop blur degradations added igt, obtaining images im. shown fig. taking example, serve flipping rotating augmen- tations, generating images in. stage- baseline model processes images obtains prelim- inary results baselineii, images flipped rotated fixed angle kmeans fusion performed obtain stage- af- ter performing fusion images get results rm. perform kmeans fusion results get final result image. training testing details. training dataset pro- vided challenge used model training. im- prove generalization, data augmentation techniques, includ- ing rotation flipping applied. model trained iterations using adamw optimizer pa- rameters single nvidia gpu. training conducted batch size learning rate patch size inference, authors adopt dual k-means fusion strategy enhance models performance. ex- perimental results indicate employing dual kmeans fusion increases score rddm baseline specifically, kmeans fusion stage one increases score baseline kmeans fusion stage two increases score baseline iic lab team developed effective frequency-aware mamba-based network image deraining, named fa- mamba, shown fig. specifically, key component proposed framework wavelet domain restora- tion module wdrm contains dual-branch fea- ture extraction block dfeb superior local per- ception global modeling capabilities prior-guided figure pipeline fa-mamba proposed team iic lab. module pgm provides refined texture detail guidance feature extraction. worth mentioning re- fined texture details obtained enhancing input high-frequency information high-frequency enhancement module hfem. training details. training, utilized adam optimizer batch size patch size total iterations. initial learning rate fixed iterations, decreased iterations. data augmentation techniques ap- plied. entire framework performed pytorch nvidia geforce rtx gpu, works end-to-end learning fashion without costly large-scale per- taining. bupt cat consistent patch transformer consistent patch transformer consistent patch transformer norm project norm project project fin fout attention map fin element-wise addition element-wise multiplication matric multiplication gelu activation reshape cpsa ffn consistent patch transformer consistent patch transformer consistent patch transformer consistent patch self-attention cpsa feed-forward network ffn down, up-sample fout iin iout aggregation expand figure pipeline method proposed team bupt cat. shown fig. team introduces novel con- sistent patch transformer cpt dual-focused day night raindrop removal task, leverages unet-based architecture designed enhance spatial consistency feature representation capability. framework com- prises multiple consistent patch transformer blocks, consisting two key components consistent patch self- attention cpsa feed-forward network ffn. model utilizes test time local converter tlc mech- anism effectively revisit global information aggrega- tion ensure robust consistent feature learning different patch sizes training testing. cpsa module responsible capturing long-range depen- dencies spatially consistent local details. instead using traditional window-based attention mechanisms, cpsa module integrates tlc-based feature aggregation scaling strategy maintains consistent patch sizes training testing, reducing spatial inconsistencies training testing. training details. reduce training gpu memory, team augments input data randomly cropping input image patches size performing strategies random rotation. testing details. testing stage, self- attention part, use tlc strategy segment aggregate full image series patches size training patch, rest model full image. setup effectively improve inconsis- tency models patch size training testing, especially self-attention part. implementation details. team utilizes pytorch framework nvidia geforce rtx dur- ing training, set total batch size initial learning rate scheduler iterations, patch size set loss function, use lloss fourier loss con- strain model weights respectively. train framework using adam optimizer set number channels network. total training duration ap- proximately hours. training test sets official datasets provided dual-focused day night rain- drop removal challenge. wirteam figure pipeline method proposed team wirteam. inspired recent advancements image restoration image deraining, team propose novel multi-scale prompt-based image deraining approach mpid incorporates local global attention mechanisms, shown fig. specifically, given degraded image rhw model produces corresponding clear image rhw -level encoder-decoder framework. shadow two layers, employ residual hybrid attention groups rhag capture detailed local features. deeper layers, integrate transformer blocks facilitate cross-channel global feature extraction. encoding phase, acknowledging advantages utilizing images various resolutions deraining tasks additionally incorporate multi-scale image information original resolution enhance auxiliary information encoding process. decoding, considering dual focus raindrop-focus background-focus images within raindrop clarity dataset introduces raindrop occlusions background blurringthey introduce specialized prompt mechanism components designed address decouple different types degradation factors. notably, prompt block, integrates prompt generation module pgm prompt interaction module pim, utilizes image-specific cues effectively guide image reconstruction process, thereby improving clarity quality restored images. training details. employ end-to-end training methodology, training model epochs. training procedure based adamw optimizer decay parameters initial learning rate gradually reduces cosine annealing strategy. horizontal vertical flips adopted data augmentation. furthermore, training merely based raindrop clarity dataset provided competition organizers, without use extra datasets pretrained models. testing details. testing phase, preprocess input images padding multiple ensures dimensions input images compatible architecture method. dont resort means test-time augmentation testing phase. gurain team addresses dual degradation challengerain blurby unifying training data single frame- work using vanilla restormer, originally designed single degradation restoration, introducing dy- namic rain-aware weighted loss, shown fig. instead employing dual-input network struggles differentiate blur rain, merge rainy blurry images one input paired clear ground truth, allowing restormer learn common mapping degradations. moreover, recognizing rain streaks affect parts image, adaptive loss function figure comparison single-degradation restormer dual-degradation method proposed team gurain. top row shows original restormer handling drop- blur- degraded images separately using standard loss. method team gurain bottom left unifies degradations within single encoder-decoder framework introduces dynamic, rain-aware weighted loss better emphasize challenging re- gions. architecture bottom right retains restormer backbone enhancing ability handle degradations jointly. degraded image predict image image blurry situation rainy situation figure visualization method proposed gurain validation image. top three rows deblurry. bot- tom three rows deraining. columns left right degraded image, predicted image ground truth image. assigns higher weights rainy regions lower weights clean areas, thereby guiding network focus challenging parts image. integrated approach leads effective deraining simultaneously mitigating blur, resulting improved overall image restora- tion performance shown fig. training testing details. train default restormer using custom dynamic, rain-aware weighted loss merged rainyblurry inputs paired clear ground truths, progressive patch size scal- ing. custom loss emphasizes challenging rainy re- gions, yielding improved quantitative metrics visu- ally cleaner, less blurred results preserving effi- ciency restormer inference, standard restormer pipeline processes single degraded images. bit ssvgg figure pipeline method proposed team bit ssvgg. figure visualizations partial image restoration results team bit ssvgg. illustrated fig method designed oper- ate rain-degraded images generate corresponding clean outputs. follows encoderdecoder architecture. given input degraded image, first apply con- volution extract initial features, processed transformer module global receptive field. since rain streaks often appear large unevenly distributed regions, essential network capture long-range dependencies. address this, adopt attention mechanism restormer enables efficient global context modeling reduced computa- tional complexity. subsequently, features fed series cnn-based modules enhance local feature rep- resentation compensate transformers limited ability model fine-grained structures. cnn blocks built upon nafnet chosen lightweight de- sign effectiveness. that, features re- fined another transformer block finally passed convolution layer. output added input image residual manner produce re- stored image. preserve spatial information encod- ing decoding, employ pixel-unshuffle pixel- shuffle operations downsampling upsampling, re- spectively, cnn block. prevents informa- tion loss resolution changes. integrating transformer cnn components, hybrid architec- ture leverages strengths global context aggrega- tion local detail preservation. moreover, due im- proved generalization ability reduced tendency over- fit small datasets, model effectively trained solely provided benchmark dataset without requiring additional data. implementation details. model implemented python using pytorch framework version ... training conducted nvidia rtx adopt adamw optimizer learning rate scheduled using cosine annealing strategy, gradually decaying course training. training dataset exclusively provided competition organizer, additional external data used. trained model epochs approx- imately hours batch size patch size standard data augmentation techniques applied, including random horizontalvertical flipping random rotations improve generalization. cisdiinfo-mfdehaznet figure framework method proposed team cisdiinfo- mfdehaznet. fig. team proposes effective lightweight model applied industrial site, named mfdehaz-net, two specially de- signed components, i.e., multi-scale fusion block point-depth wise block, helping achieve deep fusion image features different scales obtain better global un- derstanding following training description model proposed imple- mented pytorch..cuda. trained epochs nvidia geforce rtx gpu. batch size learning rate set .,respec- tively, number warmup epochs adam selected optimizer. random rotation horizontal inversion also used data augmentation methods. testing description testing, images resized fed loaded pre- trained model. mfdehaz-net two specially designed components, i.e., multi-scale fusion block mf-block point-depth wise block pd-block, help mfdehaz-net achieve deep fusion image features different scales obtain better global features. besides, reduce damage original color image caused image dehazing, mfdehaz-net integrates supervision signal frequency domain specially designed loss function. mcmaster-cv figure framework method proposed team mcmaster-cv. shown fig. framework team based esdnet backbone primarily con- sists encoder-decoder network. encoder decoder level, semantic-aligned scale-aware module sam incorporated address scale variations. ad- ditionally, team introduces histogram transformer block employs histogram self-attention dynamic range spatial attention. block placed be- tween encoder decoder achieve global effi- cient degradation removal. besides, network training, designed loss function based solely single level, corre- sponds original image resolution lloss ercep ercep represent charbonnier loss perceptual loss respectively. weighting factor set training testing details. team trained model single nvidia tigb vram gpu. training, set batch size patch size training strategy, following restormer, adopted cosineannealingrestart- cycliclr scheduler, adjusts learning rate using cosine annealing schedule restarts promote bet- ter convergence escape local minima. specifically, training consists two cycles periods iterations, respectively, minimum learn- ing rates learning rate reset initial value beginning cycle. en- hance generalization, incorporated mixup-based data augmentation, training samples linearly com- bined using beta distribution shape parameter additionally, enabled use identity mapping retain original samples training. gen- erator optimized using adam optimizer learn- ing rate standard momentum parameters testing phase, directly fed input model obtain final output. falconi figure proposed unified framework joint image derain- ing deblurring team faconi. framework composed four stages context classification, adaptive deraining, multi- task feature extraction, iterative deblurring adaptive fusion. team presents unified framework image de- raining synergistically combines classification, multi- task learning, adaptive fusion. approach lever- ages pre-trained models manage complexity en- hance performance. specifically, mobilenetv-based classifier initially categorizes images day night, guid- ing subsequent processing. pre-trained diffusion trans- formerdit, fine-tuned day night scenarios, serves core component night image adaptation. deblurring, incorporate fftformer. finally, explore traditional u-net-based fusion strategies combine intermediate outputs. entire framework trained using combination curated external datasets daynight classification dit adaptation, alongside ded- icated datasets deblurring synthetically generated samples intermediate deraining stages. shown fig. training pipeline comprises four interconnected stages stage daynight classification. initialize mobilenetv-based classifier discriminate day night scenes, providing foundational context sub- sequent stages. stage adaptive dit model. extend pre- trained dit model, initially designed night im- agery, fine-tuning mixed dataset day night images. stage incorporates pre-trained classifier stage dynamic weighting strategy, enabling context-aware learning based time day. stage multi-task branching network. construct three-branch multi-task network. mobilenetv clas- sifier stage forms one branch. penultimate layer adapted dit model stage serves shared feature backbone. two parallel, final-layer branches, specialized day night conditions respectively, in- stantiated. training, shared backbone classification network remain frozen optimization re- stricted day- night-specific branches. clas- sifier output directly dictates active branch given input. stage iterative deblurring refinement. integrate fftformer deblurring network using two-round fine-tuning process. first round utilizes dataset blurred images. second round leverages synthetic dataset generated deraining outputs dit model stage thereby aligning deblurring pro- cess specific characteristics derained images. inference, input image undergoes two-stage process deraining followed deblurring. first, image processed deraining pipeline stage subse- quently enhanced deblurring network stage effectively integrate complementary information stages, employ dual-fusion approach frequency-domain fusion. method combines de- rained deblurred images adaptively weighting frequency components based sharpness, noise, edge information, prioritizing low-frequency content derained image high-frequency details de- blurred image. learned fusion. method utilizes u-net-based neu- ral network trained diverse dataset model out- puts, learn non-linear mapping optimally fuses dataset value validation test table final evaluation results image deraining de- blurring framework team falconi. derained deblurred images, implicitly addressing weaknesses individual processing stage. ta- ble presents final evaluation results image deraining deblurring framework, validation test values reported. validation score indi- cates models strong generalization capability training phase, test score reflects performance previously unseen data. relatively small difference validation test results sug- gests model effectively avoided overfitting, main-taining robust performance across different datasets. results validate effectiveness unified frame- work, integrates daynight classification, adaptive dit model, multi-task branching, iterative deblurring refinement. team developed method based dit model, utilizing adam optimizer learning rate executed training process sin- gle nvidia gpu gb. proposed raindrop clarity dataset exclusively employed, pairs drop, clear used stages pairs blur, clear utilized stage stage trained binary classi- fier, achieving convergence minutes. stage required hours training, stage took hours. finally, stage involved training u-net, com- pleted less minutes. throughout training pro- cess, learning rate optimization employed part training strategy. dfusion figure implementation structure team dfusion. proposed dfusion method dual-branch fusion framework leverages complementary strengths two state-of-the-art pretrained modelsrdiffusion dit effectively remove raindrops dual- focused images. illustrated fig. method first generates two intermediate restored images, excelling different visual aspects example, one may deliver smooth textures preserves fine details. intermediate outputs stacked passed lightweight fusion cnn, specifically trained combine best qualities inputs. final model optimized using joint loss function in- tegrates psnr, ssim, lpips, ensuring high quan- titative accuracy strong perceptual quality. enhance computational efficiency, input images processed overlapping patches. patches re- stored separately pretrained models re- assembled prior fusion step. patch-based strat- egy speeds inference also helps preserve local details across image. fusion approach draws inspiration mixture- of-experts strategies adapting raindrop re- moval task. fusing complementary features two high-performance models, dfusion able achieve robust restoration results daytime nighttime scenarios, shown fig. figure result comparison team dfusion. training details. fusion network trained using sin- gle rtx laptop. adam optimizer used initial learning rate training, in- put image resolution split six overlap- ping patches size model optimized joint loss function combines psnr, ssim, lpips. testing details. inference stage, intermediate out- puts rdiffusion dit first generated. outputs concatenated multi-channel tensor subsequently processed lightweight fusion cnn. process results final restored image effectively preserves global structure fine local details. figure network architecture team rainmamba. rainmamba team proposes video restoration framework adapt state space models day night raindrop removal tasks, shown fig. approach ap- plies global scanning mechanism causally process temporal data linear complexity. core innova- tion method lies equipping state space mod- els ssms hilbert scanning mechanism, achieves localized scanning across temporal spa- tial dimensions. given sequence rainy video frames, cascading coarse-to-fine mamba module cfm re- ceives encoded features input causally models temporal corrections using improved state space models. cfm employs global mamba block gmb local mamba block lmb capture sequence-level global local spatio-temporal dependencies. develop novel hilbert scanning paradigm lmb promote mambas locality learning. enhance visual quality re- stored results, adopt combination psnr loss perceptual loss training process. training details. network trained nvidia rtx gpus implemented pytorch plat- form. training iteration, input frame randomly cropped spatial resolution number frames per video clip total number training iterations adopt adam optimizer polynomial scheduler power initial learn- ing rate network set batch size warm-up start iterations. testing details. testing phase, take two frames video segment input input full size frame. raindropx team proposes enhanced version restormer introducing additional constraints improving percep- tual structural integrity restored images. unlike original restormer model, trained indi- figure diagram proposed method team rain- dropx. vidual restoration tasks using loss, proposed method incorporates lpips ssim losses pre- serve perceptual quality, edge preservation, texture con- sistency. training process conducted two stages, starting fine-tuning pretrained weights blurred output prediction transitioning predicting clean images added perceptual losses. overall frame- work shown fig. training details. model trained using pretrained restormer weights two-stage training approach. stage loss used predict blurred outputs de- graded inputs containing raindrops blur. stage lpips ssim losses introduced alongside loss optimize clean image restoration. training con- ducted raindrop clarity dataset model trained using single gpu. cidaut figure proposed dropfir team cidaut ai. team proposes dropfir, encoder-decoder ar- chitecture inspired darkir tailored raindrop removal, shown fig. model introduces droploc stage based custom fourier-attention-fusion block fafblock, adapted flol extract spatial drop mask. predicted mask guides derain- ing stage, enabling decoder remove localized drops blur image restoration. training details. model trained using adamw optimizer weight decay cosine annealing learning rate schedule starting decaying training conducted two phases -epoch pretraining random crops, fol- lowed epochs full-resolution images. model trained four nvidia gpus batch size using raindrop clarity dataset dgl deraindrop figure architecture proposed gsastep framework team dgl deraindrop. team proposes global semantic attention gsa based two-step dual-focused image raindrop removal method gsastep, depicted fig. adopt two-step approach break dual-focused image raindrop removal task two sub-tasks raindrop removal clean image restoration. first step, use -width nafnet remove focused defo- cused raindrops. second step, employ global semantic attention gsa module utilizes clip semantic features guide -width nafnet recon- structing final clean image background ob- tained step gsa mechanism extracts semantic features using clip image encoder effectively in- tegrates encoded features obtained nafnet encoder guide subsequent decoding pro- cess. architecture addresses various degradations in- cluding focused defocused raindrops, well defo- cused backgrounds. training details. employ multi-stage training ap- proach reduce networks learning difficulty. initially, pretrain -width nafnet using drop blur subsets training dataset loss. subse- quently, freeze pretrained nafnet train en- tire framework using drop clear subsets, still loss. finally, unfreeze parameters first nafnet jointly fine-tune entire framework us- ing combination loss, ssim loss, perceptual loss weight coefficients re- spectively. model trained using adam opti- mizer learning rate initially set halved every iterations, final fine-tuning us- ing fixed learning rate training conducted single nvidia rtx gpu pytorch approximately days epochs additional datasets utilized. xdu figure overall framework team xdu team adopts two-stage architecture combining mask prediction network transformer-based restora- tion model, depicted fig. first stage, residual convolutional network used predict rain- drop mask input raindrop image, supervised loss using ground truth masks obtained differ- ence raindrop blur images. sec- ond stage, predicted mask fused features within transformer-based deraining network fftformer mask features concatenated block. final output optimized mse loss clear image. training details. training conducted two stages. stage one trains mask prediction network epochs using loss. stage two trains mask-prior-guided transformer restoration network epochs using mse loss. predicted mask stage one used addi- tional guidance throughout transformer layers. ad- ditional datasets pre-training used. testing details. subset image pairs training set used testing. inference, rain- drop image processed jointly mask prediction net- work fftformer evaluation based psnr, ssim, lpips metrics. edgeclear-dnsst team proposes edgeclear-dnsst, transformer- based encoder-decoder model designed raindrop re- moval dual-focused images. model integrates sparse-sampling attention efficient long-range de- pendency modeling, combined specialized modules including raindropedgeenhancer, pyramidattention, raindropfeaturemodulation address fine-grained rain- drop degradation. network adopts multi-stage struc- ture four encoder decoder stages, enhanced skip connections multi-scale feature fusion. training details. model trained scratch us- ing day night images raindropclarity dataset split training-validation ratio. training process utilizes adam optimizer ini- tial learning rate scheduled cosine annealing tmax, min batch size per gpu gradient accumulation accumulate grad batches. data augmentation includes random horizontal flips rotations. combination y-channel psnr loss, ssim loss, lpips perceptual loss, difference-weighted loss used. early stopping strategy patience applied based validation psnr. testing details. inference, employ sliding window strategy window size pixel overlap handle images arbitrary resolution. weighted averaging method used window edges suppress boundary artifacts. bilinear interpolation aligned corners adopted mitigate edge color anomalies. mplnet team builds upon multi-stage fusion network mpr- net enhances performance non-uniform illumination conditions integrating global illumination detection module illumadjust illumadjust gen- erates single-channel illumination map series convolution attention layers, embedded encoder-decoder framework via custom-designed unimetaformer unit. unimetaformer module, based metaformer replaces cab modules mpr- nets encoder decoder. comprises dynamic tanh normalization layer dyt illumination-guided channel attention block icab, convolution-based mlp. icab integrates channel spatial attention mech- anisms guided illumination map, enhancing models adaptability illumination variations. net- work retains mprnets three-stage progressive restoration employs supervised attention modules sam stage-wise feature fusion. original resolution block orb used final stage refine output. training details. team trained model using dataset loss function combines char- bonnier loss, edge loss, lpips loss balance pixel- level accuracy perceptual quality. performed vali- dation epoch saved model checkpoints based best psnr lpips scores, well fixed in- tervals. gradient clipping applied stabilize training. experiments conducted nvidia gpu. singularity figure overall pipeline proposed team singularity. team proposes two-step pipeline address rain- drop blur removal challenges, depicted fig. approach leverages distinct characteristics rain- drops blurring artifacts, require different han- dling strategies. separating tasks two stages, apply specialized techniques effectively remove type artifact, leading higher-quality image re- construction. raindrop removal, utilize fft- former effectively handle irregular patterns occlusions caused raindrops. blur removal, de- velop enhanced version fformer leveraging fractional fourier transform frft non-uniform de- blurring, shown fig. blur removal framework consists two components fhtb fdtb. specif- ically, fhtb includes fdrb similar frb fdtb, use deformable convolutions adapt varying motion blur patterns, improving edge restora- tion. fdtb uses frft-based attention mechanism, inspired integrates quantization fft- formers fsas module focus relevant frequency com- ponents, ensuring efficient handling blur pattern multiple levels. details illustrated fig. training details. team trains raindrop blur removal models separately. two models trained figure overall architecture deblurring proposed team singularity. figure self-attention mechanism fractional frequency aware self-attention fsa, frequency quantized feed forward network fq-ffn proposed team singularity. following strategy nafnet fformer use standard data augmentation, adam op- timizer default settings, learning rate e-. update model cosine annealing strategy iterations. models trained images batch size patch size used self-attention. apply loss spatial frequency domains, adversarial training applied iterations improve perceptual quality. ex- periments conducted nvidia gpu, training configurations modified nafnet. viplab team utilizes efficient unified framework two-stage training strategy explore weather-general weather-specific features separation. first train- ing stage aims learn weather-general features tak- ing images various weather conditions inputs generating coarsely restored results. second training stage aims learn adaptively expand spe- cific parameters weather type deep model, requisite positions expanding weather-specific parameters automatically learned. finally, adopt nafnet enhance textures. training details. optimize model epochs using adamw optimizer learning rate e-. employ data augmentation techniques, including random cropping flipping. agent figure architecture loss computation optimiza- tion module team agent. team builds model based pre-trained dit model. propose cost-effective effi- cient fine-tuning approach dual-focused day-and-night raindrop removal optimizing loss function. specif- ically, introduce adaptive loss function inte- grates structural similarity ssim vgg-based per- ceptual losses. ssim-based weighting dynamically emphasizes regions significant texture differences be- tween clear blurry images. meanwhile, percep- tual loss leveraging vgg features ensures visually realis- tic outcomes. additionally, consistency constraint in- corporated stabilize noise estimation clear blurred branches. process illustrated fig. utilize additional datasets, including lhp-rain divk data augmentation enhance models generalization capability unseen scenarios. training details. fine-tune pre-trained dit model provided official repository using adam optimizer initial learning rate epochs. resize original training images two scales enhance generaliza- tion ability model towards diverse resolutions. randomly crop training patches training. optimize dit model linear beta scheduling strategy ranging diffusion steps. additionally, develop hybrid loss function ssim-based adaptive weighting, vgg perceptual loss, consistency constraints, significantly improving models detail recovery capability visual quality restored images. testing details. testing phase, employed im- plicit sampling method sampling steps achieve efficient high-quality raindrop removal. output im- ages pixels. mitigate boundary artifacts introduced patch stitching, utilize grid-based overlapping strategy -pixel overlap image reconstruction. unlike training phase, crop patches testing ensure comprehensive global evaluation full-resolution images. conduct evaluations single nvidia gpu. x-l enc permuted self- attention block residual channel attention block convd psa-rca layer psa-rca psa-rca dec figure overview proposed method team x-l. team develops model following popu- lar encoder-decoder paradigm. propose enhance middle features combination permuted self- attention blocks psa residual channel at- tention blocks rca shown fig. psa-rca layer comprises two parallel branches one branch consists residual channel attention blocks designed capture local features, branch includes permuted self-attention blocks psab model global relationships low computational complexity. dual-branch architecture effectively integrates local global information, thereby enhancing precision efficiency feature extraction. following processing three psa-rca layers, refined features passed fea- ture decoder additional processing reconstruction, ultimately yielding clear detailed output. training details. model trained using pro- vided dataset, loss function employed op- timize training process. additional datasets utilized training. training conducted single nvidia gpu. testing details team performed ensemble test data, adopting strategy edsr, involves rotating images different angles achieve this. method enhances robustness quality final out- put generating predictions multiple perspectives. integrating predictions various angles, model better capture details structural informa- tion images, significantly improving performance image processing task. uit-shanks figure proposed method team uit-shanks. team proposes two-stage approach handle challenge. first stage, develop model based refusion second stage, opti- mize model incorporating gan-based training strategy, semi-supervised dataset derived test data. generate dataset training model af- ter epochs. build discriminator gan training phase based unet architecture. overall framework depicted fig. training details. first stage, adopt adamw optimizer cosineannealing scheduler optimize model. use mse loss obtain fidelity-oriented model. second stage, adopt gan-based training strategy standard gan loss vgg per- ceptual loss. additionally, leverage multiscale loss, ssim loss mask loss optimize model. finish training two gpus gpu kaggle platform. one team presents two-stage approach day night raindrop removal dual-focused images. method processes raindrop removal defocus-blurring separately. employ retrained restormer model specifically raindrop removal first stage, uti- lizing fine-tuned ipt model address defocus- blurring issues present second stage. maintain hyperparameters original restormer pa- per, number channels set respectively. implementation ipt, re- train head tail components preserving core transformer blocks fixed. training details. apply training process day night images. however, split dataset two functional subsets one containing raindrops one blur correction. develop two separate mod- els rather pursuing end-to-end solution. restormer model trained scratch specif- ically raindrop removal using challenge dataset, ipt model leveraged pretrained weights fine-tuning limited head tail components defocus-blurring subset. optimize model fol- lowing original restormer approach, adopting patch size ipt training. accomplish training nvidia geforce rtx gpus. testing details. testing pipeline consists two se- quential processing stages. first, apply trained restormer model remove raindrops input images. subsequently, feed processed images fine-tuned ipt model restore defocus-blurring. inference costs approximately single gpu proposed testing pipeline. dualbranchderainnet rainy input -ch encoder convblockspool bottleneck convblock decoder upsampleconv predicted mask -ch masknet depthwise separable u-net rainy input -ch encoder convblockspool bottleneck convblock decoder upsampleconv multiscalefusion rdb residualdenseblock restored output -ch dualbranchderainnet u-net mask guidance mask guidance conv lightattn predicted mask guided features input figure network architecture proposed team dual- branchderainnet. zoom better view. team proposes two-stage framework comprising mask estimation network masknet dual-branch deraining network dualbranchderainnet, shown fig. masknet predicts rain masks learning difference rain-focused blur- focused inputs. dualbranchderainnet takes input image predicted mask, using lightweight u-net multi-scale fusion channel attention tech- niques remove raindrops restore clear details. also adopt wgan-gp-based discriminator improve perceptual quality restored results, along fre- quency gradient losses faithful restoration. training details. train proposed model epochs using adam optimizer, batch size initial learning rate pcie gpu. adopt combination losses optimize model, including ssim, wgan-gp, frequency domain, gradient-based losses. approximate training time ob- tain model hours. testing details. evaluate model directly inferring full-resolution images. final output blend original input learned residual, guided predicted raindrop mask. qwe team develops method upon previous back- bone, transweather leveraging prior knowledge pre-trained model applying continuous learn- ing competition dataset. model architecture, algorithms, module structure remain consistent outlined original transweather paper. how- ever, preprocess dataset align specific scenarios competition introduce modifications loss function training process. training details. follow officially released code transweather fine-tune model epochs rtx gpus. acknowledgments work partially supported nsfc grant china postdoctoral science foundation-anhui joint support program grant number tah. thank challenge sponsor eastern institute advanced study, ningbo. work partially supported humboldt foundation. thank ntire sponsors bytedance, meituan, kuaishou, university wurzburg computer vision lab. organizers title ntire challenge day night raindrop removal dual-focused images members xin xin.liustc.edu.cn, yeying jin, jinyeyingu.nus.edu, xin jin jinxineitech.edu.cn, zongwei zongwei.wuuni-wuerzburg.de, bingchen lbcmail.ustc.edu.cn, yufei wang ywangsnap.com, wenhan yang yangwhpcl.ac.cn, liyuidea.edu.cn, zhibo chen chenzhiboustc.edu.cn, bihan wen bihan.wenntu.edu.sg, robby tan robby.tannus.edu.sg, radu timofte radu.timofteuni-wuerzburg.de affiliations university science technology china national university singapore tencent eastern institute technology, ningbo computer vision lab, university wurzburg snap research pengcheng laboratory idea nanyang technological university miracle title semantics-guided two-stage raindrop removal network members qiyu rong buu.edu.cn, hongyuan jing, mengmeng zhang, jinglong li, xiangyu lu, ren, yuting liu meng zhang affiliations beijing union university entrovision title two-stage multi-scale transformer day night raindrop removal members xiang chen chenxiangnjust.edu.cn, qiyuan guan, jiangxin dong, jinshan pan affiliations nanjing university science technology dalian polytechnic university iirlab title raindrop removal method based histoformer members conglin gou gou conglintju.edu.cn, qirui yang, fangpu zhang, yunlong lin, sixiang chen, guoxi huang, ruirui lin, yan zhang, jingyu yang, huanjing yue affiliations tianjin university xiamen university hong kong university science technology, guangzhou university bristol, national university singapore, singapore polyrain title finetuning dense x-restormer image deraining members jiyuan chen jiyuan.chenconnect.polyu.hk, qiaosi qiaosi.yiconnect.polyu.hk, hongjun wang, chenxi xie, shuai li, yuhui affiliations hong kong polytechnic university university tokyo hfcz title dual kmeans fusion raindrop task members kaiyi ma, jiakui jiakuihugmail.com affiliations xidian university iic lab title fa-mamba members juncheng junchenglishu.edu.cn, liwen pan, guangwei gao affiliations shanghai university nanjing university posts telecommunications bupt cat title consistent patch transformer dual-focused day night raindrop removal members wenjie lewjgmail.com, zhenyujin, heng guo, zhanyu affiliations beijing university posts telecommunications wirteam title mpid image deraining multi-scale prompt- based learning members yubo wang wangyubostu.hit.edu.cn, jinghua wang affiliations harbin institute technology shenzhen gurain title unified image restoration rain blur using restormer dynamic rain-aware weighted loss members wangzhi xing w.xinggriffith.edu.au, anjus- ree karnavar, diqi chen, mohammad aminul islam affiliations griffith university bit ssvgg title hybrid network cnn transformer rain- drop removal members hao yang bit.edu.cn, ruikun zhang, liyuan pan affiliations beijing institute technology cisdiinfo-mfdehaznet title mfdehaznet-an easily deployable image dehazing model industrial sites members qianhao luo qianhao.luocisdi.com.cn, xincao xin.a.caocisdi.com.cn affiliations cisdi information technology co., ltd mcmaster-cv title rainhistonet single-image day night rain- drop removal via histogram-guided restoration members han zhou zhouhmcmaster.ca, yan min, wei dong, jun chen affiliations department electrical computer engineering, mc- master university falconi title title members taoyi taoyiwugmail.com, weijia dou, wang, shengjie zhao affiliations tongji university dfusion title dffusion new method fuse existing solutions simple cnn members yongcheng huang y.huang- student.tudelft.nl, xingyu han x.han- student.tudelft.nl, anyan huang a.huang- student.tudelft.nl affiliations delft university technology rainmamba title rainmamba video coarse-to-fine mamba video raindrop removal members hongtao wuhongtaowestlake.edu.cn, hong wang, yefeng zheng affiliations medical artificial intelligence laboratory, west lake uni- versity school life science technology, xian jiaotong university raindropx title leveraging perceptual structural constraints dual-focused raindrop removal using restormer members abhijeet kumar eedsmail.iitm.ac.in, aman kumar, a.n. rajagopalan affiliations indian institute technology madras cidaut title fracdeformer fractional fourier aware deformable transformer day night raindrop removal members marcos conde marcos.condeuni- wuerzburg.de, paula garrido, daniel feijoo, juan benito affiliations cidaut dgl deraindrop title gsastep members guanglu dong dongguanglustu.scu.edu.cn, xin lin, siyuan liu, tianheng zheng, jiayu zhong, shouyi wang, xiangtai li, lanqing guo, chao ren affiliations sichuan university xdu title raindrop mask prior-guided deraining transformer members shuaibo wang shbwangstu.xidian.edu.c, shilong zhang, wanyu zhou, yunze wu, qinzhong tan affiliations xidian university edgeclear-dnsst team title edgeclear-dnsst edge-preserving day-night sparse-sampling transformer members jieyuan pei peijieyuanzjut.edu.cn, zhuox- uan affiliations zhejiang university technology tongji university mplnet title mplnet multi-stage progressive learning illumination-conscious dynamic transformer networks members jiayu wang std.uestc.edu.cn, haoyu bian, haoran sun affiliations university electronic science technology china singularity title fracdeformer fractional fourier aware deformable transformer day night raindrop removal members subhajit paul subhajitpaulsac.isro.gov.in affiliations space applications centre sac viplab title staged restoration deblurring first, detail en- hancement members tang stu.xmu.edu.cn, junhao huang, zihan cheng, hongyun zhu, yuehan affiliations school informatics, xiamen university agent title dit-rainremoval fine-tuning pre-trained dit model dual-focus raindrop removal members kaixin deng kaixin.deng.telms.hokudai.ac.jp, hang ouyang, tianxin xiao, fan yang, zhizun luo affiliations hokkaido university chengdu university technology x-l title permuted self-attention network image rain- drop removal members zeyu xiao zeyuxiao.com, zhuoyuan affiliations national university singapore university science technology china uit-shanks title effective raindrop removal integrated deep ensemble approach semi-supervised learning restormer denoising members nguyen pham hoang gm.uit.edu.vn, dinh thien, son luu, kiet van nguyen affiliations university information technology, vietnam national university, chi minh city, vietnam one title deraining defocus-blurring separately members ronghua tstu.cqu.edu.cn, xianmin tian, weijian zhou, jiacheng zhang affiliations school big data software engineering, chongqing university dualbranchderainnet title lightweight dual-branch raindrop removal method day night scenes members yuqian chen seu.edu.cn affiliations southeast university qwe title optimization raindrop image restoration model integrating data enhancement continuous learning members yihang duan qq.com, yujie affiliations xidian university visual signal information processing team title uformer-based dual-focus raindrop removal ntire members suresh raikwar suresh.raikwarthapar.edu, arsh garg, kritika affiliations none zheng family group title title members jianhua zheng qq.com, xi- aoshan ma, ruolin zhao, yongyu yang, yongsheng liang, guiming huang affiliations zhongkai university agriculture engineering rainclear pioneers title rainclearnet dual-stream physics-guided framework raindrop removal members qiang qiangshu.edu.cn, hongbin zhang, xiangyu zheng affiliations shanghai university references eirikur agustsson radu timofte. ntire challenge single image super-resolution dataset study. proceedings ieee conference computer vision pattern recognition workshops, pages juan benito, daniel feijoo, alvaro garcia, marcos conde. flol fast baselines real-world low-light en- hancement. arxiv preprint yue cao, jiarui xu, stephen lin, fangyun wei, han hu. global context networks. tpami, kelvin chan, xintao wang, yu, chao dong, chen change loy. basicvsr search essential com- ponents video super-resolution beyond. proceed- ings ieeecvf conference computer vision pattern recognition, pages wenhui chang, hongming chen, xin he, xiang chen, liangduo shen. uav-raink benchmark raindrop removal uav aerial imagery. cvpr, pages hanting chen, yunhe wang, tianyu guo, chang xu, yip- ing deng, zhenhua liu, siwei ma, chunjing xu, chao xu, wen gao. pre-trained image processing transformer. cvpr, pages hongming chen, xiang chen, jiyang lu, yufeng li. rethinking multi-scale representations deep deraining transformer. aaai, pages hongming chen, xiang chen, jiyang lu, yufeng li. rethinking multi-scale representations deep deraining transformer. proceedings aaai conference ar- tificial intelligence, pages liangyu chen, xiaojie chu, xiangyu zhang, jian sun. simple baselines image restoration. european con- ference computer vision, pages springer, liangyu chen, xiaojie chu, xiangyu zhang, jian sun. simple baselines image restoration. eccv, pages springer, sixiang chen, tian ye, jinbin bai, erkang chen, jun shi, lei zhu. sparse sampling transformer uncertainty- driven ranking unified removal raindrops rain streaks. proceedings ieeecvf international con- ference computer vision, pages sixiang chen, tian ye, kai zhang, zhaohu xing, yunlong lin, lei zhu. teaching tailored talent adverse weather restoration via prompt pool depth-anything constraint. european conference computer vision, pages springer, sixiang chen, tian ye, kai zhang, zhaohu xing, yunlong lin, lei zhu. teaching tailored talent adverse weather restoration via prompt pool depth-anything constraint. european conference computer vision, pages springer, tingting chen, beibei lin, yeying jin, wending yan, wei ye, yuan yuan, robby tan. dual-rain video rain removal using assertive gentle teachers. european conference computer vision, pages springer, xiang chen, jinshan pan, jiangxin dong, jinhui tang. towards unified deep image deraining survey new benchmark. arxiv preprint xiangyu chen, xintao wang, jiantao zhou, qiao, chao dong. activating pixels image super- resolution transformer. proceedings ieeecvf conference computer vision pattern recognition, pages xiangyu chen, zheyuan li, yuandong pu, yihao liu, jiantao zhou, qiao, chao dong. comparative study image restoration networks general backbone network design. european conference computer vi- sion, pages springer, xiang chen, jinshan pan, jiangxin dong. bidirec- tional multi-scale implicit neural representations image deraining. proceedings ieeecvf conference computer vision pattern recognition, pages zheng chen, zongwei wu, eduard zamfir, kai zhang, yu- lun zhang, radu timofte, xiaokang yang, hongyuan yu, cheng wan, yuxin hong, al. ntire challenge im- age super-resolution methods results. cvpr, pages zheng chen, kai liu, jue gong, jingkai wang, lei sun, zongwei wu, radu timofte, yulun zhang, al. ntire challenge image super-resolution methods results. proceedings ieeecvf conference computer vision pattern recognition cvpr work- shops, zheng chen, jingkai wang, kai liu, jue gong, lei sun, zongwei wu, radu timofte, yulun zhang, al. ntire challenge real-world face restoration methods results. proceedings ieeecvf conference computer vision pattern recognition cvpr work- shops, xiaojie chu, liangyu chen, chengpeng chen, xin lu. improving image restoration revisiting global informa- tion aggregation. european conference computer vision, pages springer, marcos conde, radu timofte, al. ntire challenge raw image restoration super-resolution. proceed- ings ieeecvf conference computer vision pattern recognition cvpr workshops, marcos conde, radu timofte, al. raw image reconstruc- tion rgb smartphones. ntire challenge re- port. proceedings ieeecvf conference com- puter vision pattern recognition cvpr workshops, egor ershov, sergey korchagin, alexei khalin, artyom panshin, arseniy terekhin, ekaterina zaychenkova, georgiy lobarev, vsevolod plokhotnyuk, denis abramov, elisey zhdanov, sofia dorogova, yasin mamedov, nikola banic, georgii perevozchikov, radu timofte, al. ntire challenge night photography rendering. pro- ceedings ieeecvf conference computer vision pattern recognition cvpr workshops, daniel feijoo, juan benito, alvaro garcia, marcos conde. darkir robust low-light image restoration. arxiv preprint evan frick, connor chen, joseph tennyson, tianle li, wei-lin chiang, anastasios angelopoulos, ion sto- ica. prompt-to-leaderboard, xueyang fu, jiabin huang, delu zeng, yue huang, xing- hao ding, john paisley. removing rain single images via deep detail network. proceedings ieee conference computer vision pattern recogni- tion, pages yuqian fu, xingyu qiu, bin ren yanwei fu, radu timofte, nicu sebe, ming-hsuan yang, luc van gool, al. ntire challenge cross-domain few-shot object detection methods results. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, ning gao, xingyu jiang, xiuhui zhang, yue deng. ef- ficient frequency-domain image deraining contrastive regularization. european conference computer vi- sion, pages springer, yun guo, xueyao xiao, chang, shumin deng, luxin yan. sky ground large-scale bench- mark simple baseline towards real rain removal. proceedings ieeecvf international conference computer vision, pages shuhao han, haotian fan, fangyuan kong, wenjie liao, chunle guo, chongyi li, radu timofte, al. ntire challenge text image generation model qual- ity assessment. proceedings ieeecvf confer- ence computer vision pattern recognition cvpr workshops, zhixiang hao, shaodi you, li, kunming li, feng lu. learning synthetic photorealistic raindrop sin- gle image raindrop removal. iccv workshops, pages xiaowei hu, chi-wing fu, lei zhu, pheng-ann heng. depth-attentional features single-image rain removal. proceedings ieeecvf conference computer vi- sion pattern recognition, pages wenfeng huang, guoan xu, wenjing jia, stuart perry, guangwei gao. revivediff universal diffusion model restoring images adverse weather conditions. arxiv preprint varun jain, zongwei wu, quan zou, louis florentin, henrik turbell, sandeep siddhartha, radu timofte, al. ntire challenge video quality enhancement video conferencing datasets, methods results. pro- ceedings ieeecvf conference computer vision pattern recognition cvpr workshops, kui jiang, zhongyuan wang, peng yi, chen chen, baojin huang, yimin luo, jiayi ma, junjun jiang. multi-scale progressive fusion network single image deraining. proceedings ieeecvf conference computer vi- sion pattern recognition, pages yeying jin, aashish sharma, robby tan. dc- shadownet single-image hard soft shadow removal using unsupervised domain-classifier guided network. proceedings ieeecvf international conference computer vision, pages yeying jin, wending yan, wenhan yang, robby tan. structure representation network uncertainty feedback learning dense non-uniform fog removal. proceed- ings asian conference computer vision, pages yeying jin, wenhan yang, robby tan. unsuper- vised night image enhancement layer decomposi- tion meets light-effects suppression. european confer- ence computer vision, pages springer, yeying jin, ruoteng li, wenhan yang, robby tan. estimating reflectance layer single image integrat- ing reflectance guidance shadowspecular aware learn- ing. proceedings aaai conference artificial intelligence, pages yeying jin, beibei lin, wending yan, yuan yuan, wei ye, robby tan. enhancing visibility nighttime haze images using guided apsf gradient adaptive convolu- tion. proceedings acm international confer- ence multimedia, pages yeying jin, xin li, jiadong wang, yan zhang, malu zhang. raindrop clarity dual-focused dataset day night raindrop removal. european conference computer vision, pages springer, yeying jin, wei ye, wenhan yang, yuan yuan, robby tan. des adaptive attention-driven self soft shadow removal using vit similarity. proceedings aaai conference artificial intelligence, pages justin johnson, alexandre alahi, fei-fei. perceptual losses real-time style transfer super-resolution. computer visioneccv european conference, amsterdam, netherlands, october pro- ceedings, part pages springer, diederik kingma jimmy ba. adam method stochastic optimization. arxiv preprint lingshun kong, jiangxin dong, jianjun ge, mingqiang li, jinshan pan. efficient frequency domain-based trans- formers high-quality image deblurring. proceedings ieee conference computer vision pattern recognition cvpr, lingshun kong, jiangxin dong, jianjun ge, mingqiang li, jinshan pan. efficient frequency domain-based trans- formers high-quality image deblurring. cvpr, pages wei-sheng lai, jia-bin huang, narendra ahuja, ming- hsuan yang. fast accurate image super-resolution deep laplacian pyramid networks. ieee transactions pattern analysis machine intelligence, sangmin lee, eunpil park, angel canelo, hyunhee park, youngjo kim, hyungju chun, xin jin, chongyi li, chun- guo, radu timofte, al. ntire challenge efficient burst hdr restoration datasets, methods, results. proceedings ieeecvf conference computer vision pattern recognition cvpr work- shops, bingchen li, xin li, yiting lu, sen liu, ruoyu feng, zhibo chen. hst hierarchical swin transformer com- pressed image super-resolution. european conference computer vision, pages springer, bingchen li, xin li, yiting lu, ruoyu feng, mengxi guo, shijie zhao, zhang, zhibo chen. promptcir blind compressed image restoration prompt learning. pro- ceedings ieeecvf conference computer vision pattern recognition, pages bingchen li, xin li, hanxin zhu, yeying jin, ruoyu feng, zhizheng zhang, zhibo chen. sed semantic-aware discriminator image super-resolution. proceedings ieeecvf conference computer vision pattern recognition, pages bingchen li, xin li, yiting lu, zhibo chen. hybrid agents image restoration. arxiv preprint ruoteng li, loong-fah cheong, robby tan. heavy rain image restoration integrating physics model conditional adversarial learning. proceedings ieeecvf conference computer vision pattern recognition, pages siyuan li, iago breno araujo, wenqi ren, zhangyang wang, eric tokuda, roberto hirata junior, roberto cesar-junior, jiawan zhang, xiaojie guo, xiaochun cao. single image deraining comprehensive bench- mark analysis. proceedings ieeecvf confer- ence computer vision pattern recognition, pages xin li, xin jin, jianxin lin, sen liu, yaojun wu, tao yu, wei zhou, zhibo chen. learning disentangled fea- ture representation hybrid-distorted image restoration. eccv, pages springer, xin li, bingchen li, xin jin, cuiling lan, zhibo chen. learning distortion invariant representation im- age restoration causality perspective. proceedings ieeecvf conference computer vision pat- tern recognition, pages xin li, yulin ren, xin jin, cuiling lan, xingrui wang, wenjun zeng, xinchao wang, zhibo chen. diffusion models image restoration enhancementa compre- hensive survey. arxiv preprint xin li, yeying jin, xin jin, zongwei wu, bingchen li, yufei wang, wenhan yang, li, zhibo chen, bihan wen, robby tan, radu timofte, al. ntire challenge day night raindrop removal dual-focused images methods results. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, xin li, xijun wang, bingchen li, kun yuan, yizhen shao, suhang yao, ming sun, chao zhou, radu timofte, zhibo chen. ntire challenge short-form ugc video quality assessment enhancement kwaisr dataset study. proceedings ieeecvf conference computer vision pattern recognition cvpr work- shops, xin li, kun yuan, bingchen li, fengbin guan, yizhen shao, zihao yu, xijun wang, yiting lu, wei luo, suhang yao, ming sun, chao zhou, zhibo chen, radu timofte, al. ntire challenge short-form ugc video qual- ity assessment enhancement methods results. proceedings ieeecvf conference computer vi- sion pattern recognition cvpr workshops, li, robby tan, xiaojie guo, jiangbo lu, michael brown. rain streak removal using layer priors. proceedings ieee conference computer vision pattern recognition, pages jingyun liang, jiezhang cao, guolei sun, kai zhang, luc van gool, radu timofte. swinir image restoration using swin transformer. proceedings ieeecvf international conference computer vision, pages jie liang, radu timofte, qiaosi yi, zhengqiang zhang, shuaizheng liu, lingchen sun, rongyuan wu, xindong zhang, hui zeng, lei zhang, al. ntire restore image model raim wild challenge. proceedings ieeecvf conference computer vi- sion pattern recognition cvpr workshops, bee lim, sanghyun son, heewon kim, seungjun nah, kyoung lee. enhanced deep residual networks single image super-resolution. proceedings ieee conference computer vision pattern recogni- tion workshops, pages beibei lin, yeying jin, wending yan, wei ye, yuan yuan, robby tan. nighthaze nighttime image dehazing via self-prior learning. arxiv preprint beibei lin, yeying jin, wending yan, wei ye, yuan yuan, shunli zhang, robby tan. nightrain nighttime video deraining via adaptive-rain-removal adaptive- correction. proceedings aaai conference ar- tificial intelligence, pages yunlong lin, zixu lin, haoyu chen, panwang pan, chenxin li, sixiang chen, yeying jin, wenbo li, xinghao ding. jarvisir elevating autonomous driving per- ception intelligent image restoration. proceedings ieeecvf conference computer vision pat- tern recognition cvpr, jianzhao liu, jianxin lin, xin li, wei zhou, sen liu, zhibo chen. lira lifelong image restoration un- known blended distortions. european conference computer vision, pages springer, jiawei liu, qiang wang, huijie fan, yinong wang, yan- dong tang, liangqiong qu. residual denoising diffu- sion models. cvpr, pages pengju liu, hongzhi zhang, jinghui wang, yuzhi wang, dongwei ren, wangmeng zuo. robust deep ensemble method real-world image denoising. neurocomputing, xiaohong liu, xiongkuo min, qiang hu, xiaoyun zhang, jie guo, al. ntire xgc quality assessment challenge methods results. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, xiaoning liu, zongwei wu, florin-alexandru vasluianu, hailong yan, bin ren, yulun zhang, shuhang gu, zhang, zhu, radu timofte, al. ntire challenge low light image enhancement methods results. proceedings ieeecvf conference computer vi- sion pattern recognition cvpr workshops, yiting lu, xin li, bingchen li, zihao yu, fengbin guan, xinrui wang, ruling liao, yan ye, zhibo chen. aigc- vqa holistic perception metric aigc video quality assessment. proceedings ieeecvf conference computer vision pattern recognition, pages ziwei luo, fredrik gustafsson, zheng zhao, jens sjolund, thomas schon. refusion enabling large- size realistic image restoration latent-space diffusion models. proceedings ieeecvf conference computer vision pattern recognition, pages ozan ozdenizci robert legenstein. restoring vision adverse weather conditions patch-based denoising diffusion models. ieee transactions pattern analysis machine intelligence, ozan ozdenizci robert legenstein. restoring vision adverse weather conditions patch-based denoising diffusion models. ieee transactions pattern analysis machine intelligence, yingxue pang, xin li, xin jin, yaojun wu, jianzhao liu, sen liu, zhibo chen. fan frequency aggregation net- work real image super-resolution. computer vision eccv workshops glasgow, uk, august proceedings, part iii pages springer, paszke. pytorch imperative style, high-performance deep learning library. arxiv preprint subhajit paul, sahil kumawat, ashutosh gupta, deepak mishra. fformer fractional fourier meets deep wiener deconvolution selective fre- quency transformer image deblurring. arxiv preprint subhajit paul, sahil kumawat, ashutosh gupta, deepak mishra. fformer fractional fourier meets deep wiener deconvolution selective frequency trans- former image deblurring. ieeecvf winter conference applications computer vision wacv, william peebles saining xie. scalable diffusion mod- els transformers. iccv, pages horia porav, tom bruls, paul newman. see clearly image restoration via de-raining. icra, pages ieee, vaishnav potlapalli, syed waqas zamir, salman khan, fahad shahbaz khan. promptir prompting all- in-one image restoration. advances neural information processing systems, rui qian, robby tan, wenhan yang, jiajun su, jiay- ing liu. attentive generative adversarial network rain- drop removal single image. proceedings ieee conference computer vision pattern recogni- tion, pages rui qian, robby tan, wenhan yang, jiajun su, jiay- ing liu. attentive generative adversarial network rain- drop removal single image. cvpr, pages ruijie quan, xin yu, yuanzhi liang, yang. remov- ing raindrops rain streaks one go. proceedings ieeecvf conference computer vision pattern recognition, pages alec radford, jong wook kim, chris hallacy, aditya ramesh, gabriel goh, sandhini agarwal, girish sastry, amanda askell, pamela mishkin, jack clark, al. learn- ing transferable visual models natural language super- vision. international conference machine learning, pages pmlr, bin ren, hang guo, lei sun, zongwei wu, radu tim- ofte, yawei li, al. tenth ntire efficient super-resolution challenge report. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, qiyu rong, hongyuan jing, mengmeng zhang, jinlong li, mengfei han. strrnet semantics-guided two- stage raindrop removal network. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, olaf ronneberger, philipp fischer, thomas brox. net convolutional networks biomedical image segmen- tation. medical image computing computer-assisted interventionmiccai international conference, munich, germany, october proceedings, part iii pages springer, nickolay safonov, alexey bryntsev, andrey moskalenko, dmitry kulikov, dmitriy vatolin, radu timofte, al. ntire challenge ugc video enhancement meth- ods results. proceedings ieeecvf confer- ence computer vision pattern recognition cvpr workshops, yiyang shen, mingqiang wei, yongzhen wang, xueyang fu, jing qin. rethinking real-world image deraining via unpaired degradation-conditioned diffusion model. arxiv preprint vera soboleva oleg shipitko. raindrops wind- shield dataset lightweight gradient-based detection al- gorithm. ieee ssci, pages ieee, yuda song, zhuqing he, hui qian, xin du. vision transformers single image dehazing. ieee transactions image processing, lei sun, andrea alfarano, peiqi duan, shaolin su, kaiwei wang, boxin shi, radu timofte, danda pani paudel, luc van gool, al. ntire challenge event-based image deblurring methods results. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, lei sun, hang guo, bin ren, luc van gool, radu timo- fte, yawei li, al. tenth ntire image denoising challenge report. proceedings ieeecvf confer- ence computer vision pattern recognition cvpr workshops, shangquan sun, wenqi ren, xinwei gao, rui wang, xiaochun cao. restoring images adverse weather con- ditions via histogram transformer. european conference computer vision, pages springer, shangquan sun, wenqi ren, xinwei gao, rui wang, xiaochun cao. restoring images adverse weather condi- tions via histogram transformer. eccv, pages springer, zhengzhong tu, hossein talebi, han zhang, feng yang, peyman milanfar, alan bovik, yinxiao li. maxim multi-axis mlp image processing. proceedings ieeecvf conference computer vision pattern recognition, pages jeya maria jose valanarasu, rajeev yasarla, vishal patel. transweather transformer-based restoration im- ages degraded adverse weather conditions. proceed- ings ieeecvf conference computer vision pattern recognition, pages florin-alexandru vasluianu, tim seizinger, zhuyun zhou, cailian chen, zongwei wu, radu timofte, al. ntire image shadow removal challenge report. proceed- ings ieeecvf conference computer vision pattern recognition cvpr workshops, florin-alexandru vasluianu, tim seizinger, zhuyun zhou, zongwei wu, radu timofte, al. ntire ambi- ent lighting normalization challenge. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, tianyu wang, xin yang, xu, shaozhe chen, qiang zhang, rynson lau. spatial attentive single-image deraining high quality real rain dataset. proceed- ings ieeecvf conference computer vision pattern recognition, pages yingqian wang, zhengyu liang, fengyuan zhang, lvli tian, longguang wang, juncheng li, jungang yang, radu timofte, yulan guo, al. ntire challenge light field image super-resolution methods results. pro- ceedings ieeecvf conference computer vision pattern recognition cvpr workshops, zhou wang, alan bovik, hamid sheikh, eero simoncelli. image quality assessment error visibility structural similarity. ieee transactions image pro- cessing, chen wei, wenjing wang, wenhan yang, jiaying liu. deep retinex decomposition low-light enhancement. arxiv preprint hongtao wu, yijun yang, huihui xu, weiming wang, jinni zhou, lei zhu. rainmamba enhanced local- ity learning state space models video deraining. acmmm, pages jie xiao, xueyang fu, aiping liu, feng wu, zheng- jun zha. image de-raining transformer. ieee transac- tions pattern analysis machine intelligence, kangning yang, jie cai, ling ouyang, florin-alexandru vasluianu, radu timofte, jiaming ding, huiming sun, lan fu, jinlong li, chiu man ho, zibo meng, al. ntire challenge single image reflection removal wild datasets, methods results. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, wenhan yang, robby tan, jiashi feng, jiaying liu, zongming guo, shuicheng yan. deep joint rain detec- tion removal single image. proc. ieee conf. comput. vis. pattern recognit. workshops, pages wenhan yang, robby tan, jiashi feng, jiaying liu, zongming guo, shuicheng yan. deep joint rain de- tection removal single image. proceedings ieee conference computer vision pattern recog- nition, pages tian ye, sixiang chen, jinbin bai, jun shi, chenghao xue, jingxia jiang, junjie yin, erkang chen, yun liu. ad- verse weather removal codebook priors. proceed- ings ieeecvf international conference com- puter vision, pages xin yu, peng dai, wenbo li, lan ma, jiajun shen, jia li, xiaojuan qi. towards efficient scale-robust ultra- high-definition image demoireing. european conference computer vision, pages springer, zihao yu, fengbin guan, yiting lu, xin li, zhibo chen. sf-iqa quality similarity integration generated image quality assessment. proceedings ieeecvf conference computer vision pattern recognition, pages pierluigi zama ramirez, fabio tosi, luigi stefano, radu timofte, alex costanzino, matteo poggi, samuele salti, stefano mattoccia, al. ntire challenge depth images specular transparent surfaces. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, syed waqas zamir, aditya arora, salman khan, munawar hayat, fahad shahbaz khan, ming-hsuan yang, ling shao. multi-stage progressive image restoration. cvpr, pages syed waqas zamir, aditya arora, salman khan, mu- nawar hayat, fahad shahbaz khan, ming-hsuan yang. restormer efficient transformer high-resolution image restoration. proceedings ieeecvf conference computer vision pattern recognition, pages zhang vishal patel. density-aware single image de-raining using multi-stream dense network. proceed- ings ieee conference computer vision pattern recognition, pages richard zhang, phillip isola, alexei efros, eli shecht- man, oliver wang. unreasonable effectiveness deep features perceptual metric. cvpr, pages yulun zhang, kunpeng li, kai li, lichen wang, bineng zhong, yun fu. image super-resolution using deep residual channel attention networks. eccv, pages yiyuan zhang, kaixiong gong, kaipeng zhang, hong- sheng li, qiao, wanli ouyang, xiangyu yue. meta- transformer unified framework multimodal learning. arxiv preprint yupeng zhou, zhen li, chun-le guo, song bai, ming- ming cheng, qibin hou. srformer permuted self- attention single image super-resolution. iccv, pages jiachen zhu, xinlei chen, kaiming he, yann lecun, zhuang liu. transformers without normalization. arxiv preprint zhen zou, yu, jie huang, feng zhao. freqmamba viewing mamba frequency perspective image deraining. proceedings acm international conference multimedia, pages", "published_date": "2025-04-17T07:35:35+00:00"}
{"id": "2504.12018v1", "title": "Instruction-augmented Multimodal Alignment for Image-Text and Element Matching", "authors": ["Xinli Yue", "JianHui Sun", "Junda Lu", "Liangchao Yao", "Fan Xia", "Tianyi Wang", "Fengyun Rao", "Jing Lyu", "Yuetang Deng"], "summary": "rapid advancement text-to-image generation models, assessing semantic alignment generated images text descriptions become significant research challenge. current methods, including based visual question answering vqa, still struggle fine-grained assessments precise quantification image-text alignment. paper presents improved evaluation method named instruction-augmented multimodal alignment image-text element matching imatch, evaluates image-text semantic alignment fine-tuning multimodal large language models. introduce four innovative augmentation strategies first, qalign strategy creates precise probabilistic mapping convert discrete scores multimodal large language models continuous matching scores. second, validation set augmentation strategy uses pseudo-labels model predictions expand training data, boosting models generalization performance. third, element augmentation strategy integrates element category labels refine models understanding image-text matching. fourth, image augmentation strategy employs techniques like random lighting increase models robustness. additionally, propose prompt type augmentation score perturbation strategies enhance accuracy element assessments. experimental results show imatch method significantly surpasses existing methods, confirming effectiveness practical value. furthermore, imatch first place cvpr ntire text image generation model quality assessment track image-text alignment.", "full_text": "instruction-augmented multimodal alignment image-text element matching xinli yue jianhui sun junda liangchao yao fan xia tianyi wang fengyun rao jing lyu yuetang deng wuhan university wechat abstract rapid advancement text-to-image genera- tion models, assessing semantic alignment gen- erated images text descriptions become signifi- cant research challenge. current methods, including based visual question answering vqa, still struggle fine-grained assessments precise quantification image-text alignment. paper presents improved evaluation method named instruction-augmented multi- modal alignment image-text element matching imatch, evaluates image-text semantic alignment fine-tuning multimodal large language models. in- troduce four innovative augmentation strategies first, qalign strategy creates precise probabilistic mapping convert discrete scores multimodal large language models continuous matching scores. second, vali- dation set augmentation strategy uses pseudo-labels model predictions expand training data, boosting models generalization performance. third, element augmentation strategy integrates element category labels refine models understanding image-text matching. fourth, image augmentation strategy employs techniques like random lighting increase models robustness. ad- ditionally, propose prompt type augmentation score perturbation strategies enhance accuracy element assessments. experimental results show imatch method significantly surpasses existing methods, confirming effectiveness practical value. further- more, imatch first place cvpr ntire text image generation model quality assessment track image-text alignment. introduction recent years, rapid development deep learning technologies led significant breakthroughs text-to- equal contribution. corresponding author. image generation models demonstrated powerful image generation capabilities. however, objectively accurately assessing semantic alignment generated images text descriptions increasingly become critical issue significant challenge research field. metrics based visual language models assess semantic matching image text mea- suring cosine similarity embedding spaces. several ap- proaches utilize human-annotated data emulate human judgments image-text alignment, others analyze texts broken elements evaluated vqa models. innovations introduction question templates improve alignment vqa model human preferences although meth- ods form foundation evaluating image-text alignment, lack unified approach comprehensive detailed matching assessments, missing systematic exploration complex relationships text images. hand, emergence multimodal large lan- guage models mllms recent years provided robust technical support image-text matching tasks. recently, models internvl. qwen.-vl ovis series shown ex- ceptional performance multimodal understanding tasks. nonetheless, exploration needed fully leverage models accurate assessments image-text matching. addressing challenges highlighted above, study proposes enhanced method comprehensive fine-grained image-text matching assessment, named instruction-augmented multimodal alignment image- text element matching imatch, aimed precisely measuring alignment generated images textual descriptions. specifically, approach encom- passes several innovative aspects firstly, employ fine- tuning strategy based mllms, utilizing fine-grained image-text matching annotations evalmuse-k dataset explicitly guide model learning nuanced correspondences text images. fur- cs.cv apr figure instruction set augmentation process proposed imatch. ther enhance model performance, propose four augmen- tation strategies qalign strategy strategy defines mapping function textual rating levels specific numerical scores, coupled soft mapping model prediction probabilities, accurately convert overall image-text matching scores. validation set aug- mentation strategy initially use model predict validation set training, generating high-quality pseudo-labels, merged back original training set re-training enhance models gener- alization performance. element augmentation strategy training, explicitly input element labels addi- tional features user query, helping model deduce overall matching score finer-grained information chain-of-thought-like manner image aug- mentation strategy introduce three data augmentation techniques expand diversity training set images enhance models robustness image variations. additionally, element matching task, propose two additional augmentation techniques prompt type aug- mentation explicitly integrate prompt type real synthetic user query, aiding model dis- tinguishing intrinsic characteristics different prompt sources. score perturbation augmentation applying slight random perturbations target labels element matching, reduce risk model overfitting spe- cific training labels, improving models general- ization capabilities. synergistic effect methods led outstanding performance evalmuse-k dataset ntire challenge validat- ing effectiveness practicality proposed meth- ods image-text matching tasks. summary, contributions follows present imatch, innovative image-text match- ing method enhances semantic matching accuracy fine-tuned multimodal models strategic aug- mentations. introduce four augmentation strategiesqalign, val- idation set augmentation, element augmentation, im- age augmentationto improve models adaptability generalization image-text tasks. develop two techniquesprompt type augmentation score perturbation augmentationto boost model performance stability. extensive testing evalmuse-k dataset ntire challenge shows imatch surpasses existing methods across multiple metrics. related work image-text alignment recent advancements text-to-image generation models empha- sized importance accurately assessing alignment images text descriptions. clipscore utilizes pretrained clip model measure cosine similarity embeddings, providing initial auto- matic evaluation. blipscore follows similar ap- proach enhanced evaluation. imagereward pickscore refine assessments fine-tuning extensive human feedback, aligning closely hu- man perceptions. tifa break text element-level questions answered vqa models, fo- cusing detailed matching. fga-blip improves tailored question templates direct vqa models towards crucial text content, enhancing element-level accu- racy. despite advances, challenges remain provid- ing unified framework detailed overall image-text alignment. multimodal large language models recent years, advanced mllms shown remarkable performance mul- timodal tasks. internvl. series based in- ternvl retains original architecture introduces improvements training, testing, data quality. internvl.-mpo model employs mixed preference opti- mization mpo enhance reasoning multimodal tasks. qwen.-vl series demonstrates strong visual linguistic skills, qwen.-vl-b-instruct model excelling visual benchmarks agentive tasks. ovis models successors ovis., feature upgraded dataset organization training, boosting reasoning smaller models. developments mllms advance cross-modal understanding generation, setting stage future advancements. methodology baseline model propose fine-tuning approach based mllms address fine-grained image-text matching task image generation quality assessment. utilizing evalmuse-k dataset provides extensive fine- grained image-text matching annotations, method ex- figure construction process element instruction set augmentation. map elements integers incorporate user query. additionally, confidence scores also integrated user query. plicitly guides model learning detailed correspon- dences text images. specifically, given text description corresponding generated image, model tasked predicting overall image-text match- ing score element-level matching hits. ... problem definition let given text description corresponding generated image evalmuse-k dataset pro- vides overall image-text matching score stotal set fine-grained element matching scores sein image-text pair here, stotal ranges rep- resenting overall matching degree, element matching score sei ranges indicating whether specific element accurately represented image. therefore, task formally defined hat ext total ftext total theta hat element left phi right tau right quad i,, ldots ftotal model predicting overall image-text matching score, felement model predicting element matching scores, model parameters, threshold determining element hits. ... instructional fine-tuning strategy enhance models capability learning fine-grained image-text correspondences, designed specific in- structional fine-tuning strategy transforms task predicting image-text matching scores classification problem. steps follows overall image-text matching score stotal, first per- form linear scaling otal roundleft frac stext total times right point, total transformed discrete set in- tegers subsequently, map integer range alphabet set ..., serves target labels multimodal model instructional fine-tuning. inference phase, models predicted letter la- bels mapped back range linearly scaled original range final prediction result. element matching task, illustrated figure discretize element matching scores sei categories roundsei times inference, use threshold converting predicted categories binary classification task, scores greater considered hits others considered misses additionally, incorporate information provided evalmuse-k dataset prompt meaning- lessness, split confidence, attribute confidence problem formulation, shown figure instruction-based fine-tuning strategy, significantly en- hance multimodal models performance image-text matching tasks, especially terms element-level recog- nition judgment. image-text matching augmented model ... qalign augmentation enhance accuracy image-text matching tasks, introduce probability distribution-based aug- mentation strategy meticulously simulate human scoring behavior, illustrated figure specifically, adopt post-processing strategy similar qalign method converts model output scoring levels precise final matching scores. firstly, define mapping function letters nu- meric scores ldots ..., example, scoring level corresponds numeric score level corresponds numeric score next, calculate probabilities pli predicted mllms. specifically, logits output language model scoring level xli use closed-set softmax function compute probability distribution level rac exlisum exlj, figure inference process image-text matching aug- mented model. inference, extract closed-set probabili- ties rating levels perform weighted average obtain mllm-predicted score. pli consequently, models final continuous predicted score stotal hat ext total sum pli cdot gli. ... validation set augmentation ntire competition development phase validation set, final phase test set. fur- ther enhance models generalization ability final test phase, design pseudo-labeling strategy fully utilizes validation set data augment training process. development phase, first use trained model predict validation set, generating pseudo la- bels, denoted hat then, final phase, combine pseudo-labeled validation set original training set construct augmented training dataset subse- quent model retraining. new training objective expressed mathcal text enha dtheta athcal ltext trainyt, hat theta mathcal ltext pseudohat yv, hat theta mathcal ltext train denotes loss original training set, mathcal ltext pseudo denotes loss supervised pseudo labels, models prediction validation set samples. figure process validation set augmentation. use model trained training set generate pseudo-labels validation set development phase, use pseudo-labeled validation data augment training dataset. ... element augmentation enhance models understanding judgment capabilities image-text matching, propose strat- egy based element feature augmentation. specifically, shown figure training phase, element- level scores explicitly embedded user query additional features facilitate reasoning process similar chain-of-thought cot however, testing phase, due lack real element labels, cannot directly employ aug- mentation method. therefore, adopt pseudo-label pre- diction strategy. initially, use model trained section predict pseudo-labels element level hat yei test set. specifically, model first predicts ele- ment scores sei text elementi, phi subsequently, predicted element scores hat sei em- bedded input image-text matching task, form- ing prompt pseudo-labeled element scores predict final image-text matching score stotal hat stext total ftext totali, hat sei theta ... image augmentation enhance models generalization performance improve robustness image variations, introduce image data augmentation strategy, illustrated fig- ure specifically, randomly select sam- ples training set apply one three differ- ent types data augmentation methods. augmented samples used conjunction original data training, effectively enriching diversity train- ing dataset. specific data augmentation methods in- clude random lighting augmentation. randomly adjust brightness images simulate variations lighting conditions, thereby improving models robustness changes illumination. defined ext light text brightnessi, alpha alpha sim u., represents original image, alpha randomly sampled lighting intensity factor. random grid distortion. apply random grid defor- mations image enhance models robustness spatial transformations. specifically defined text text gridi, beta beta sim u., beta random factor controlling degree grid distortion. random crop augmentation. randomly crop im- ages simulate partial occlusions changes perspec- tive. specifically defined text text cropi, gamma gamma sim u., gamma determines cropping scale. augmentation strategies implemented fol- lows randomly selected subset dtext subset training dataset dtext train begin plit text gmented ,mid dtext subset, qquad quad ttext brightness, ttext grid, ttext crop end split subsequently, merge enhanced dataset orig- inal dataset construct final augmented training set ext nal dtext train cup dtext augmented. element matching augmented model building upon baseline model, propose el- ement matching augmented model aimed improving models fine-grained understanding predictive accuracy regarding image-text element correspondences. specifi- cally, introduce two augmentation strategies, including incorporation prompt type information intro- duction score perturbation. ... prompt type augmentation richly characterize problem features el- ement matching task, propose integrating prompt type information evalmuse-k dataseteither real synthetic generated using gpt-explicitly user query. explicitly introducing prompt type informa- tion text real, text synth expand input formulation, resulting enhanced element matching task representa- tion yei text elementi, ei, phi figure examples image augmentation. employ three types augmentations random lighting augmentation, ran- dom grid distortion, random crop augmentation. figure model ensemble. ensemble five image-text match- ing augmented models four element matching augmented models improve models generalization performance. ... score perturbation enhance generalization capability el- ement matching model prevent overfitting specific score annotations training data, propose pertur- bation strategy based element labels. specifically, dur- ing training phase, apply slight perturbations mapped element labels, discrete numerical range increase diversity training data. let discretized score element training set denoted sei text discrete introduce perturbation fac- tor epsilon add random perturbation delt -epsilon epsilon elements discretized score. perturbed element score seit ext perturbed expressed sei perturb itext discrete delta quad delta sim u-epsilon epsilon table performance comparison different methods evalmuse-k validation set image-text matching tasks. method srcc plcc clipscore blipvscore imagereward pickscore hpsv vqascore fga-blip internvl.-b-mpo zero-shot imatch experiments experimental setup dataset. experimental studies employ evalmuse- dataset consists images generated different text-to-image generation models based approximately prompts. image-text pair annotated two levels scoring information overall prompt-level image-text matching score fine-grained element-level matching score. dataset divided training set, validation set, test set. training set contains image-text pairs model parameter learning validation set includes pairs tuning model parameters selecting hy- perparameters test set comprises approximately pairs final performance evaluation. models. fine-tune variety advanced mllms, including internvl.-b-mpo qwen.-vl-b- instruct ovis-b ovis-b ovis- comprehensively validate effectiveness generalization capability proposed methods. training parameters. implement experiments us- ing ms-swift framework employing low-rank adaptation lora fine-tuning, lora rank set additionally, unfreeze vision transformer vit cross-modal aligner fully leverage cross-modal information. specific training param- eters include maximum image pixel count initial learning rate e-, weight decay factor learning rate warm-up proportion learning rate decay strategy using cosine annealing. models trained one epoch ensure fair consistent experi- mental conditions. evaluation metrics. comprehensively assess model performance, utilize three metrics spearman rank correlation coefficient srcc, pearson linear cor- relation coefficient plcc, element-level accuracy table performance comparison different methods evalmuse-k validation set element matching tasks. in- dicates method uses fixed step search select optimal binary classification threshold, aiming maximize over- accuracy. method mllms acc tifa llava. mplug-owl qwen-vl llava. mplug-owl qwen-vl pn-vqa llava. mplug-owl qwen-vl fga-blip blip zero-shot internvl.-b-mpo imatch internvl.-b-mpo qwen.-vl-b-instruct ovis-b acc. combined final performance score, calculated final score plcc srcc acc. comparative approaches. comparative schemes include clipscore blipvscore imagere- ward pickscore hpsv vqascore tifa pn-vqa fga-blip experimental results results validation set. conduct comparative ex- periments evalmuse-k validation set sev- eral mainstream methods evaluate effectiveness proposed imatch. performance results presented tables shown table imatch achieves significant gains srcc plcc, outperforming baselines. com- pared traditional metrics like clipscore, improves srcc plcc respectively. even strongest fine-tuned baseline, fga-blip, imatch achieves absolute improvements srcc plcc, demonstrating effectiveness targeted fine-tuning strategy capturing fine-grained se- mantic alignment. also report zero-shot results using backbone internvl.-b-mpo. zero-shot model per- forms well, fine-tuned imatch surpasses no- table margin, highlighting importance task-aware fine-tuning. table results ntire text image generation model quality assessment track image-text alignment. rank team main score srcc plcc acc ih-vqa evalthon hcmus micv sjtu-mmlab sjtumm yag sprank aiig joe icost table imatch also achieves best accuracy el- ement matching task, outperforming baseline zero- shot models. validates effectiveness approach improving global detailed image-text alignment. results ntire challenge. validated ef- fectiveness method ntire chal- lenge text-to-image generation model quality assess- ment track image-text alignment. shown fig- ure final solution leveraged robust model ensemble strategy played key role success. table summarizes results. imatch method ranked first, outperforming competitors across key met- rics srcc, plcc, acc. compared second-place evalthon team, imatch improved main score achieved gains srcc, plcc, acc, respectively. performance margin even larger relative teams, highlighting strength consistency approach. ablation study results image-text matching augmented model. evaluate effectiveness four augmentation strategies proposed section conduct ablation ex- periments evalmuse-k test set. results shown table introduction qalign strategy significantly improved accuracy image-text matching. addition validation set augmentation strategy enhanced models generalization performance, indicated improvements srcc plcc metrics. element augmentation strategy, incor- porates element-level information, notably increased models understanding fine-grained aspects image-text table results ablation study component image-text matching augmented model evalmuse-k test set. base qalign aug. validation aug. element aug. image aug. internvl.-b-mpo qwen.-vl-b-instruct ovis-b srcc plcc srcc plcc srcc plcc table results ablation study component element matching augmented model evalmuse-k test set. base type aug. score perturbation internvl.-b-mpo qwen.-vl-b-instruct ovis-b acc acc acc table inference latency throughput imatch rtxd images resolution. batch size fixed larger batches result oom. model latency sthroughput images internvl.-b-mpo qwen.-vl-b-instruct ovis-b relations, achieving highest improvements. conversely, employing image augmentation strategy alone showed effectiveness enhancing models robustness im- age variations, although element augmentation strategy proved effective detailed information capture. results element matching augmented model. assess impact two augmentation strategies pro- posed section conduct ablation experiments evalmuse-k test set. results presented ta- ble introducing prompt type augmentation strategy significantly improved accuracy, highlighting effec- tiveness aiding models understanding image-text element relationships, thereby boosting predictive perfor- mance. incorporating score perturbation strat- egy yielded best performance, demonstrating value enhancing model generalization robustness intro- ducing moderate label variations. inference cost table report inference latency throughput imatch rtx gpus. fixed batch size input resolution fastest model achieves latency seconds throughput images per second, demonstrating efficient large-scale im- age processing potential real-world deployment. conclusion paper, tackle challenges image quality semantic matching text-to-image generation models multimodal approach called imatch, quantifies overall detailed matching relationships images text descriptions. developed framework using instructional fine-tuning, alongside several innovative strategies qalign, validation set augmentation, el- ement augmentation, image augmentation. addition- ally, introduced prompt type augmentation score perturbation methods element matching, enhancing models generalization precision element level. comprehensive tests evalmuse-k dataset ntire challenge show imatch outperforms existing methods overall detailed image-text match- ing, particularly excelling semantic understanding fine-grained evaluations. future work focus refin- ing image-text matching strategies integrating ad- vanced multimodal models broaden applicability methods practical settings. references josh achiam, steven adler, sandhini agarwal, lama ah- mad, ilge akkaya, florencia leoni aleman, diogo almeida, janko altenschmidt, sam altman, shyamal anadkat, al. gpt- technical report. arxiv preprint vladimir arkhipkin, andrei filatov, viacheslav vasilev, anastasia maltseva, said azizov, igor pavlov, julia aga- fonova, andrey kuznetsov, denis dimitrov. kandinsky technical report. arxiv preprint jinze bai, shuai bai, shusheng yang, shijie wang, sinan tan, peng wang, junyang lin, chang zhou, jingren zhou. qwen-vl versatile vision-language model un- derstanding, localization, text reading, beyond. arxiv preprint shuai bai, keqin chen, xuejing liu, jialin wang, wenbin ge, sibo song, kai dang, peng wang, shijie wang, jun tang, al. qwen. -vl technical report. arxiv preprint blackforestlabs. flux.. blackforestlabs.ai, zhe chen, weiyun wang, yue cao, yangzhou liu, zhang- wei gao, erfei cui, jinguo zhu, shenglong ye, hao tian, zhaoyang liu, al. expanding performance boundaries open-source multimodal models model, data, test- time scaling. arxiv preprint zhe chen, jiannan wu, wenhai wang, weijie su, guo chen, sen xing, muyan zhong, qinglong zhang, xizhou zhu, lewei lu, al. internvl scaling vision foundation mod- els aligning generic visual-linguistic tasks. cvpr, alexey dosovitskiy, lucas beyer, alexander kolesnikov, dirk weissenborn, xiaohua zhai, thomas unterthiner, mostafa dehghani, matthias minderer, georg heigold, syl- vain gelly, al. image worth words trans- formers image recognition scale. arxiv preprint dreaminaai. dreamina. dreamina capcut.com, patrick esser, sumith kulal, andreas blattmann, rahim entezari, jonas muller, harry saini, yam levi, dominik lorenz, axel sauer, frederic boesel, al. scaling recti- fied flow transformers high-resolution image synthesis. icml, yutong feng, biao gong, chen, yujun shen, liu, jingren zhou. ranni taming text-to-image diffu- sion accurate instruction following. arxiv preprint shuhao han, haotian fan, jiachen fu, liang li, tao li, junhui cui, yunqiu wang, yang tai, jingwei sun, chunle guo, al. evalmuse-k reliable fine-grained benchmark comprehensive human annotations text- to-image generation model evaluation. arxiv preprint shuhao han, haotian fan, fangyuan kong, wenjie liao, chunle guo, chongyi li, radu timofte, al. ntire challenge text image generation model quality assess- ment. proceedings ieeecvf conference com- puter vision pattern recognition cvpr workshops, wanggui he, siming fu, mushui liu, xierui wang, wenyi xiao, fangxun shu, wang, lei zhang, zhelun yu, haoyuan li, al. mars mixture auto-regressive mod- els fine-grained text-to-image synthesis. arxiv preprint jack hessel, ari holtzman, maxwell forbes, ronan bras, yejin choi. clipscore reference-free evaluation met- ric image captioning. arxiv preprint david holz. midjourney. com, edward hu, yelong shen, phillip wallis, zeyuan allen- zhu, yuanzhi li, shean wang, wang, weizhu chen, al. lora low-rank adaptation large language models. iclr, yushi hu, benlin liu, jungo kasai, yizhong wang, mari os- tendorf, ranjay krishna, noah smith. tifa accurate interpretable text-to-image faithfulness evaluation question answering. iccv, yuval kirstain, adam polyak, uriel singer, shahbuland ma- tiana, joe penna, omer levy. pick-a-pic open dataset user preferences text-to-image generation. neurips, baiqi li, zhiqiu lin, deepak pathak, jiayao li, yixin fei, kewen wu, xide xia, pengchuan zhang, graham neubig, deva ramanan. evaluating improving composi- tional text-to-visual generation. cvpr, daiqing li, aleks kamko, ehsan akhgari, ali sabet, lin- miao xu, suhail doshi. playground three in- sights towards enhancing aesthetic quality text-to-image generation. arxiv preprint junnan li, dongxu li, silvio savarese, steven hoi. blip- bootstrapping language-image pre-training frozen image encoders large language models. icml, zhimin li, jianwei zhang, qin lin, jiangfeng xiong, yanxin long, xinchi deng, yingfang zhang, xingchao liu, minbin huang, zedong xiao, al. hunyuan-dit powerful multi-resolution diffusion transformer fine-grained chi- nese understanding. arxiv preprint haotian liu, chunyuan li, qingyang wu, yong jae lee. visual instruction tuning. neurips, haotian liu, chunyuan li, yuheng li, yong jae lee. improved baselines visual instruction tuning. cvpr, haotian liu, chunyuan li, yuheng li, li, yuanhan zhang, sheng shen, yong jae lee. llavanext improved reasoning, ocr, world knowledge, haoyu lu, wen liu, zhang, bingxuan wang, kai dong, liu, jingxiang sun, tongzheng ren, zhuoshu li, hao yang, al. deepseek-vl towards real-world vision- language understanding. arxiv preprint shiyin lu, yang li, qing-guo chen, zhao xu, weihua luo, kaifu zhang, han-jia ye. ovis structural embed- ding alignment multimodal large language model. arxiv preprint alec radford, jong wook kim, chris hallacy, aditya ramesh, gabriel goh, sandhini agarwal, girish sastry, amanda askell, pamela mishkin, jack clark, al. learn- ing transferable visual models natural language super- vision. icml, aditya ramesh, prafulla dhariwal, alex nichol, casey chu, mark chen. hierarchical text-conditional image gen- eration clip latents. arxiv preprint robin rombach, andreas blattmann, dominik lorenz, patrick esser, bjorn ommer. high-resolution image syn- thesis latent diffusion models. cvpr, axel sauer, dominik lorenz, andreas blattmann, robin rombach. adversarial diffusion distillation. eccv, yongliang shen, kaitao song, tan, dongsheng li, weiming lu, yueting zhuang. hugginggpt solving tasks chatgpt friends huggingface. neurips, gemini team, rohan anil, sebastian borgeaud, jean- baptiste alayrac, jiahui yu, radu soricut, johan schalkwyk, andrew dai, anja hauth, katie millican, al. gemini family highly capable multimodal models. arxiv preprint kolors team. kolors effective training diffusion model photorealistic text-to-image synthesis. arxiv preprint, jason wei, xuezhi wang, dale schuurmans, maarten bosma, fei xia, chi, quoc le, denny zhou, al. chain-of-thought prompting elicits reasoning large lan- guage models. neurips, olivia wiles, chuhan zhang, isabela albuquerque, ivana kajic, wang, emanuele bugliarello, yasumasa onoe, chris knutsen, cyrus rashtchian, jordi pont-tuset, al. revisiting text-to-image evaluation gecko met- rics, prompts, human ratings. arxiv preprint chenfei wu, shengming yin, weizhen qi, xiaodong wang, zecheng tang, nan duan. visual chatgpt talking, drawing editing visual foundation models. arxiv preprint haoning wu, zicheng zhang, weixia zhang, chaofeng chen, liang liao, chunyi li, yixuan gao, annan wang, erli zhang, wenxiu sun, al. q-align teaching lmms visual scoring via discrete text-defined levels. icml, xiaoshi wu, yiming hao, keqiang sun, yixiong chen, feng zhu, rui zhao, hongsheng li. human preference score solid benchmark evaluating human preferences text-to-image synthesis. arxiv preprint jiazheng xu, xiao liu, yuchen wu, yuxuan tong, qinkai li, ming ding, jie tang, yuxiao dong. imagere- ward learning evaluating human preferences text- to-image generation. neurips, michal yarom, yonatan bitton, soravit changpinyo, roee aharoni, jonathan herzig, oran lang, eran ofek, idan szpektor. see read? improving text- image alignment evaluation. neurips, jiabo ye, haiyang xu, haowei liu, anwen hu, ming yan, qian, zhang, fei huang, jingren zhou. mplug- owl towards long image-sequence understanding multi- modal large language models. iclr, yuze zhao, jintao huang, jinghan hu, xingjun wang, yun- lin mao, daoze zhang, zeyinzi jiang, zhikai wu, baole ai, ang wang, wenmeng zhou, yingda chen. swifta scal- able lightweight infrastructure fine-tuning, deyao zhu, jun chen, xiaoqian shen, xiang li, mo- hamed elhoseiny. minigpt- enhancing vision-language understanding advanced large language models. arxiv preprint", "published_date": "2025-04-16T12:21:49+00:00"}
{"id": "2504.11326v2", "title": "PVUW 2025 Challenge Report: Advances in Pixel-level Understanding of Complex Videos in the Wild", "authors": ["Henghui Ding", "Chang Liu", "Nikhila Ravi", "Shuting He", "Yunchao Wei", "Song Bai", "Philip Torr", "Kehuan Song", "Xinglin Xie", "Kexin Zhang", "Licheng Jiao", "Lingling Li", "Shuyuan Yang", "Xuqiang Cao", "Linnan Zhao", "Jiaxuan Zhao", "Fang Liu", "Mengjiao Wang", "Junpei Zhang", "Xu Liu", "Yuting Yang", "Mengru Ma", "Hao Fang", "Runmin Cong", "Xiankai Lu", "Zhiyang Chen", "Wei Zhang", "Tianming Liang", "Haichao Jiang", "Wei-Shi Zheng", "Jian-Fang Hu", "Haobo Yuan", "Xiangtai Li", "Tao Zhang", "Lu Qi", "Ming-Hsuan Yang"], "summary": "report provides comprehensive overview pixel-level video understanding wild pvuw challenge, held conjunction cvpr summarizes challenge outcomes, participating methodologies, future research directions. challenge features two tracks mose, focuses complex scene video object segmentation, mevis, targets motion-guided, language-based video segmentation. tracks introduce new, challenging datasets designed better reflect real-world scenarios. detailed evaluation analysis, challenge offers valuable insights current state-of-the-art emerging trends complex video segmentation. information found workshop website", "full_text": "pvuw challenge report advances pixel-level understanding complex videos wild henghui ding, chang liu, nikhila ravi, shuting he, yunchao wei, song bai, philip torr kehuan song, xinglin xie, kexin zhang, licheng jiao, lingling li, shuyuan yang xuqiang cao, linnan zhao, jiaxuan zhao, fang liu mengjiao wang, junpei zhang, liu, yuting yang, mengru hao fang, runmin cong, xiankai lu, zhiyang chen, wei zhang tianming liang, haichao jiang, wei-shi zheng, jian-fang haobo yuan, xiangtai li, tao zhang, qi, ming-hsuan yang abstract report provides comprehensive overview pixel-level video understanding wild pvuw chal- lenge, held conjunction cvpr summarizes challenge outcomes, participating methodologies, future research directions. challenge features two tracks mose, focuses complex scene video object segmentation, mevis, targets motion- guided, language-based video segmentation. tracks introduce new, challenging datasets designed better reflect real-world scenarios. detailed evaluation analysis, challenge offers valuable insights current state-of-the-art emerging trends complex video segmentation. information found workshop website introduction pixel-level understanding dynamic complex visual scenes remains core yet unresolved problem computer vision traditional research predominantly focused semantic segmentation within static images approaches fall short capturing temporal continuity real world. contrast, video segmentation offers realistic framework, aligning better applications demand spatiotemporal reasoningsuch autonomous driving, aerial navigation, mobile video editing. use cases underscore growing shift toward scene under- authors cvpr pvuw challenge organizers. others challenge participants top- teams mose mevis tracks. corresponding henghui ding henghui.dinggmail.com, institute big data, fudan university, shanghai, china. standing methods spatially precise also temporally coherent. advance research direction, introduce pixel-level video understanding wild pvuw workshop, emphasizes challenges posed unconstrained, real-world environ- ments pvuw seeks narrow gap static dynamic scene understanding, encouraging development robust algorithms generalize across diverse, time-varying visual conditions. ini- tiative, aim catalyze innovation toward deploying perception systems capable reliable operation wild. recent advances large language models mul- timodal llms significantly reshaped computer vi- sion alongside, foundational models like sam leveraged large-scale data achieve strong gener- alization. notably, progress tasks video ob- ject segmentation vos referring video object segmentation rvos highlights fields continued momentum toward robust unified vision systems. building developments, goal work- shop challenge keep pace cutting-edge re- search, offer challenging, yet realistic benchmark eval- uate state-of-the-art models, provide valuable insights current trends future directions video understanding. following past challenges, aim con- tinuously provide challenging diverse benchmarking data taken real world, year, added latest data first time released. pvuw challenge year, center challenge around two focused tracks mose track, benchmarks advanced vos methods complex densely populated scenes mevis track, evaluates models language-guided cs.cv apr table mose track results top final rankings. rank team brainybots deepsegma jio scu leung wulutuluman mima stelatos maxbitter xiaomiyu menghaoran zjy keeper zhaojinhui luxeedr huaweiaitom yulinlin cchub zhimu ppbb video segmentation, particular emphasis motion- guided language expressions. two video segmentation tracks track mose track complex video object segmentation mose aims track segment objects videos complex environ- ments. track based mose dataset, new video object segmentation benchmark designed study object tracking segmentation complex, real-world scenes. unlike previous video segmentation datasets focus salient isolated objects, mose features crowded environments, frequent occlu- sions, object disappearances. consists video clips objects across categories, high-quality segmentation masks. mose chal- lenges existing vos models highlights performance gap complex scenarios, encouraging research robust segmentation techniques. years testing set part mose testing set, challenging newly taken data added. ground truths videos testing sets confidential never released before. year, teams registered mose track platform, teams submitted results testing phase. top results shown table top three teams imaplus, kirinczw, dumplings. first place team achieved score testing set. track mevis track motion expression guided video segmentation mevis focuses segmenting objects video based table mevis track results top final rankings. rank team mvp-lab referdino-isee sava pengsong ssams strong kimchi seilvik yiweima xmu maclab xinming zhangtao-whu yiweima transvg xmu-xiaoma myolo kker x-clip tbao luqilxx mengyuan sentence describing motion objects, based mevis dataset. mevis dataset large-scale benchmark designed motion-guided language-based video object segmentation. unlike previous referring image segmentation referring video segmenta- tion works focus static object attributes, mevis emphasizes motion-centric language expressions identify segment target objects complex video scenes. features wide range motion expressions paired videos containing crowded dynamic environments. benchmarking results show existing referring video object segmentation methods struggle task, highlighting need new methods better leverage motion primary cue language-guided video segmentation. similarly, testing set track comes mevis testing set, newly added videos confidential ground-truths. mevis track, year attracted teams registered, teams participated testing phase. top three teams mvp-lab, referdino-plus, harbory, shown table evaluation tracks evaluated using standard metrics consistent prior pvuw challenges benchmarks davis youtube-vos specifically, adopt region similarity contour accuracy average serving primary ranking metric. evaluations conducted publicly accessible codalab platform. sec. sec. presents solutions top- teams mose track mevis track, respectively. mose track top solution team mose track brainybots title stseg members kehuan song, xinglin xie, kexin zhang, licheng jiao, lingling li, shuyuan yang affiliation xidian university, china optimize solution across training inference stages. training, fine-tune sam tmo mose dataset better adapt challenges video object segmentation complex environments. inference, leverage ensemble five modelssam, tmo, cutie, xmem, livoson mose test set. predicted masks models aggregated construct rich pseudo-labels. based these, dynami- cally select suitable model per video instance ensure optimal segmentation quality. detailed fine-tuning strategies provided full report. adaptive pseudo-labels guided model refinement pipeline analyzing dataset, found challeng- ing achieve good results scenarios using single model. therefore, propose adaptive pseudo-labels guided model refinement pipeline pgmr, shown fig specific implementation steps follows multi-model inference independent processing result collection. video frame segmentation track- ing tasks, first employ multi-model independent in- ference process set video frames. model demonstrates unique performance advantages dif- ferent scenarios based design features training data. fully leverage strengths model, designed parallel inference framework ensures model operate independently produce results without interference models. framework allows multiple models perform inferences set video frames simultaneously, enabling model perform best without influenced others. output results model collected separately include segmentation masks, tracking ids, confi- dence scores. segmentation masks used accurately delineate boundaries target objects within video frames tracking ids employed continuously track positional changes target objects throughout video sequence confidence scores reflect models assessment prediction. pseudo-label fusion generating baseline result. optimize performance video frame segmentation tracking tasks, crucial integrate inference results multiple models comprehensive pseudo- label. pseudo-label serves key baseline figure overview pgmr framework. inference pseudo-label-based model selection employing five models conduct inference operations, model optimal performance different video contents intelligently selected. subsequent optimization process helps identify model performs optimally different video contents. generation pseudo-label involves several steps firstly, consistency check carried comparing segmentation masks tracking ids different models identify regions model results consistent inconsistent. then, confidence weighting performed. weights as- signed model based historical performance confidence scores associated predictions. finally, voting mechanism employed regions models produce conflicting results, conflict resolution strategy adopted. fused pseudo-label, key intermediate link, bridges gap outputs individual models performance unified optimization system. enables intelligent selection model demon- strates best performance different video contents. model recommendation mechanism intelligent task allocation. based generated pseudo-label, developed dynamic model recommendation mechanism ensure video frame processed suitable model. first, feature extraction conducted analyze video frames extract key information scene complexity, number objects, distribution object sizes. subsequently, established compact model per- formance database record historical performance model across various feature scenarios. finally, recommendation algorithm employed recommend optimal model video frame based extracted frame features information stored model performance database. implementing model recommendation mecha- nism, system able dynamically allocate tasks suitable model video frame. team mose track deepsegma title deepsegma members xuqiang cao, linnan zhao, jiaxuan zhao, fang liu affiliation key laboratory intelligent perception image understanding, china method. overview framework presented figure better align characteristics mose dataset, construct tailored dataset, mose, introduce set targeted data augmentation strate- gies mimic real-world variations appearance, pose, illumination, structural consistency. inference, employ mask confidence control mechanism, followed temporal fusion across frames generate final segmentation outputs. component detailed below. baseline model. use transformer-based segmen- tation framework object-guided attention, mask-aware memory, spatiotemporal reasoning. model effec- tively captures temporal cues spatial details dual memory modules multi-scale decoding, enabling robust performance challenging scenarios like occlu- sion, motion blur, small-object clutter. strong base- line lays solid foundation enhancement strategies. loss function. achieve high-precision segmentation temporal consistency, design multi-task loss combines pixel-wise accuracy, region-level overlap, classification discriminability, robustness occlusion. total loss defined mat cal ltotal lambda mathcal lce lambda mathcal ldice lambda mathcal lsim lambda mathcal lmaskiou, lce denotes cross-entropy loss foreground- background classification, ldice enhances region consis- tency, lsim enforces similarity memory query features, lmaskiou constrains predicted mask quality. losses computed across multiple frames can- didate masks jointly supervise spatiotemporal modeling. data augmentation. improve generalization robustness, introduce set targeted augmentation strategies training. unlike static image tasks, video segmentation demands consistency across frames simulating realistic variations. approach integrates frame-consistent frame-inconsistent perturbations consistent geometric transformations random hori- zontal flipping, affine transformations rotation, shear, multi-scale resizing applied across frames clip simulate viewpoint object deformation. mixed color perturbations brightness, contrast, saturation changes applied globally, grayscale conversion inconsistent color jittering selectively applied individual frames, enhancing robustness lighting changes visual ambiguity. figure overview team deepsegmas method. normalization images transformed tensors normalized using imagenet mean standard deviation stable convergence pretrained compatibility. augmentations significantly improve models ability handle structure variation, appearance change, dynamic scenes mose-like scenarios. inference strategy. improve model robustness adaptability complex video scenarios, introduce set tailored strategies inference. mask confidence control strategy. observe quality predicted masks significantly affected post-processing different scenarios, small ob- jects, heavy occlusions, target overlaps. address this, adopt control strategy based dynamic adjustment mask output distribution, using two key parameters sigmoid scale sigmoid bias. sigmoid scale controls sharpness output boundaries, sigmoid bias adjusts overall activation level, thereby influencing target coverage boundary quality. experiments validation set show setting sigmoid scale sigmoid bias yields best performance. data. improve generalization target modeling complex scenarios, construct enhanced training set named mose, based original mose dataset. augmented set composed video segments multiple public vos datasets, selected match char- acteristics mose, including frequent occlusions, dense small objects, object reappearance, high similarity among targets. specifically, integrate carefully chosen sequences datasets burst davis ovis youtubevis unify annotations resolution formats, seamlessly merge mose form consistent training set, enhancing semantic understanding robustness. please refer main technical report deepsegma model training experiment details. team mose track jio title fvos members mengjiao wang, junpei zhang, liu, yuting yang, mengru affiliation international joint research center intelligent perception computation, china transformer blocks query frame query encoder conv mask mask decoder prediction mask memory encoder memory frames figure network architecture fvos. method. approach primarily consists three components model fine-tuning training, morphological post-processing, multi-scale segmentation result fusion. figure illustrates network architecture adopted framework, primarily relies transformers feature extraction attention computation. mose fine-tuning. training process follows first, fine-tune pre-trained model mose dataset total epochs, submitting results validation set epoch. best-performing model stage selected pre-trained model begin new round training. second stage, conduct training total epochs, selecting best-performing model testing optimal parameters. finally, single best-performing model selected generate initial single-model segmentation results. morphological post-processing. training, noticed exists distinct gap adjacent objects. model predicts separate objects individually merging inference, thus edge regions well aligned. address problem, propose using morphological operations, especially di- lation, post-processing inference network, binary segmen- tation masks object first obtained collected. current object, dilation operations performed object objects. adjacency objects current object determined checking whether dilated masks overlap. objects deemed adjacent, overlapping regions filled applied current object. finally, object mask merg- ing performed following rule prioritizing higher- indexed objects, yielding final segmentation results. figure test time data augmentation multi-scale magnification operations. original image. clockwise clockwise clockwise horizontal flipping. multi-scale magnification. based experiments, using kernel size yields better improvements segmentation results. multi-scale results fusion. also adopted common test-time data augmentation methods, including rotating original image clockwise horizontal flipping, well multi-scale processing resizing image several scales, shown figure specifically, starting original size, resized dataset increments reconstruct multiple scales. experimenting several scales, finally selected different scales ranging fusion. mevis track top solution team mevis track mvp-lab title unleashing potential large multi- modal models referring video segmen- tation members hao fang, runmin cong, xiankai lu, zhiyang chen, wei zhang affiliation shandong university input rvos contains video sequence rhw frames corresponding referring expression tll words. baseline. adopt sava baseline obtain mask sequences mtn correlated language descriptions math cal mathcal frvosleft mathcal mathcal tright frvos denotes sava model. overall architecture sava shown fig. contains two parts llava-like model sam pre-trained lmms. sava adopts pre-trained llava- like models lmms. contains one visual encoder, one visual projection layer, one llm. visual encoder takes input images, video, sub-images inputs. visual projection layer maps inputs visual tokens. tokens, combined input text tokens, input llms llms generate text token llm text image tokenizer image encoder visual prompt prompt encoder sam encoder lora image video text decoder language embd image embd video image encoder image encoder video embd prompt embd image caption video caption image video sam decoder visual features language features language outputs image outputs video outputs referring image segmentation referring video segmentation seg token figure architecture sava model first encodes input texts, visual prompts, images, videos token embeddings. tokens processed large language model llm. output text tokens used generate seg token associated language outputs. sam decoder receives image video features sam encoder, along seg token, generate corresponding image video masks. prediction based them. note sava adopts pre- trained lmms following previous works lever- age strong capability. applies pipeline image video chat datasets without modification. decoupled design. sava append sam alongside pre-trained llava model. take sam out- put tokens visual features decoder outputs llm. three reasons. sava makes combination simple possible without increasing extra computation costs. adding extra tokens needs extra alignment process. via design, fully make work plug-in-play framework utilize pre-trained lmms since lmm community goes fast. thus, sava adopts de- coupled design without introducing communication llava sam tuning sam decoder via seg tokens. sava connects sam lmm via special token seg. hidden states seg token used new type prompt fed sam decoder generate segmentation masks. hidden states seg seen novel spatial-temporal prompt sam sam segments corresponding object mask image video based spatial-temporal prompt. training, sam decoder tuned understand spatial- temporal prompt, gradients backpropagated seg token lmm, allowing output spatial-temporal prompt better. inference. rvos tasks, sava designs simple framework achieve strong results public benchmarks. particular, giving input video, adopts seg token generate masks key frames. then, uses memory encoded key frame features generate mask remaining frames. sava defaults extracting first five frames input video key frames llm, mevis long video dataset, results significant loss video information. address algorithm rvos inference pipeline input video length number key frames video frames x,. language description output sequence masks m,. run sava model rvos uniform sampling extract key frames visual embeddings encodersm language embeddings encodert answers llmev, prompt embedding linearfinda, seg sam feature encoderx mask decoderpl, update memory mem cross-attentionmem, sam feature encoderx mask decodermem, update memory mem cross-attentionmem, emit m,. this, shown algorithm uniformly sample key frames across entire video provide llm comprehensive temporal context. key frames fed clip flattened visual sequential tokens llm processing. llm takes visual language tokens input uses tokens extract information video generate seg token. sam prompt encoder encodes boxes clicks prompt embeddings object referring. different sam sava use two linear layers project seg token language prompt embedding, serves extension sam prompt encoders. language prompt embedding, uses sam decoder generate masks key frames. then, sava use memory encoder sam generate memory based output key-frame masks. finally, memory attention sam- uses memory, along prior non-key-frame masks, generate remaining frame masks. aggregation. find sava necessarily perform better larger number parameters sampling frames, configuration strengths different videos. videos can- accurately segmented lmms, classic rvos model may handle well. integrate results multiple expert models mitigate erroneous predictions single model math mathcal ffuseleft mathcal mkright sets mask sequences output sava models different configurations rvos models ffuse denotes pixel-level binary mask voting. pixels value equal divide pixel foreground, otherwise, divided background. team mevis track referdino-isee title referdino-plus referdino sam members tianming liang, haichao jiang, wei-shi zheng, jian-fang affiliation sun yat-sen university referdino bison attacked lions. sam conditional mask fusion mask figure overview referdino-plus. video- description pair, input referdino derive object masks corresponding scores across frames. then, select mask highest score prompt sam, producing refined masks ms. finally, fuse two series masks conditional mask fusion strategy. overall framework solution referdino-plus presented figure video-description pair, input referdino derive object masks corresponding scores across frames. then, select mask highest score prompt sam, producing refined masks. finally, fuse two series masks conditional mask fusion strategy, generate final masks frame. cross-modal dense reasoning via referdino. refer- dino strong rvos model inheriting object-level vision-language knowledge groundingdino endowed pixel-level dense prediction cross-modal spatiotemporal reasoning. given video clip frames text description, referdino performs cross-modal reasoning segmentation, deriving mask sequence corresponding scores throughout video. following previous works combine multiple object masks scores higher preset threshold handle multi-object cases. post enhancement sam. sam power- ful prompt-based segmentation model capable efficiently generating high-quality object masks across video frames given cues clicks, bounding boxes, masks. integrate sam enhance mask precision temporal consistency referdino predictions. obtaining frame-wise masks associated confidence scores, select highest-scoring mask reference prompt. using reference frame mask, sam propagates refines segmentation across entire video, yielding sequence masks conditional mask fusion. although masks sam reliable stable, observe sams overall performance mevis significantly weaker referdino. experiments, identify main reason that, multi-object mask prompts, sam tends degenerate single-object masks, leading substantial target loss subsequent frames. address issue, design conditional mask fusion cmf principle single-object cases, output masks sam multi-object cases, combine masks referdino sam. however, remains challenging determine whether expression involves multiple objects. solution, define multi-object case mask area sam less referdinos. formally, process described follows begin cases text mathcal ams frac mathcal amr, text otherwise, end cases indicates mask area. note cmf applied individually frame, empirically achieves better performance. team mevis track sava title sava members haobo yuan, xiangtai li, tao zhang, qi, ming-hsuan yang affiliation merced bytedance wuhan university meta architecture. shown fig. sava consists mllm sam. mllm accepts inputs images, videos, text instructions, outputs text responses based text instructions. user instruction requires model output segmentation re- sults, text response include segmentation token seg. segmentation tokens hidden states serve implicit prompts converted sam image video-level object segmentation masks. mllm. sota mllm internvl adopted mllm, demonstrating powerful capabilities single-image, multi-image, video understanding conversation. internvl adopts llava-like architecture, consisting internvit mlp projector, large language model. high-resolution images first divided several sub-images thumbnail, encoded internvit vision tokens, mapped one mlp combined text tokens input llm. llm autoregressively output text responses, may include segmentation tokens. segmentation tokens hidden states last llm transformer layer processed mlp serve prompt input sam sam. sam generates object segmentation results high-resolution video frames based segmentation prompts output mllm. subsequently, sam propagates frame segmentation results obtain object segmentation results entire video. sava model training. original sava co- trained multiple datasets, including imagevideo vqa datasets, caption datasets, imagevideo referring seg- mentation datasets, including mevis. challenge, fine-tune model mevis, focus test time modifications sava. naive ref-vos inference pipeline. origin pipeline sava begins extracting first five frames set respectively input video keyframes, ensuring capture critical context following processing. key frames fed clip flattened visual sequential tokens llm processing. llm takes visual language tokens input uses tokens extract information video generate seg token. sam- prompt encoder encodes boxes clicks prompt embeddings object referring. different sam-, use two linear layers project seg token language prompt embedding, serves extension sam- prompt encoders. using language prompt embedding, employ sam- decoder generate key- frame masks. encode masks memory via sam-s memory encoder. finally, memory attention module produces remaining masks based key- frame prior non-key-frame masks. test time augmentation sava mevis long-interleaved inference. naive ref-vos inference pipeline directly uses first several frames keyframes. however, may lead suboptimal performance initial frames lack sufficient context accurate reference embedding. especially evident language prompt requires longer temporal reasoning. address issue, propose inference strategy named long-interleaved inference lii. intentionally lengthen time duration key frames capture context video. specifically, interleave keyframes across longer temporal window rather selecting consecutively beginning. sample keyframes fixed intervals throughout video, ensuring early late contextual signals incorporated reference embedding. keep whole method simple overly dependent hyperparameters, use interval videos. whole algorithm similar naive ref-vos inference pipeline, main difference key frame selection strategy. set fixed set values execution entire pipeline. long-interleaved inference strategy, keyframes longer clustered beginning spread across longer video clip. design encourages model capture long-term dependencies, particularly beneficial scenarios object appearance scene context changes time. attempts. also try model ensembling strategy competition, shows performance degradation adopted final result. model ensembling strategy, use two separate sam- decoders inference. first one sava, trained one-shot instruction tuning process different original sam- decoder shown figure one original sam-. process predicting key frame masks, use sam- decoder sava need use seg token prompt. input key frame masks second sam- decoder infer rest masks. hope use approach separate reasoning tracking. however, observe performance drop apply strategy. conclusion discussion years pvuw challenge attracted record number participants. high level engagement highlights growing interest relevance pixel-level video understanding within research community. top-performing methods, several key insights emerge. first, observe critical importance high-quality data. datasets mose mevis, offer fine- grained annotations, enable methods powered large- scale pre-trained models like sam achieve strong performance. second, multi-modal large language models llms beginning demonstrate significant potential video understanding, particularly language-guided video tasks. continued evolution llms, expect play increasingly vital role field. findings offer clear directions future research. importance scalingboth model capacity quality training datahas reinforced across many submissions. llms continue improve multimodal capabilities, believe advance state video understanding. looking ahead, continue updating training testing sets mose mevis datasets, remain committed pushing boundaries pixel-level video understanding. references ali athar, jonathon luiten, paul voigtlaender, tarasha khurana, achal dave, bastian leibe, deva ramanan. burst benchmark unifying object recognition, segmentation tracking video. wacv, zhe chen, weiyun wang, yue cao, yangzhou liu, zhang- wei gao, erfei cui, jinguo zhu, shenglong ye, hao tian, zhaoyang liu, al. expanding performance boundaries open-source multimodal models model, data, test- time scaling. arxiv preprint zhe chen, jiannan wu, wenhai wang, weijie su, guo chen, sen xing, muyan zhong, qinglong zhang, xizhou zhu, lewei lu, al. internvl scaling vision foundation models aligning generic visual-linguistic tasks. cvpr, mary comer edward delp iii. morphological operations color image processing. journal electronic imaging, henghui ding, xudong jiang, bing shuai, qun liu, gang wang. context contrasted feature gated multi- scale aggregation scene segmentation. proceedings ieee conference computer vision pattern recognition, pages henghui ding, xudong jiang, qun liu, nadia magnenat thalmann, gang wang. boundary-aware feature propagation scene segmentation. proceedings ieeecvf international conference computer vision, pages henghui ding, xudong jiang, bing shuai, qun liu, gang wang. semantic correlation promoted shape-variant context segmentation. proceedings ieeecvf conference computer vision pattern recognition, pages henghui ding, chang liu, suchen wang, xudong jiang. vision-language transformer query generation referring segmentation. proceedings ieeecvf international conference computer vision, pages henghui ding, chang liu, shuting he, xudong jiang, chen change loy. mevis large-scale benchmark video segmentation motion expressions. proceedings ieeecvf international conference computer vision, pages henghui ding, chang liu, shuting he, xudong jiang, philip torr, song bai. mose new dataset video object segmentation complex scenes. proceedings ieeecvf international conference computer vision, pages henghui ding, chang liu, suchen wang, xudong jiang. vlt vision-language transformer query generation referring segmentation. ieee transactions pattern analysis machine intelligence, henghui ding, lingyi hong, chang liu, ning xu, linjie yang, yuchen fan, deshui miao, yameng gu, xin li, zhenyu he, al. lsvos challenge report large-scale complex long video object segmentation. eccv workshop, henghui ding, chang liu, yunchao wei, nikhila ravi, shuting he, song bai, philip torr, deshui miao, xin li, zhenyu he, al. pvuw challenge complex video understanding methods results. eccv workshop, hao fang, feiyu pan, xiankai lu, wei zhang, runmin cong. uninext-cutie solution lsvos challenge rvos track. arxiv preprint shuting henghui ding. decoupling static hierarchical motion perception referring video segmentation. proceedings ieeecvf conference computer vision pattern recognition, pages shuting henghui ding. refmaskd language- guided transformer referring segmentation. proceedings acm international conference multimedia, pages shuting he, henghui ding, chang liu, xudong jiang. grec generalized referring expression comprehension. arxiv preprint shuting he, henghui ding, xudong jiang, bihan wen. segpoint segment point cloud via large language model. european conference computer vision, pages springer, syed ariff syed hesham, yun liu, guolei sun, henghui ding, jing yang, ender konukoglu, xue geng, xudong jiang. exploiting temporal state space sharing video semantic segmentation. proceedings ieeecvf conference computer vision pattern recognition, lei ke, henghui ding, martin danelljan, yu-wing tai, chi-keung tang, fisher yu. video mask transfiner high-quality video instance segmentation. european conference computer vision, pages springer, lei ke, martin danelljan, henghui ding, yu-wing tai, chi-keung tang, fisher yu. mask-free video instance segmentation. proceedings ieeecvf conference computer vision pattern recognition, pages xin lai, zhuotao tian, yukang chen, yanwei li, yuhui yuan, shu liu, jiaya jia. lisa reasoning segmentation via large language model. proceedings ieeecvf conference computer vision pattern recognition, pages xiangtai li, henghui ding, haobo yuan, wenwei zhang, jiangmiao pang, guangliang cheng, kai chen, ziwei liu, chen change loy. transformer-based visual segmentation survey. ieee transactions pattern analysis machine intelligence, tianming liang, kun-yu lin, chaolei tan, jianguo zhang, wei-shi zheng, jian-fang hu. referdino referring video object segmentation visual grounding foundations. arxiv preprint chang liu, xudong jiang, henghui ding. instance- specific feature propagation referring segmentation. ieee tmm, chang liu, henghui ding, xudong jiang. gres generalized referring expression segmentation. proceedings ieeecvf conference computer vision pattern recognition, pages chang liu, henghui ding, yulun zhang, xudong jiang. multi-modal mutual attention iterative interaction referring image segmentation. ieee tip, chang liu, xudong jiang, henghui ding. primitivenet decomposing global constraints referring segmenta- tion. visual intelligence, chang liu, xiangtai li, henghui ding. referring image editing object-level image editing via referring expressions. proceedings ieeecvf conference computer vision pattern recognition, pages haotian liu, chunyuan li, qingyang wu, yong jae lee. visual instruction tuning. advances neural information processing systems, shilong liu, zhaoyang zeng, tianhe ren, feng li, hao zhang, jie yang, qing jiang, chunyuan li, jianwei yang, hang su, al. grounding dino marrying dino grounded pre-training open-set object detection. european conference computer vision, pages springer, jordi pont-tuset, federico perazzi, sergi caelles, pablo arbelaez, alexander sorkine-hornung, luc van gool. davis challenge video object segmentation. jiyang qi, yan gao, yao hu, xinggang wang, xiaoyu liu, xiang bai, serge belongie, alan yuille, philip torr, song bai. occluded video instance segmentation benchmark. international journal computer vision, nikhila ravi, valentin gabeur, yuan-ting hu, ronghang hu, chaitanya ryali, tengyu ma, haitham khedr, roman radle, chloe rolland, laura gustafson, al. sam segment anything images videos. arxiv preprint xincheng shuai, henghui ding, xingjun ma, rongcheng tu, yu-gang jiang, dacheng tao. survey multimodal-guided image editing text-to-image diffusion models. guolei sun, yun liu, henghui ding, thomas probst, luc van gool. coarse-to-fine feature mining video semantic segmentation. proceedings ieeecvf conference computer vision pattern recognition, pages guolei sun, yun liu, henghui ding, min wu, luc van gool. learning local global temporal contexts video semantic segmentation. ieee transactions pattern analysis machine intelligence, peng wang, shuai bai, sinan tan, shijie wang, zhihao fan, jinze bai, keqin chen, xuejing liu, jialin wang, wenbin ge, al. qwen-vl enhancing vision-language models perception world resolution. arxiv preprint yaxian wang, henghui ding, shuting he, xudong jiang, bifan wei, jun liu. hierarchical alignment-enhanced adaptive grounding network generalized referring expression comprehension. aaai, changli wu, yihang liu, jiayi ji, yiwei ma, haowei wang, gen luo, henghui ding, xiaoshuai sun, rongrong ji. d-gres generalized referring expression segmentation. proceedings acm international conference multimedia, pages jianzong wu, xiangtai li, xia li, henghui ding, yunhai tong, dacheng tao. towards robust referring image segmentation. ieee transactions image processing, jianzong wu, xiangtai li, shilin xu, haobo yuan, henghui ding, yibo yang, xia li, jiangning zhang, yunhai tong, xudong jiang, al. towards open vocabulary learning survey. ieee transactions pattern analysis machine intelligence, ning xu, linjie yang, yuchen fan, jianchao yang, dingcheng yue, yuchen liang, brian price, scott cohen, thomas huang. youtube-vos sequence-to-sequence video object segmentation. eccv, cilin yan, haochen wang, shilin yan, xiaolong jiang, yao hu, guoliang kang, weidi xie, efstratios gavves. visa reasoning video object segmentation via large language models. european conference computer vision, pages springer, linjie yang, yuchen fan, ning xu. large- scale video object segmentation challenge video object segmentation track, haobo yuan, xiangtai li, tao zhang, zilong huang, shilin xu, shunping ji, yunhai tong, qi, jiashi feng, ming-hsuan yang. sava marrying sam llava dense grounded understanding images videos. arxiv preprint hui zhang henghui ding. prototypical matching open set rejection zero-shot semantic segmentation. proceedings ieeecvf international conference computer vision iccv, pages", "published_date": "2025-04-15T16:02:47+00:00"}
{"id": "2504.10254v1", "title": "MASSeg : 2nd Technical Report for 4th PVUW MOSE Track", "authors": ["Xuqiang Cao", "Linnan Zhao", "Jiaxuan Zhao", "Fang Liu", "Puhua Chen", "Wenping Ma"], "summary": "complex video object segmentation continues face significant challenges small object recognition, occlusion handling, dynamic scene modeling. report presents solution, ranked second mose track cvpr pvuw challenge. based existing segmentation framework, propose improved model named masseg complex video object segmentation, construct enhanced dataset, mose, includes typical scenarios occlusions, cluttered backgrounds, small target instances. training, incorporate combination inter-frame consistent inconsistent data augmentation strategies improve robustness generalization. inference, design mask output scaling strategy better adapt varying object sizes occlusion levels. result, masseg achieves score score score mose test set.", "full_text": "masseg technical report pvuw mose track xuqiang cao linnan zhao jiaxuan zhao fang liu puhua chen wenping key laboratory intelligent perception image understanding xian, china abstract complex video object segmentation continues face significant challenges small object recognition, occlu- sion handling, dynamic scene modeling. report presents solution, ranked second mose track cvpr pvuw challenge. based ex- isting segmentation framework, propose improved model named masseg complex video object segmen- tation, construct enhanced dataset, mose, includes typical scenarios occlusions, cluttered back- grounds, small target instances. training, incorporate combination inter-frame consistent inconsistent data augmentation strategies improve ro- bustness generalization. inference, design mask output scaling strategy better adapt varying object sizes occlusion levels. result, masseg achieves score score score mose test set.the code available introduction pixel-level scene understanding fundamental task computer vision, aiming recognize category, seman- tics, instance pixel. rapid growth video content, task extended static images dynamic videos, drawing increasing attention video object segmentation vos. vos requires segmenting tracking target objects across frames first- frame mask provided. widely applied au- tonomous driving, augmented reality, video editing, data annotation. task complexity increases, memory-based vos meth- ods become mainstream models main- tain memory bank store visual features adopt atten- tion feature matching segment targets future frames. representative works include stm introduces explicit memory pixel-level matching stcn figure representative challenges complex video object seg- mentation. examples showcase small object motion left, ap- pearance confusion occlusion middle, densely cluttered scenes right, reflect typical situations encountered mose dataset. improves efficiency unmasked frames affinity xmem mimics human memory sensory, working, long-term stages. cutie en- hances robustness crowded scenes via object-level mem- ory object transformers, achieving state-of-the-art re- sults youtube-vos davis. better evaluate model robustness real-world sce- narios, henghui ding al. proposed organized mose mevis tracks. tracks target two distinct complementary aspects video understanding mose focuses complex video object segmentation un- der challenging visual conditions, mevis emphasizes motion expression-guided segmentation based natural language descriptions. better evaluate model robustness real-world sce- narios, henghui ding al., collaboration cvpr, introduced pixel-level video understanding wild pvuw challenge. challenge focuses pixel- level scene understanding features two complemen- tary tracks mose targets complex multi- object segmentation challenging visual conditions, mevis emphasizes motion expression- guided segmentation based natural language descrip- tions. mose track achieved significant progress pvuw challenge mose dataset includes videos an- notated instances categories, pixel- level masks. mose emphasizes real-world challenges cs.cv apr figure overview method. occlusions, fast motion, dense small objects, similar ap- pearances, frequent reappearance, providing rigorous benchmark beyond prior datasets like davis youtube- vos. illustrated figure mose dataset contains challenging scenarios small objects, heavy occlu- sions, similar instances, significantly impact seg- mentation robustness. mainstream methods, e.g., cutie, achieve youtube-vos drop significantly mose, reveal- ing generalization limitations. mose thus emerged valuable platform driving progress robust, real-world vos. address mose-specific challenges, propose improved method based sam framework incorporating mask output control strategy retrain- ing enhanced dataset mose, includes di- verse small object instances complex scenes. fur- ther adopt inter-frame consistent inconsistent augmen- tations tuned inference strategies improve robustness across scales occlusions. method achieves score score mose test set, ranking overall. method overall solution illustrated figure based data characteristics mose dataset, construct customized dataset named mose design series targeted data augmentation strategies, aiming simu- late appearance variations, pose perturbations, illumination changes, structural blur commonly found real-world video scenarios. inference, mask confidence con- trol strategy adopted, followed fusion process video frames obtain final segmentation results. detailed components described below. baseline model adopt transformer-based segmentation framework featuring object-guided attention, mask-aware memory, spatiotemporal reasoning. model effectively captures temporal cues spatial details dual memory modules multi-scale decoding, enabling robust perfor- mance challenging scenarios occlusion, mo- tion blur, small-object clutter. strong baseline lays solid foundation enhancement strategies. loss function achieve high-precision segmentation temporal consistency, design multi-task loss combines pixel-wise accuracy, region-level overlap, classification dis- criminability, robustness occlusion. total loss defined ltotal lce ldicelsimlmaskiou lce denotes cross-entropy loss foreground- background classification, ldice enhances region consis- tency, lsim enforces similarity memory query features, lmaskiou constrains predicted mask quality. losses computed across multiple frames can- didate masks jointly supervise spatiotemporal modeling. implementation, weights empirically set balanced optimization. formulation improves segmentation performance small occluded targets supports temporal consistency long sequences. data augmentation improve generalization robustness, introduce set targeted augmentation strategies training. figure visualization data augmentation strategies. column shows example top bottom applying augmentation. left right geometric transformation e.g., affine distortion, color jittering inconsistent color shift, grayscale conversion. augmentations simulate realistic variations pose, illumination, appearance, improving robustness generalization model complex scenarios. unlike static image tasks, video segmentation demands consistency across frames simulating realistic vari- ations. approach integrates frame-consistent frame-inconsistent perturbations consistent geometric transformations random hori- zontal flipping, affine transformations rotation, shear, multi-scale resizing applied across frames clip simulate viewpoint object deformation. mixed color perturbations brightness, contrast, saturation changes applied globally, grayscale conversion inconsistent color jittering selectively applied individual frames, enhancing robustness lighting changes visual ambiguity. normalization images transformed tensors normalized using imagenet mean standard deviation stable convergence pretrained compatibility. augmentations significantly improve models ability handle structure variation, appearance change, dynamic scenes mose-like scenarios. inference strategy enhance robustness adaptability model complex video object segmentation, design apply series strategies inference. mask confidence control strategy. observe quality predicted masks significantly affected post-processing different scenarios, small ob- jects, heavy occlusions, target overlaps. address this, adopt control strategy based dynamic adjustment mask output distribution, using two key parameters sig- moid scale sigmoid bias. sigmoid scale controls sharpness output boundaries, sigmoid bias adjusts overall activation level, thereby influencing target coverage boundary quality. extensive experiments validation set show setting sigmoid scale sigmoid bias yields best performance mose dataset terms metrics. addition, parameter combi- nations explored improve local segmentation perfor- mance specific conditions dense small objects complex backgrounds. final predictions obtained merging results different local configurations achieve globally optimal segmentation. implementation details data. improve generalization target modeling complex scenarios, construct enhanced training set named mose, based original mose dataset. augmented set composed video segments mul- tiple public vos datasets, selected match character- istics mose, including frequent occlusions, dense small objects, object reappearance, high similarity among tar- gets. specifically, integrate carefully chosen sequences datasets burst davis ovis youtubevis unify annotations resolu- tion formats, seamlessly merge mose form consistent training set, enhancing semantic under- standing robustness. training. train model end-to-end mose dataset. batch contains samples, adopt adamw optimizer initial learning rate e-. input image resolution clip con- taining frames supporting object instances. employ automatic mixed precision amp apply gradient clipping max norm stabilize conver- gence. training conducted three nvidia figure qualitative results method challenging mose test sequences. model accurately segments small objects, handles severe occlusions, maintains temporal consistency across fast-moving cluttered scenes. nvlink gpus memory epochs, taking approximately hours total. main results method imaplus kirinczw dumplings table leaderboard mose track pvuw chal- lenge method achieved second place mose track cvpr pvuw challenge, score consisting region similarity contour accuracy leaderboard summa- rized tab. demonstrating robustness effective- ness method complex video object segmentation scenarios. ablation study evaluate contribution component, con- duct ablation studies mose validation set. shown tab. start public baseline cutie, achieved mosesignificantly lower performance traditional datasets like youtube- vos, indicating challenges posed complex scenes. method cutie val baseline val baseline val baseline mss val baseline mss test table ablation study results mose validation test sets. evaluate baseline framework multi- scale encoders, enhanced memory, mask mechanisms, improves adding mose proposed data augmentation strategy raises performance verifying effectiveness en- hanced training data. finally, introduce mask scal- ing strategy mss dynamically adjust output mask distribution using sigmoid scale bias, boosts performance validation set test set.as shown qualitative results fig. optimized method demonstrates superior performance mose test set, particularly segmenting small ob- jects, occluded targets, objects within complex clut- tered scenes. conclusion work, proposed robust solution complex video object segmentation integrating enhanced train- ing strategies mask confidence control mechanism. based mose dataset, approach incorporates targeted data augmentation adaptive mask output cali- bration improve segmentation performance chal- lenging scenarios. method achieved score ranked second mose track cvpr pvuw challenge. references ali athar, jonathon luiten, paul voigtlaender, tarasha khu- rana, achal dave, bastian leibe, deva ramanan. burst benchmark unifying object recognition, segmentation tracking video. wacv, yuxin cheng, yihao liu, zeyu wang, li, yu-wing tai, chi-keung tang. xmem long-term video object segmentation atkinson-shiffrin memory model. eccv, henghui ding, xudong jiang, bing shuai, qun liu, gang wang. semantic segmentation context encoding multi-path decoding. ieee transactions image pro- cessing, henghui ding, chang liu, shuting he, xudong jiang, chen change loy. mevis large-scale benchmark video segmentation motion expressions. proceedings ieeecvf international conference computer vi- sion iccv, pages october henghui ding, chang liu, shuting he, xudong jiang, philip torr, song bai. mose new dataset video object segmentation complex scenes. iccv, henghui ding, chang liu, yunchao wei, nikhila ravi, shuting he, song bai, philip torr, deshui miao, xin li, zhenyu he, yaowei wang, ming-hsuan yang, zhensong xu, jiangtao yao, chengjing wu, ting liu, luoqi liu, xinyu liu, jing zhang, kexin zhang, yuting yang, licheng jiao, shuyuan yang, mingqi gao, jingnan luo, jinyu yang, jungong han, feng zheng, bin cao, yisi zhang, xuanxu lin, xingjian he, zhao, jing liu, feiyu pan, hao fang, xiankai lu. pvuw challenge complex video understanding methods results, seoung wug oh, joon-young lee, ning xu, seon joo kim. video object segmentation using space-time memory networks. iccv, jordi pont-tuset, federico perazzi, sergi caelles, pablo ar- belaez, alexander sorkine-hornung, luc van gool. davis challenge video object segmentation. jiyang qi, yan gao, yao hu, xinggang wang, xiaoyu liu, xiang bai, serge belongie, alan yuille, philip torr, song bai. occluded video instance segmentation bench- mark. international journal computer vision, nikhila ravi, valentin gabeur, yuan-ting hu, ronghang hu, chaitanya ryali, tengyu ma, haitham khedr, roman radle, chloe rolland, laura gustafson, eric mintun, junt- ing pan, kalyan vasudev alwala, nicolas carion, chao- yuan wu, ross girshick, piotr dollar, christoph feicht- enhofer. sam segment anything images videos. arxiv preprint tao tang, xiaoyang wu, zhihao chen, xinggang wang, wenyu liu, xiang bai. associating objects trans- formers video object segmentation. cvpr, angtian wang, linjie yang, zhe lin, kevin barnes, humphrey shi. rethinking space-time networks im- proved memory coverage efficient video object segmen- tation. neurips, linjie yang, yuchen fan, ning xu. large-scale video object segmentation challenge video object segmen- tation track, oct.", "published_date": "2025-04-14T14:15:46+00:00"}
{"id": "2504.09960v1", "title": "Dual-Path Enhancements in Event-Based Eye Tracking: Augmented Robustness and Adaptive Temporal Modeling", "authors": ["Hoang M. Truong", "Vinh-Thuan Ly", "Huy G. Tran", "Thuan-Phat Nguyen", "Tram T. Doan"], "summary": "event-based eye tracking become pivotal technology augmented reality human-computer interaction. yet, existing methods struggle real-world challenges abrupt eye movements environmental noise. building efficiency lightweight spatiotemporal network-a causal architecture optimized edge devices-we introduce two key advancements. first, robust data augmentation pipeline incorporating temporal shift, spatial flip, event deletion improves model resilience, reducing euclidean distance error vs. baseline challenging samples. second, propose knightpupil, hybrid architecture combining efficientnet-b backbone spatial feature extraction, bidirectional gru contextual temporal modeling, linear time-varying state-space module adapt sparse inputs noise dynamically. evaluated benchmark, framework achieved euclidean distance private test set event-based eye tracking challenge cvpr demonstrating effectiveness practical deployment arvr systems providing foundation future innovations neuromorphic vision.", "full_text": "dual-path enhancements event-based eye tracking augmented robustness adaptive temporal modeling hoang truong vinh-thuan huy tran thuan-phat nguyen tram doan university science, vnu-hcm, chi minh city, vietnam vietnam national university, chi minh city, vietnam student.hcmus.edu.vn, dttramhcmus.edu.vn abstract event-based eye tracking become pivotal technol- ogy augmented reality human-computer interac- tion. yet, existing methods struggle real-world chal- lenges abrupt eye movements environmental noise. building efficiency lightweight spa- tiotemporal networka causal architecture optimized edge deviceswe introduce two key advancements. first, robust data augmentation pipeline incorporating tempo- ral shift, spatial flip, event deletion improves model re- silience, reducing euclidean distance error vs. baseline challenging samples. second, propose knightpupil, hybrid architecture combining efficientnet-b backbone spatial feature extraction, bidirectional gru contextual temporal modeling, linear time-varying state-space module adapt sparse inputs noise dynamically. evaluated benchmark, framework achieved euclidean dis- tance private test set event-based eye tracking challenge cvpr demonstrating effectiveness practical deployment arvr systems providing foundation future innovations neuromorphic vision. introduction human eyes remarkable agility capable execut- ing saccades speeds exceeding presents inspiration challenge machine vision sys- tems. augmented reality headsets evolve toward gaze- contingent rendering neurological diagnostics increas- ingly rely precise oculomotor analysis limita- tions conventional frame-based eye tracking become ap- parent temporal resolution power efficiency fun- damentally constrain applications demanding real-time re- sponsiveness. event cameras, bio-inspired asyn- chronous operation offer paradigm shift. sen- sors achieve microsecond temporal resolution con- suming merely two orders magnitude effi- cient high-speed cameras however, sparse, irregular output streams defy traditional computer vision approaches. core challenge lies developing algo- rithms extract robust gaze estimates noisy, intermittent signals maintaining real-time per- formance edge devices recent research significantly advanced event-based eye tracking. ding al. introduced facet, uses fast ellipse fitting enhance gaze estimation accu- racy, particularly applications. however, may face challenges occlusion varying lighting con- ditions. similarly, chen al. proposed et, utilizing change-based convlstm network preserve tempo- ral precision raw event data. despite strong perfor- mance, recurrent nature convlstm introduce latency issues resource-constrained edge devices. ad- ditionally, lightweight spatiotemporal network demonstrated effectiveness causal architecture efficient fifo-based processing, though practical de- ployment highlights challenges blink-induced ar- tifacts interference eyewear. overcoming limitations holds great potential consumer arvr, en- abling always-on gaze interaction without draining battery life, clinical settings, could aid monitor- ing digital biomarkers neurodegenerative diseases current event-based eye tracking methods typically fol- low one two paths either compressing events frame- like representations processing raw events specialized architectures latter approach preserves temporal precision, implementations strug- gle balance computational efficiency adaptability challenging real-world conditions. recent literature re- flects growing interest hybrid approaches. works combine convolutional recurrent networks, others cs.cv apr explore novel spiking neural architectures however, often introduce substantial computational overhead require complex training protocols, limiting practical applicability. lightweight spatiotemporal network offers ef- ficient event processing via causal design, fifo buffering, depthwise convolutions, making well-suited con- strained environments. however, experiments show fixed temporal receptive field limits performance variable eye movements real-world noise. explore two complementary solutions. first, im- prove original network using data augmentation tech- niques temporal shifting spatial flipping, en- hancing robustness without compromising efficiency. second, propose knightpupil, novel architecture com- bining efficientnet-based spatial encoding, bidirectional grus, ltv-ssm adaptive temporal modeling. distinct, strategies contribute advancing real- time, robust event-based gaze estimation balancing effi- ciency adaptability. building upon recent advances event-based vision, make three key contributions augmented robustness spatiotemporal networks strategic data augmentation temporal shift, spatial flip, event deletion improves resilience real-world pertur- bations preserving computational efficiency. adaptive hybrid architecture knightpupil integrates spatial feature extraction efficientnet-b, bidirectional temporal modeling gru, dynamic state adaptation ltv-ssm noise-resistant gaze estimation. dual-strategy framework combines deployable op- timized baseline exploratory architecture, ad- vancing practical deployment adaptive temporal modeling research. related work lightweight spatiotemporal network event-based vision gained significant attention due high temporal resolution sparse nature, making suitable real-time applications eye tracking traditional approaches often rely frame-based process- ing, lose valuable temporal information ap- plied event-based data. address this, spatiotemporal neural networks developed leverage spa- tial temporal event patterns. lightweight spatiotemporal network proposed domain notable advancement proposed network employs causal spatiotemporal convolutional ar- chitecture designed perform online inference efficiently. key components include causal spatiotemporal convolutional blocks network utilizes sequence spatiotemporal blocks, figure compact spatiotemporal model integrating data aug- mentation spatial temporal processing blocks. convolu- tional layers extract spatial temporal features efficiently. figure illustration spatiotemporal processing temporal block applies temporal convolution across frames, spa- tial block extracts spatial features using convolutional filters. consisting temporal convolution followed spatial convolution. structure ensures efficient tem- poral feature extraction maintaining causality, al- lowing real-time inference. causal event binning unlike conventional binning techniques introduce latency, method causally processes events, reducing buffering requirements improving efficiency streaming data applications. hybrid normalization strategy network alter- nates batch normalization group nor- malization balance training stability adapt- ability across different batch sizes, enhancing generaliza- tion. sparse activation regularization regulariza- tion term applied activations encourage sparsity, significantly reducing computational complexity maintaining accuracy. experimental results demonstrate lightweight architecture achieves state-of-the-art performance event- based eye-tracking tasks, particularly terms ac- curacy computational efficiency. integrating efficient event binning, causal convolutions, activation sparsity techniques makes well-suited deployment edge de- vices. work builds upon foundation incorporating advanced data augmentation strategy, enhanc- ing model robustness generalization. lightweight spatiotemporal architecture inference pipeline shown fig. fig. voxel grid representation event-based pro- cessing voxel grids widely adopted event-based vision spatiotemporal feature encoding. unlike frame-based methods rely regularly sampled intensity values, voxel grids aggregate asynchronous events structured tensors, temporal dimension discretized multiple bins. representation effectively bridges gap event-based sensing deep learning, enabling efficient feature extraction preserving fine- grained motion information. early works explored voxel-based accumu- lation enhance event representation motion estima- tion. subsequent research extended approach, inte- grating voxel grids convolutional neural networks tasks like object recognition optical flow estimation recently, voxel-based encoding utilized transformer-based models event-based classification, demonstrating versatility despite effectiveness, voxel grids introduce trade- temporal resolution memory efficiency. se- lecting appropriate number bins crucial bins may discard valuable temporal details, many increase computational overhead. approach em- ploys fixed -bin voxel grid, ensuring compatibility efficientnets input structure maintaining balance temporal precision computational efficiency. efficientnet efficientnet family cnns designed opti- mize accuracy efficiency compound scaling. achieves state-of-the-art performance significantly fewer parameters compared traditional deep networks. among variants, efficientnet-b provides strong bal- ance model complexity inference speed, mak- ing well-suited vision tasks requiring real-time pro- cessing. work, leverage efficientnet-b feature extractor event-based eye tracking, providing compact yet expressive representation input data. integrating efficientnet-b sequential model, aim cap- ture spatial temporal dependencies, enabling ro- bust efficient gaze estimation. state-space models sequential modeling state-space model ssm mathematical framework models dynamic systems using latent states evolve time. ssms widely explored sequence modeling due ability capture long-range depen- dencies efficiently. compared recurrent neural networks rnns, ssms offer structured representations temporal dynamics, making particularly useful ap- plications memory efficiency stability cru- cial. recent works extended ssms event-based vision tasks, demonstrating potential modeling sparse asynchronous data streams however, conventional ssms often involve complex parameterization high computational cost, making less practical real-time applications. address this, introduce simplified version linear time-variant state-space model ltv-ssm, specifically designed complement efficientnet-b gru architecture. keeping formulation lightweight preserving key temporal modeling capabilities, approach maintains computational efficiency without sacri- ficing predictive power. enables model effec- tively capture spatial temporal dependencies event- based eye tracking, improving robustness accuracy. methodology approach combines data augmentation lightweight neural architecture enhance event-based eye-tracking. first apply augmentation techniques improve model robustness temporal spatial varia- tions. then, introduce knightpupil, integrates voxel grid encoding, efficientnet-b spatial features, bi-gru temporal modeling, ltv-ssm adap- tive state transitions, enabling precise efficient gaze estimation. data augmentation enhance robustness generalizability event- based eye-tracking model, employ series augmenta- tion techniques tailored event streams. augmen- tations include temporal shift, spatial flip, event dele- tion, designed introduce realistic variations figure overview data augmentation techniques spatial flip mirrors event coordinates, temporal shift modifies event timing, event deletion simulates sensor noise. preserving label consistency. augmentation tech- niques visually overviewed fig. temporal shift. given asynchronous nature event- based data, temporal augmentation crucial improving model resilience timing variations. apply random shift event timestamps within range mil- liseconds delt quad delta sim mathcal u-, text ms, ua, denotes uniform distribution. since labels sampled every ms, recompute label indices lfloor delta rfloor ensuring accurate correspondence events la- bels. spatial flip. introduce spatial invariance, horizon- tally vertically flip event coordinates quad width height sensor, respectively. corresponding pupil center labels undergo transformation xtext label, ytext label xtext label, ytext label. event deletion. simulate real-world sensor noise occlusions, randomly remove events ptex quad set events. label sequence re- mains unchanged, ensuring model learns handle miss- ing data effectively. figure voxel grid representation event data, raw events segmented windows accumulated spatial bins time. augmentation applied independently train- ing set, generating multiple augmented versions orig- inal data. final dataset comprises original augmented samples, ensuring diversity spatial temporal domains maintaining label accuracy. knightpupil proposed eye-tracking solution integrates efficientnet- spatial feature extraction, bi-gru dropout temporal modeling, linear time-varying state-space model ltv-ssm adaptive state transitions. archi- tecture designed effectively process event-based data efficiently capturing spatial temporal depen- dencies. name model knightpupil reflect strengths knight symbolizes robustness adaptability, highlight- ing resilience dynamic event-based data. pupil signifies focus eye-tracking, drawing direct connec- tion primary task. knightpupil embodies vigilant efficient system gaze estimation, capable pre- cisely tracking rapid eye movements. architecture illustrated fig. affine transformation knightpupil knight- pupil, chose use affine transformation preprocessing stage, unlike augmentation technique used model proposed pei al. instead, approach leverages raw event data directly augmenta- tion. method allows maintain integrity temporal spatial details crucial event-based eye- tracking, ensuring efficient training avoiding unnec- essary transformations could distort key charac- teristics knightpupils architecture. voxel grid representation since event cameras produce stream asynchronous events rather conventional frames, employ voxel grid encoding transform events structured representation. given set events mathcal xi, yi, ti, piin xi, represents spa- tial coordinates, timestamp, de- notes polarity, construct voxel grid representation mathbb times times discretizing time domain bins accumulating events within bin. illustration voxel grid shown fig. following implementation tonic, voxel grid computed cdot max lef left frac ttext minttext max ttext min right right temporal bin index computed using linear in- terpolation scheme distribute events across adjacent bins. ensures smooth temporal representation pre- serving event polarity information. voxel grid nor- malized bin contains values scaled improving numerical stability training. transformation enables knightpupil efficiently process event-based eye-tracking data, capturing spa- tial temporal event distributions enhanced resolu- tion robustness. spatial feature extraction. use efficientnet-b backbone extract spatial features event-based eye-tracking framework. optimized architecture bal- ances efficiency representational power, making ideal capturing spatial patterns. efficientnet scales depth, width, resolution using compound coefficient phi beta phi gamma phi represent depth, width, in- put resolution, respectively. constants pha beta gamma de- termined via grid search, phi controls overall scal- ing. efficientnet-b, specific values phi alpha beta gamma resulting deeper net- work channels higher resolution compared efficientnet-b. initialize efficientnet-b pre-trained imagenet weights process event-based eye-tracking data. unlike partial fine-tuning approaches freeze lower layers, perform full fine-tuning, allowing layers updated training. ensures model fully adapts event-based eye-tracking task rather relying solely generic imagenet features. given input voxel grid representation extracted feature representation text effnetbv mathbb times sequence extracted spatial fea- tures. chose efficientnet-b commonly used convolutional networks resnet several rea- sons. first, efficientnet employs compound scaling, al- lowing achieve superior trade-off accuracy efficiency compared traditional architectures. contrast, resnet scales depth independently, may lead redundancy feature extraction without proportion- ally improving performance. additionally, efficientnets depthwise-separable convolutions significantly reduce com- putational cost, making suitable real-time appli- cations like eye tracking. another crucial factor efficientnets superior param- eter efficiency. resnet architectures, particularly deeper variants resnet- resnet-, contain many parameters, increasing memory footprint inference latency. contrast, efficientnet-b achieves comparable better accuracy fewer parameters, enabling efficient deployment resource-constrained environments. moreover, efficientnet demonstrated strong generaliza- tion various vision tasks, making robust choice event-based processing spatial features must effi- ciently captured. leveraging efficientnet-b feature extractor, ensure optimal balance accuracy, computa- tional efficiency, real-time applicability, making well- suited event-based eye tracking. temporal modeling extracted feature sequence obtained efficientnet-b, frame en- coded -dimensional feature vector. case, efficientnet-b. sequence passed bidirectional gru two layers hidden size per direction, resulting total hidden size dropout applied gru layers im- prove generalization. gru update equations sigma sigma tanh odot ht- bh, odot ht- odot tilde ht. reset update gates, repre- sents hidden state. since employ bidirectional gru, figure overview knightpupil architecture. model consists three key components efficientnet-b backbone spatial feature extraction, bidirectional gru bi-gru temporal modeling, linear time-varying state-space model ltv-ssm adaptive state transitions. design enables robust event-based gaze estimation efficiently capturing spatial temporal dependencies. final output concatenated forward back- ward directions. gru outputs feature sequence shape fed ltv-ssm sequential modeling. several reasons, adopt grultv-ssm combi- nation temporal models like lstms trans- formers. lstms widely used, additional gating mechanisms cell states increase computational complexity inference latencycritical factors real- time applications like event-based eye tracking. grus of- fer comparable performance reduced overhead, mak- ing better fit lightweight setup. despite strength modeling long-range dependen- cies, transformers require self-attention computations scale quadratically sequence length lack intrin- sic temporal bias, relying instead positional encodings. limitations make less suitable long, high- frequency event sequences. contrast, hybrid model balances efficiency expressiveness grus handle short- term dependencies, ltv-ssm component cap- tures dynamic state transitions, enabling robust efficient gaze estimation. adaptive state-space modeling. employ linear time-varying state-space model ltv-ssm enhance sequential representation. unlike traditional state-space models fixed transition matrices, approach dy- namically learns state transitions based gru output ht. specifically, parameterize transition matrices elta del lta delta odot bt, delta delta bt. dynamically learned input using linear transformation. here, ensures numerical stability, refines hidden state representation. additionally, maintain identity matrix stabi- lize transformations, resulting final output mathbf dt. comparison ltv-ssm mamba gated lin- ear attention gla. ltv-ssm shares conceptual sim- ilarities recent state-space sequence models mamba gla particularly ability selectively process input sequences. similar mamba, ltv-ssm leverages learned transformations adjust state transitions based input features dynamically. however, unlike mamba, employs selective long-range con- volutions gating mechanisms, ltv-ssm maintains structurally simpler formulation element-wise multi- plicative updates, making computationally lightweight still capturing long-range dependencies. compared gla, integrates explicit gating sequence model- ing, ltv-ssm avoids additional gate parameters, reducing overall model complexity preserving adaptabil- ity. furthermore, gla explicitly designed hardware- efficient training leveraging structured gating mecha- nisms optimize memory computational efficiency. contrast, ltv-ssm prioritizes lightweight design fewer learnable parameters, making suitable sce- narios model simplicity efficiency critical. inclusion bigru architecture, use ltv-ssm becomes even suitable, comple- ments bidirectional processing temporal dependen- cies already captured bigru. ltv-ssms simpler, computationally efficient formulation avoids need complex state-space model, effectively models long-range dependencies element-wise multiplica- tive updates. makes ltv-ssm lightweight appropriate choice combination bigru, eliminating need additional gating mechanisms state-space models, maintaining adaptability reducing model complexity. ltv-ssm training process rely parallel scan techniques. model trained using stan- dard sequential operations typical deep learning train- ing loop, sequences processed forward pass gradients computed via backward pass. parallel scan used, element-wise updates ltv- ssm allow efficient computation without need complex parallelization strategies. final prediction layer. final gaze coordinates obtained applying fully connected layer transformed sequence hidden states ltv-ssm mathbb times sequence hidden states output ltv-ssm, maps features coordi- nate space. dropout layer helps regularize predictions preventing overfitting. integrating efficientnet-b spatial feature ex- traction, bi-gru dropout temporal modeling, ltv-ssm adaptive state transitions, solution max- imizes benefits pre-training adapt- ability fine-tuning, leading robust high- performing eye-tracking model. voxel grid caching. improve training effi- ciency, employ disk caching mechanism using diskcacheddataset utility tonic library. approach minimizes redundant computations sub- sequent epochs future training sessions, significantly reducing data preprocessing overhead. initial transformation incurs latency, caching allows model bypass repeated voxel grid construction, leading much faster data loading later iterations. tonics caching mechanism stores preprocessed voxel grid representations disk instead ram, ensuring ef- ficient retrieval training. particularly bene- fits large-scale event-based datasets, on-the-fly trans- formations computationally expensive. further- more, caching system designed automatically de- tect changes data transformations modifications bin size augmentations, preventing outdated cached files reused incorrectly. approach accelerates training convergence elim- inating need redundant computations. also fa- cilitates experimentation different hyperparameters model configurations without excessive preprocessing de- lays. cached dataset integrates seamlessly training pipeline, allowing efficient reproducible data loading. optimization effectively balances pre- processing time computational efficiency, making well-suited large-scale event-based eye-tracking appli- cations. experiment datasets conduct experiments datasets comprising subjects, recording sessions. datasets share event resolution contain five activity classes random, sac- cades, reading, smooth pursuit, blinks. ground truth annotations provided hz, includ- ing pupil center coordinates binary blink indi- cator close, denotes open eye closed eye. evaluation based average euclidean dis- tance predicted ground truth pupil center coordinates. dataset, percentage cases average euclidean distance less used. implementation details conduct experiments single nvidia tesla gpu provided kaggle. lightweight spatiotemporal network, adopt original training configuration pei al. specifically, train model using batch size containing event frames. network optimized epochs using adamw optimizer base learning rate weight decay employ cosine decay learning rate schedule linear warmup, warmup phase spans total training steps. additionally, leverage automatic mixed- precision amp, pytorch compilation accel- erate training improve efficiency. knightpupil, adopt structured training approach optimize performance event-based eye-tracking. model trained epochs batch size using adam optimizer initial learning rate steplr scheduler reduces learning rate every epochs. handle event-based input effi- ciently, convert raw event streams voxel grids num time bins sample spans time window train length temporal subsample factor events within window binned three equal sub-intervals each, forming voxel grid due spatial downsampling factor em- ploy sliding window strategy, training samples stride train stride ensuring overlap, validation samples use stride val stride non- overlapping evaluation. model validated regular intervals, save best checkpoint based lowest euclidean error validation loss, along final model last epoch. results ablation study effect augmentation. evaluate impact augmentation techniques architectures integrated study. results presented tab. year method augmentation dist. knightpupil knightpupil spatiotemporal spatiotemporal knightpupil knightpupil spatiotemporal spatiotemporal table comparison euclidean distance accuracy across different methods. results demonstrate effectiveness augmen- tation techniques improving model performance across different architectures. dataset, observe marginal improve- ment accuracy augmentation applied. knightpupil model experiences slight decrease suggesting augmentation may intro- duce minor variability predictions. however, spa- tiotemporal model, augmentation provides clear improve- ment, increasing accuracy suggests spatiotemporal model benefits augmentation, potentially due ability leverage enhanced temporal patterns. dataset, assess euclidean distance dist., lower values indicate better tracking accu- racy. augmentation consistently improves performance models. knightpupil model slightly reduces dis- tance spatiotemporal model benefits significantly, reducing improvements indicate augmentation helps re- fine feature extraction spatial-temporal representations, particularly spatiotemporal model. overall, findings suggest augmentation crucial refining tracking accuracy. spatiotemporal model benefits augmentation compared knightpupil, likely inherent architecture better suited handling variations temporal patterns. highlights importance carefully selecting augmentation strate- gies tailored specific model architectures. data augmentation techniques. evaluate impact different augmentation strategies, conduct ablation study selectively removing specific augmentations measuring effect model performance. results presented tab. augmentation dist. knightpupil spatiotemporal temporal shift spatial flip event deletion full augmentation table ablation study impact different augmentation techniques. report euclidean distance dist. set- ting. knightpupil trained epochs instead full epoch setting. removing temporal shift leads noticeable increase euclidean distance, dropping performance knightpupil spa- tiotemporal model. highlights importance tem- poral alignment event-based eye tracking. similarly, disabling spatial flip slightly degrades per- formance, increasing euclidean distance knightpupil spatiotemporal model. suggests spatial variations contribute model robust- ness, though impact less pronounced temporal shift. event deletion removed, performance also dete- riorates knightpupil spatiotempo- ral model, indicating selectively removing events helps reduce noise improve generalization. overall, full augmentation strategy achieves best performance, reducing euclidean distance knightpupil spatiotemporal model. results emphasize effectiveness augmen- tation pipeline enhancing tracking accuracy. conclusion work, enhance lightweight spatiotemporal network via temporal shift, spatial flip, event deletion mitigate real-world noise, knightpupil combines efficientnet-b spatial encoding, bidirectional gru tem- poral modeling, ltv-ssm adaptive dynamics ad- dress sparse event patterns. together, strategies bal- ance deployable efficiency algorithmic flexibility neuromorphic vision systems. hope future work explore unifying approaches investigating asyn- chronous processing harmonizes sparse nature event cameras. efforts aim contribute evolv- ing dialogue temporal modeling neuromorphic sys- tems, striving balance real-world applicability algo- rithmic creativity. references qinyu chen, zuowen wang, shih-chii liu, chang gao. efficient event-based eye tracking using change-based convlstm network. ieee biomedical circuits systems conference biocas, pages ieee, qinyu chen, chang gao, min liu, daniele perrone, al. event-based eye tracking. event-based vision work- shop. proceedings ieeecvf conference com- puter vision pattern recognition workshops, xavier clady, sio-hoi ieng, ryad benosman. motion- based feature event-based pattern recognition. frontiers neuroscience, junyuan ding, ziteng wang, chang gao, min liu, qinyu chen. facet fast accurate event-based eye tracking using ellipse modeling extended reality. arxiv preprint guillermo gallego, tobi delbruck, garrick orchard, chiara bartolozzi, brian taba, andrea censi, stefan leutenegger, andrew davison, jorg conradt, kostas daniilidis, davide scaramuzza. event-based vision survey. ieee transactions pattern analysis machine intelligence, guillermo gallego, tobi delbruck, garrick orchard, chiara bartolozzi, brian taba, andrea censi, stefan leutenegger, andrew davison, jorg conradt, kostas daniilidis, davide scaramuzza. event-based vision survey. ieee transactions pattern analysis machine intelligence, daniel gehrig, mathias gehrig, javier hidalgo-carrio, davide scaramuzza. end-to-end learning representations asynchronous event-based data. ieee international conference computer vision iccv, pages albert tri dao. mamba linear-time sequence modeling selective state spaces. arxiv preprint albert gu, karan goel, christopher re. efficiently mod- eling long sequences structured state spaces. inter- national conference learning representations iclr, ana maqueda, antonio loquercio, guillermo gallego, narciso garcia, davide scaramuzza. event-based vision meets deep learning steering prediction self-driving cars. ieeecvf conference computer vision pattern recognition, page ieee, yan pei, sasskia bruers, sebastien crouzet, douglas mclelland, olivier coenen. lightweight spatiotempo- ral network online eye tracking event camera, ken pfeuffer, jason alexander, hans gellersen. multi- user gaze-based interaction techniques collaborative touchscreens. acm symposium eye tracking research applications, new york, ny, usa, association computing machinery. christoph posch, teresa serrano-gotarredona, bernabe linares-barranco, tobi delbruck. retinomorphic event- based vision sensors bioinspired cameras spiking out- put. proceedings ieee, henri rebecq, rene ranftl, vladlen koltun, davide scaramuzza. high speed high dynamic range video event camera, martin rolfs. microsaccades small steps long way. vision research, thomas stoffregen, guillermo gallego, tom drummond, davide scaramuzza. reducing computational com- plexity event-based optical flow. ieee transactions pattern analysis machine intelligence tpami, mingxing tan quoc le. efficientnet rethinking model scaling convolutional neural networks. pro- ceedings international conference machine learning icml, pages pmlr, zuowen wang, chang gao, zongwei wu, marcos conde, radu timofte, shih-chii liu, qinyu chen, al. event- based eye tracking. ais challenge survey. pro- ceedings ieeecvf conference computer vision pattern recognition workshops, songlin yang, bailin wang, yikang shen, rameswar panda, yoon kim. gated linear attention trans- formers hardware-efficient training. arxiv preprint bojian yin, federico corradi, sander bohte. effec- tive efficient computation multiple-timescale spik- ing recurrent neural networks, alex zihao zhu, liangzhe yuan, kevin chaney, kostas daniilidis. unsupervised event-based learning optical flow, depth, egomotion. ieee conference com- puter vision pattern recognition cvpr, pages nikola zubic, mathias gehrig, davide scaramuzza. state space models event cameras. proceedings ieeecvf conference computer vision pattern recognition cvpr,", "published_date": "2025-04-14T07:57:22+00:00"}
{"id": "2504.09887v1", "title": "Enhanced Semantic Extraction and Guidance for UGC Image Super Resolution", "authors": ["Yiwen Wang", "Ying Liang", "Yuxuan Zhang", "Xinning Chai", "Zhengxue Cheng", "Yingsheng Qin", "Yucai Yang", "Rong Xie", "Li Song"], "summary": "due disparity real-world degradations user-generated contentugc images synthetic degradations, traditional super-resolution methods struggle generalize effectively, necessitating robust approach model real-world distortions. paper, propose novel approach ugc image super-resolution integrating semantic guidance diffusion framework. method addresses inconsistency degradations wild synthetic datasets separately simulating degradation processes lsdir dataset combining official paired training set. furthermore, enhance degradation removal detail generation incorporating pretrained semantic extraction model sam fine-tuning key hyperparameters improved perceptual fidelity. extensive experiments demonstrate superiority approach state-of-the-art methods. additionally, proposed model second place cvpr ntire short-form ugc image super-resolution challenge, validating effectiveness. code available", "full_text": "enhanced semantic extraction guidance ugc image super resolution yiwen wang ying liang yuxuan zhang xinning chai zhengxue cheng yingsheng qin yucai yang rong xie song shanghai jiao tong university, china transsion, china evonwang, forest, keudyhsi, chaixinning, zxcheng, xierong, song lisjtu.edu.cn yingsheng.qin, yucai.yangtranssion.com abstract due disparity real-world degradations user-generated contentugc images synthetic degra- dations, traditional super-resolution methods struggle generalize effectively, necessitating robust approach model real-world distortions. paper, propose novel approach ugc image super-resolution inte- grating semantic guidance diffusion framework. method addresses inconsistency degradations wild synthetic datasets separately simulating degradation processes lsdir dataset combining official paired training set. furthermore, enhance degradation removal detail generation in- corporating pretrained semantic extraction model sam fine-tuning key hyperparameters improved percep- tual fidelity. extensive experiments demonstrate su- periority approach state-of-the-art meth- ods. additionally, proposed model second place cvpr ntire short-form ugc image super- resolution challenge report validat- ing effectiveness. code available github.commoonsofangntire--srlab. introduction single image super-resolution sisr fundamental task computer vision focuses reconstructing high- resolution images low-resolution counterparts. primary goal sisr recover fine- grained details high-frequency textures lost dur- ing downsampling process, ultimately enhancing perceptual quality fidelity upscaled images. early deep learning-based approaches predominantly re- lied convolutional neural networks cnns leverage hierarchical feature extraction corresponding author figure objective subjective results ntire short- form ugc image challenge. top six methods in- cluded. horizontal axis represents objective score, computed score psnr ssim lpips musiq maniqa clipiqa. vertical axis represents subjective score calculated five experts. results provided competition organizer. learn lr-to-hr mappings. methods achieve impressive psnr, often produce overly smooth tex- tures due reliance pixel-wise loss functions, fail- ing capture high-frequency details essential percep- tual realism. adversarial training, gan-based methods synthesize visually plausible textures, significantly improving perceptual quality. however, tendency hallucinate visually plausible yet semantically inconsistent details remains critical limitation faithful super-resolution. unlike gans, diffusion modelsdms iteratively re- fine images markov chain-based denoising pro- cess. progressive nature allows finer control trade-off distortion perceptual quality, making particularly well-suited high-fidelity im- age generation. additionally, inherent stochasticity enables generation diverse high-resolution sam- cs.cv apr ples single low-resolution input, distinct ad- vantage traditional deterministic approaches cnns gans, typically produce single fixed output. recently, growing number diffusion-based super-resolution methods emerged, demonstrating effectiveness framework enhancing image quality restoring fine details. however, despite promising capabilities, diffusion models still face several critical challenges applied real-world super-resolution tasks. first, existing methods rely synthetically gener- ated low-resolution high-resolution training pairs, degradation processes, bicubic downsam- pling, fail accurately replicate complex hetero- geneous degradations found real-world images. dis- crepancy creates domain gap, making difficult mod- els trained synthetic data generalize effectively real- world scenarios. user-generated content ugc images, instance, often suffer diverse unpredictable degra- dation patterns, including noise, compression artifacts, mo- tion blur, varying lighting conditions, compli- cating super-resolution process. result, models op- timized artificially degraded datasets may struggle re- store authentic details applied in-the-wild data. second, diffusion models excel generating plau- sible high-frequency details, often struggle maintain delicate balance degradation removal seman- tic fidelity. many cases, model may introduce unnat- ural artifacts fail preserve structural coherence objects, leading inconsistencies restored images. issue particularly pronounced cases ex- treme degradation erased fine-grained textures, making challenging model reconstruct missing informa- tion visually meaningful way. address challenges, propose diffusion- based super-resolution framework integrates real- istic degradation modeling, semantic-aware refinement, perceptual optimization. method aims bridge gap synthetic real-world wild degradation scenar- ios constructing representative training dataset incorporating advanced architectural components en- hance restoration quality. specifically, introduce train- ing strategy combines synthetic training set pro- vided competition lsdir dataset better simulate real-world conditions, process lsdir dataset applying controlled degradation, ensur- ing model learns handle diverse degradation pat- terns effectively. approach leverages strong generative prior stable diffusion provides powerful latent space high-quality image synthesis. en- hance reconstruction fidelity, integrate controlnet enables precise spatial conditioning improves structural detail preservation. additionally, incorpo- rate semantic-aware module refine structural contextual information, ensuring generated high- resolution images maintain perceptual quality se- mantic coherence. specifically, leverage semantic extraction capabilities sam recover fine-grained details embedding high-level semantic information latent space. integration allows framework strike balance effective degradation removal realistic detail generation, making robust diverse real-world degradations producing visually coherent semantically meaningful reconstructions. contributions follows. address inconsistency degradations wild synthetic datasets, simulate synthetic wild degradation processes lsdir separately. resulting datasets combined offi- cially provided paired training set form final train- ing dataset. enhance degradation removal semantic detail gen- eration, incorporate pretrained semantic extraction model sam, helps refine structural contex- tual understanding input data. better control visual quality generated re- sults, fine-tune specific hyperparameters model, optimizing performance improved perceptual fi- delity. related works recent years, advancement deep learning technol- ogy significantly propelled progress single image super-resolution sisr. based variety deep learn- ing networks, capability sisr models improve quality low-resolution images greatly enhanced. commonly employed architectures domain in- cludes convolutional neural networks cnns, generative adversarial networks gan, transformer, diffusion model cnn-based sisr cnn subclass feedforward neural networks in- corporate convolutional operation. characteristics lo- cal connectivity weight sharing endow powerful feature-learning capabilities. notable examples cnn-based single-image super- resolution sisr models include srcnn first deep learning-based sisr model, bsrn first place ntire efficient track, cvanet important research direction cnn- based sisr models enhancing network depth width improve models ability capture fine image details. gan-based sisr generative adversarial networks gans consist gen- erator discriminator. adversarial training, gans effectively learn underlying distribution im- ages, e.g., ldl realsr gan-based sisr models. gan-based sisr models demonstrated sig- nificant advantages terms perceptual quality realis- tic visual effects. however, large number network pa- rameters leads unstable training relatively long iner- ence time. enhancing reconstruction capabilities gan- based models designing lightweight networks sta- bilize training process become key research direc- tion. transformer-based sisr transformer architecture, originally designed nat- ural language processing, neural network framework relies entirely self-attention mechanisms capture global dependencies inputs outputs. compared cnn rnn, self-attention enables better modeling long-range relationships mitigating vanishing gra- dient problem. additionally, transformers support parallel computation, significantly improving network efficiency. transformer network consists encoder-decoder structure, multi-head self-attention core compo- nent. models grl hipa esrt based transformer architectures. despite ad- vantages, transformer-based sisr models still face chal- lenges high computational costs artifacts affect perceptual quality. addressing issues remains important research direction improving applica- tion transformers image super-resolution. diffusion-based sisr recent years, diffusion models emerged pow- erful approach based principle destruction re- construction, incorporating forward backward dif- fusion processes. notable diffusion-based sisr models in- clude idm diffir seesr etc. existing diffusion-based sisr models adopt unet architecture denoising diffusion probabilistic models ddpms employ entirely novel structures, transformer-integrated diffusion mod- els. designing unified foundational diffusion model architecture promising research direction sisr field. however, introduction stochastic noise backward diffusion process often results unstable out- puts. therefore, developing stable efficient diffusion- based sisr model forward-looking research challenge, aiming balance reconstruction quality computa- tional efficiency. method preliminaries diffusion model diffusion models class generative models achieve goal generating target data samples gaussian noise diffusion models based two-step process forward diffusion process reverse denoising process. forward diffusion process, data sample gradually perturbed adding gaussian noise timesteps qmathb mathb xt- mathcal nmathbf sqrt beta mathbf xt-, beta mathbf variance schedule controlling noise level timestep, noised image timestep reverse process, neural network xt, trained predict added noise, enabling generation new samples denoising random gaussian sample ptheta mathbf athbf mathcal nmathbf xt- theta mathbf xt, sigma theta mathbf xt, xt, typically parameterized using pre- dicted noise xt, noise schedule. practice, denoising models adopt denoising network xt, estimate noise training, network parameters denoising network optimized minimizing loss function mathca lma thbb etext x,t,epsilon left epsilon epsilon theta text xt, right recent advancements, classifier-free guidance enhanced capability diffusion mod- els, making state-of-the-art various generative mod- eling tasks. overview proposed method built upon diffusion frame- work. overall pipeline approach illustrated fig. given low-quality input image ilr, first encode latent representation using vae encoder progressively refine latent representation, employ denoising u-net, iteratively removes noise sequence denoising steps. additionally, in- corporate controlnet architecture enforce structural con- sistency low-resolution input. controlnet en- ables precise conditioning, ensuring super-resolved output maintains fidelity original image bene- fiting generative power diffusion models. key challenge real-world super-resolution tasks handling images varying degrees degradation different scaling factors, particularly dealing synthetic wild test sets. address this, in- tegrate segment anything model sam figure overview proposed method. approach builds upon diffusion framework, incorporating mechanism enforce structural consistency preserve fidelity original image. additionally, leverage sam semantic-guided refinement, extracting high-level semantic embeddings enhance adaptability diverse degradation conditions. architecture pca sca module. architecture sam image encoder. encoder comprises trunk neck. trunk extracts multi-scale features low-resolution image four stages varying numbers msbmulti-scale block layers. neck applies convolutions scales performs top-down feature fusion low-resolution features. output includes refined multi-scale feature maps corresponding positional encodings. framework. sam utilized extract high-level se- mantic embeddings input images, providing addi- tional contextual information aids denoising reconstruction process. semantic embeddings help model adapt diverse image conditions, improving ro- bustness across different datasets. enriched latent rep- resentation, augmented semantic information, iteratively refined denoising u-net denois- ing steps. training, minimize denoising objective mathcal lmath text ,tex xlr,t,c,csem,epsilon left epsilon epsilon theta text xt, text xlr, csem right xlr represents low-resolution latent, de- notes tag prompt, csem semantic embedding. noise estimation network responsible predict- ing noise semantic-aware module ensure robustness across inputs ilr different levels degradation, illustrated fig. introduce semantic-aware module, extracts high-level seman- tic embeddings frame using pre-trained sam model. specifically, formulated eq. input im- ages processed frozen image semantic ex- tractor obtain corresponding semantic representa- tions csem bel semanticeq begin aligned csem texttt semantic extractoritext end aligned extracted semantic embeddings effectively capture essential structural contextual information, even presence severe degradation. features subse- quently integrated denoising u-net se- mantic attention mechanism, facilitating informed restoration process. particular, l-th layer net, compute query vector spatial feature map key value vectors derived semantic embedding csem, defined eq. lab qsemanti gin aligned quad wkcsem, quad wvcsem texttt attentionq,k,v texttt softmax left frac qktsqrt dright end aligned incorporating semantic priors extracted sam, semantic-aware module enables u-net lever- age high-level contextual cues, thereby improving re- construction fine details preserving structural consis- tency. integration significantly enhances models ability handle complex degradations, resulting realistic visually coherent super-resolved outputs. experiment experimental setup train dataset enhance models performance short-form user-generated content ugc scenarios, training dataset constructed integrating synthetic ugc training set provided competition ls- dir dataset leveraging multiple data sources, training set designed capture diverse realistic range ugc image degradations. dataset composed three complementary compo- nents. first, lsdir training set, high-resolution image undergoes either downsampling degradation pro- cess equal probability ensuring balanced distribution degradation patterns. second, synthetic ugc training dataset provides additional high-resolution low-resolution training pairs, existing paired images cropped overlapping sub- images enhance sample diversity preserving local structural details. cropped patches retain orig- inal hr-lr correspondence, ensuring consistency within dataset. finally, increase degradation diver- sity, high-resolution images synthetic ugc dataset first cropped patches subsequently subjected dedicated degradation process, simulating wild ugc distortions. validation dataset test dataset validation testing phase, evaluation conducted using of- ficial validation test sets provided ntire challenge short-form ugc image super-resolution ensuring comprehensive assessment general- ization ability synthetic real-world ugc scenarios. validation test sets consist two sub- sets synthetic dataset wild dataset. synthetic dataset comprises paired low-resolution high- resolution images, images generated downsampling followed degradation, simu- lating distortions commonly found real-world ugc con- tent. model performance synthetic dataset assessed using psnr, ssim, lpips wild dataset consists unpaired real-world ugc images collected short-video platforms. images exhibit authentic distortions lack corresponding references, undergo degradation without downsampling. employ no-reference metrics, including musiq maniqa clipiqa nrqm hyperiqa evaluate models restoration capability setting. additionally, utilize divk validation dataset apply degradation pipeline used training generate images. training details training, model optimized using integrated training dataset. balance learning effectiveness computational efficiency, training con- ducted steps nvidia rtx gpu. adam optimizer utilized learning rate times preserve integrity pre-trained stable diffu- sion parameters, remain frozen throughout train- ing. optimization applied exclusively controlnet component semantic-aware module. training strategy enables model adapt effectively short-form ugc images preserving pretrained features diffusion backbone. testing details testing, analyze three key param- eters start point, guidance scale, positive negative prompts assess impact models performance across synthetic wild datasets. goal determine optimal configuration final test settings. first, compare two initialization methods gaussian noise noised low-resolution latent, assessing im- pact super-resolved image quality. next, examine effect positive negative prompts. additional positive prompts clean, high-resolution, ultra-detailed, ultra-realistic introduced enhance image sharpness fidelity. conversely, negative prompts including dot- ted, noise, blur, low-resolution, smooth, unrealistic physics, unnatural shadows tested suppress artifacts im- prove overall coherence. finally, analyze impact varying guidance scale result. multiple values tested, resulting images evaluated deter- mine optimal balance fidelity input adherence learned priors diffusion model. based findings experiments, iden- tify best-performing combination parameters shown tab. adopted final test settings ensure optimal performance. datasets start point text prompt synthetic prompt wild noise positive prompt table hyperparameter selection testing validation. abbreviation guidance scale. quantitative results compare proposed method several state-of- the-art sisr methods, including gan-based methods bsr- gan real-esrgan diffusion-based meth- ods faithdiff invsr xpsr pisa-sr seesr qualitative results shown tab. synthetic validation dataset, although models performance outstanding particular metric, consistently ranks mid-to-high level across metrics, overall, performs well among models. wild validation dataset, method consistently achieves best performance across metrics, surpassing previous sota methods demonstrating robustness handling real-world degradations. divk dataset, compare proposed method models using reference-based metrics. model ranks third psnr second ssim. al- though model performs worse real objective metrics, achieves best performance subjective eval- uation. highlights effectiveness semantic-guided diffusion framework reconstructing fine details pre- serving structural integrity perceptual consistency. furthermore, results validate capability ap- proach bridge gap synthetic real-world super-resolution scenarios, making promising solution practical ugc image enhancement. dataset metrics bsrgan real-esrgan faithdiff invsr xpsr pisa-sr seesr synthetic musiq maniqa clipiqa nrqm hyperiqa wild musiq maniqa clipiqa nrqm hyperiqa divk psnr ssim table comparison different sisr methods synthetic, wild divk validation dataset. red blue represent best second score, respectively. qualitative results images restoration process pre- sented fig. illustrated fig. proposed model demonstrates strong capability reconstructing fine de- tails within low-resolution images. furthermore, re- stored images exhibit improved visual clarity structural integrity, contributing realistic perceptually pleasing outcome. suggests model en- hances objective image quality also maintains high sub- jective fidelity, making suitable applications requiring detail preservation natural visual appearance. also compared perceived realism proposed method existing state-of-the-art methods wild synthetic data illustrated figures fig. im- ages produced proposed method exhibit higher de- gree naturalness realism compared generated approaches based generative adversarial net- works bsrgan real-esrgan diffusion models faithdiff invsr xpsr pisa-sr seesr demonstrates effectiveness method synthesizing visually coherent perceptu- ally convincing results. ablation study text prompt evaluate impact text prompts model performance, conducted ablation experiments introducing additional positive negative prompts inference. results across various guidance scale settings in- dicate positive negative prompts contribute final output comparable manner. shown tab. wild dataset, incorporating additional positive prompts yields highest scores, sug- gesting improvement model performance. contrast, synthetic dataset, prompts minimal effect enhancing results. based observations, inference strategy follows wild dataset, include additional positive prompts refine outputs, whereas synthetic dataset, additional prompts introduced, con- tribute improvements. dataset text prompt score wild prompt positive prompt negative prompt synthetic prompt positive prompt negative prompt table impact additional text prompts model perfor- mance across datasets. positive prompt includes clean, high- resolution, ultra-detailed, ultra-realistic. negative prompt in- cludes dotted, noise, blur, lowres, smooth, unrealistic physics, unnatural shadows. wild dataset, score computed score musiq maniqa clipiqa. synthetic dataset, score computed score psnr ssim lpips guidance scale evaluated final scores gener- ated results across different guidance scale values, shown tab. synthetic dataset, highest score achieved guidance scale set indicating setting optimally enhances performance sce- nario. contrast, wild dataset, positive correlation guidance scale final evalua- tion score. analyze impact guidance scale image perceptual quality, examine results wild test set values synthetic test set values illustrated fig. synthetic dataset, guidance scale set figure comparison images wild synthetic datasets super-resolution processing model. figure comparison images different models synthetic wild validation dataset. dataset score dataset score wild synthetic table impact guidance scale wild synthetic dataset. abbreviation guidance scale around generated images exhibit minimal percep- tual differences. wild dataset, increasing guid- ance scale generally enhances evaluation score. how- ever, beyond certain threshold, generated images ex- hibit noticeable artifacts, resulting degradation per- ceptual quality. balance evaluation metrics percep- tual quality, set guidance scale wild test set synthetic test set. semantic-aware module evaluate effectiveness proposed semantic-aware module, compare dape module used seesr ensure fair comparison, standardize parameter settings models conduct qualitative quantitative analyses assess performance. wild dataset, evaluate performance using musiq, maniqa, clipiqa metrics. shown tab. semantic-aware module outperforms dape across metrics, indicating sam module enhances models ability process wild images com- pared dape module. synthetic dataset, due lack images, focus subjective evalua- tions, shown fig. example shown fig. model demonstrates superior capability restoring text details natural texture compared seesr. overall, results suggest proposed semantic-aware module significantly improves performance wild synthetic datasets. model musiqmaniqaclipiqa score dape table ablation study semantic-aware model wild dataset. figure image restoration results wild synthetic datasets different values. figure comparison effects dape sam modules synthetic dataset. results two models obtained parameter setting gs. withou extra positive prompt. conclusion paper, propose novel approach ugc image super-resolution incorporating semantic guidance diffusion-based framework. leveraging semantic em- beddings, method enriches model high-level contextual information, enabling accurate reconstruc- tion fine details mitigating artifacts. address disparity synthetic real-world degradations, separately simulate types lsdir dataset integrate official training set, resulting diverse representative dataset. additionally, utilize pretrained sam model extract refined struc- tural semantic features, enhancing quality super-resolved images. meticulous selection key hyperparameters, optimize model perceptual fidelity, ensuring visually coherent high-quality results. approach effectively bridges gap synthetic real-world super-resolution, contributing ro- bust, perceptually faithful, practically applicable image enhancement. despite promising results, method still presents several limitations. first, exhibits notable difficulties reconstructing text regions conditions severe degradation occlusion. secondly, regions sub- stantial corruption low resolution, method may inad- vertently introduce excessive high-frequency components, potentially leading degradation subjective per- ceptual quality. current approach certain con- strains, provides strong foundation future extensions domain. acknowledgements work partly supported science tech- nology commission shanghai municipality no. shanghai key laboratory dig- ital media processing transmission grant dz, project bp. references eirikur agustsson radu timofte. ntire challenge single image super-resolution dataset study. ieee conference computer vision pattern recogni- tion cvpr workshops, qing cai, yiming qian, jinxing li, jun lyu, yee-hong yang, feng wu, david zhang. hipa hierarchical patch transformer single image super resolution. ieee trans- actions image processing, junyang chen, jinshan pan, jiangxin dong. faithd- iff unleashing diffusion priors faithful image super- resolution. arxiv preprint chao dong, chen change loy, kaiming he, xiaoou tang. image super-resolution using deep convolutional net- works. ieee transactions pattern analysis machine intelligence, sicheng gao, xuhui liu, bohan zeng, sheng xu, yan- jing li, xiaoyan luo, jianzhuang liu, xiantong zhen, baochang zhang. implicit diffusion models continu- ous super-resolution. proceedings ieeecvf con- ference computer vision pattern recognition, pages jonathan tim salimans. classifier-free diffusion guidance. arxiv preprint jonathan ho, ajay jain, pieter abbeel. denoising dif- fusion probabilistic models. advances neural information processing systems, xiaozhong ji, yun cao, ying tai, chengjie wang, jilin li, feiyue huang. real-world super-resolution via kernel estimation noise injection. proceedings ieeecvf conference computer vision pattern recognition workshops, pages junjie ke, qifei wang, yilin wang, peyman milanfar, feng yang. musiq multi-scale image quality transformer. proceedings ieeecvf international conference computer vision, pages jiwon kim, jung kwon lee, kyoung lee. deeply- recursive convolutional network image super-resolution. proceedings ieee conference computer vision pattern recognition, pages jiwon kim, jung kwon lee, kyoung lee. accurate image super-resolution using deep convolutional net- works. proceedings ieee conference computer vision pattern recognition, pages diederik kingma, max welling, al. auto-encoding vari- ational bayes, christian ledig, lucas theis, ferenc huszar, jose caballero, andrew cunningham, alejandro acosta, andrew aitken, alykhan tejani, johannes totz, zehan wang, al. photo- realistic single image super-resolution using generative ad- versarial network. proceedings ieee conference computer vision pattern recognition, pages xin li, xijun wang, bingchen li, kun yuan, yizhen shao, suhang yao, ming sun, chao zhou, radu timofte, zhibo chen. ntire challenge short-form ugc video quality assessment enhancement kwaisr dataset study. proceedings ieeecvf conference com- puter vision pattern recognition cvpr workshops, xin li, kun yuan, bingchen li, fengbin guan, yizhen shao, zihao yu, xijun wang, yiting lu, wei luo, suhang yao, ming sun, chao zhou, zhibo chen, radu timofte, al. ntire challenge short-form ugc video quality assessment enhancement methods results. pro- ceedings ieeecvf conference computer vision pattern recognition cvpr workshops, yawei li, yuchen fan, xiaoyu xiang, denis demandolx, rakesh ranjan, radu timofte, luc van gool. effi- cient explicit modelling image hierarchies image restoration. proceedings ieeecvf conference computer vision pattern recognition, pages yawei li, kai zhang, jingyun liang, jiezhang cao, liu, rui gong, yulun zhang, hao tang, yun liu, denis deman- dolx, al. lsdir large scale dataset image restoration. proceedings ieeecvf conference computer vision pattern recognition, pages zheyuan li, yingqi liu, xiangyu chen, haoming cai, jinjin gu, qiao, chao dong. blueprint separable residual network efficient image super-resolution. proceedings ieeecvf conference computer vision pattern recognition, pages jie liang, hui zeng, lei zhang. details artifacts locally discriminative learning approach realistic im- age super-resolution. proceedings ieeecvf con- ference computer vision pattern recognition, pages bee lim, sanghyun son, heewon kim, seungjun nah, kyoung lee. enhanced deep residual networks single image super-resolution. proceedings ieee confer- ence computer vision pattern recognition workshops, pages xinqi lin, jingwen he, ziyan chen, zhaoyang lyu, dai, fanghua yu, qiao, wanli ouyang, chao dong. diff- bir toward blind image restoration generative diffusion prior. european conference computer vision, pages springer, zhisheng lu, juncheng li, hong liu, chaoyan huang, lin- lin zhang, tieyong zeng. transformer single im- age super-resolution. proceedings ieeecvf con- ference computer vision pattern recognition, pages chao ma, chih-yuan yang, xiaokang yang, ming- hsuan yang. learning no-reference quality metric single-image super-resolution. computer vision image understanding, yunpeng qu, kun yuan, kai zhao, qizhi xie, jinhua hao, ming sun, chao zhou. xpsr cross-modal priors diffusion-based image super-resolution. european con- ference computer vision, pages springer, nikhila ravi, valentin gabeur, yuan-ting hu, ronghang hu, chaitanya ryali, tengyu ma, haitham khedr, roman radle, chloe rolland, laura gustafson, al. sam segment anything images videos. arxiv preprint robin rombach, andreas blattmann, dominik lorenz, patrick esser, bjorn ommer. high-resolution image synthesis latent diffusion models. proceedings ieeecvf conference computer vision pattern recognition, pages robin rombach, andreas blattmann, dominik lorenz, patrick esser, bjorn ommer. high-resolution image synthesis latent diffusion models. proceedings ieeecvf conference computer vision pattern recognition, pages jascha sohl-dickstein, eric weiss, niru maheswaranathan, surya ganguli. deep unsupervised learning using nonequilibrium thermodynamics. international confer- ence machine learning, pages pmlr, shaolin su, qingsen yan, zhu, cheng zhang, xin ge, jinqiu sun, yanning zhang. blindly assess image qual- ity wild guided self-adaptive hyper network. proceedings ieeecvf conference computer vi- sion pattern recognition, pages lingchen sun, rongyuan wu, zhiyuan ma, shuaizheng liu, qiaosi yi, lei zhang. pixel-level semantic-level adjustable super-resolution dual-lora approach. arxiv preprint jianyi wang, kelvin chan, chen change loy. ex- ploring clip assessing look feel images. pro- ceedings aaai conference artificial intelligence, pages xintao wang, yu, shixiang wu, jinjin gu, yihao liu, chao dong, qiao, chen change loy. esrgan en- hanced super-resolution generative adversarial networks. proceedings european conference computer vision eccv workshops, pages xintao wang, liangbin xie, chao dong, ying shan. real-esrgan training real-world blind super-resolution pure synthetic data. proceedings ieeecvf inter- national conference computer vision, pages rongyuan wu, tao yang, lingchen sun, zhengqiang zhang, shuai li, lei zhang. seesr towards semantics- aware real-world image super-resolution. proceedings ieeecvf conference computer vision pattern recognition, pages bin xia, yulun zhang, shiyin wang, yitong wang, xing- long wu, yapeng tian, wenming yang, luc van gool. diffir efficient diffusion model image restoration. proceedings ieeecvf international conference computer vision, pages sidi yang, tianhe wu, shuwei shi, shanshan lao, yuan gong, mingdeng cao, jiahao wang, yujiu yang. maniqa multi-dimension attention network no-reference image quality assessment. proceedings ieeecvf conference computer vision pattern recognition, pages tao yang, rongyuan wu, peiran ren, xuansong xie, lei zhang. pixel-aware stable diffusion realistic image super-resolution personalized stylization. european conference computer vision, pages springer, fanghua yu, jinjin gu, zheyuan li, jinfan hu, xiangtao kong, xintao wang, jingwen he, qiao, chao dong. scaling excellence practicing model scaling photo- realistic image restoration wild. proceedings ieeecvf conference computer vision pattern recognition, pages zongsheng yue, jianyi wang, chen change loy. resshift efficient diffusion model image super- resolution residual shifting. advances neural infor- mation processing systems, zongsheng yue, kang liao, chen change loy. arbitrary-steps image super-resolution via diffusion inver- sion. arxiv preprint kai zhang, jingyun liang, luc van gool, radu timofte. designing practical degradation model deep blind im- age super-resolution. proceedings ieeecvf inter- national conference computer vision, pages richard zhang, phillip isola, alexei efros, eli shecht- man, oliver wang. unreasonable effectiveness deep features perceptual metric. proceedings ieee conference computer vision pattern recogni- tion, pages weidong zhang, wenyi zhao, jia li, peixian zhuang, hai- han sun, yibo xu, chongyi li. cvanet cascaded visual attention network single image super-resolution. neural networks, yulun zhang, kunpeng li, kai li, lichen wang, bineng zhong, yun fu. image super-resolution using deep residual channel attention networks. proceedings european conference computer vision eccv, pages yulun zhang, yapeng tian, kong, bineng zhong, yun fu. residual dense network image super-resolution. proceedings ieee conference computer vision pattern recognition, pages zhicun zhang, han, linlin zhu, xiaoqi xi, lei li, meng- nan liu, siyu tan, bin yan. network architecture single image super-resolution comprehensive review comparison. iet image processing,", "published_date": "2025-04-14T05:26:24+00:00"}
{"id": "2504.05178v1", "title": "The 1st Solution for 4th PVUW MeViS Challenge: Unleashing the Potential of Large Multimodal Models for Referring Video Segmentation", "authors": ["Hao Fang", "Runmin Cong", "Xiankai Lu", "Zhiyang Chen", "Wei Zhang"], "summary": "motion expression video segmentation designed segment objects accordance input motion expressions. contrast conventional referring video object segmentation rvos, places emphasis motion well multi-object expressions, making arduous. recently, large multimodal models lmms begun shine rvos due powerful vision-language perception capabilities. work, propose simple effective inference optimization method fully unleash potential lmms referring video segmentation. firstly, use sava baseline, unified lmm dense grounded understanding images videos. secondly, uniformly sample video frames inference process enhance models understanding entire video. finally, integrate results multiple expert models mitigate erroneous predictions single model. solution achieved mevis test set ranked place pvuw challenge mevis track cvpr", "full_text": "solution pvuw mevis challenge unleashing potential large multimodal models referring video segmentation hao fang, runmin cong, xiankai lu, zhiyang chen, wei zhang shandong university team mvp-lab abstract motion expression video segmentation designed seg- ment objects accordance input motion expres- sions. contrast conventional referring video ob- ject segmentation rvos, places emphasis motion well multi-object expressions, making arduous. recently, large multimodal models lmms begun shine rvos due powerful vision-language per- ception capabilities. work, propose simple effective inference optimization method fully unleash potential lmms referring video segmentation. firstly, use sava baseline, unified lmm dense grounded understanding images videos. secondly, uniformly sample video frames inference process enhance models understanding entire video. finally, integrate results multi- ple expert models mitigate erroneous predictions single model. solution achieved mevis test set ranked place pvuw challenge mevis track cvpr introduction referring video object segmentation rvos contin- ually evolving task aims segment target objects video, referred linguistic expressions. recently, new large-scale dataset called motion expressions video segmentation mevis proposed, fo- cuses segmenting objects video content based sentence describing motion objects. compared traditional rvos datasets, emphasizes motion multi- object expression, making challenging. early rvos approaches adopt multi-stage complex pipelines take bottom-up top-down paradigms segment frame separately. meanwhile, compared rely complicated pipelines, mttr referformer first adopt end-to-end framework model- ing task sequence prediction problem, greatly simplifies pipeline. based end-to-end architecture transformer, soc mutr achieve excellent performance efficiently aggregating intra inter-frame information. example, place solution mevis track pvuw workshop involve fine-tuning mutr mevis lmpm dshmp add motion ex- pression understanding module video instance seg- mentation framework, specifically designed processing motion expression video segmentation. fully utilize capabilities existing video segmentation mod- els, place solution lsvos challenge rvos track integrate strengths leading rvos vos models build refine fine- tuning pipeline motion expression video segmentation. thanks achievements large language mod- els llms, large multimodal models lmms seen unprecedented development. recent studies explored implementation lmms produce object masks novel reasoning segmentation task, en- hances applicability real-world applications. inspired this, visa villa introduce new task reasoning video object segmentation, aims seg- ment track objects videos given implicit texts. collect large-scale datasets propose reasoning video segmentation models based lmms. sava first unified model dense grounded understanding images videos. sava combines sam foundation video segmentation model, llava advanced vision-language model, unifies text, image, video shared llm token space. experiments show sava achieves state-of-the-art across multiple tasks, particularly rvos, highlighting potential motion expression video segmentation. work, propose simple effective inference optimization method fully unleash potential lmms referring video segmentation. firstly, use sava baseline, uses llm generate instruction tokens guide sam producing precise masks, en- abling grounded, multi-modal understanding static cs.cv apr llm text image tokenizer image encoder visual prompt prompt encoder sam encoder lora image video text decoder language embd image embd video image encoder image encoder video embd prompt embd image caption video caption image video sam decoder visual features language features language outputs image outputs video outputs referring image segmentation referring video segmentation seg token figure architecture sava model first encodes input texts, visual prompts, images, videos token embeddings. tokens processed large language model llm. output text tokens used generate seg token associated language outputs. sam decoder receives image video features sam encoder, along seg token, generate corresponding image video masks. dynamic visual content. secondly, inference process, sava defaults inputting first frames llm, mevis long video dataset, results significant loss video information. uniformly sample video frames enhance models understand- ing entire video. finally, find sava necessarily perform better larger number param- eters sampling frames, configuration strengths different videos. videos cannot accurately segmented lmms, classic rvos model may handle well. integrate results multiple expert models mitigate erro- neous predictions single model. year, pixel-level video understanding wild challenge pvuw challenge adds two new tracks, com- plex video object segmentation track based mose motion expression guided video segmentation track based mevis two new tracks, additional videos annotations feature challenging elements provided, disappearance reappearance objects, inconspicuous small objects, heavy occlusions, crowded environments mose moreover, new motion expression guided video segmentation dataset mevis provided study natural language-guided video understanding complex environments. new videos, sentences, annotations enable foster development comprehensive robust pixel- level understanding video scenes complex environ- ments realistic scenarios. thanks superior per- formance sava uninext-cutie so- lution achieved mevis test set ranked place pvuw challenge mevis track cvpr method input rvos contains video sequence rhw frames corresponding referring expression tll words. so- lution consists three parts baseline sec. infer- ence sec. aggregation sec. baseline adopt sava baseline obtain mask se- quences mtn correlated language descriptions math cal mathcal frvosleft mathcal mathcal tright frvos denotes sava model. overall archi- tecture sava shown fig. contains two parts llava-like model sam pre-trained lmms. sava adopts pre-trained llava- like models lmms. contains one visual encoder, one visual projection layer, one llm. visual en- coder takes input images, video, sub-images inputs. visual projection layer maps inputs visual tokens. tokens, combined input text tokens, input llms llms generate text token pre- diction based them. note sava adopts pre-trained lmms following previous works leverage algorithm rvos inference pipeline input video length number key frames video frames x,. language description output sequence masks m,. run sava model rvos uniform sampling extract key frames visual embeddings encodersm language embeddings encodert answers llmev, prompt embedding linearfinda, seg sam feature encoderx mask decoderpl, update memory mem cross-attentionmem, sam feature encoderx mask decodermem, update memory mem cross-attentionmem, emit m,. strong capability. image video chat datasets, follows pipeline without modification. decoupled design. sava append sam alongside pre-trained llava model. take sam out- put tokens visual features decoder outputs llm. three reasons. first, sava makes combi- nation simple possible without increasing extra com- putation costs. secondly, adding extra tokens needs ex- tra alignment process. thirdly, via design, fully make work plug-in-play framework utilize pre- trained lmms since lmm community goes fast. thus, sava adopts decoupled design without introducing fur- ther communication llava sam tuning sam decoder via seg tokens. sava con- nects sam lmm via special token seg. hidden states seg token used new type prompt fed sam decoder, decoded segmentation masks. hidden states seg seen novel spatial-temporal prompt sam sam segments corresponding object mask image video based spatial-temporal prompt. training, sam decoder tuned un- derstand spatial-temporal prompt, gradients backpropagated seg token lmm, al- lowing output spatial-temporal prompt better. inference rvos tasks, sava designs simple framework achieve strong results public benchmarks. particular, giving input video, adopts seg token gener- ate masks key frames. then, uses memory encoded key frame features generate mask remaining frames. sava defaults extracting first five frames input video key frames llm, mevis long video dataset, results significant loss video information. described algorithm uniformly sample video frames key frames en- hance lmms understanding entire video. key frames fed clip flattened vi- sual sequential tokens llm processing. llm takes visual language tokens input uses to- kens extract information video generate seg token. sam prompt encoder encodes boxes clicks prompt embeddings object referring. different sam sava use two linear layers project seg token language prompt embed- ding, serves extension sam prompt encoders. language prompt embedding, uses sam decoder generate masks key frames. then, sava use memory encoder sam gener- ate memory based output key-frame masks. finally, memory attention sam generates remaining masks using memory generated key-frame previous non-key-frame masks. aggregation find sava necessarily perform better larger number parameters sampling frames, configuration strengths differ- ent videos. videos cannot accurately segmented lmms, classic rvos model may handle well. integrate results multiple ex- pert models mitigate erroneous predictions single model math mathcal ffuseleft mathcal mkright sets mask sequences output sava models different configurations rvos models ffuse denotes pixel-level binary mask voting. pixels value equal divide pixel foreground, other- wise, divided background. experiments datasets metrics dataset. mevis newly established dataset targeted motion information analysis contains video clips high-quality object segmenta- tion masks, sentences indicating objects complex environments. videos divided training videos, validation videos test videos. evaluation metrics. employ region similarity aver- age iou, contour accuracy mean boundary similarity, average mathcal mathcal evaluation metrics. implementation details use trained weights provided sava combine sota lmms models like internvl. table leaderboard mevis test set. team mathcal mathcal mvp-lab referdino-plus harbory pengsong ssams strong kimchi table sava mevis validation set. model sampling number mathcal mathcal sam sava trained four types datasets, including image qa, video qa, image segmentation, video segmentation datasets. video-level referring ex- pression segmentation, sava used existing rvos data ref-youtubevos mevis revos self-built ref-sav dataset. conduct test- ing nvidia gpu memory. main results shown tab. solution achieves mevis test set ranks place pvuw challenge mevis track cvpr ablation study validate effectiveness model, conduct simple ablation studies. shown tab. model parame- ter quantity sava, sampling whether uniform sam- pling, number refers number frames sampled video. sampling first frames, im- provement sava-b mathcal mathcal compared sava-b, indicating potential model fully utilized. shown third row, using uniform sampling improved mathcal mathcal indicating crucial correctly understanding entire video. increasing sampling frame number still effec- tive, achieving highest performance mathcal mathcal frames. shown tab. without post- processing semi-supervised learning, sava reachs level comparable uninext-cutie table comparison sava uninext-cutie mevis val set. team mathcal mathcal sava-b uninext-cutie conclusion work, propose simple effective inference optimization method fully unleash potential lmms referring video segmentation. firstly, use sava baseline, unified lmm dense grounded understanding images videos. secondly, uniformly sample video frames inference process enhance models understanding en- tire video. finally, integrate results multiple ex- pert models mitigate erroneous predictions single model. solution achieved mevis test set ranked place pvuw challenge mevis track cvpr references adam botach, evgenii zheltonozhskii, chaim baskin. end-to-end referring video object segmentation multi- modal transformers. proceedings ieeecvf con- ference computer vision pattern recognition, pages zhe chen, weiyun wang, yue cao, yangzhou liu, zhang- wei gao, erfei cui, jinguo zhu, shenglong ye, hao tian, zhaoyang liu, al. expanding performance boundaries open-source multimodal models model, data, test- time scaling. arxiv preprint kei cheng, seoung wug oh, brian price, joon-young lee, alexander schwing. putting object back video object segmentation. proceedings ieeecvf conference computer vision pattern recognition, pages henghui ding, chang liu, shuting he, xudong jiang, chen change loy. mevis large-scale benchmark video segmentation motion expressions. proceedings ieeecvf international conference computer vi- sion, pages henghui ding, chang liu, shuting he, xudong jiang, philip torr, song bai. mose new dataset video object segmentation complex scenes. proceedings ieeecvf international conference computer vi- sion, pages henghui ding, lingyi hong, chang liu, ning xu, lin- jie yang, yuchen fan, deshui miao, yameng gu, xin li, zhenyu he, al. lsvos challenge report large-scale com- plex long video object segmentation. arxiv preprint henghui ding, chang liu, yunchao wei, nikhila ravi, shuting he, song bai, philip torr, deshui miao, xin li, zhenyu he, al. pvuw challenge complex video understanding methods results. arxiv preprint hao fang, feiyu pan, xiankai lu, wei zhang, runmin cong. uninext-cutie solution lsvos challenge rvos track. arxiv preprint hao fang, peng wu, yawei li, xinxin zhang, xiankai lu. unified embedding alignment open-vocabulary video instance segmentation. european conference com- puter vision, pages springer, hao fang, tong zhang, xiaofei zhou, xinxin zhang. learning better video query sam video instance seg- mentation. ieee transactions circuits systems video technology, shuting henghui ding. decoupling static hier- archical motion perception referring video segmentation. proceedings ieeecvf conference computer vision pattern recognition, pages miran heo, sukjun hwang, seoung wug oh, joon-young lee, seon joo kim. vita video instance segmentation via object token association. advances neural information processing systems, anna khoreva, anna rohrbach, bernt schiele. video object segmentation language referring expressions. computer visionaccv asian conference computer vision, perth, australia, december re- vised selected papers, part pages springer, xin lai, zhuotao tian, yukang chen, yanwei li, yuhui yuan, shu liu, jiaya jia. lisa reasoning segmentation via large language model. proceedings ieeecvf conference computer vision pattern recognition, pages chen liang, wu, tianfei zhou, wenguan wang, zongxin yang, yunchao wei, yang. rethinking cross-modal interaction top-down perspective referring video object segmentation. arxiv preprint haotian liu, chunyuan li, qingyang wu, yong jae lee. visual instruction tuning. advances neural information processing systems, zhuoyan luo, yicheng xiao, yong liu, shuyan li, yi- tong wang, yansong tang, xiu li, yujiu yang. soc semantic-assisted object cluster referring video object segmentation. advances neural information processing systems, feiyu pan, hao fang, runmin cong, wei zhang, xi- ankai lu. video object segmentation via sam solution lsvos challenge vos track. arxiv preprint nikhila ravi, valentin gabeur, yuan-ting hu, ronghang hu, chaitanya ryali, tengyu ma, haitham khedr, roman radle, chloe rolland, laura gustafson, al. sam segment anything images videos. arxiv preprint zhongwei ren, zhicheng huang, yunchao wei, yao zhao, dongmei fu, jiashi feng, xiaojie jin. pixellm pixel reasoning large multimodal model. proceedings ieeecvf conference computer vision pattern recognition, pages seonguk seo, joon-young lee, bohyung han. urvos unified referring video object segmentation network large-scale benchmark. computer visioneccv european conference, glasgow, uk, august proceedings, part pages springer, peng wang, shuai bai, sinan tan, shijie wang, zhihao fan, jinze bai, keqin chen, xuejing liu, jialin wang, wenbin ge, al. qwen-vl enhancing vision-language models perception world resolution. arxiv preprint jiannan wu, jiang, peize sun, zehuan yuan, ping luo. language queries referring video object seg- mentation. proceedings ieeecvf conference computer vision pattern recognition, pages bin yan, jiang, jiannan wu, dong wang, ping luo, ze- huan yuan, huchuan lu. universal instance percep- tion object discovery retrieval. proceedings ieeecvf conference computer vision pattern recognition, pages cilin yan, haochen wang, shilin yan, xiaolong jiang, yao hu, guoliang kang, weidi xie, efstratios gavves. visa reasoning video object segmentation via large language models. european conference computer vision, pages springer, shilin yan, renrui zhang, ziyu guo, wenchao chen, wei zhang, hongyang li, qiao, hao dong, zhongjiang he, peng gao. referred multi-modality unified tem- poral transformer video object segmentation. proceed- ings aaai conference artificial intelligence, pages haobo yuan, xiangtai li, tao zhang, zilong huang, shilin xu, shunping ji, yunhai tong, qi, jiashi feng, ming-hsuan yang. sava marrying sam llava dense grounded understanding images videos. arxiv preprint rongkun zheng, qi, chen, wang, kun wang, qiao, hengshuang zhao. villa video reasoning segmentation large language model. arxiv preprint", "published_date": "2025-04-07T15:24:54+00:00"}
{"id": "2503.23725v1", "title": "Exploring Temporal Dynamics in Event-based Eye Tracker", "authors": ["Hongwei Ren", "Xiaopeng Lin", "Hongxiang Huang", "Yue Zhou", "Bojun Cheng"], "summary": "eye-tracking vital technology human-computer interaction, especially wearable devices ar, vr, xr. realization high-speed high-precision eye-tracking using frame-based image sensors constrained limited temporal resolution, impairs accurate capture rapid ocular dynamics, saccades blinks. event cameras, inspired biological vision systems, capable perceiving eye movements extremely low power consumption ultra-high temporal resolution. makes promising solution achieving high-speed, high-precision tracking rich temporal dynamics. paper, propose tdtracker, effective eye-tracking framework captures rapid eye movements thoroughly modeling temporal dynamics implicit explicit perspectives. tdtracker utilizes convolutional neural networks capture implicit short-term temporal dynamics employs cascaded structure consisting frequency-aware module, gru, mamba extract explicit long-term temporal dynamics. ultimately, prediction heatmap used eye coordinate regression. experimental results demonstrate tdtracker achieves state-of-the-art sota performance synthetic seet dataset secured third place cvpr event-based eye-tracking challenge code available", "full_text": "exploring temporal dynamics event-based eye tracker hongwei ren, xiaopeng lin, hongxiang huang, yue zhou, bojun cheng mics thrust hong kong university science technology guangzhou hren, xlin, hhuang, yzhouconnect.hkust-gz.edu.cn, bochenghkust-gz.edu.cn abstract eye-tracking vital technology human-computer interaction, especially wearable devices ar, vr, xr. realization high-speed high- precision eye-tracking using frame-based image sensors constrained limited temporal resolution, im- pairs accurate capture rapid ocular dynamics, saccades blinks. event cameras, inspired biologi- cal vision systems, capable perceiving eye movements extremely low power consumption ultra-high tem- poral resolution. makes promising solution achieving high-speed, high-precision tracking rich temporal dynamics. paper, propose tdtracker, effective eye-tracking framework captures rapid eye movements thoroughly modeling temporal dynam- ics implicit explicit perspectives. tdtracker utilizes convolutional neural networks capture im- plicit short-term temporal dynamics employs cas- caded structure consisting frequency-aware module, gru, mamba extract explicit long-term temporal dy- namics. ultimately, prediction heatmap used eye co- ordinate regression. experimental results demonstrate tdtracker achieves state-of-the-art sota performance synthetic seet dataset secured third place cvpr event-based eye-tracking challenge code available tdtracker. introduction eye tracking technology pivotal advancement human-computer interaction, finding extensive application augmented reality ar, extended reality xr, medi- cal diagnostics, psychological research pro- vides precise measurements rapid ocular movements, en- abling sophisticated, intuitive gaze-based interactions, no- tably advanced wearable platforms. however, equal contribution, order determined random seed. corresponding author. rapid velocities exceeding substantial ac- celerations characteristic human eye movements necessitate sampling rates kilo-hertz level. realizing high precision within wearable de- vices technically challenging due strict limitations re- garding power consumption, latency, data processing capacity. hence, development high-speed high-precision eye-tracking systems represents in- dispensable requirement contemporary technological in- novation. traditional frame-based eye-tracking systems exhibit tracking delays milliseconds insufficient capturing rapid eye movements accurately kilo-hertz rates. achieving high sampling rates significantly in- creases power consumption, resulting large data volumes require substantial bandwidth energy real- time processing event cameras provide ef- fective solution eliminating redundant information focusing solely dynamic elements within scene. event cameras type bio-inspired vision sensor respond local changes illumination intensity ex- ceeding predefined threshold, offering several advantages traditional frame-based cameras feature low latency, high dynamic range, asynchronous opera- tion, making highly suitable applications involving rapid movements varying lighting conditions specifically, event cameras capture brightness changes microsecond precision provide high temporal resolution, crucial eye tracking even fastest eye movements, saccades, ac- curately captured tracked. characteristics en- able event cameras deliver sparse informative data stream highly efficient eye tracking. however, despite advantages, current algorithms eye track- ing using event cameras still face several challenges. one primary issues effective extraction utilization temporal dynamics inherent event data. temporal dynamics, characteristic eye move- ments states evolve time, play crucial role design robust eye-tracking systems ability cs.cv mar accurately track eye coordinates depends spa- tial information also temporal changes occur eye movements. dynamics, blinking, gaze shifts, reset, saccade, introduce variability disrupt conventional tracking methods, making essential eye trackers account temporal factors maintain continuous accurate tracking another per- spective, temporal dynamics enable tracker adapt short-term long-term fluctuations eye state. instance, eye blinks moves rapidly, tracker must able predict next gaze point without losing track eye. failing capture temporal patterns lead tracking errors, poor user experience, drop performance. thus, effectively modeling temporal dy- namics improves robustness reliability eye track- ers, allowing perform well wider range conditions. paper, sufficiently explore temporal dynam- ics event-based eye tracker named tdtracker. tdt- track principally composed two distinct components implicit temporal dynamic itd explicit temporal dy- namic etd. itd component implicitly extracts short- term temporal features leveraging convolution neu- ral networks, effectively capturing nuanced variations temporal patterns brief periods. hand, etd component explicitly extracts long-term tempo- ral features cascading structure three ad- vanced temporal models frequency-aware module, gru, mamba. cascading approach enables model capture complex sustained temporal dependen- cies, enhancing overall performance dynamic track- ing tasks. unlike eye trackers directly regress coordinates, tdtracer generates heatmaps employs kullback-leibler divergence training, enabling post-processing based probability distributions. con- duct comprehensive validation synthetic real datasets, tdtracker achieved state-of-the-art sota per- formance seet dataset, fewer floating point operations flops previous sota, eventmamba. additionally, tdtracker secured third place cvpr event-based eye tracking challenge. related work eye tracking traditional frame-based eye-tracking methods rely frame-based cameras categorized model-based appearance-based approaches model-based meth- ods identify eye geometry align key features predefined models often require manual calibration struggle variations lighting eye anatomy appearance-based techniques utilize deep learning analyze eye images, demanding significant com- putational resources extensive training data additionally, frame-based cameras generally operate fre- quencies hz, pushing beyond threshold sig- nificantly boosts power consumption, thus exceeding energy limitations mobile wearable systems event-based eye tracking methods utilize intrinsic properties event data deliver high frame rates minimal bandwidth, enhancing energy efficiency compared conventional frame-based systems recent advance- ments event-based eye tracking demonstrate significant methodological evolution across multiple research teams. al. propose three-channel temporal encod- ing framework coupled lightweight convolution neu- ral network low-latency pupil event prediction, estab- lishing new paradigm real-time processing. subse- quently, chen al. implemented temporal binning transformation event data, developing novel cross- bottleneck convlstm cb-convlstm architecture outperforms conventional cnn models spatiotemporal feature extraction. parallel, ryan al. innovatively integrated gated recurrent units grus adapted yolov framework, enabling robust eye tracking voxel grid representations asynchronous event streams. ais challenge event-based eye tracking catalyzed methodological innovation domain, participants employing advanced architectures includ- ing convlstms mamba-based models spa- tiotemporal processing raw event data. competitive so- lutions demonstrated sophisticated data conversion tech- niques ranging dynamic binary encoding neuromor- phic point cloud representations notably, recent pub- lications demonstrate efficacy spiking neural networks snns extracting temporal features, thereby enhancing tracking precision achieving lower compu- tational overhead compared conventional artificial neu- ral networks anns. time series model event-based eye tracking, time series analysis critical. accurately extracting short-term long-term, well local global temporal information, contributes signifi- cantly understanding users fixation behaviors visual attention patterns. traditionally, recurrent neural networks rnns variants, long short-term memory lstm gated recurrent units gru widely utilized processing eye-tracking data. significantly increases network sparsity reduces computational load maintaining accuracy introducing change-based hidden state input. employs gru core component network archi- tecture model eye-movement event sequences de- tect onsets offsets eye-movement events, thereby classifying eye-movement data. however, models may encounter problems like vanishing exploding gra- dients handling long sequences, negatively impacting model performance. unlike rnns, mamba model, selective state- space model, demonstrates unique advantages time se- ries modeling. integrating recursive characteristics rnns parallel computation abilities cnns, mamba maintains linear complexity enhancing infor- mation filtering capabilities, enabling efficient processing long sequences capturing global patterns. currently, mamba applied various event-related tasks, effectively enhances extraction explicit temporal fea- tures event sequences introducing mamba. models hidden states eye movement patterns dual recurrent module selectively focus valid eye mo- tion phases, thereby improving stability accuracy eye tracking. however, fixed hidden-state dimension mamba might insufficient represent extremely long se- quences effectively. regardless whether using mamba rnns, models may face problem informa- tion forgetting long-term modeling, necessitating improvements. better tackle problem long-term information forgetting, studies introduced fast fourier transform fft based principle multipli- cation frequency domain equivalent convolution time domain, fft enables models learn holistic information across entire time domain helps miti- gate information forgetting subsequent temporal model- ing tasks. currently, fft employed tasks, super-resolution low-level tasks human pose estimation however, research yet applied fft methods temporal information extraction eye-tracking tasks, presenting novel direction future research. method section, provide detailed explanation td- tracker, covering representations raw events, two key modules implicit temporal dynamic explicit temporal dynamic capturing within network architec- ture, well loss function specifically designed heatmap prediction. event representations event cameras produce raw events capture changes environments illumination, encoding information terms spatial dimensions, temporal dimension, polarity s-t-p. raw events, denoted formally expressed xi, yi, ti, formulation, specifies spatial location event emission, refers timestamp, denotes polarity, indicates index ith element within event stream. numerous approaches involve converting raw events event frame, serves static representation dynamic event stream. frame created aggre- gating number events polarity pixel location following equation. fx, tit,tn ti, ei, here, denote spatial coordinates within event frame, ranges ranges signifying width height frame resolution, respectively. event frame encapsulates collected event data discrete time intervals, effectively summarizing temporal evolution events. ag- gregated information crucial subsequent processing steps analytical procedures, providing comprehensive representation event dynamics across defined spa- tial grid. another widely used representation voxel, denoted defined following set equations. time axis divided discrete intervals, referred time bins, boundaries bins specified lk, corresponds index kth time bin. particular, boundary computed using following formula represent start end times event sequence, respectively, denotes total number bins. voxel representation constructed aggre- gating event data ti, pi, event consists timestamp polarity pi. events grouped respective time bins lk, lk, boundaries kth time bin, follows vx, tilk,lk ti, ei, furthermore, binary map representation combines binary frames generate sequence bits pixel pixel, binary map considers sequence bits representation different num- ber system. instance, sequence eight binary frames encoded single -bit unsigned integer frame. additionally, binary map approach extended manner similar binary event frames, producing se- quence frames, represented b-bit numbers. approach indirectly yields sparse representation, re- lying binary event images. figure architecture tdtracker. tdtracker primarily comprises two components, implicit temporal dynamic itd explicit temporal dynamic etd, structure featuring three itd components ensure effective feature abstraction. employs cascaded architecture three distinct time series models comprehensively capture temporal information. implicit temporal dynamic itd tdtracker leverages convolution applied frame- based representation dimensions allow- ing model implicitly abstract temporal dimension spatio-temporal features effectively capture temporal spatial patterns within data. feature extraction process structured three sequential stages, designed progressively increase receptive field shown fig. method chooses binary map representation preferred form representation. input tdtracker represented frame- based tensor rct denotes tem- poral dimension, represent spatial dimen- sions. begin feature extraction process, tdtracker applies convolution operation input capture spatial features using kernel size ks, ks, represents spatial kernel size. convolution op- eration expressed mathematically convdi rksks convolution kernel, correspond- ing bias term, convd denotes convolution op- eration performed spatial dimensions following this, tdtracker applies another convolu- tion operation intermediate feature map ex- tract temporal features using kernel size kt, kt, kt, temporal kernel size. call convo- lution named implicit-conv. operation abstracts temporal dimension spatio-temporal features, cap- turing dynamic changes temporal domain well spatial patterns within events convdfs rktktkt step allows model encode implicit temporal dynamics alongside spatial characteristics data. network deepens, tdtracker progressively in- creases receptive field stage. increase en- ables model better capture larger contexts complex dependencies within data. order mit- igate risk overfitting control rapid growth parameters, tdtracker employs average downsampling feature maps spatial dimensions stage. downsampling operation, typically im- plemented pooling, reduces spatial dimensions preserving essential features. downsampling strategy minimizes computational cost also aids preservation critical spatial information, thus improv- ing models generalization capability. passing multiple stages convolution downsampling, final feature map produced itd ef- fectively captures relevant implicit temporal spatial patterns within data, providing model nec- essary features make accurate predictions. explicit temporal dynamic etd tdtracker utilizes combination three distinct types time-series models stacked together effectively extract explicit temporal features sequences. frequency-aware module, gru, mamba. follow- ing section describes using mathematical methods. ... frequency-aware module one-dimensional discrete fourier transform dft em- ployed convert features frequency domain following formulation xnej kn, imaginary unit, represents signals temporal domain, denotes spectrum different fre- quencies, length temporal signals addi- tionally, inverse dft recover spectrum temporal signals following formulation xkej kn, mathematically, xk, means specturm conjugate symmetric. therefore, transformed frequency domain spectrum needs long enough recovered original signal. spectrum obtained dft, ini- tialize learnable filter dimensionality matching spectrum perform hadamard product spectrum filter. summary, specific process frequency-aware module follows fftx, ifft nonlinear activation function means hadamard product. ... gated recurrent unit gru gated recurrent unit block designed effec- tively capture sequential dependencies incorporating gat- ing mechanisms control flow information. model particularly adept learning long-term temporal patterns, making suitable time-series data intri- cate relationships time. gru processes inputs updates hidden states follows wrxt urht wzxt uzht tanhwhxt uhrt reset gate, update gate, candidate hidden state, current hidden state, input time step hidden state previ- ous time step. denotes nonlinear activation function, tanh hyperbolic tangent activation function, represents element-wise multiplication. ... mamba block mainly integrates sixth version state space model ssm, well able parallel focus long time series information, temporal corre- lations sequences. ssm extracts explicit temporal features expressed follows aht bxt, cht, expa, aexpa xt, ht, ssms discrete inputs, states, outputs. continuous system param- eters, parameters discrete system zero-order hold rule. num- ber dimension events current stage, respec- tively. whole globalfe extractor represented formula mambasa resbst resb residual block, mamba extracts ex- plicit temporal features dimension, resb abstracts spatial temporal. loss function several works utilized rgb images cnn model shown transforming labels heatmap predicting heatmap provides effective approach regression tasks. transform labels two heat vectors corre- sponding evaluation metric resolution size. vh, method resolution representation param flops msepx tdtracker mambapupil eventmamba fapnet pepnet pepnettiny pointmlpelite pointnet pointnet cnn tenns cb-convlstm convlstm table results tdtracker synthetic seet dataset. figure visualization heatmap generated tdtracker. vectors one-hot encoded blurred using gaussian kernel, shapes respec- tively. exp utilize divergence loss measure differ- ence two probability distributions, commonly used loss function various machine learn- ing models quantify one probability distribution di- verges second reference distribution. lklp log represents probability index distribution represents probability in- dex distribution summation runs possible indices. so, final loss obtained predicted scatter true labels ltotal lklx lkly final output coordinates highest proba- bility visualized heatmap shown fig. experiment experiments, proposed method evaluated using two datasets real-world event dataset pro- vided challenge synthetic event dataset seet. effectiveness eye-tracking method assessed measuring accuracy pupil location prediction. euclidean distance predicted pupil location ground truth label serves evaluation metric. tracking success defined instance dis- tance error falls within pixels, subsequently used calculate tracking rate. dataset seet dataset synthetic eye-tracking dataset created rgb dataset called labeled pupils wild event streams generated using simulator resolution time step ms, ensuring synchronization frame rate original rgb dataset. dataset event-based eye-tracking dataset features real-world events captured using dvxplorer mini camera. includes data partic- ipants, involved recording sessions. sessions, participants engage five types activities random movements, saccades, reading text, smooth pursuit, blinking. total data size gb. ground truth labels provided hz. implement details server leverages pytorch deep learning framework selects adamw optimizer initial learning figure visualization results tdtracker. green dot figure stands ground truth label yellow dot prediction results genetated tdtracker. figure visualization trajectory comparison ground truth label prediction results gererated tdtracker. rate set employs cosine decay strat- egy, accompanied weight decay parameter configuration meticulously chosen enhance models convergence performance adaptive learning rate adjustments. training conducted nvidia geforce rtx gpu memory, enabling batch size results seet dataset present results tdtracker synthetic seet dataset, shown tab. tdtracker designed resolution utilizing frame-based represen- tation. model achieves parameter million requires million flops. terms tracking accu- racy, tdtracker demonstrates superior performance high score indicates robust capability maintaining precise track- ing time. furthermore, model achieves mse pixels, highlighting accuracy localizing eye within event stream. comparison state-of- the-art methods, tdtracker performs favorably, especially considering trade-off accuracy com- putational efficiency. instance, mambapupil yields lower also substan- tially larger number parameters million, result- ing higher mse pixels. additionally, methods eventmamba report better accuracy significantly computational complexity million flops. results demonstrate tdtracker achieves strong balance model efficiency accuracy, making promising candidate real-time eye-tracking tasks resource-constrained environments. results dataset performance tdtracker dataset presented tab. tdtracker achieves strong results score mse pixels. compared mambapupil, higher parameter count .m, tdtracker performs efficiently lower computational cost better tracking accuracy mse pixels. tenns computationally efficient, tracking accuracy method resolution representation param flops msepx tdtracker mambapupil tenns table results tdtracker dataset. ground truth obtained interpolation result comparison model open-source code reproduction version. method param flops msepx fft mamba implict-conv table abalation study tdtracker dataset. ground truth obtained interpolation representation msepx event frame voxel binary map table ablation different representation dataset. augment method employed. lower, mse pixels. over- all, tdtracker strikes good balance efficiency accuracy. whats more, tdtrackers inference time rtx draw serval predict coordinates trajectory tdtracker figs. ablation study ... key module ablation ablation study key module tdtracker dataset, shown tab. demonstrates impact different modules performance. full model denoted all, achieves best performance pixels. removing fft module fft results slight drop performance, particularly still maintaining strong precision scores excluding mamba mamba reduces accuracy, especially leads mse pixels. significant performance degradation occurs implicit-conv module removed implicit-conv, precision scores drop drasti- cally, mse increases pixels. results highlight critical role component enhancing tracking accuracy efficiency tdtracker. ... representation ablation ablation study different representations td- tracker shows choice representation significantly affects models performance tab. among tested options, binary map representation consistently outper- forms others, offering best balance accuracy precision. event frame voxel representations also yield strong results, tend slightly lower performance, particularly terms precision error. binary map representation demonstrates ro- bust tracking performance, suggesting superior ability capture relevant features tdtracker. overall, findings highlight importance selecting appropri- ate representation optimize tracking accuracy. challenge post-process competition, found using sequence training, sequence testing highest accuracy mse however, since parameters frequency-aware module tied sequence length, canceled module competition. addition, since model consider open closed eye cases, simply use ratio number events basis judgement set current ratio smaller value, inference eye coordinate changed sample overwritten in- ference value closest sample. whats more, differ directly regressing coordinate information using predicted probability density map, provides additional probability model predicting image shown fig. probability less believe predicted result. post-processing, mse optimized conclusion paper, present tdtracker, event-based eye tracker effectively captures implicit explicit temporal dynamics. leveraging cnns cas- caded architecture frequency-aware module, gru, mamba models, tdtracker improves tracking per- formance enhances robustness. experiments demon- strate sotas performance seet dataset third place cvpr event-based eye-tracking chal- lenge. references murat bagci, rashid ansari, khokhar, cetin. eye tracking using markov models. proceedings in- ternational conference pattern recognition, icpr pages ieee, sami barchid, jose mennesson, chaabane djeraba. bina-rep event frames simple effective representa- tion event-based cameras. ieee international conference image processing icip, pages ieee, pietro bonazzi, sizhen bian, giovanni lippolis, yawei li, sadique sheik, michele magno. retina low-power eye tracking event camera spiking hardware. pro- ceedings ieeecvf conference computer vision pattern recognition, pages jiadi chen, chunjiang duanmu, huanhuan long. large kernel frequency-enhanced network efficient single im- age super-resolution. proceedings ieeecvf con- ference computer vision pattern recognition, pages qinyu chen, zuowen wang, shih-chii liu, chang gao. efficient event-based eye tracking using change-based convlstm network. ieee biomedical circuits systems conference biocas, pages ieee, bojun cheng, zhou yue, yuetong fang, raphael gisler, hongwei ren, haotian fu, zelin ma, yulong huang, ren- jing xu, alexandre bouhelier, al. memristive blinking neuron enabling dense scalable photonically-linked neu- ral network. kyunghyun cho, bart van merrienboer, caglar gulcehre, dzmitry bahdanau, fethi bougares, holger schwenk, yoshua bengio. learning phrase representations using rnn encoder-decoder statistical machine translation. arxiv preprint james cooley john tukey. algorithm machine calculation complex fourier series. mathematics computation, yongjian deng, hao chen, hai liu, youfu li. voxel graph cnn object classification event cameras. proceedings ieeecvf conference computer vi- sion pattern recognition, pages junyuan ding, ziteng wang, chang gao, min liu, qinyu chen. facet fast accurate event-based eye tracking using ellipse modeling extended reality. arxiv preprint jeffrey elman. finding structure time. cognitive sci- ence, elias daniel guestrin moshe eizenman. general theory remote gaze estimation using pupil center corneal reflections. ieee transactions biomedical engineering, sepp hochreiter jurgen schmidhuber. long short-term memory. neural computation, yuhuang hu, shih-chii liu, tobi delbruck. video frames realistic dvs events. proceedings ieeecvf conference computer vision pattern recognition, pages khadija iddrisu, waseem shariff, peter corcoran, noel oconnor, joe lemley, suzanne little. event camera based eye motion analysis survey. ieee access, yizhou jiang, wenwei wang, lei yu, chu he. eye tracking based event camera spiking neural network. electronics, xin jin, suyu chai, jie tang, xianda zhou, kai wang. eye-tracking arvr technological review future di- rections. ieee open journal immersive displays, joohwan kim, michael stengel, alexander majercik, shalini mello, david dunn, samuli laine, morgan mcguire, david luebke. nvgaze anatomically-informed dataset low-latency, near-eye gaze estimation. proceedings chi conference human factors computing systems, pages taewoo kim, hoonhee cho, kuk-jin yoon. frequency- aware event-based video deblurring real-world motion blur. proceedings ieeecvf conference com- puter vision pattern recognition, pages robert konrad, anastasios angelopoulos, gordon wet- zstein. gaze-contingent ocular parallax rendering virtual reality. acm transactions graphics tog, chih-chuan lai, sheng-wen shih, yi-ping hung. hy- brid method gaze tracking using glint contour features. ieee transactions circuits systems video technology, junxuan li, shaodi you, antonio robles-kelly. frequency domain neural network fast image super- resolution. international joint conference neural networks ijcnn, pages ieee, nealson li, ashwin bhat, arijit raychowdhury. track eye tracking event camera extended reality applications. ieee international confer- ence artificial intelligence circuits systems aicas, pages ieee, patrick lichtsteiner, christoph posch, tobi delbruck. latency asynchronous temporal con- trast vision sensor. ieee journal solid-state circuits, xiaopeng lin, hongwei ren, bojun cheng. fapnet effective frequency adaptive point-based eye tracker. pro- ceedings ieeecvf conference computer vision pattern recognition, pages feng lu, yusuke sugano, takahiro okabe, yoichi sato. adaptive linear regression appearance-based gaze esti- mation. ieee transactions pattern analysis machine intelligence, fei ma, yucheng yuan, yifan xie, hongwei ren, ivan liu, ying he, fuji ren, fei richard yu, shiguang ni. gener- ative technology human emotion recognition scoping review. information fusion, page ma, qin, haoxuan you, haoxi ran, yun fu. rethinking network design local geometry point cloud simple residual mlp framework. arxiv preprint xintian mao, yiming liu, fengze liu, qingli li, wei shen, yan wang. intriguing findings frequency selection image deblurring. proceedings aaai conference artificial intelligence, pages pier luigi mazzeo, dilan damico, paolo spagnolo, cosimo distante. deep learning based eye gaze estimation prediction. international conference smart sustainable technologies splitech, pages ieee, clara mestre, josselin gautier, jaume pujol. robust eye tracking based multiple corneal reflections clinical applications. journal biomedical optics, carlos morimoto marcio mimica. eye gaze track- ing techniques interactive applications. computer vision image understanding, alejandro newell, kaiyu yang, jia deng. stacked hour- glass networks human pose estimation. computer visioneccv european conference, amster- dam, netherlands, october proceedings, part viii pages springer, yan pei, sasskia bruers, sebastien crouzet, douglas mclelland, olivier coenen. lightweight spatiotem- poral network online eye tracking event camera. proceedings ieeecvf conference computer vi- sion pattern recognition, pages charles qi, hao su, kaichun mo, leonidas guibas. pointnet deep learning point sets classification segmentation. proceedings ieee conference computer vision pattern recognition, pages charles ruizhongtai qi, yi, hao su, leonidas guibas. pointnet deep hierarchical feature learning point sets metric space. advances neural information processing systems, vilayanur ramachandran. encyclopedia human brain. elsevier, henri rebecq, rene ranftl, vladlen koltun, davide scaramuzza. high speed high dynamic range video event camera. ieee transactions pattern analysis machine intelligence, hongwei ren, yue zhou, yulong huang, haotian fu, xi- aopeng lin, jie song, bojun cheng. spikepoint efficient point-based spiking neural network event cam- eras action recognition. arxiv preprint hongwei ren, yue zhou, jiadong zhu, haotian fu, yu- long huang, xiaopeng lin, yuetong fang, fei ma, hao yu, bojun cheng. rethinking efficient effective point- based networks event camera classification regres- sion eventmamba. arxiv preprint hongwei ren, jiadong zhu, yue zhou, haotian fu, yulong huang, bojun cheng. simple effective point- based network event camera -dofs pose relocalization. arxiv preprint cian ryan, amr elrasad, waseem shariff, joe lemley, paul kielty, patrick hurney, peter corcoran. real-time multi- task facial analytics event cameras. ieee access, waseem shariff, mehdi sefidgar dilmaghani, paul kielty, mohamed moustafa, joe lemley, peter corcoran. event cameras automotive sensing review. ieee access, niklas stein, diederick niehorster, tamara watson, frank steinicke, katharina rifai, siegfried wahl, markus lappe. comparison eye tracking latencies among sev- eral commercial head-mounted displays. i-perception, kang wang qiang ji. real time eye gaze tracking deformable eye-face model. proceedings ieee international conference computer vision, pages zuowen wang, chang gao, zongwei wu, marcos conde, radu timofte, shih-chii liu, qinyu chen, zheng-jun zha, wei zhai, han han, al. event-based eye tracking. ais challenge survey. proceedings ieeecvf con- ference computer vision pattern recognition, pages zhong wang, zengyu wan, han han, bohao liao, yu- liang wu, wei zhai, yang cao, zheng-jun zha. mam- bapupil bidirectional selective recurrent model event- based eye tracking. proceedings ieeecvf con- ference computer vision pattern recognition, pages shih-en wei, varun ramakrishna, takeo kanade, yaser sheikh. convolutional pose machines. proceedings ieee conference computer vision pattern recogni- tion, pages ziyi yang, kehan liu, yiru duan, mingjia fan, qiyue zhang, zhou jin. three challenges reram-based process-in-memory neural network. ieee international conference artificial intelligence circuits systems aicas, pages ieee, raimondas zemblys, diederick niehorster, kenneth holmqvist. gazenet end-to-end eye-movement event detec- tion deep neural networks. behavior research methods, qitao zhao, zheng, mengyuan liu, pichao wang, chen chen. poseformerv exploring frequency domain efficient robust human pose estimation. proceed- ings ieeecvf conference computer vision pattern recognition, pages", "published_date": "2025-03-31T04:57:13+00:00"}
{"id": "2503.23509v2", "title": "ReferDINO-Plus: 2nd Solution for 4th PVUW MeViS Challenge at CVPR 2025", "authors": ["Tianming Liang", "Haichao Jiang", "Wei-Shi Zheng", "Jian-Fang Hu"], "summary": "referring video object segmentation rvos aims segment target objects throughout video based text description. task attracted increasing attention field computer vision due promising applications video editing human-agent interaction. recently, referdino demonstrated promising performance task adapting object-level vision-language knowledge pretrained foundational image models. report, enhance capabilities incorporating advantages sam mask quality object consistency. addition, effectively balance performance single-object multi-object scenarios, introduce conditional mask fusion strategy adaptively fuses masks referdino sam. solution, termed referdino-plus, achieves mathcaljmathcalf mevis test set, securing place mevis pvuw challenge cvpr code available", "full_text": "cs.cv may referdino-plus solution pvuw mevis challenge cvpr tianming liang haichao jiang wei-shi zheng jian-fang sun yat-sen university abstract referring video object segmentation rvos aims seg- ment target objects throughout video based text de- scription. task attracted increasing attention field computer vision due promising applications video editing human-agent interaction. recently, referdino showcases promising performance task adapting object-level vision-language knowledge pretrained foundational image models. re- port, extend capabilities incorporating sam enhance mask quality object consistency. ef- fectively balance performance single-object multi-object scenarios, introduce conditional mask fu- sion strategy adaptively combines masks refer- dino sam. solution, termed referdino-plus, achieves mevis test set, securing place mevis pvuw challenge cvpr code available laboratoryreferdino-plus. introduction referring video object segmentation rvos aims seg- ment target objects throughout video based text description. task bridges gap vision- language understanding pixel-level video analysis, of- fering significant value many down-stream applications, video editing human-agent interaction systems. previous rvos datasets like refer-youtube-vos ref-davis focused segmenting salient video objects described static attributes e.g., color shape simple spatial relationships, overlooking complex, dynamic properties real-world scenarios. encourage efforts towards challenging yet practical pixel-level video understanding, pvuw workshop cvpr presents challenging rvos benchmark mevis competition. different previous rvos datasets, mevis focuses understanding temporal mo- tion rvos task. mevis, videos often contain corresponding author. multiple objects similar static appearances differ- ent motion attributes, object descriptions mevis mainly focus motion temporal expressions. ad- dition, mevis includes numerous multi-object expressions, allowing referral unlimited number target objects video. features make mevis chal- lenging reflective real-world scenarios. overcome challenges, strong cross-modal spatiotemporal ca- pability necessary understand motion properties videos descriptions. early works rvos tend directly apply referring image segmentation methods rvos. however, manner ignores temporal information often result inconsistent object prediction. after- wards, mttr introduced detr paradigm rvos. building this, referformer proposed gen- erate queries directly text description. subse- quent works focused modular im- provements enhance cross-frame consistency tem- poral understanding. despite efforts, current rvos models still struggle insufficient vision- language understanding, often failing handle complicated object descriptions, especially involving composite appear- ance, location attributes. recently, referdino proposed address limitation leveraging pretrained vision-language knowledge founda- tional visual-grounding model groundingdino enable end-to-end adaptation rvos data, referdino incorpoarates cross-modal temporal enhancer well- designed mask decoder. combining components, referdino achieves state-of-the-art performance across various rvos benchmarks. solution, termed referdino-plus, two-stage strategy built upon referdino sam specifi- cally, first stage, employ referdino perform cross-modal object identification spatiotemporal dense reasoning. given video object description, refer- dino generates masks binary scores candidate target. however, due lack training large-scale segmentation data, mask quality may unsatisfactory. therefore, second stage, apply sam mask re- finement augmentation, regarding frame mask highest binary score prompts. two-stage process, obtain two series masksone referdino sam. intuitively, masks sam reliable stable. however, observe sam tends degenerate multi-object mask single-object mask, resulting performance degradation multi-object scenarios. address issue, design conditional mask fusion cmf strategy. single-object cases, output masks sam multi-object cases, combine masks referdino sam. however, remains challenging determine whether expression involves multiple objects. experiment, define multi-object case mask area sam less referdinos. solution straight-forward yet effective. without finetuning additional pseudo labels validationtest data solution achieves mevis val- idation set, mevis test set, securing final ranking mevis track cvpr pvuw challenge. related works referring video object segmentation rvos aims segment objects throughout video based text descriptions. works attempt directly apply referring image segmentation meth- ods rvos. however, manner unable cap- ture temporal information often result inconsistent object prediction. mttr firstly introduces detr paradigm rvos. furthermore, referformer proposes produce queries text description. top pipeline, follow-up works focus modular improvements improve cross-frame consistency temporal understanding. example, soc aggregates video content textual guidance semantic integration module unified temporal modeling cross-modal alignment. dshmp de- couples video-level referring expression understanding static motion perception, customized module enhance temporal comprehension. despite notable progress specific datasets, models limited insufficient vision-language understanding, often struggle un- seen objects scenarios. recently, referdino over- comes limitation leveraging pretrained vision- language knowledge groundingdino ex- tending capabilities dense perception spatio- temporal reasoning integrating effective mask de- coder temporal enhancer. semi-supervised video object segmentation conventional semi-supervised video object segmenta- tion aims propagate ground-truth object masks given frame throughout video. many existing ap- proaches employ memory mechanism store past features tracking segmenting future frames. early deep learning-based methods mainly em- ployed online adaptation strategies, models fine-tuned either initial frame frames spe- cialize target object. reduce computation over- heads, many works focus offline training conditioned solely first frame incorporating temporal depen- dencies preceding frames. development strong segment anything model sam many ef- forts attempt combine sam video trackers based masks perform segmentation throughout video. however, manner ineffective since combination end-to-end differentiable. address limitation, sam proposed, aiming accommodate spe- cific demand semi-supervised video object segmentation. sam showcases strong performance object tracking segmentation, attracted attention computer vision field. referdino-plus overall framework solution referdino-plus presented figure video-description pair, input referdino derive object masks corresponding scores across frames. then, select mask highest score prompt sam, produc- ing refined masks. finally, fuse two series masks conditional mask fusion strategy, generate final masks frame. cross-modal reasoning referdino referdino strong rvos model inher- object-level vision-language knowledge ground- ingdino endowed pixel-level dense prediction cross-modal spatiotemporal reasoning. formally, given video clip frames text descrip- tion, referdino performs cross-modal reasoning seg- mentation, deriving mask sequence cor- responding scores throughout video. follow- ing practice previous works combine multiple object masks scores higher preset threshold handle multi-object cases. mask refinement sam sam strong prompt-based segmentation model, efficiently produce high-quality object masks throughout video based given prompts, form clicks, bounding boxes masks. work, utilize sam refine mask quality object consis- tency referdino. specifically, obtaining masks corresponding scores across frames, select mask maximum score prompt. based referdino bison attacked lions. sam conditional mask fusion mask figure overview solution referdino-plus. video-description pair, input referdino derive object masks corresponding scores across frames. then, select mask highest score prompt sam, producing refined masks ms. finally, fuse two series masks conditional mask fusion strategy. best view color. prompt frame mask, sam produces refined mask sequence throughout video. conditional mask fusion although masks sam reliable stable, observe sams overall performance mevis significantly weaker referdino. experiments, identify main reason that, multi-object mask prompts, sam tends degenerate single-object masks, leading substantial target loss subsequent frames. address issue, design conditional mask fusion cmf principle single- object cases, output masks sam multi-object cases, combine masks refer- dino sam. however, remains challenging determine whether expression involves multiple objects. solution, define multi-object case mask area sam less referdinos. formally, process described follows ams amr otherwise indicates mask area. note cmf conducted individually frame, empirically achieves better performance. experiment dataset metrics mevis large rvos dataset comprising videos text descriptions. competition, provided test set includes videos language descrip- tions. descriptions may correspond single ob- ject, multiple objects, even non-objects within videos, making dataset significantly challenging. employ re- gion similarity average iou, contour accuracy mean boundary similarity, average evalu- ation metrics. implementation details pretrain referdino referring image seg- mentation datasets refcocog first, train combination refer-youtube-vos ref-davis finally, finetune training set mevis. referdino, use mm- groundingdino-swinb backbone. sam, use sam. hiera large backbone. set threshold hyper-parameters original referdino. unlike solutions pre- vious challenges, use additional pseudo labels validation test data finetuning. competition results shown table solution achieves mevis test set, securing final ranking mevis track cvpr pvuw challenge. turtle coming right swimming backwards. sea turtle swimming rightward direction. parrot motion, changing position. parrot eating without moving position. cows moving left. truck moving left right. cat laying looking around. cat moving around. cat lying right. figure visualization results solution mevis test set. team mvp-lab referdino-plus harbory pengsong ssams strong kimchi table leaderboard mevis test set. method referdino sam samcmfv samcmf table ablation stuides mevis validation set. ablation studies conduct abaltion studies mevis validation set explore effects individual components solu- tion. shown table refinement sam im- proves perform cmf entire video termed cmfv result improved performing cmf individual frames, result improved results demonstrate effectiveness components. visualization figure present several visualization results referdino-plus mevis test set. shows method effectively segment targets based cor- responding text descriptions throughout videos. results demonstrate accurate high-quality masks generated referdino-plus. line figure show case mult-object referring, demonstrates effectiveness conditional mask fusion strategy. conclusion work, propose referdino-plus address problem motion expression guided video object segmen- tation. two-stage strategy. first stage, employs referdino perform cross-modal object identi- fication spatiotemporal dense reasoning. second stage, integrates sam mask refinement object tracking. address multi-object collapse problem, design conditional mask fusion strategy post segmentation ensemble. without finetuning additional pseudo labels validationtest data, solu- tion achieves place pvuw mevis challenge cvpr references bellver, ventura, silberer, kazakos, torres, giro-i nieto. refvos closer look referring expressions video object segmentation adam botach, evgenii zheltonozhskii, chaim baskin. end-to-end referring video object segmentation multi- modal transformers. proceedings ieeecvf con- ference computer vision pattern recognition, pages nicolas carion, francisco massa, gabriel synnaeve, nicolas usunier, alexander kirillov, sergey zagoruyko. end-to- end object detection transformers. european confer- ence computer vision, pages springer, kei cheng alexander schwing. xmem long- term video object segmentation atkinson-shiffrin memory model. european conference computer vi- sion, pages springer, kei cheng, seoung wug oh, brian price, joon-young lee, alexander schwing. putting object back video object segmentation. proceedings ieeecvf conference computer vision pattern recognition, pages henghui ding, chang liu, suchen wang, xudong jiang. vision-language transformer query generation refer- ring segmentation. proceedings ieeecvf interna- tional conference computer vision, pages henghui ding, chang liu, shuting he, xudong jiang, chen change loy. mevis large-scale benchmark video segmentation motion expressions. proceedings ieeecvf international conference computer vi- sion, pages hao fang, feiyu pan, xiankai lu, wei zhang, runmin cong. uninext-cutie solution lsvos challenge rvos track. arxiv preprint kirill gavrilyuk, amir ghodrati, zhenyang li, cees snoek. actor action video segmentation sentence. proceedings ieee conference computer vision pattern recognition, pages mingfei han, yali wang, zhihui li, lina yao, xiaojun chang, qiao. html hybrid temporal-scale mul- timodal learning framework referring video object seg- mentation. proceedings ieeecvf international conference computer vision, pages shuting henghui ding. decoupling static hier- archical motion perception referring video segmentation. proceedings ieeecvf conference computer vision pattern recognition, pages sahar kazemzadeh, vicente ordonez, mark matten, tamara berg. referitgame referring objects pho- tographs natural scenes. proceedings con- ference empirical methods natural language processing emnlp, pages anna khoreva, anna rohrbach, bernt schiele. video object segmentation language referring expressions. computer visionaccv asian conference computer vision, perth, australia, december re- vised selected papers, part pages springer, alexander kirillov, eric mintun, nikhila ravi, hanzi mao, chloe rolland, laura gustafson, tete xiao, spencer white- head, alexander berg, wan-yen lo, al. segment any- thing. proceedings ieeecvf international con- ference computer vision, pages tianming liang, kun-yu lin, chaolei tan, jianguo zhang, wei-shi zheng, jian-fang hu. referdino referring video object segmentation visual grounding founda- tions. arxiv preprint shilong liu, zhaoyang zeng, tianhe ren, feng li, hao zhang, jie yang, chunyuan li, jianwei yang, hang su, jun zhu, al. grounding dino marrying dino grounded pre-training open-set object detection. european con- ference computer vision, zhuoyan luo, yicheng xiao, yong liu, shuyan li, yi- tong wang, yansong tang, xiu li, yujiu yang. soc semantic-assisted object cluster referring video object segmentation. proceedings international con- ference neural information processing systems, pages junhua mao, jonathan huang, alexander toshev, oana camburu, alan yuille, kevin murphy. generation comprehension unambiguous object descriptions. proceedings ieee conference computer vision pattern recognition, pages miao, mohammed bennamoun, yongsheng gao, ajmal mian. spectrum-guided multi-granularity referring video object segmentation. proceedings ieeecvf international conference computer vision, pages nikhila ravi, valentin gabeur, yuan-ting hu, ronghang hu, chaitanya ryali, tengyu ma, haitham khedr, roman radle, chloe rolland, laura gustafson, eric mintun, junt- ing pan, kalyan vasudev alwala, nicolas carion, chao- yuan wu, ross girshick, piotr dollar, christoph feicht- enhofer. sam segment anything images videos. arxiv preprint seonguk seo, joon-young lee, bohyung han. urvos unified referring video object segmentation network large-scale benchmark. computer visioneccv european conference, glasgow, uk, august proceedings, part pages springer, jiannan wu, jiang, peize sun, zehuan yuan, ping luo. language queries referring video object seg- mentation. proceedings ieeecvf conference computer vision pattern recognition, pages shilin yan, renrui zhang, ziyu guo, wenchao chen, wei zhang, hongyang li, qiao, hao dong, zhongjiang he, peng gao. referred multi-modality unified tem- poral transformer video object segmentation. proceed- ings aaai conference artificial intelligence, pages zhao yang, jiaqi wang, yansong tang, kai chen, heng- shuang zhao, philip torr. lavt language-aware vision transformer referring image segmentation. pro- ceedings ieeecvf conference computer vision pattern recognition, pages linfeng yuan, miaojing shi, zijie yue, qijun chen. losh long-short text joint prediction network referring video object segmentation. proceedings ieeecvf conference computer vision pattern recognition, pages xiangyu zhao, yicheng chen, shilin xu, xiangtai li, xin- jiang wang, yining li, haian huang. open com- prehensive pipeline unified object grounding detec- tion. arxiv preprint", "published_date": "2025-03-30T16:43:04+00:00"}
{"id": "2503.15697v1", "title": "Technical Report for the 5th CLVision Challenge at CVPR: Addressing the Class-Incremental with Repetition using Unlabeled Data -- 4th Place Solution", "authors": ["Panagiota Moraiti", "Efstathios Karypidis"], "summary": "paper outlines approach clvision challenge cvpr, addresses class-incremental repetition cir scenario. contrast traditional class incremental learning, novel setting introduces unique challenges research opportunities, particularly integration unlabeled data training process. cir scenario, encountered classes may reappear later learning experiences, experience may involve subset overall class distribution. additionally, unlabeled data provided training may include instances unseen classes, irrelevant classes ignored. approach focuses retaining previously learned knowledge utilizing knowledge distillation pseudo-labeling techniques. key characteristic method exploitation unlabeled data training, order maintain optimal performance instances previously encountered categories reduce detrimental effects catastrophic forgetting. method achieves average accuracy pre-selection phase final evaluation phase, outperforming baseline accuracy provide implementation code", "full_text": "technical report clvision challenge cvpr addressing class-incremental repetition using unlabeled data place solution panagiota moraiti efstathios karypidis, tech hive labs national technical university athens archimedesathena research center panagiotamoraitigmail.com, stathiskaripidisgmail.com abstract paper outlines approach clvision chal- lenge cvpr, addresses class-incremental repetition cir scenario. contrast traditional class incremental learning, novel setting introduces unique challenges research opportunities, particularly integration unlabeled data training process. cir scenario, encountered classes may reappear later learning experiences, experience may in- volve subset overall class distribution. ad- ditionally, unlabeled data provided training may include instances unseen classes, irrelevant classes ignored. approach focuses re- taining previously learned knowledge utilizing knowl- edge distillation pseudo-labeling techniques. key characteristic method exploitation unla- beled data training, order maintain optimal performance instances previously encountered cate- gories reduce detrimental effects catastrophic forgetting. method achieves average accuracy pre-selection phase dur- ing final evaluation phase, outperforming baseline accuracy provide implementation code challenge-. introduction clvision challenges designed promote re- search visual continual learning emerging field deep learning focuses enabling models learn continuously stream visual data. participants encouraged address critical issues catastrophic forgetting adapting models new tasks without requiring retraining scratch. conventional deep learning models tend suffer catastrophic forgetting problem training new classes causes lose ability recall earlier ones. continual learning, also known life- long learning incremental learning, involves training model incrementally stream experiences con- tinuous manner class incremental learning fo- cuses enabling pre-trained model learn new classes incrementally, means model extend knowledge integrating new categories time without forgetting previously learned ones. class incremen- tal learning repetition cir previously learned classes reappear later experiences, simulating realistic learning scenario. new experience may con- tain mix old entirely new, unseen categories, repre- senting subset overall class distribution. years challenge required participants lever- age unlabeled data training process. un- labeled data may include instances unseen irrelevant classes, ignored. address prob- lem, extend baseline strategy, already em- ploys well-known learning without forgetting lwf method unlabeled data streams. approach also utilizes lwf labeled data incorporates less forgetting learning lfl strategy la- beled unlabeled data streams, aiming retain previ- ously learned knowledge reduce catastrophic forget- ting. exploitation unlabeled data pseudo- labeling technique enables method maintain optimal performance previously encountered categories, model tends forget trained new experiences. approach achieves average accuracy pre-selection phase final evaluation phase, outperforming baseline accuracy result, method ranks final evalu- ation. methodology approach consists three main components two focus distilling knowledge models trained previous experiences new one, third leverages unla- cs.cv mar beled data training pseudo-labeling technique overview proposed method provided fig. continual learning framework, process la- beled samples unlabeled samples experience model parameters experi- ence denoted representing fea- ture extractor denoting classification head. labeled data stream, following baseline method, employ standard supervised learning minimizing cross-entropy loss models predictions ground truth labels. let ground truth label labeled sample xl. classification loss formulated lsup logpi softmaxgtftxli represents predicted probability class total number classes seen experience indicator function equals condition true otherwise. next outline principal modules proposed method. crucial component involves maintaining copy model, referred old model, exclusively trained previous experiences, current model trained new experiences data. end experience, parameters retained model updated align current model. maintaining networks capabilities learning without forgetting lwf approach focuses retaining knowledge previous tasks learning new one. unlike replay-based methods store examples earlier tasks, lwf trains net- work exclusively new data correspond old cate- gories, aiming preserving performance prior tasks. functions regularizer minimizing discrepancy models output old tasks learning new task. model every new experience includes extra nodes output layer accommodate new classes, fully connected previous layer weights randomly initialized. experiments shown outputs old model new data similar outputs new model, capabilities previous tasks maintained means model able integrate new knowledge without forgetting previ- ously learned classes. approach, extend baseline strategy, already employs lwf method unlabeled data streams, also incorporating labeled data lwf framework. since lwf utilizes knowledge distillation, require labels therefore available data utilized mitigate negative effects catastrophic forgetting. knowledge distillation term defined kullback-leibler divergence cur- rent old models logits lkdz, zold zold zold logit outputs current old model, softmax function temperature parameter. total lwf loss, denoted llwf combines con- tributions labeled unlabeled data streams llwf lkdzl, zold lkdzu, zold weights distillation losses applied labeled unlabeled data, respectively. notations gtftxl zold refer logit outputs current previous model labeled data, gtftxu zold denote correspond- ing outputs unlabeled data. preserving feature consistency less forgetting learning lfl strategy simi- lar lwf, instead preventing changes out- puts new model, focuses minimizing changes models intermediate representations learning process. inspired lfl, method leverages representa- tions labeled unlabeled data. new expe- rience, features extracted new old mod- els, discrepancies penalized. un- like approach proposed lfl, none networks weights frozen. observed freezing certain layers helps prevent catastrophic forgetting, also results significant degradation models performance new classes. implement strategy using mean squared error mse loss, applied labeled un- labeled data streams. lfl loss, denoted llfl for- mulated llfl msehl, hold msehu, hold weight mean squared error losses labeled unlabeled data streams. terms hold indicate feature representations labeled data current previous models, hold refer correspoding representations unla- baled data. prototype generation pseudo-labeling technique method involves generating prototypes labeled data stream beginning experience facilitate figure overview proposed method input image passed model frozen instance model previous experience old model. addition standard cross-entropy loss models predictions ground truth labels, two additional losses introduced mitigate catastrophic forgetting. lwf loss penalizes difference logit outputs current model old model, lfl loss encourages feature similarity intermediate features models. final component, cross-entropy loss computed using pseudo-labels generated unlabeled data, based saved class prototypes. representations updated beginning experience, new ones computed average old class prototypes recent experience. losses combined form total loss. overall method enables model effectively leverage labeled unlabeled data. pseudo-labeling unlabeled data. class pro- totype computed average feature vectors associated class hl,i number labeled samples class prototypes, along corresponding labels, stored buffer direct reference. beginning experience updated average old proto- types recent experience. unlabeled sample, pseudo-label as- signed, based highest cosine similarity features class prototype arg max hupc pseudo-label assigned similarity exceeds predefined threshold ensuring sample doesnt belong unseen distractor class present labeled data. pseudo-labels assigned, cross-entropy loss applied lpseudo logpi softmaxgtftxui predicted proba- bility pseudo-labeled class weight pseudo- labeled loss number unlabeled samples. approach enables leverage previously encoun- tered classes labeled data stream appear unlabeled data future experiences, thereby expand- ing dataset available training. total loss function combination classi- fication, distillation, less forgetting learning lfl, pseudo-labeling losses. expressed ltotal lsup llwf llfl lpseudo comprehensive loss function ensures model ef- fectively learns labeled unlabeled data, maintaining past knowledge expanding capability incorporate new classes. experiments experimental setup data. experiments conducted imagenet- like computer vision dataset comprising classes, struc- tured class-incremental repetition cir learn- ing framework. dataset presents sequential experi- ences, containing labeled images unla- beled images data streams, balanced classes within experience. total categories, designated learnable, serve distractors. challenge explores three scenarios increasing complex- ity scenario labeled unlabeled streams con- tain identical class distributions scenario un- labeled stream includes samples current future learnable classes scenario adds samples distractor categories unlabeled stream maintaining structure scenario evaluation performed balanced test set containing novel instances previously seen classes, excluding distractor classes. incremental setup allows com- prehensive assessment continual learning strategies un- der varying degrees unlabeled data complexity. implementation details. implementation built top challenge devkit based avalanche li- brary utilize resnet- backbone network, mandated competition guidelines, extends baseline learning without forgetting lwf ap- proach applying labeled unlabeled data streams. employ adam optimizer learning rate implement steplr scheduler step size gamma equals learning rate de- cay. training batch size set test batch size leverage unlabeled data effectively, maintain buffer size store class prototypes, feature dimension along corresponding labels. pseudo-labeling unlabeled samples, em- ploy confidence threshold samples threshold excluded prevent contamination unseen distractor classes. regarding loss term weights use pre- vent overfitting current classes, limit training maximum epochs per experience implement early stopping based validation performance. results phase scenario scenario scenario average accuracy baseline pre-selection final table performance across different scenarios phases. results tab. present results study, showcas- ing improvements achieved approach across three distinct scenarios two phases pre-selection final evaluation. results compared baseline method provided organizers challenge. pre-selection phase, observe significant improvement average accuracy, increases reaching final evaluation phase, approach shows improvement, average accuracy rising reaching result, method ranks final evaluation. notably, observe significant performance enhance- ments scenarios explained fact unlabeled data scenarios may include samples classes encountered before, helps model remember mitigate for- getting. using learning without forgetting lwf less forgetting learning lfl labeled un- labeled data streams, approach effectively minimizes changes models outputs intermediate feature rep- resentations, thereby reducing issue forgetting. more- over, assigning pseudo-labels unlabeled data based similarity class prototypes, leverage prior knowledge sustain high performance old tasks. results highlight effectiveness integrating tech- niques enhance learning process, demonstrating value utilizing unlabeled data reinforce previously ac- quired knowledge. conclusion paper, presented approach clvi- sion challenge cvpr, focused addressing class- incremental repetition cir scenario using unlabeled data. method successfully tackles catastrophic forget- ting combining three key components applying learn- ing without forgetting lwf labeled unlabeled data streams, implementing less forgetting learning lfl preserve feature consistency across model iterations, leveraging prototype-based pseudo-labeling technique effectively utilize unlabeled data. experimental results demonstrate effectiveness approach. method shows particular strength scenarios unlabeled data contains samples previous future learnable classes distractor classes, highlighting value pseudo-labeling strategy complex learning environments. work contributes field contin- ual learning showcasing unlabeled data ef- fectively leveraged maintain performance previously encountered categories accommodating new knowl- edge. future work could explore sophisticated prototype generation techniques, adaptive threshold mechanisms pseudo-labeling, integration memory-efficient re- play methods enhance performance challenging continual learning scenarios. references clvision cvpr workshop. comviewclvisionchallenge, benedikt bagus alexander gepperth. investiga- tion replay-based approaches continual learning. international joint conference neural networks ijcnn, pages ieee, antonio carta, lorenzo pellegrini, andrea cossu, hamed hemati, vincenzo lomonaco. avalanche pytorch li- brary deep continual learning. journal machine learn- ing research, kaiming he, xiangyu zhang, shaoqing ren, jian sun. deep residual learning image recognition. proceed- ings ieee conference computer vision pattern recognition, pages hamed hemati, andrea cossu, antonio carta, julio hurtado, lorenzo pellegrini, davide bacciu, vincenzo lomonaco, damian borth. class-incremental learning repetition, geoffrey hinton, oriol vinyals, jeff dean. distill- ing knowledge neural network. arxiv preprint heechul jung, jeongwoo ju, minju jung, junmo kim. less-forgetting learning deep neural networks, diederik kingma jimmy ba. adam method stochastic optimization. international conference learning representations, iclr san diego, ca, usa, may conference track proceedings, zhizhong derek hoiem. learning without forgetting. computer vision eccv pages cham, springer international publishing. zhizhong derek hoiem. learning without forgetting. ieee transactions pattern analysis machine intelli- gence, vincenzo lomonaco, lorenzo pellegrini, andrea cossu, an- tonio carta, gabriele graffieti, tyler hayes, matthias lange, marc masana, jary pomponi, gido van ven, mar- tin mundt, she, keiland cooper, jeremy forest, eden belouadah, simone calderara, german parisi, fabio cuz- zolin, andreas tolias, simone scardapane, luca antiga, subutai amhad, adrian popescu, christopher kanan, joost van weijer, tinne tuytelaars, davide bacciu, da- vide maltoni. avalanche end-to-end library continual learning. proceedings ieee conference computer vision pattern recognition, gido van ven, tinne tuytelaars, andreas to- lias. three types incremental learning. nature machine intelligence, liyuan wang, xingxing zhang, hang su, jun zhu. comprehensive survey continual learning theory, method application. ieee transactions pattern analysis machine intelligence, felix wiewel bin yang. localizing catastrophic forget- ting neural networks, xiangli yang, zixing song, irwin king, zenglin xu. survey deep semi-supervised learning. ieee transac- tions knowledge data engineering, da-wei zhou, qi-wei wang, zhi-hong qi, han-jia ye, de- chuan zhan, ziwei liu. class-incremental learning survey ieee transactions pattern analysis machine intelligence,", "published_date": "2025-03-19T21:11:57+00:00"}
{"id": "2503.12260v1", "title": "Enhancing Facial Expression Recognition through Dual-Direction Attention Mixed Feature Networks and CLIP: Application to 8th ABAW Challenge", "authors": ["Josep Cabacas-Maso", "Elena Ortega-Beltr\u00e1n", "Ismael Benito-Altamirano", "Carles Ventura"], "summary": "present contribution abaw challenge cvpr tackle valence-arousal estimation, emotion recognition, facial action unit detection three independent challenges. approach leverages well-known dual-direction attention mixed feature network ddamfn three tasks, achieving results surpass proposed baselines. additionally, explore use clip emotion recognition challenge additional experiment. provide insights architectural choices contribute strong performance methods.", "full_text": "enhancing facial expression recognition dual-direction attention mixed feature networks clip application abaw challenge josep cabacas-maso, elena ortega-beltran, ismael benito-altamirano,, carles ventura ehealth center, faculty computer science, multimedia telecommunication, universitat oberta catalunya, barcelona, spain mindinub, department electronic biomedical engineering, universitat barcelona, barcelona, spain jcabacas, eortegabeltran, ibenitoal, cventuraroyuoc.edu abstract present contribution abaw challenge cvpr tackle valence-arousal estimation, emotion recognition, facial action unit detection three independent challenges. approach leverages well-known dual-direction attention mixed feature net- work ddamfn three tasks, achieving results surpass proposed baselines. additionally, explore use clip emotion recognition challenge additional experiment. provide insights archi- tectural choices contribute strong performance methods. introduction facial emotion recognition emerged pivotal area research within affective computing, driven poten- tial applications fields ranging human-computer in- teraction psychological research clinical diagnostics. since ekmans classification human expression faces emotions many studies emerged recent years. calvo al. baltrusaities al. kaya al. laid foundational framework understanding facial expressions window emotional states. contem- porary research, liu al. kim al. continued refine expand methodologies, synthesizing insights cognitive psychology, computer vision, machine learning, researchers made significant strides enhancing accuracy applicability facial emo- tion recognition systems. addition, integration dimensions valence arousal added depth interpretation emotional states, allowing nu- anced insight human affective experiences. action unit detection complemented efforts parsing facial expressions discrete muscle move- ments, facilitating finer-grained analysis emotional expressions across cultures contexts. advance- ments improved reliability automated emo- tion recognition systems, also opened possibility personalize affective computing applications fields mental health monitoring user experience de- sign tackle challenges, researchers explored innovative architectures ddamfn dual- direction attention mixed feature network novel approach integrates attention mechanisms mixed feature extraction enhancing networks abil- ity capture intricate details within facial expressions. increasing need develop machines capa- ble understanding appropriately responding hu- man emotions real-world, day-to-day applications. ad- dressing challenge, series competitions titled af- fective behavior analysis in-the-wild abaw organized abaw challenge cvpr six competitions in- troduced valence-arousal estimation, expression expr recognition, action unit detection, com- pound expression recognition, emotional mimicry intensity emi estimation, ambivalencehesitancy recognition. work, present approach estimation, expr recognition, detec- tion challenges, adapted ddamfn architec- ture challenge additionally leveraged clip embedding space enhance emotion recognition. cs.cv mar methodology dataset curation abaw challenge, organization provided following datasets estimation augmented version aff- wild database used. audiovisual av, in-the-wild dataset consists videos, totaling ap- proximately million frames, annotations va- lence arousal. expression recognition aff-wild database used, comprising videos around mil- lion frames. dataset annotated six basic expressions anger, disgust, fear, happiness, sadness, surprise, along neutral state cate- gory, represents affective states beyond six ba- sic emotions. action unit detection dataset includes videos, also totaling approximately million frames. annotated action units au, au, au, au, au, au, au, au, au, au, au, au. outlined dataset guidelines, preprocessed data filtering frames contained annotation values outside specified acceptable ranges. specifically, frames valencearousal values expression values action unit values excluded consideration analysis. process ensured frames valid annotations retained model training evaluation. filtering criteria number frames removed summarized table table annotation ranges invalid values. annotation range invalid valencearousal expressions action units aus applied rigorous filtering train-validation splits across three tasks estimation, expression recognition, detection. summary dataset filtering shown table network architecture ... dual-direction attention mixed feature network abaw challenge, adapted dual-direction attention mixed feature network ddamfn three separate tasks. task fully-connected layer end network one valence-arousal prediction output units, another emotion recognition output units, third action unit prediction output units. table dataset summary filtering estimation frames curated frames training validation test expr recognition training validation test detection training validation test figure shows diagram network, features base mobilefacenet mfn architecture feature extraction, followed dual-direction attention dda modulewith two attention headsand global depth- wise convolution gdconv layer. output gd- conv layer reshaped fed corresponding fully- connected layers individual task. ... contrastive language-image pretraining addition leveraging dual-direction attention mixed feature network ddamfn abaw challenge, also employed clip contrastive language-image pretraining model. clip widely recognized vision-language model trained millions image-text pairs, text consists image captions, using contrastive learning approach. essence, clip learns two types embeddings one images one text. model trained map matching image-text pairs similar points embedding space, based co- sine similarity, simultaneously pushing apart em- beddings mismatched image-text pairs. followed approach built clip embedding space adding two fully connected lay- ers top image encoder allow deformations embedding space. used contrastive loss cosine similarity collection text prompts. training, clip image text encoders frozen, preserving learned features adapting embed- ding space sentiment analysis tasks. training ddamfn initially, ddamfn pretrained model obtained stock versions available source code repository original work, trained affectnet- mfn features dda dda max gdconv reshape lstm figure ddamfn architecture abaw challenge mobilefacenet mfn feature extraction grey, dual- direction attention dda module green, global depthwise convolution gdconv layer red, one fully-connected layer yellow specific task addressed valence-arousal prediction, emotion recognition, action unit detection. figure clip architecture overview diagram, architecture divided several key components. visual path, high- lighted orange, processes image input. text path, shown green, processes textual input. white areas represent fully connected layers bridge paths. finally, similarity outputs, purple, demonstrate model calculates relationship image text inputs. dataset experiments, preserved pre- trained weights feature extraction layers, attention mechanisms, gdconv layer, components already optimized extracting meaningful fea- tures input data. however, fully-connected lay- ers reinitialized random weights facilitate adaptation model specific task. enhance models performance across various chal- lenges, introduced custom classifier individual task. classifiers designed address unique requirements challenge, allowing special- ized learning. classifier trained independently separately, goal adapting pretrained feature extraction attention components specific charac- teristics new tasks. strategy ensured model could benefit strong generalization capabil- ities ddamfn architecture still tailoring output specific demands challenge. training custom classifier separately, able achieve fine-tuned balance leveraging pre- trained knowledge optimizing model indi- vidual task, leading improved overall performance experiments. loss functions calculated following criteria valence-arousal prediction, loss function cal- culated using concordance correlation coefficient ccc. ccc measure evaluates agreement two time series assessing precision well observations agree accuracy well observations match true values. emotion recognition, cross-entropy, commonly employed classification tasks measure difference predicted probability distribu- tion true distribution. action unit detection, binary cross-entropy loss used, suitable binary classifica- tion tasks, measuring difference predicted probability actual binary outcome action unit. furthermore, action unit task, global thresh- old initially tested across aus, followed individual optimization however, initial round experiments, ob- served model could benefit improve- ments capturing temporal coherence con- secutive frames. address this, decided replace fully connected layers long short-term mem- ory lstm network. lstm specifically chosen ability retain model long-term dependencies sequential data, enabling model maintain temporal coherence across frames. clip contrastive following work done used architec- ture loss. clip contrastive model fig. introduced two fully connected layers top image encoder allow adjustments embedding space. used contrastive loss cosine similarity set text prompts. training, clip image text encoders kept frozen. unique prompt generated emotion based structure face showing emotion. clip contrastive architecture, added fully connected layer units relu activation top clip image encoder, followed another fully con- nected layer units linear activation function loss function. linear activation function selected retain negative activations, original clip- trained embedding space contains negative posi- tive values. train model, employed con- trastive loss function used original clip approach specifically, given batch image-text pairs ii, ti, losses image text computed follows limg log expiei, tei expiei, tej ltext log exptei, iei exptei, iej iei, tejis cosine similarity em- bedding vectors i-th image sample iei j-th text sample tej, iei, teiis cosine similarity be- tween i-th image sample corresponding text cap- tion. contrastive loss lco computed lco limg ltext loss drives model enhance similarity be- tween embedding vector sample associ- ated text description reducing similarity embedding vectors sample samples batch. previously experiments using ddamfn, decided replace fully connected layers lstm maintain temporal coherence. results metrics evaluated challenge concor- dance correlation coefficient ccc valence, arousal, combination valence-arousal, score emo- tion classification score action unit detec- tion cccv ccca fexpr fau table presents performance metrics chal- lenge across different architectures tested. metrics include combined cccv valence-arousal, fexpr expression classification, fau action unit classifica- tion, fauopt optimized action unit classification af- ter threshold optimization. baseline model shows lowest performance across tasks, cccv fexpr fau fauopt value reported base- line. ddamfnfc model improves upon baseline, achieving cccv however, fexpr fau values respectively remain lower compared architectures. fauopt score threshold optimization ddamfnlstm model achieves highest per- formance overall, highest cccv fau fauopt results indi- cate architecture performs best terms emotional valence-arousal prediction action unit clas- sification. clipfc cliplstm models provided promis- ing results expression classification. specifically, cliplstm achieved fexpr highest among rest architectures, demonstrat- ing strong performance task. table performance metrics challenge. table shows fexpr fau already normalized number classes classification task. cccav combined ccc valence arousal. fauopt result optimizing thresholds architecture cccv fexpr fau fauopt baseline ddamfnfc ddamfnlstm clipfc cliplstm conclusion performance models attributed several key architectural choices design considerations. ddamfnfc model demonstrated significant improve- ment baseline emotional valence-arousal pre- diction, achieving cccv however, per- formance expression classification fexpr action unit classification fau remained rela- tively lower compared models, indicating model struggles fine-grained expression action unit recognition. may due limitations fully connected layers capturing complex spatial temporal features required tasks. architecture appears perform better simpler, less dynamic tasks like valence-arousal prediction, temporal spatial dependencies less crucial. contrast, ddamfnlstm model performed best overall, achieving highest cccv fau fauopt integration lstm layers model allowed capture temporal de- pendencies inherent facial expressions emotional states, providing distinct advantage action unit classification valence-arousal prediction. lstms ability maintain memory previous frames crucial tasks involving dynamic data, emotion recogni- tion, progression emotional states time plays critical role. temporal awareness, com- bined ddamfns feature extraction capabilities, en- abled ddamfnlstm model excel pre- diction emotional valence arousal well clas- sification facial action units, subtle changes fa- cial expressions key. clipfc cliplstm models showed promis- ing results, particularly domain expression classifi- cation. cliplstm model, fexpr outperformed models task, demonstrating strong ability capture nuances facial expres- sions. success attributed clip models capability process visual contextual informa- tion, combined lstms ability capture temporal dependencies. synergy enabled model recognize facial expressions evolve time, making espe- cially well-suited task expression classification, understanding transitions facial states cru- cial. moreover, clip models, known robust per- formance image-based tasks, likely enhanced models ability interpret diverse facial expressions complex visual features, leading notable improvement expres- sion classification accuracy. summary, superior performance ddamfnlstm cliplstm models largely attributed ability handle temporal spatial features effectively. models performed best tasks required understanding dynamics emotional valence-arousal subtlety facial expressions. lstms strength learning temporal patterns, combined advanced feature extraction meth- ods like employed ddamfn clip, provided models significant advantage. highlights importance incorporating temporal modeling dynamic feature extraction architectures designed complex tasks emotion recognition facial action unit classification. references tadas baltrusaitis, amir zadeh, yagmur lim, louis- philippe morency. openface facial behavior analysis toolkit. ieee winter conference applications com- puter vision wacv, cristina bustos, carles civit, brian du, albert sole-ribalta, agata lapedriza. use vision-language mod- els visual sentiment analysis study clip. international conference affective computing intelligent interaction acii, pages manuel calvo lauri nummenmaa. faces feelings universal recognition emotions. annu. rev. psychol., sheng chen, yang liu, xiang gao, zhen han. mobile- facenets efficient cnns accurate real-time face verifica- tion mobile devices. biometric recognition chi- nese conference, ccbr urumqi, china, august proceedings pages springer, paul ekman. darwin facial expression century re- search review. general psychology, paul ekman wallace friesen. facial action coding sys- tem technique measurement facial movement. consulting psychologists press, qibin hou, daquan zhou, jiashi feng. coordinate at- tention efficient mobile network design. proceedings ieeecvf conference computer vision pattern recognition, pages huseyin kaya emre gurpinar. exploring deep convo- lutional neural networks facial action unit recognition. neurocomputing, jihye kim, seung-chan lee, jong-soo kim, chee sun yoo. survey facial emotion recognition approaches, databases, challenges. pattern recognition letters, dimitrios kollias. abaw valence-arousal estimation, ex- pression recognition, action unit detection multi-task learning challenges. proceedings ieeecvf con- ference computer vision pattern recognition, pages dimitrios kollias. abaw learning synthetic data multi-task learning challenges. european conference computer vision, pages springer, dimitrios kollias. multi-label compound expression recog- nition c-expr database network. proceedings ieeecvf conference computer vision pattern recognition, pages dimitrios kollias stefanos zafeiriou. expression, affect, action unit recognition aff-wild, multi-task learning arcface. arxiv preprint dimitrios kollias stefanos zafeiriou. affect analysis in-the-wild valence-arousal, expressions, action units unified framework. arxiv preprint dimitrios kollias stefanos zafeiriou. analysing affec- tive behavior second abaw competition. proceed- ings ieeecvf international conference com- puter vision, pages kollias, schulc, hajiyev, zafeiriou. analysing affective behavior first abaw competition. ieee international conference automatic face gesture recognition fg, pages dimitrios kollias, panagiotis tzirakis, alan cowen, irene kotsia, cogitat, eric granger, marco pedersoli, simon bacon, alice baird, chunchang shao, al. advancements affective behavior analysis abaw workshop competition. dimitrios kollias, panagiotis tzirakis, alan cowen, ste- fanos zafeiriou, irene kotsia, eric granger, marco peder- soli, simon bacon, alice baird, chris gagne, chun- chang shao, guanyu hu, soufiane belharbi, muham- mad haseeb aslam. advancements affective behav- ior analysis abaw workshop competition. dimitrios kollias, viktoriia sharmanska, stefanos zafeiriou. face behavior carte expressions, af- fect action units single network. arxiv preprint dimitrios kollias, panagiotis tzirakis, mihalis nicolaou, athanasios papaioannou, guoying zhao, bjorn schuller, irene kotsia, stefanos zafeiriou. deep affect prediction in-the-wild aff-wild database challenge, deep architec- tures, beyond. international journal computer vision, pages dimitrios kollias, viktoriia sharmanska, stefanos zafeiriou. distribution matching heterogeneous multi- task learning large-scale face study. arxiv preprint dimitrios kollias, panagiotis tzirakis, alice baird, alan cowen, stefanos zafeiriou. abaw valence-arousal esti- mation, expression recognition, action unit detection emo- tional reaction intensity estimation challenges. proceed- ings ieeecvf conference computer vision pattern recognition, pages dimitrios kollias, viktoriia sharmanska, stefanos zafeiriou. distribution matching multi-task learning classification tasks large-scale study faces beyond. proceedings aaai conference artificial intelli- gence, pages dimitrios kollias, panagiotis tzirakis, alan cowen, ste- fanos zafeiriou, irene kotsia, alice baird, chris gagne, chunchang shao, guanyu hu. affective behav- ior analysis in-the-wild abaw competition. proceedings ieeecvf conference computer vision pat- tern recognition, pages dimitrios kollias, stefanos zafeiriou, irene kotsia, abhinav dhall, shreya ghosh, chunchang shao, guanyu hu. abaw competition multi-task learning compound expression recognition. arxiv preprint xiaoying liu, xianhua song, zheng-hua tan. deep learning emotion recognition comprehensive review. neurocomputing, patrick lucey, jeffrey cohn, takeo kanade, jason saragih, zara ambadar, iain matthews. extended cohn- kanade dataset complete dataset action unit emotion-specified expression. ieee conference com- puter vision pattern recognition workshops cvprw, rosalind picard, elias vyzas, jennifer healey. to- ward machine emotional intelligence analysis affective physiological states. ieee transactions pattern analysis machine intelligence, radford, kim, hallacy, ramesh, goh, agarwal, sastry, askell, mishkin, clark, al. learning transferable visual models natural language supervision. proceedings international con- ference machine learning, pages pmlr, andrey savchenko. frame-level prediction facial ex- pressions, valence, arousal action units mobile de- vices, mohammad soleymani, david garcia, brendan jou, bjorn schuller. deep learning affective computing text- based emotion recognition decision support. ieee trans- actions affective computing, stefanos zafeiriou, dimitrios kollias, mihalis nicolaou, athanasios papaioannou, guoying zhao, irene kot- sia. aff-wild valence arousal in-the-wildchallenge. computer vision pattern recognition workshops cvprw, ieee conference on, pages ieee, zhihong zeng, maja pantic, glenn roisman, thomas huang. survey affect recognition methods audio, visual, spontaneous expressions. ieee transactions pattern analysis machine intelligence, saining zhang, yuhang zhang, zhang, yufei wang, zhigang song. dual-direction attention mixed feature net- work facial expression recognition. electronics,", "published_date": "2025-03-15T21:03:03+00:00"}
{"id": "2503.11935v3", "title": "Design of an Expression Recognition Solution Based on the Global Channel-Spatial Attention Mechanism and Proportional Criterion Fusion", "authors": ["Jun Yu", "Yang Zheng", "Lei Wang", "Yongqi Wang", "Shengfan Xu"], "summary": "facial expression recognition challenging classification task holds broad application prospects field human-computer interaction. paper aims introduce method adopt affective behavioral analysis wild abaw competition, held conference computer vision pattern recognition cvpr .first all, apply frequency masking technique method extracting data equal time intervals conduct targeted processing original videos. then, based residual hybrid convolutional neural network multi-branch convolutional neural network respectively, design feature extraction models image audio sequences. particular, propose global channel-spatial attention mechanism enhance features initially extracted audio image modalities respectively.finally, adopt decision fusion strategy based proportional criterion fuse classification results two single modalities, obtain emotion probability vector, output final emotional classification. also design coarse fine granularity loss function optimize performance entire network, effectively improves accuracy facial expression recognition.in facial expression recognition task abaw competition, method ranked third official validation set. result fully confirms effectiveness competitiveness method proposed.", "full_text": "design expression recognition solution based global channel-spatial attention mechanism proportional criterion fusion jun yu, yang zheng, lei wang, yongqi wang, shengfan university science technology china macau university science technology harryjun,wanglustc.edu.cn zhengyang,wangyongqimail.ustc.edu.cn eyki.com abstract facial expression recognition challenging classifica- tion task holds broad application prospects field human-computer interaction. paper aims intro- duce method adopt affective be- havioral analysis wild abaw competition, held conference computer vision pattern recognition cvpr first all, apply frequency masking technique method extract- ing data equal time intervals conduct targeted pro- cessing original videos. then, based residual hybrid convolutional neural network multi-branch convolutional neural network respectively, design fea- ture extraction models image audio sequences. particular, propose global channel-spatial attention mechanism enhance features initially extracted audio image modalities respectively. finally, adopt decision fusion strategy based propor- tional criterion fuse classification results two single modalities, obtain emotion probability vector, output final emotional classification. also design coarse fine granularity loss function optimize per- formance entire network, effectively improves accuracy facial expression recognition. facial expression recognition task abaw competition, method ranked third official validation set. result fully confirms effectiveness competitiveness method proposed. introduction problem facial expression recognition involves mul- tiple research fields artificial intelligence, bio- corresponding author science, psychology, popular research branch within field artificial intelligence. traditional facial expression recognition mainly focuses research single modality facial images. however, emotional information covered limited, different modal- ities data also different sensitivities emotional states. therefore, integrating multiple modalities be- come research trend. mainly three strategies integration multiple modalities feature-level fusion, decision-level fusion, model fusion. among them, decision-level fusion method require strict tempo- ral synchronization modalities, also solves problem different feature reliabilities different modalities. therefore, popular choice modern emo- tion recognition architectures. researchers conduct modeling analysis human emotions audio act data audio, text, facial images, well physiological signal data electroencephalogram eeg body temperature. among them, two modalities audio facial im- ages account conveyed emotional factors. two usually complementary pro- vide relatively comprehensive information emotion recognition task. recent years, deep learning related algorithms convolutional neural network cnn, residual network resnet, long short term memory lstm, attention mechanisms widely applied research audio signal feature extraction audio emotion classification. example, kumaran al. proposed deep convolutional recurrent neural network model chen al. proposed bidirectional long short term memory bilstm network based atten- tion mechanism. however, currently, dual modality fusion models still problems complex network struc- tures, large number model parameters, difficulty training. cs.cv mar order promote interdisciplinary cooperation ad- dress key research issues spanning affective computing, ma- chine learning, multimodal signal processing, kollias al. took lead launching affective behavior anal- ysis wild abaw initiative. abaw workshop competition scheduled held con- junction ieee cvpr conference aiming emotion recognition problems, paper con- ducts research based audio static facial images, main contributions follows .we propose channel spatial attention mechanism suitable audio visual emotion recognition. com- bining channel attention, channel shuffling, spatial attention mechanisms, mechanism aims capture global dependencies feature maps enhance feature representation ability extracted original audio images. .we use proportional decision fusion strategy ob- tain final emotion classification results. coarse fine granularity loss function designed optimize perfor- mance entire network. .in abaw competition, algorithm achieved excellent results official validation set. result fully confirms effectiveness competitiveness proposed method. related work image-based facial expression recognition various visual descriptors extracted facial mor- phology detect expressions video streams. example, nguyen al. extracted facial landmark points constructed geometric descriptors train support vector machine svm classifier differentiating different emotional categories. frame- work proposed reference based facial action units aus, continuously detects emotional states aus. development deep learning technology, visual based emotion recognition systems using convolutional neural network cnn architectures, taking video frames sequences input, higher recognition rates compared traditional frame aggregation based methods. two dimensional convolutional neural networks like emotional deep attention network emotional- dan aim solve problems emotion, valence, landmark recognition one step. spatial trans- former network stn detect main regions interest video frames correct spatial variations, similar framework proposed reference used cap- ture facial landmark points facial visual saliency maps. facial expression recognition fer video streams regards series frames within temporal analysis win- dow single input. example, network used reference, utilizes convolutional kernels shared weights along time axis, widely applied dynamic facial expression recognition.there also methods design facial expression recognition models trained single images. reference proposed supervised self supervised learning methods im- prove classification accuracy fine grained wild scenarios. visual transformer feature fusion vtff reference recognize emotions un- der extreme conditions. transformer based facial ex- pression recognition transfer method reference extracts rich relation aware representations visual descriptors. reference proposed graph convolutional network gcn framework exploit dependencies be- tween two types emotion recognition tasks. abbasi al. constructed graph based representation chil- drens facial expression recognition. reference intro- duced video based facial expression recognition method improve descriptiveness embedding features us- ing emotion wheel information. overall, analysis advanced methods shows important distin- guish peak non peak video frames video sequences. although deep recurrent neural networksd convolutional neural networks encode temporal de- pendencies consecutive frames, performance satisfactory difficult train. audio integrated emotion recognition number studies applied multimodal audio-visual analysis emotion prediction. example, nguyen al. integrated d-cnn deep belief networks, fused visual audio feature vectors using bilinear pooling theory recognize emotions. reference, dif- ferent acoustic visual features input cnn extended rnn, average fusion carried decision-making level. kahou al. integrated multiple deep neural networks different data modalities predict emotions.vaanet used specific cnn architecture combined late fusion strategy, ignored interaction features. aver, wang al., reference, reference, etc. pro- cessed multimodal information different ways respec- tively. recently, reference proposed model fuses audio-visual features model level. al., memobert, reference, multiple transformer- based frameworks also made explo- rations. experiments shown performance multimodal concatenation better. overall, performance multimodal emotion recog- nition superior unimodal recognition. model-level fusion good effect. impor- tance information video sequences varies. meth- ods adopt two-stage shallow processing pipeline make use complementarity modalities. moreover, existing methods consider correlation emotion categories, polarity defined mikels emotion wheel. propose cross-modal audio-visual fusion framework, employs global channel-spatial attention mechanism decision fu- sion strategy based proportional criterion. except initial pre-training, auxiliary data required. operations splitting video stream extracting audio images, multiple attention mechanisms utilized process features predict emotions. mean- while, coarse fine granularity loss function designed optimize performance entire network. next section provide detailed introduction method modules. method overview overall framework audio-visual emotion recog- nition training network paper shown figure entire network implemented end-to-end man- ner. according processing flow task, divided three stages data preprocessing, feature ex- traction feature enhancement, modality fusion classification. first data preprocessing stage. face image sequences audio files extracted orig- inal audio-visual data. frequency masking technique applied conduct targeted processing original audio data. time, image data content se- lected equal time intervals ensure representative- ness regularity samples. then, based residual hybrid convolutional neural network multi- branch convolutional neural network methods respectively, feature extraction models face image sequences au- dio sequences designed. subsequently, obtained se- quence features enhanced using global channel- spatial attention module. finally, proportional decision fusion strategy adopted fuse classification results two single modalities, obtaining emotion proba- bility vector, final emotion classification output based emotion probability vector. order opti- mize model efficiently, coarse fine granularity loss function designed, coarse granularity penalty coefficient determined according emotion probabil- ity vector. feature extraction paper employs multi-branch convolutional neural network mcnn residual-based hybrid convolu- tional neural network rhcnn respectively ex- traction emotional features audio facial images. shown following formula, audio features extracted mcnn denoted fa, refers input audio sequence. changes emotional features. mcnna mcnn structure composed branches extract- ing features local, temporal, spatial dimensions. different branch consists two convolutional blocks different receptive field sizes. way, receptive fields various sizes utilized fully obtain im- portant emotional context information feature maps. enriches input features also enhances robustness model similarly, visual features extracted rhcnn denoted fv, refers input image sequence. rhcnnv rhcnn structure implemented based concept residual connections depthwise separable convolu- tions. firstly, image data input two convolutional blocks. convolutional block composed convo- lutional layer receptive field size nn, batch normalization layer bn, relu activation function layer, used extract primary features in- crease nonlinearity features. residual con- nection block composed convolution, depthwise separable convolution, convolution. convolution block used change number channels increase nonlinear representation network, making easier residual connections. re- ceptive field size capture spatial informa- tion, channel mixing operation used subsequently enhance information exchange among channels feature maps. global channel-spatial attention completing preliminary feature extraction, or- der enhance feature representation ability, con- duct enhancement processing extracted audio vi- sual features. designed global channel-spatial attention gcsa module enhance representational ability input feature maps. module combines channel attention, channel shuffling, spatial attention mechanisms, aiming capture global dependencies feature maps. ... channel attention sub-module traditional convolutional neural networks, relation- ships channels may overlooked. however, relationships crucial capturing global information. dependencies channels taken ac- count, model may fail fully utilize informa- tion feature maps, resulting insufficient capture figure proposed framework expression recognition. global features.to enhance dependencies chan- nels, adopt multi-layer perceptron mlp chan- nel attention sub-module. implementing channel attention mlp reduce redundant information be- tween channels highlight important features. first, permute dimensions input feature map channel dimension last one. then, process two-layer mlp. first layer, number channels reduced original. relu activa- tion function used introduce non linearity. sec- ond layer, number channels restored original dimension. way, global dependencies channels better captured. finally, perform inverse permutation restore original dimensions generate channel attention map sigmoid activa- tion function. input feature map multiplied element element channel attention map obtain enhanced feature map. fchannel mlppermutefinput finput ... channel shuffle even enhancing channel attention, may still problem information channels fully mixed. information channels ad- equately mixed, feature representation ability may limited, effect channel attention cannot fully realized. mix share information, channel shuffle operation applied. enhanced feature map divided groups, group containing channels. transpose operation performed grouped feature map disrupt channel order within group. subse- quently, shuffled feature map restored original shape chw. way, feature information better mixed, feature representation ability enhanced. fshuffle channelshufflefchannel ... spatial attention sub module relying solely channel attention channel shuffle op- erations may fully utilize spatial information. however, spatial information equally important capturing local global features image. information spatial dimension considered, model may over- look important details feature maps. spatial attention sub module, input feature map passes convolutional layer, num- ber channels reduced original. then, batch normalization relu activation function applied non linear transformation. next, second convo- lutional layer restores number channels origi- nal dimension followed another batch normalization layer. finally, sigmoid activation function used gen- erate spatial attention map. shuffled feature map multiplied element element spatial attention map obtain final output feature map. fspatial convbnreluconvfshuffle fshuffle decision fusion based proportion cri- terion mainly two common multimodal fusion methods feature fusion decision fusion. former fuses features modality makes emotion judg- ment fused features. latter, however, fuses emotion state judgments modality outputs fi- nal decision. compared feature fusion, decision fusion doesnt need consider semantic differences modalities, simpler effective. pa- per, decision fusion strategy based proportion cri- terion adopted fuse audio image modalities. obtain emotion state judgments audio image, audio features video features respectively input fully connected layers. then, softmax function used convert probability values, thus com- pleting mapping features emotion probability vectors.the emotion probability vectors audio image denoted respectively. according proportion criterion, emotion probability vectors two modalities fused output probabilities discrete emotion. formula proportion criterion follows mpa npv among them, emotion probability vector au- dio video modality. proportion parameters two modalities, coarse fine granularity loss discrete emotion classification, cross-entropy loss function mostly used train model. loss func- tion calculates relative entropy predicted value true value based probability vector. predicted value true value measured terms discrete emotion category regarded calculation fine-grained classifica- tion loss contains single emotion category. denote pi, pi, pim probability vector, pie predicted probability data sample belongs true category cross-entropy loss function defined follows yie logpie among them, represents number data samples, represents number emotion categories. yie indicator function. predicted category data sample true category yie otherwise, yie discrete emotion categories coarsely grained, qualitative analysis emotion categories achieved, fine-grained categories quantita- tive division emotion categories. theoretically, qualitative error event lead chain reaction errors. therefore, loss caused misjudgment coarse-grained emotion categories ampli- fied. response this, paper adds coarse grained penalty coefficient basis cross entropy loss maintaining fine grained classification loss, designs coarse fine granularity loss function, shown follows lcf yie logpie among them, coarse grained penalty coefficient. based four coarse grained emotion cat- egories divided four quadrants, situations coarse grained misjudgment. simplify calculation, two events misjudging misjudging merged confusion event en, is, nm. therefore, paper defines six coarse grained penalty coefficients, denoted values deter- mined specific error events. experiment section, provide detailed description used datasets, experiment setup, experimental results. datasets workshop competition affective be- havioral analysis wild announced aff-wild database, key collection extracted series studies. resource serves core expr classification challenge, audio-visual dataset consists videos approxi- mately million frames, annotated six basic facial expressions, well neutral cate- gories. enrich depth dataset, integrated data afew databases. afew dataset corpus dynamic temporal facial expres- sion data near real-world environment extracted movies. contains video clips avi format, divided seven discrete emotions. dataset pro- vides data subjects, filmed labo- ratory environment. includes videos six universal expressions neutral expression, expression labels categorical discrete values. integration aims enhance comprehensiveness practicality analysis. using generative adversarial network gan technology, new expression video samples gen- erated based data features afew ck. samples conform overall data distribution characteris- tics aff-wild, thereby increasing diversity quan- tity data enhancing comprehensiveness dataset. setup experiments paper developed based python pytorch deep learning frame- work. gpu rtx model video memory used train test model. batch size set model iterated rounds. stochastic gradient descent sgd optimizer selected, learning rate learning rate weight parameters audio facial images set dropout fully connected layer set momentum set coarse fine granularity loss chosen loss function. smaller loss, higher prediction accuracy model. table ablation study results validation set method score modality official base base gcsa base gcsa base base gcsa base gcsa metrics line competition requirements, employ average score evaluation metric, robust class frequency variations particularly suitable imbalanced class distributions. calculation average score follows precision recall precision recall represents number classes means c-th class. results according official competition results, method submitted ranked third. verify effectiveness method, conducted ablation studies component strategy within method. studies car- ried respectively single-modal data dual-modal data, results shown table .it evident application technology significantly improved recognition performance, recognition accuracy even higher dual-modal scenario, validates feature extraction enhancement unique fusion strategy improve accuracy. addition, in- corporating global channel-spatial attention mechanism increase accuracy highlighting impor- tance feature enhancement. ultimately, post- processing, accuracy model reached conclusion paper, propose method fusing audio fea- ture expressions improve ability facial expres- sion recognition. first stage data preprocessing stage. video processing pipeline, preprocess- ing stage plays crucial foundational role. stage, frequency masking technique first applied conduct targeted processing original audio data, thus filter- ing optimizing information. subsequently, im- age data content selected equal time intervals en- sure representativeness regularity samples. second stage feature extraction enhancement stage. address issue insufficient modal repre- sentation capabilities, adopt global channel-spatial attention mechanism enhance features initially ex- tracted audio image modalities, thereby improving representation capabilities modali- ties. finally, decision-making classifica- tion stage. adopt decision fusion strategy based proportional principle obtain final emotional clas- sification result. coarse fine granularity loss function designed optimize performance entire net- work. ultimately, third place expres- sion recognition track affective behavioral analysis wild abaw competition. future, continue explore ways achieve accurate dynamic facial expression recognition classification re- sults. references nida itrat abbasi, siyang song, hatice gunes. statis- tical, spectral graph representations video-based fa- cial expression recognition children. icassp ieee international conference acoustics, speech signal processing icassp, pages ieee, panagiotis antoniadis, panagiotis paraskevas filntisis, petros maragos. exploiting emotional dependencies graph convolutional networks facial expression recogni- tion. ieee international conference auto- matic face gesture recognition pages ieee, qiupu chen guimin huang. novel dual attention- based blstm hybrid features speech emotion recog- nition. engineering applications artificial intelligence, weidong chen, xiaofeng xing, xiangmin xu, jichen yang, jianxin pang. key-sparse transformer multimodal speech emotion recognition. icassp ieee international conference acoustics, speech signal processing icassp, pages ieee, jadisha cornejo helio pedrini. bimodal emotion recog- nition based audio facial parts using deep con- volutional neural networks. ieee interna- tional conference machine learning applications icmla, pages ieee, esam ghaleb, jan niehues, stylianos asteriadis. mul- timodal attention-mechanism temporal emotion recogni- tion. ieee international conference image pro- cessing icip, pages ieee, dou hu, xiaolong hou, lingwei wei, lianxin jiang, yang mo. mm-dfn multimodal dynamic fusion network emotion recognition conversations. icassp ieee international conference acoustics, speech signal processing icassp, pages ieee, max jaderberg, karen simonyan, andrew zisserman, al. spatial transformer networks. advances neural informa- tion processing systems, vijay john yasutomo kawanishi. audio video- based emotion recognition using multimodal transformers. international conference pattern recogni- tion icpr, pages ieee, samira ebrahimi kahou, christopher pal, xavier bouthillier, pierre froumenty, aglar gulcehre, roland memisevic, pas- cal vincent, aaron courville, yoshua bengio, raul chan- dias ferrari, al. combining modality specific deep neural networks emotion recognition video. proceedings acm international conference multimodal interaction, pages dimitrios kollias. abaw learning synthetic data multi-task learning challenges. european conference computer vision, pages springer, dimitrios kollias. abaw valence-arousal estimation, ex- pression recognition, action unit detection multi-task learning challenges. proceedings ieeecvf con- ference computer vision pattern recognition, pages dimitrios kollias. multi-label compound expression recognition c-expr database network. proceed- ings ieeecvf conference computer vision pattern recognition, pages dimitrios kollias stefanos zafeiriou. expression, affect, action unit recognition aff-wild, multi-task learning arcface. arxiv preprint dimitrios kollias stefanos zafeiriou. analysing af- fective behavior second abaw competition. proceedings ieeecvf international conference computer vision, pages dimitrios kollias stefanos zafeiriou. affect analysis in- the-wild valence-arousal, expressions, action units unified framework. arxiv preprint dimitrios kollias, viktoriia sharmanska, stefanos zafeiriou. face behavior carte expressions, affect action units single network. arxiv preprint dimitrios kollias, panagiotis tzirakis, mihalis nicolaou, athanasios papaioannou, guoying zhao, bjorn schuller, irene kotsia, stefanos zafeiriou. deep affect prediction in-the-wild aff-wild database challenge, deep archi- tectures, beyond. international journal computer vision, dimitrios kollias, attila schulc, elnar hajiyev, stefanos zafeiriou. analysing affective behavior first abaw competition. ieee international confer- ence automatic face gesture recognition pages ieee, dimitrios kollias, viktoriia sharmanska, stefanos zafeiriou. distribution matching heterogeneous multi- task learning large-scale face study. arxiv preprint dimitrios kollias, panagiotis tzirakis, alice baird, alan cowen, stefanos zafeiriou. abaw valence-arousal estimation, expression recognition, action unit detection emotional reaction intensity estimation challenges. proceedings ieeecvf conference computer vi- sion pattern recognition, pages dimitrios kollias, viktoriia sharmanska, stefanos zafeiriou. distribution matching multi-task learning classification tasks large-scale study faces beyond. proceedings aaai conference artificial intelli- gence, pages dimitrios kollias, panagiotis tzirakis, alan cowen, ste- fanos zafeiriou, irene kotsia, alice baird, chris gagne, chunchang shao, guanyu hu. affective be- havior analysis in-the-wild abaw competition. pro- ceedings ieeecvf conference computer vision pattern recognition, pages dimitrios kollias, stefanos zafeiriou, irene kotsia, abhinav dhall, shreya ghosh, chunchang shao, guanyu hu. abaw competition multi-task learning compound expression recognition. arxiv preprint dimitrios kollias, panagiotis tzirakis, cowen, zafeiriou, kotsia, eric granger, marco pedersoli, ba- con, alice baird, gagne, al. advancements affective behavior analysis abaw workshop com- petition. jean kossaifi, georgios tzimiropoulos, sinisa todorovic, maja pantic. afew-va database valence arousal estimation in-the-wild. image vision computing, kumaran, radha rammohan, senthil murugan nagara- jan, prathik. fusion mel gammatone frequency cepstral coefficients speech emotion recognition using deep c-rnn. international journal speech technology, hoai-duy le, guee-sang lee, soo-hyung kim, seungwon kim, hyung-jeong yang. multi-label multimodal emo- tion recognition transformer-based fusion emotion- level representation learning. ieee access, patrick lucey, jeffrey cohn, takeo kanade, jason saragih, zara ambadar, iain matthews. extended cohn- kanade dataset complete dataset action unit emotion-specified expression. ieee computer soci- ety conference computer vision pattern recognition- workshops, pages ieee, fuyan ma, bin sun, shutao li. facial expression recog- nition visual transformers attentional selective fu- sion. ieee transactions affective computing, haotian miao, yifei zhang, weipeng li, haoran zhang, dal- ing wang, shi feng. chinese multimodal emotion recog- nition deep traditional machine leaming approaches. first asian conference affective computing intelligent interaction acii asia, pages ieee, joseph mikels, barbara fredrickson, gregory larkin, casey lindberg, sam maglio, patricia reuter- lorenz. emotional category data images interna- tional affective picture system. behavior research methods, ryo miyoshi, shuichi akizuki, kensuke tobitani, noriko nagata, manabu hashimoto. convolutional neural tree video-based facial expression recognition embedding emotion wheel inductive bias. ieee international conference image processing icip, pages ieee, muralikrishna, induja, meera, anju philip, kumar, pranita bhatele, nalini, ravi mishra, sileesh mullasseri. human emotion identification using deep learn- ing, binh nguyen, minh trinh, tan phan, hien nguyen. efficient real-time emotion detection using cam- era facial landmarks. seventh international conference information science technology icist, pages ieee, dung nguyen, kien nguyen, sridha sridharan, david dean, clinton fookes. deep spatio-temporal feature fu- sion compact bilinear pooling multimodal emotion recognition. computer vision image understanding, srinivas parthasarathy shiva sundaram. detecting ex- pressions multimodal transformers. ieee spo- ken language technology workshop slt, pages ieee, mahdi pourmirzaei, gholam ali montazer, farzaneh esmaili. using self-supervised auxiliary tasks im- prove fine-grained facial representation. arxiv preprint ivona tautkute tomasz trzcinski. classifying visu- alizing emotions emotional dan. fundamenta informat- icae, tran, lubomir bourdev, rob fergus, lorenzo torresani, manohar paluri. learning spatiotemporal features convolutional networks. proceedings ieee inter- national conference computer vision, pages panagiotis tzirakis, jiaxin chen, stefanos zafeiriou, bjorn schuller. end-to-end multimodal affect recognition real-world environments. information fusion, ashish vaswani, noam shazeer, niki parmar, jakob uszko- reit, llion jones, aidan gomez, ukasz kaiser, illia polosukhin. attention need. advances neural information processing systems, giuseppe vecchio, simone palazzo, concetto spamp- inato. surfacenet adversarial svbrdf estimation sin- gle image. proceedings ieeecvf international conference computer vision, pages yanan wang, jianming wu, panikos heracleous, shinya wada, rui kimura, satoshi kurihara. implicit knowl- edge injectable cross attention audiovisual model group emotion recognition. proceedings interna- tional conference multimodal interaction, pages stefanos zafeiriou, dimitrios kollias, mihalis nicolaou, athanasios papaioannou, guoying zhao, irene kotsia. aff-wild valence arousal in-the-wild challenge. proceedings ieee conference computer vision pattern recognition workshops, pages jianfeng zhao, xia mao, jian zhang. learning deep facial expression features image optical flow se- quences using cnn. visual computer, jinming zhao, ruichen li, qin jin, xinchao wang, haizhou li. memobert pre-training model prompt-based learning multimodal emotion recognition. icassp ieee international conference acoustics, speech signal processing icassp, pages ieee, sicheng zhao, yunsheng ma, yang gu, jufeng yang, tengfei xing, pengfei xu, runbo hu, hua chai, kurt keutzer. end-to-end visual-audio attention network emotion recognition user-generated videos. proceed- ings aaai conference artificial intelligence, pages", "published_date": "2025-03-15T00:59:34+00:00"}
{"id": "2503.06107v1", "title": "Feature Fusion Attention Network with CycleGAN for Image Dehazing, De-Snowing and De-Raining", "authors": ["Akshat Jain"], "summary": "paper presents novel approach image dehazing combining feature fusion attention ffa networks cyclegan architecture. method leverages supervised unsupervised learning techniques effectively remove haze images preserving crucial image details. proposed hybrid architecture demonstrates significant improvements image quality metrics, achieving superior psnr ssim scores compared traditional dehazing methods. extensive experimentation reside densehaze cvpr dataset, show approach effectively handles synthetic real-world hazy images. cyclegan handles unpaired nature hazy clean images effectively, enabling model learn mappings even without paired data.", "full_text": "feature fusion attention network cyclegan image dehazing, de-snowing de-raining akshat jain biitb.ac.in abstractthis paper presents novel approach image dehazing combining feature fusion attention ffa net- works cyclegan architecture. method leverages supervised unsupervised learning techniques effectively remove haze images preserving crucial image details. proposed hybrid architecture demonstrates significant im- provements image quality metrics, achieving superior psnr ssim scores compared traditional dehazing methods. extensive experimentation reside dense- haze cvpr dataset, show approach effectively handles synthetic real-world hazy images. cyclegan handles unpaired nature hazy clean images effectively, enabling model learn mappings even without paired data. introduction image dehazing remains critical challenge computer vision, affecting various applications autonomous driving surveillance systems. project addresses specific challenge constructing clean images degraded images affected adverse weather conditions fog, rain, snow. unique aspect approach lies handling task extremely limited clean image samples source domain. key objectives project include developing model capable generating clean images weather-degraded images training severely constrained dataset maximum clean samples creating multiple model variants different sample sizes clean samples implementing evaluating performance across variants developing practical web interface model testing comparison traditional methods rely physical models like atmospheric scattering model, approach combines two powerful deep learning architectures feature fusion attention ffa networks supervised learning cyclegan unsupervised domain adaptation combination allows leverage paired unpaired data, making solution robust generaliz- able real-world scenarios, particularly working limited clean samples. web-based interface, implemented using flask html, enables real-time testing different model variants provides immediate feedback ssim psnr metrics. i-a problem statement project addresses following specific requirements initial model training clean images particular domain creation utilization dataset containing weather- degraded images fograinsnow development mechanism construct clean images using limited clean source domain samples maximum source-trained model shifted domain samples progressive reduction clean samples development appropriate quantitative metrics implementation user interface model testing comparison i-b contributions main contributions include novel approach combining ffa networks cycle- gan limited-sample image restoration comprehensive analysis model performance across varying sample sizes development web-based testing interface real- time metric calculation extensive experimentation real-world road datasets comparative analysis model variants across different sample constraints workflow development process followed structured approach initial setup data preparation dataset organization cvpr paired, its- reside unpaired hazy images unpaired clean images implementation data loading preprocessing pipelines environment setup pytorch required de- pendencies model development implementation ffa network attention mechanisms integration cyclegan architecture custom loss function development cs.cv mar training pipeline two-phase training strategy implementation gradient accumulation better memory efficiency mixed precision training setup evaluation optimization psnr ssim metric implementation model performance analysis hyperparameter tuning iii related work iii-a unpaired image-to-image translation using cycle- consistent adversarial networks paper presents approach image-to-image trans- lation, challenging vision graphics problem aims learn mapping input image source domain output image target domain. typically, image translation tasks require training set paired images however, many practical applications, paired data available. address this, authors propose method learning mapping source domain target domain without paired training examples. primary goal learn function distribution translated images indistinguishable distribution images achieved using adversarial loss drives translation process. iii-b dehazenet end-to-end system single im- age haze removal paper based cnn-based model, dehazenet, removing haze single images. estimates medium transmission map, used reconstruct clear image via atmospheric scattering model. dehazenet incorporates bilateral rectified linear unit brelu, novel activation function improving image restoration limiting search space aiding convergence. system automates learning com- ponents traditionally reliant hand-crafted priors, achieving superior dehazing performance efficiently. dehazenet automates medium transmission map estima- tion single-image dehazing using deep cnn. brelu activation function enhances image restora- tion accuracy bilateral restraint. dehazenet aligns improves upon established image dehazing assumptions automatic learning. system efficient outperforms traditional meth- ods without complex manual priors. iii-c deep joint rain detection removal single image paper presents novel method rain detection removal single image using deep learning techniques. introduces new rain image model incorporates binary map rain streak locations, enhancing separation rain background textures. proposed recurrent rain detection removal network iteratively cleans images addressing rain streak accumulation various shapes overlapping streaks. experiments show approach significantly improves visibility images affected heavy rain compared existing methods. rain image model introduces binary map better represent visible rain streaks atmospheric effects. multi-task deep learning combines rain detection removal tasks enhance preservation background details. contextualized dilated network utilizes multiple dilated convolutions expand receptive field improve contextual understanding. iterative processing employs recurrent approach progressively eliminate rain streaks improve image clarity complex conditions. methods approaches iv-a pre-review phase work identification data sets pre-processing augmentation code dataset reading different research paper hazing removing mist snow rain analysis different approaches learning different types model used research papers cvpr dehazenet cyclegan preliminary code cyclegan model network incorporates multiple attention blocks feature refinement channel pixel attention mechanisms residual learning better gradient flow iv-b post-review phase work creation structure model different parts optimizer, loss function, metrics ssim psnr ,generator, discriminator. training testing function pipeline assess results model creation final stucture model following initial ffa model trained clean images trained ffa model used image generator cyclegan first trained lot unclean images ffa cyclegan model fine tuned pair clean hazy image steps creation web interface using python flash html test model different parameters iv-c model functions methods utilized iv-c imports initial setup initial imports include essential libraries py- torch torch, numpy, image processing modules. optim modules pytorch particularly important handle deep learning model components optimization functions. iv-c defining basic building blocks default convolution layer default conv defines standard convolutional layer customizable kernel size, stride, padding, bias. function used multiple parts model base convolu- tional layer. pixel attention layer palayer attention layer assigns weights pixels, highlighting important spatial areas feature map. layer uses convolution followed sigmoid activation compute spatial attention across pixels. channel attention layer calayer channel attention layer gives channel different weight, capturing inter-channel dependencies. pro- cess particularly useful emphasizing feature maps relevant dehazing task. iv-c constructing residual blocks groups residual block block core building block ffa network, consisting two convolutional layers relu activation function between. also includes pixel channel attention layers, enhancing ability selectively amplify important features suppressing less relevant details. group residual blocks group group encapsulates multiple residual blocks con- nects residual manner. groups help network learn complex patterns, improving overall performance stability deep architectures. iv-c defining ffa network architecture ffa network assembled stacking multiple groups residual blocks. overall structure includes head layer initial convolutional layer transforms input image compatible feature dimension. body multiple group instances collectively refine extract relevant features residual learning. tail layer final layer projects processed features back original image space. forward pass forward function defines input data flows part network, head body tail. iv-c training evaluation metrics ffa loss function code uses loss training, computes mean absolute difference predicted target images. loss commonly used image restoration tasks, tends preserve overall image structure. metrics ssim psnr metrics calculated epoch evaluate structural similarity signal-to-noise ratio predicted target images, essential assessing image quality. iv-c data loading preprocessing dataset loader reside performs random crop- ping augments dataset taking random sections images, aiding generalization. normalization ensures pixel values scaled helps model learn efficiently stabilizing gradients. iv-c training loop ffa clean images epoch structure epoch, model goes batch training data, performs forward pass, calculates loss, updates weights using adam optimizer. optimizer advantageous adaptive learning rate momentum features, promoting faster stable convergence. logging evaluation end epoch, ssim psnr metrics recorded training validation sets, providing insights models performance guiding adjustments. iv-c gan loss functions cyclegan loss cycleganloss class defines com- posite loss cyclegan gan loss mse loss penalizes discrepancies generated target domains e.g., fake vs. real images. cycle consistency loss loss weighted lambda cycle ensuring converting hazy clear back hazy vice versa yields image similar original, preserving consistency. combined loss drives network generate images look realistic maintaining fidelity original domain. iv-c ots reside cvrp dataset loader dataset class cvrp dataset class prepares data paired hazy clear image pairs unpaired training hazy images paired images loads spec- ified number paired hazy clear images, supporting supervised training. dataset class ots reside unclean images sam- ples subset hazy images create unpaired dataset, aiding unsupervised learning. transformations resizes images consistent resolution converts tensors compatibility pytorch. data loaders initializes dataloader instances paired unpaired datasets, using pinned memory efficiency gpu-based training. iv-c loading ffa model cyclegan ffa model reloaded saved checkpoint, configured data parallelism leverage multiple gpus available. iv-c cyclegan components generator discriminator models generatorffa class extends ffa model, adapting generator cyclegan framework. two genera- tor instances created generator converts hazy clear images. generator converts clear hazy images. discriminator convolutional neural network clas- sifies images real fake, distinguishing au- thentic generated samples. uses sequential convolu- tional layers leakyrelu activations batchnorm stabilize training. optimizers sets adam optimizers gen- erator discriminator networks learning rate momentum parameters effective conver- gence. iv-c fine tuning model defined number paired images forward cycle hazy clean hazy backward cycle clean hazy clean generates clean images hazy images reconstructs back, vice versa, refining cyclegans ability retain image quality across transformations. gan loss computation evaluates gan loss generators discriminators, promoting accurate trans- formation hazy clean domains. model saving monitoring saves generated images models periodically monitor progress. iv-c testing model perfomance evaluation tests ffa model passing single hazy image generator create dehazed version saves output image. psnr peak signal-to-noise ratio ssim structural similarity index calculated measure quality generated images compared clean reference images, focusing detail retention overall image similarity. iv-c implementation interface implementation cyclegan using flask html offers user-friendly web interface enhancing images affected weather conditions haze, snow, rain. users upload original images select various fine-tuning parameters, including number paired fine-tuning images selection allows users customize image processing according preferences. processing, application displays original cleaned images side side, providing clear visual comparison. side-by-side presentation helps users assess restoration quality effectively, making easier understand impact different weather conditions image clarity detail. additionally, application calculates presents key metrics like structural similarity index ssim peak signal-to-noise ratio psnr. quantitative statis- tics enable users evaluate effectiveness image enhancement process based chosen fine-tuning image count, facilitating informed decisions level improvement required images intuitive manner. reason particular choice ffa cyclegan attention mechanisms ffa ffa model leverages channel pixel attention layers calayer palayer, allowing empha- size essential features suppressing irrelevant infor- mation. dehazing, means model better focus regions heavily impacted haze, enhancing details improving contrast needed. combination residual blocks attention makes ffa highly efficient tasks requiring nuanced feature extraction, enabling capture global channel- wise local pixel-wise dependencies. ffa feature-enhancing generator cycle- gan cyclegan setup, ffa model function powerful generator converting hazy images clear images. inherent design allows extract complex features hazy images, making suitable challenging real-world data. ffas capability adapt attention across different regions image helps learn mappings accurately represent clear images, especially useful unpaired cyclegan training ground-truth labels may always available. cyclegan domain adaptation cycle consistency loss core advantage cyclegan cycle consistency loss, ensures images translated hazy clear back hazy retain structural fidelity. essential dehazing, preserving integrity original image crucial. unsupervised learning capability cyclegan, model doesnt require paired data. especially bene- ficial dehazing, clear-hazy image pairs may difficult obtain. ffa-cyclegan framework leverage unpaired images, making scalable large, diverse datasets. data vi-a dataset description model trained dense-haze cvpr dataset paired training paired images dataset commonly used benchmark image dehazing tasks high resolution images size dataset image approx image width image height also contains snowy, rainy, hazy images indoor training set reside-standard unpaired hazy images clean images reside-standards indoor training set consists clear images corresponding hazy image size dataset entire image approx image width image height used image clean ffa hazy dataset cyclegan various indoor hazy images outdoor training set ots reside-b reside outdoor training set ots consists images clean images medium resolution images, dataset image approx image width image height used image clean ffa hazy dataset cyclegan various outdoor scenes full diversity snowy, rainy, hazy image vi-b preprocessing data preprocessing includes transform transforms.compose transforms.resize, transforms.totensor data augmentation includes random horizontal flips random rotations normalization mean., std., vii experiments vii-a training configuration hardware gpu cuda support disk cpu gpu batch size due memory constraints learning rate optimizer adam vii-b experiments done tried train various learning rate, steps epochs part problem statement different fine tuning parameters vii-c learning rate learning rate parameters tried ffa model following outputs table performance metrics different learning rates learning rate ssim psnr gradient descent behavior learning rate low like resulting slow convergence. model got stuck local minima, failing make substantial progress. hand, learning rate caused optimizer overshoot minimum, leading divergence erratic updates hinder learning. learning rate defines much adjust weights response gradient loss function. optimal learning rate like case strikes balance, allowing sufficient movement opti- mization landscape without overshooting stagnating. model update weights effectively, maintaining stability training allowing convergence towards better solution. learning rates low high may introduce instability insufficient exploration loss surface. vii-d fine tuning ssim psnr values different number images fine tuning table performance metrics different number im- ages number images ssim psnr ssim psnr columns provide example perfor- mance metrics might typically reflect decreasing trend fewer images used, demonstrating impact dataset size model performance. viii results proposed ffa cyclegan model demonstrates no- table performance across various metrics based fine-tuning decreasing number paired images, showcasing ro- bustness even limited clean data. following improve- ments achieved compared baseline methods referenced study al. fig. performance different dehazing model. adapted al., unsupervised haze removal high- resolution optical remote-sensing images based improved generative adversarial networks, remote sensing, vol. online. viii-a quantitative results psnr improvement model achieves psnr paired images, improvement cycledehazes compara- ble highest baseline intermediate result db. ssim improvement model reaches ssim paired images, surpassing cycledehazes closely approaching dark channel methods training convergence training convergence achieved approximately epochs, ensuring effi- cient training process given models complexity resource constraints. viii-b qualitative results visual assessments reveal model works dehazing de-snow de-rain demisti- fying model removes elements haze, snow rain meticulously, hence great model achiev- ing image cleanup process. preserves fine details ffa cyclegan approach effectively retains intricate textures, particularly areas dense haze. produces natural color reproduction model restores colors appear true-to-life compared methods. reduces artifacts challenging conditions ob- serve fewer halos ringing artifacts, particularly complex backgrounds, highlighting models stability challenging cases. results underscore models capability high- quality haze removal utilizing approximately im- ages training, demonstrating efficiency data-limited scenarios. viii-b rainy images rainy image clean image psnr ssim rainy image clean image psnr ssim fig. comparison rainy hazy clean images pro- cessed ffa cyclegan model viii-b snowy images snowy image clean image psnr ssim snowy image clean image psnr ssim fig. comparison snowy hazy clean images pro- cessed ffa cyclegan model viii-b foggy images foggy image clean image psnr ssim foggy image clean image psnr ssim fig. comparison foggy hazy clean images pro- cessed ffa cyclegan model plan novelty assessment future work focus comparison emerging state-of-the-art mod- els beyond baseline comparisons, include evaluations number images ssim psnr table iii performance ffa cyclegan model varying numbers paired images newer architectures, like diffusion models transformer-based image restoration networks, could reveal unique strengths limitations ffa- cyclegan model. real-world applicability varied scenarios assess performance diverse, real-world settings e.g., dense fog, rainstorms various imaging devices, low- resolution surveillance cameras high-resolution sen- sors. demonstrate robustness across practical use cases. user-driven metrics interpretability introduce user-friendly metrics like human perception scores automated quality metrics beyond ssim psnr, especially subjective image quality assessments gauge user satisfaction real-time applications. plan action comparing metrics different models testing model different dataset improvement metrics interpretability in- terface conclusion combined ffa-cyclegan approach demonstrates promising results image dehazing, effectively leveraging paired unpaired data. integration attention mechanisms cycle consistency leads robust performance across various scenarios. key achievements include successful combination supervised unsupervised learning improved performance metrics memory-efficient training implementation references zhu, j.-y., park, t., isola, p., efros, unpaired image-to- image translation using cycle-consistent adversarial networks. arxiv. cai, b., xu, x., jia, k., qing, c., tao, de- hazenet end-to-end system single image haze re- moval. ieee transactions image processing, yang, w., tan, t., feng, j., liu, j., guo, z., yan, deep joint rain detection removal single image. arxiv. ancuti, o., ancuti, c., sbert, m., timofte, dense haze benchmark image dehazing dense-haze haze-free images. ieee international conference image processing icip, taipei, taiwan. ancuti, o., ancuti, c., timofte, r., van gool, l., zhang, l., yang, m.-h. ntire image dehazing challenge report. proceedings ieee conference computer vision pattern recognition workshops, long beach, us. li, b., ren, w., fu, d., tao, d., feng, d., zeng, w., wang, benchmarking single-image dehazing beyond. ieee transactions image processing, li, b., ren, w., fu, d., tao, d., feng, d., zeng, w., wang, benchmarking single-image dehazing beyond. ieee transactions image processing, hu, a., xie, z., xu, y., xie, m., wu, l., qiu, unsupervised haze removal high-resolution optical remote-sensing images based improved generative adversarial networks. remote sensing,", "published_date": "2025-03-08T07:18:42+00:00"}
{"id": "2502.18495v2", "title": "A Comprehensive Survey on Composed Image Retrieval", "authors": ["Xuemeng Song", "Haoqiang Lin", "Haokun Wen", "Bohan Hou", "Mingzhu Xu", "Liqiang Nie"], "summary": "composed image retrieval cir emerging yet challenging task allows users search target images using multimodal query, comprising reference image modification text specifying users desired changes reference image. given significant academic practical value, cir become rapidly growing area interest computer vision machine learning communities, particularly advances deep learning. best knowledge, currently comprehensive review cir provide timely overview field. therefore, synthesize insights publications top conferences journals, including acm tois, sigir, cvpr particular, systematically categorize existing supervised cir zero-shot cir models using fine-grained taxonomy. comprehensive review, also briefly discuss approaches tasks closely related cir, attribute-based cir dialog-based cir. additionally, summarize benchmark datasets evaluation analyze existing supervised zero-shot cir methods comparing experimental results across multiple datasets. furthermore, present promising future directions field, offering practical insights researchers interested exploration. curated collection related works maintained continuously updated", "full_text": "comprehensive survey composed image retrieval xuemeng song, shandong university, china haoqiang lin, shandong university, china haokun wen, harbin institute technology shenzhen china city university hong kong, china bohan hou, shandong university, china mingzhu xu, shandong university, china liqiang nie, harbin institute technology shenzhen, china composed image retrieval cir emerging yet challenging task allows users search target images using multimodal query, comprising reference image modification text specifying users desired changes reference image. given significant academic practical value, cir become rapidly growing area interest computer vision machine learning communities, particularly advances deep learning. best knowledge, currently comprehensive review cir provide timely overview field. therefore, synthesize insights publications top conferences journals, including acm tois, sigir, cvpr. particular, systematically categorize existing supervised cir zero-shot cir models using fine-grained taxonomy. comprehensive review, also briefly discuss approaches tasks closely related cir, attribute-based cir dialog-based cir. additionally, summarize benchmark datasets evaluation analyze existing supervised zero-shot cir methods comparing experimental results across multiple datasets. furthermore, present promising future directions field, offering practical insights researchers interested exploration. curated collection related works maintained continuously updated awesome-cir repository. ccs concepts information systems image search. additional key words phrases composed image retrieval multimodal retrieval multimodal fusion acm reference format xuemeng song, haoqiang lin, haokun wen, bohan hou, mingzhu xu, liqiang nie. comprehensive survey composed image retrieval. acm trans. inf. syst. march pages. introduction image retrieval fundamental task computer vision database management since serving cornerstone various applications, face recognition fashion retrieval person re-identification traditional image retrieval systems primarily rely unimodal queries, using either text work submitted acm possible publication. copyright may transferred without notice, version may longer accessible. authors contact lnformation xuemeng song, shandong university, qingdao, china, sxmustcgmail.com haoqiang lin, shandong university, qingdao, china, zichaohqgmail.com haokun wen, harbin institute technology shenzhen shenzhen, china city university hong kong, hong kong, china, whenhaokungmail.com bohan hou, shandong university, qingdao, china, bohanhoufoxmail.com mingzhu xu, shandong university, jinan, china, xumingzhusdu.edu.cn liqiang nie, harbin institute technology shenzhen, shenzhen, china, nieliqianggmail.com. permission make digital hard copies part work personal classroom use granted without fee provided copies made distributed profit commercial advantage copies bear notice full citation first page. copyrights components work owned others authors must honored. abstracting credit permitted. copy otherwise, republish, post servers redistribute lists, requires prior specific permission andor fee. request permissions permissionsacm.org. copyright held ownerauthors. publication rights licensed acm. manuscript submitted acm manuscript submitted acm cs.mm mar xuemeng song al. supervised learning paradigm zero-shot learning paradigm related tasks fig. number papers trending task composed image retrieval related task since images convey users search intent however, users often struggle clearly express search intent single text query find perfect image accurately represents it. address limitations provide greater flexibility, composed image retrieval cir emerged allows users express search intent reference image combined textual description specifying desired modifications. enabling users utilize nuanced search queries, cir offers significant potential enhance search experiences across domains, e-commerce internet search engines concept cir, allows users utilize multimodal query express search intent, easily adapted various real-world retrieval scenarios. example, reference image could replaced reference video enable composed video retrieval, single-turn cir could evolve dialog-based multi-turn image retrieval. since introduction cir garnered increasing research attention due potential value across various domains. illustrated figure number publications cir increasing rapidly. summarize past current achievements rapidly developing field, present comprehensive overview work conducted november existing studies primarily focus addressing following key challenges. multimodal query fusion. cir, modification text reference image play complementary roles conveying users search intent. modification text typically specifies changes certain attributes reference image. instance, given modification requirement, want dress black professional, color style dress reference image changed, attributes reference image kept unchanged. due nature, achieve effective multimodal fusion accurately comprehending multimodal query poses first challenge. target images matching. semantic gap multimodal query target images presents significant challenge due heterogeneous representations. additionally, brevity modification texts lead ambiguity. example, text want change dress longer sleeves yellow color could multiple interpretations sleeves could change sleeveless either short long, color could range light dark yellow. ambiguity suggests multiple target images could satisfy given query. therefore, bridging semantic gap managing one-to-many query-to-target matching relationship crucial accurate query-target matching. scale training data. training cir models typically requires triplets manuscript submitted acm comprehensive survey composed image retrieval composed image retrieval section supervised composed image retrieval section zero-shot composed image retrieval section related tasks composed image retrieval section benchmarks experiments section discussion, future directions, conclusion feature extraction image-text fusion target matching data augmentation pseudo-triplet-based training-free attribute-based sketch-based remote sensing- based dialog-based video-based datasets metric experimental results textual-inversion-based fig. organization present survey. form reference image, modification text, target image. triplet, reference-target image pair often generated using heuristic strategy, modification text usually annotated humans. creating training samples costly labor-intensive, significantly restricts size benchmark datasets. consequently, addressing issue insufficient training data improve models generalization capabilities remains significant challenge. existing work area broadly divided two main categories supervised learning-based approaches zero-shot learning-based approaches. key distinction methods lies availability annotated training triplets. supervised approaches rely annotated triplets dataset train model, zero-shot approaches leverage large-scale, easily accessible data, image-text pairs, pre-training without requiring annotated triplets optimization. facilitate deeper analysis, establish fine-grained taxonomy category. supervised cir approaches, summarize existing methods based four key components general framework feature extraction, image-text fusion, target matching, data augmentation. zero-shot composed image retrieval zs-cir approaches, classify methods three groups textual-inversion-based, pseudo-triplet- based, training-free. previously mentioned, concept using composed multimodal query adapted various scenarios. beyond primary task cir, several related tasks also involve composed queries, reference image plus attribute manipulation, sketch plus modification text, video plus modification text. since tasks closely related cir, include recent advancements provide comprehensive review topic. based type multimodal query, categorize related tasks five groups attribute-based, sketch-based, remote sensing-based, dialog-based, video-based. summary, main contributions follows best knowledge, paper presents first comprehensive review cir, incorporating primary studies. aims provide timely insightful overview guide future research rapidly advancing field. systematically organize research findings, technical approaches, benchmarks, experiments deepen understanding field. additionally, propose elaborate taxonomy methods, catering diverse needs readers. manuscript submitted acm xuemeng song al. want dress black professional. reference image modification text neural network- based fusion image-text fusion basic metric learning image difference alignment negative mining uncertainty modeling re-ranking target matching target image traditional encoder vlp-based encoder feature extraction feature extraction idc model-based llm-based query unification- based data augmentation gradient-based image replacement- based prototype image generation-based fusion reverse objective- based explicit combination- based fusion fig. illustration standard framework supervised composed image retrieval. cir remains emerging area research. based surveyed literature, identify several key research challenges propose potential future directions, offering forward-looking guidance researchers domain. remainder paper organized depicted figure sections review supervised cir models zero-shot cir models, respectively. section introduces tasks related cir. section describes currently available datasets, evaluation metrics used, experimental results existing approaches. finally, discuss possible future research directions section conclude work section supervised composed image retrieval section, first provide problem statement task supervised cir, present existing approaches. generally, depicted figure existing models involve four key components feature extraction, image- text fusion, target matching, data augmentation. first three essential components cir, last one optional aimed enhancing model performance. existing supervised cir methods summarized table problem statement. given reference image modification text, cir aims retrieve target images collection gallery images. supervised learning setting, existing methods rely training samples triplet form, i.e., reference image, modification text, target image. let denote set triples, reference image, modification text, signifies target image, total number triplets. then, based training dataset existing methods aim learn multimodal fusion function effectively combines multimodal query visual feature embedding function ensure composed query corresponding target image close embedding space. formalized follows, represents multimodal fusion function mapping multimodal query latent space, denotes feature embedding function target image. manuscript submitted acm comprehensive survey composed image retrieval table summarization main supervised composed image retrieval approaches. fusion strategy method year image encoder text encoder aspect explicit combination-based fusion transformed image-and-residual tirg resnet- lstm css dataset val resnet-, mobilenet lstm hierarchical matching jvsm mobilenet lstm joint visual semantic matching datir resnet-, mobilenet lstm hierarchical matching dcnet resnet- glovemlp image difference alignment mgf resnet- lstm online groups matching mcr resnet- lstm image difference alignment clvc-net resnet- lstm mutual enhancement sac resnet- bert matching optimization eer resnet- lstm semantic space alignment crn swin transformer lstm cross relation retrieval mlclsap resnet- lstm image difference alignment tg-cir clip-b clip-b target similarity guidance ranking-aware cliprn cliprn uncertainty modeling mcem resnet-, resnet- lstm negative example mining dwc resnet-, cliprn lstm, cliprn mutual enhancement css-net resnet-, resnet- roberta collaborative matching alret resnet-, cliprn lstm, cliprn composition decomposition content-and-style cosmo resnet-, resnet- lstm lsctcir resnet- gru pcasm resnet-, resnet- lstm spirit cliprnx cliprnx patch-level graph reasoning neural network-based fusion mlp-based composeae resnet- bert rotational symmetry combiner cliprnx cliprnx clipcir cliprnx cliprnx fine-tune strategy plcir cliprn, clip-b cliprn, clip-b fashion-based fine-tuning artemis resnet-, resnet- bi-gru, lstm clipcir cliprnx cliprnx fine-tune strategy dscn resnet-, resnet- bi-gru hierarchical matching clip-cd cliprnx cliprnx pseudo triplet generation blipcir blip blip reverse learning cmap resnet- bi-gru, lstm hierarchical matching caff cliprn cliprn fashion-based fine-tuning manme resnet- bi-gru, lstm hierarchical matching nsfse resnet-, resnet- bi-gru, lstm negative sensitive framework shaf fashionclip fashionclip hierarchical alignment clip-probcr clip clip uncertainty modeling dmot blip blip dqu-cir clip-h clip-h data augmentation sadn cliprnx cliprnx neighborhood distillation cross-attention-based lbf faster r-cnn tep coarse fine retrieval maaf resnet- lstm provla swin transformer bert negative example mining comqueryformer swin transformer bert hierarchical matching lgli resnet- lstm acnet resnet- bi-gru image difference alignment re-ranking blip blip re-rank case blip blip reverse learning sdqur blip blip uncertainty regularization iudc clip clip llm-based data augmentation self-attention-based cirplant resnet- cirr dataset fashionvlp resnet-, resnet- bert asymmetric design fashionvil resnet- bert multi-task pre-training fad-vlp cliprn cliprn multi-task pre-training amc resnet- lstm dynamic router mechanism aacl swin transformer distilbert revised shoppingk dataset fame-vil clip-b clip-b multi-task pre-training neucore resnet bi-gru multi-modal concept alignment lmga vit visual-bert gradient attention fashionern clip-b clip-b modifier enhancement sdfn resnet- lstm dynamic router mechanism ssn clip-b clip-b limn clip-l clip-l pseudo triplet generation sprc blip blip auxiliary loss vista eva-clip--base bge-base-v. syncmask vit-b bert multi-task pre-training unifashion clip clip multi-task pre-training graph-attention-based jamma resnet- bi-gru gscmr resnet- bi-gru prototype image generation-based fusion synthtripletgan resnet- bi-gru tis inception-v lstm plug-and-play rtic resnet- lstm gcns stream jpm resnet- lstm image difference alignment resnet- lstm gradient augmentation vqacir re-rank cir-mu resnet- roberta uncertainty modeling cala image difference alignment sda llm-based data augmentation spn positive example generation manuscript submitted acm xuemeng song al. feature extraction task cir, feature extraction plays crucial role deriving meaningful embeddings input query target image. since feature extraction extensively studied fields natural language processing computer vision, existing methods cir leverage established textual visual feature extraction backbones encode input query target image. categorize encoder backbones two primary types traditional encoders vision-language pre-trained vlp model-based encoders. traditional encoder. textual feature extraction, commonly used encoders cir tasks include rnn-based encoders transformer-based encoders. representative rnn-based encoders used cir studies bidirectional gated recurrent units bigrus long short-term memory networks lstms, proven effective capturing long-term dependencies text sequences. specifically, existing cir studies employ bigrus text encoders process sequences bidirectionally, enriching feature embedding capturing context past future tokens. meanwhile, several studies utilize lstms, introduce gated mechanisms standard rnn structure, effectively managing long-range dependencies modification text feature extraction. emergence transformers growing number cir studies adopt transformer-based encoders, bert variants e.g., roberta distilbert text encoders. encoders leverage self-attention mechanisms capture global context across entire text sequence, enabling parallel processing producing deeper contextual embeddings. overall, compared rnn-based encoders, transformer-based encoders demonstrate superior capabilities textual embedding cir tasks, particularly pre-trained extensive corpora. similarly, traditional image encoders used cir studies categorized cnn-based transformer- based encoders. cnn-based encoders initially popular due ability capture spatial hierarchies convolution operations, preserving crucial spatial information providing robust hierarchical feature embeddings. many cir methods extract image features pre-trained cnn-based encoders, resnet googlenet mobilenet yield generalizable feature embeddings pre-trained large-scale datasets like imagenet contrast cnn-based encoders, directly feed entire image encoder, transformer-based encoders redefine image encoding segmenting images non-overlapping patches employing self-attention model spatial relationships. one commonly used transformer- based encoder cir vision transformer vit captures nuanced visual details self-attention mechanism image patches. additionally, several cir methods employ swin transformers adopt window-based self-attention mechanism local interactions within window, reducing computational complexity. typically, transformer-based encoders offer superior representational capabilities compared cnn-based ones, particularly pre-trained extensive datasets. vlp-based encoder. recently, advancements vision-language pre-training led prevalence vlp-based encoders, preferred encoding multimodal data due ability align visual textual modalities. example, several studies adopt clip feature extraction backbone, leverages contrastive learning large-scale image-text datasets demonstrates exceptional flexibility across diverse domains. addition, studies utilize blip unifies vision-language understanding generation multimodal mixture encoder-decoder framework. moreover, blip- also adopted feature extraction recent cir studies bridges modality manuscript submitted acm comprehensive survey composed image retrieval gap lightweight querying transformer q-former achieves state-of-the-art performance various vision- language tasks. collectively, vlp-based encoders provide robust feature embeddings cir tasks, compared traditional encoders. image-text fusion reference image modification text input query separately encoded, subsequent crucial step involves designing effective image-text fusion strategy integrate complementary information modalities, order precisely represent input query conduct target image retrieval. towards end, existing methods categorized three groups explicit combination-based fusion, neural network-based fusion, prototype image generation-based fusion. explicit combination-based fusion. first group methods aims accomplish image-text fusion explicit combination operations, categorized two types transformed image-and-residual combination content-and-style combination. transformed image-and-residual. key idea group methods keep image feature dominant component, achieve image-text fusion learning two parts transformed reference image feature residual feature. group methods typically expressed img txt img res. here, img txt represent reference image embedding modification text embedding, respectively. notably, img txt may direct output feature extraction component. enhance image-text fusion, various types imagetext features, global features local features hierarchical features decoupled features explored. function neural network derives parameters applied reference image achieve modification operation, denotes element-wise multiplication. res refers residual offsetting information. based method used obtain res, approaches branch divided two major categories. first category methods, including tirg ,val jvsm mgf dcnet clvc-net css-net crn directly fuse image text features derive residual offsetting information, i.e., res img txt. representative method tirg designs gating function sigmoid activation function adaptively preserve unchanged information reference image, simple mlp-based neural network fuse image text features derive residual offsetting information. given simple yet effective image-text fusion paradigm, following jvsm, mgf, dcnet directly adopt manner tirg does, val, clvc-net, crn share similar spirits tirg, except utilize attention mechanism instead gating function. example, val devises joint-attention mechanism suppress highlight visual content based spatial channel dimensions reference image feature maps. meanwhile, combines self-attention learning capture crucial visio-linguistic cues residual part. clvc-net devises two attention mechanism-based streams derive local-wise global-wise perspectives. subsequently, two streams made learn one another mutual learning order obtain comprehensive image-text fusion result. particular, css-net adopts two different forms compositor, namely img res text res. former primarily aims discern elements modify within reference image, guided modification text latter emphasizes determining retain within modification text given reference image. another group methods adheres formula expressed res img txt txt learn residual part. typically, scaled fall within range strategy works preserving manuscript submitted acm xuemeng song al. unchanged part within reference image, also integrating information modification text replace certain aspects. tg-cir dwc alret eer follow criteria achieve image-text fusion. several methods adopt straightforward approach directly adding img txt, mcem ranking-aware sake convenience, also classify group considering i.e., identity mapping. likewise, lgli sac also categorized group. notably, instead using element-wise multiplication transform image features, sac relies attentional transformation derive text-conditioned image representation adds modification text representation obtain fused feature, expressed img txt txt. denotes attentional transformation network. content-and-style. considering image well characterized content style group methods typically hypothesizes image style content modified accordance modification text. thereby, modifications achieved style content spaces, finally combined obtain fusion output. example, cosmo initially devises content modulator equipped disentangled multimodal non-local block effecting content modifications. subsequently, content adjusted, style modulator designed incorporate modified style information, ensuring sequential progression content style adaptation. differently, lsctcir pcasm pursue content style modifications parallel. specifically, decompose reference image corresponding style content features, perform semantic replacement within feature spaces according modification text. finally, content style features fed combination module parallel image-text fusion. spirit primarily centers style aspect. introduces explicit definition style commonality difference among local patches image. concept mind, spirit first segments reference image multi-granularity patches. subsequently, two modules devised model local patches commonality difference, respectively. two style features combined enrich representation reference image, fused text features obtain final query representation. neural network-based fusion. contrast methods former group, methods group rely entirely neural networks fuse reference image modification text without employing explicit combination operations. methods categorized four subgroups mlp-based, cross-attention-based, self-attention-based, graph-attention-based approaches. mlp-based. methods branch primarily rely multi-layer perceptron mlp fulfill image-text fusion. example, plcir introduces multi-stage learning framework progressively acquire complex knowledge necessary multimodal image retrieval employs mlp-based query adaptive weighting strategy dynamically balance influence image text. pioneers applying clip cir tasks, baldrati al. introduce classic multimodal fusion network, i.e., combiner, integrates clip-based reference image modification text features using mlp-based weighted summing feature concatenation. later, also propose several fine-tuning strategies within framework alleviate domain discrepancy clips pre-training data downstream task data. combiner network directly adopted many subsequent methods refined later studies considering differing contribution input reference image modification text, dwc introduces editable modality de-equalizer emd. module employs two modality editors equipped spatial word attention mechanisms refine image text features, respectively. utilizes mlp-based adaptive weighting module manuscript submitted acm comprehensive survey composed image retrieval assign modality weights based contributions. additionally, dwc incorporates clip-based mutual enhancement module, effectively mitigates modality discrepancies promotes similarity learning. achieve precise alignment visual linguistic features, shaf initially employs attention mechanism text image feature realignment multiple levels encoding reference image modification text using fashionclip subsequently, mlp-based dynamic feature fusion strategy used integrating multimodal information emphasizing critical features weight allocation feature enhancement mechanisms. cross-attention-based. cir methods branch primarily adopt cross-attention model interaction word modification text every local region reference image, thereby enhancing fusion. approach, one modality serves query, acts key value. among them, lgli introduces localization mask, derived object detection model faster r-cnn additional input image-text fusion, enabling precise local modifications reference image. recognizing generated localization mask may always reliable, authors additionally introduce channel cross-modal attention mechanism spatial cross-modal attention mechanism effectively localize to-be-modified regions. acnet proposes multi-stage compositional framework sequentially modifies reference image based textual semantics. stage, framework first enhances image features using self-attention layer applied images regional features. applies cross-attention-based relation transformation layer establish cross-modal associations image regions word features. address comprehensive modification intent reasoning, iudc introduces dual-channel matching model comprising semantic matching module visual matching module. semantic matching module utilizes gate-based attention mechanism fuse attributes reference image, generated large language model llm, modification text semantic reasoning. visual matching module uses affine transformation image-text fusion, preceded cross-attention mechanism enhance interaction two input modalities. two modules trained collaboratively, transferring knowledge mutual enhancement. contrast, capture fine-grained deterministic many-to-many correspondence composed query target, sdqur leverages q-former module blip achieve adaptive fine-grained image-text fusion word modification text every local region reference image. specifically, incorporates set learnable queries first interact word tokens modification text self-attention layers exchange information visual patch features cross-attention layers, thereby learning diverse semantic aspects input query. unlike previous vlp encoder-based methods compose multimodal query late fusion manner, case introduces cross-attention driven shift encoder based blips image-grounded text encoder, i.e., bert encoder intermediate cross-attention layers. image first encoded vit injected cross-attention layers shift encoder, enabling early image-text fusion. self-attention-based. contrary cross-attention, self-attention mechanism gained prominence rise transformer, input sequence simultaneously plays roles query, key, value, computing attention scores input element. consequently, self-attention enables learning dependencies among different elements within single sequence. methods branch typically feed concatenation encoded reference image feature modification text feature self- attention-based network, like transformer, fully learn interaction promoting image-text fusion. among them, aacl refines standard transformer encoder additive self-attention layer, utilizes additive attention mechanism capture contextual information selectively suppress highlight representation token, thereby facilitating retention modification reference image information. different previous work treats modification text single description, ssn treats modification text manuscript submitted acm xuemeng song al. instruction hence explicitly decomposes semantic transformation conveyed modification text two steps degradation upgradation. specifically, first uses mlp-based degradation network degrade reference image visual prototype retains to-be-preserved visual attributes. then, transformer-based upgrading network adopted upgrade visual prototype final desired target image. processes guided modification text. contrary methods compose multimodal query based extracted features, methods first concatenate encoded reference image features word tokens modification text feed concatenated token sequence transformer-based model image-text fusion. different methods adopt single fusion strategy, amc sdfn consider multiple fusion strategies, self-attention mechanism acts one fusion option, incorporates dynamic routing mechanism adaptive image-text fusion. especially, methods focus addressing various vision-and-language tasks fashion domain, including cross-modal retrieval cir, within transformer-based foundation model. pretraining multiple meticulously designed tasks, transformer-based frameworks exhibit competitive performance across range heterogeneous fashion-related tasks. graph-attention-based. graph attention mechanisms specifically designed handling graph-structured data. unlike cross-attention self-attention, primary distinction lies deal data structured graphs rather sequences, take account relationships edges nodes within graph. manipulate visual features reference image according semantics modification text attribute level, jamma leverages pretrained faster r-cnn extract visual attribute features referencetarget image. features utilized vertices construct graph, edge established based relative size location relationships two attributes subsequently, jamma employs jumping graph attention network infuse semantic information modification text attribute graph, dynamically assigning higher weights attributes relevant text. finally, adopts global semantic reasoning module, follows idea gate memory mechanism filter redundant attributes, thereby yielding discriminative global query feature. jointly modeling geometric information image visual-semantic relationship input image text, gscmr initially learns cross-modal embedding composed query geometry-aware way rectifies visual feature guidance modification text multi-head graph attention network prototype image generation-based fusion. apart first two groups, methods aim achieve multimodal fusion directly synthesizing prototype image satisfies requirements multimodal query. approach effectively converts cir image-to-image retrieval problem. recognizing lack interpretability traditional methods directly combine inputs multimodal query representation target image retrieval, synthtripletgan pioneers integrate generative adversarial networks gans cir, triplet loss used metric learning. contrast, tis introduces multi-stage gan-based structure embeds retrieval model within gan framework. learn discriminative composed query feature, tis uses two distinct discriminators one targeting global differences generated target images, identifying local modifications generated images. unlike two methods concentrate cir task, unifashion focuses developing unified framework leveraging llms diffusion models enhance performance multimodal retrieval generation tasks mutual task reinforcement. manuscript submitted acm comprehensive survey composed image retrieval target matching target matching module aims accurately retrieve images match given multimodal query. one fundamental technique target matching cir metric learning, establishes feature space distances effectively represent semantic similarities differences. enhance metric learning, several strategies developed, categorized five groups basic metric learning, image difference alignment, negative mining, uncertainty modeling, re-ranking. basic metric learning. existing cir studies primarily utilize three types loss functions batch-based classification bbc loss function, soft triplet-based loss function, hinge-based triplet ranking function. batch-based classification loss. loss function widely used metric learning current cir studies loss function aims bring query embedding closer annotated target image embedding treating target images batch negative samples, pushing away query embedding. formulated follows, log subscript refers -th triplet sample mini-batch, represent combined feature input query target feature, respectively. batch size, serves cosine similarity function, denotes temperature factor. soft triplet-based loss. loss function specific variant batch-based classification loss function, single candidate image batch selected negative example iteration. widely adopted metric learning several cir studies specifically, loss function formulated follows, log xt,m, subscript refers -th triplet sample mini-batch, xt,m represents -th selected negative sample, repeat times evaluate every possible set. hinge-based triplet ranking loss. loss focuses optimizing hard negative samples adopted several cir studies similar loss, primary objective cluster matched query-target pairs separating unmatched ones embedding space. concentrating hard negatives, loss function loss function effectively addresses challenges high redundancy slow convergence associated random triplet sampling process inherent soft triplet-based loss functions. formally, objective function defined follows, xt, subscript refers -th triplet sample dataset, denotes semantic similarity function, margin value, represent hard negatives positive pair xt. cir methods typically rely single-granularity matching, optimizing model using one three aforementioned loss functions minimize distance final combined feature target image. however, since visual elements vary substantially scale several studies explored hierarchical matching improve alignment input query target image. methods start sampling multi-granular visual features visual encoder separately integrating modification manuscript submitted acm xuemeng song al. text feature. then, minimize distance composed features corresponding granularity-level features target image based respective loss functions optimize model. image difference alignment. mentioned above, mainstream methods model task query-target matching task, i.e., encoding multimodal query single feature aligning target image. however, learning paradigm explores straightforward relationship within triplet. fact, beyond query-target matching relationship, exists latent relationship reference-target image pair modification text. intuitively, modification text capture visual difference reference image target image. acts implicit transformation convert reference image target image. accordingly, several studies explore image difference alignment, i.e., aligning difference reference image target image modification text, boosting metric learning. achieve this, several studies adapt conventional bbc loss image difference alignment follows, log vd, vd, subscript refers -th triplet sample mini-batch, represent visual difference representation reference-target image pair modification text representation, respectively. typically, derived neural networks, mlp cross-attention networks. differently, jpm adopts mean squared error mse loss narrow distance image differences text modifications follows, subscript refers -th triplet sample dataset. different methods, mcr formulates image difference alignment modification text generation problem. inputs features reference target images lstm, attempting generate modification text directly. commonly used cross-entropy loss text generation adopted optimize process. furthermore, enhance image difference alignment, neucore designs multimodal concept alignment, targets mining aligning visual concepts reference target images semantic concepts modification text. one hand, neucore extracts keywords modification text semantic concepts, embedding using glove hand, learns visual concepts present reference target images using transformer-based model. since modification text typically concise contains limited semantic concepts, whereas images convey wealth visual concepts, neucore employs asymmetric loss supervise multimodal concept alignment. loss function ensures effective alignment accounting inherent imbalance richness concepts textual visual modalities, formulated follows vrt wc, sisi nsj sj, subscript refers -th triplet sample dataset, vrt represent joint visual concept feature reference-target image pair embedding one semantic concept extracted modification text, respectively. positive negative sets, respectively. hyper-parameters balance importance positive negative concepts, respectively. manuscript submitted acm comprehensive survey composed image retrieval negative mining. mainstream bbc loss helps models learn associations composed queries target images, treats examples within batch equally negative samples. leads issue false negative samples, cir tasks, query might correspond multiple target images, even though one annotated positive example. moreover, loss overlooks varying impact different negative samples metric learning. intuitively, using hard negative samples, i.e., negative examples particularly challenging classify, significantly benefit model optimization. address limitations, researchers proposed several negative mining techniques. deal false negative issue, tg-cir first utilizes visual similarity distribution ground-truth target image features candidate image features within batch regularize models metric learning. addition, nsfse flexibly learns boundaries matched triplets mismatched triplets using gaussian distributions, flexible threshold learned distinguish positive target images negative ones. furthermore, sadn first calculates similarity composed query candidate target image select top-most relevant candidate images neighborhood. adaptively aggregates features neighborhood target images refine query feature. incorporation neighborhood target features effectively mitigates adverse impact caused false negative samples. effectively mine hard negative samples, provla introduces moment queue-based hard negative mining mechanism uses momentum-based distillation dynamically store recent embeddings composed images, reference images, target images. stored embeddings serve hard negative samples triplets, enabling selection hard negatives across multiple batches. later, instead constructing conventional query-level hard negative samples, mcem proposes two strategies generating component-level hard negative samples. first strategy involves directly replacing entire modification text training triplet create hard negative sample. second strategy narrows replacement scope replacing partial dimensions modification text embedding, resulting challenging negative samples. particular, mcem introduces mask vector controlled bernoulli distribution parameter modification text embedding replacement. parameter controls similarity newly generated modification text embedding original version. uncertainty modeling. existing cir datasets, one target image per query annotated. however, mentioned earlier, inherent ambiguity arising general modification text often leads many-to-many relationships input queries target images. address limitation, ranking-aware introduces novel ranking-aware uncertainty approach, employs stochastic mappings instead deterministic ones capture many-to-many correspondences. specifically, images text encoded deterministic features distributions feature space. approach optimizes many-to-many ranking three key components in- sample uncertainty, cross-sample uncertainty, distribution regularization. contrast, cir-mu sdqur retain one-to-one matching paradigm integrating gaussian-based uncertainty modeling uncertainty regularization accommodate diverse retrieval requirements. uncertainty modeling simulates true range uncertainty within effective domain, estimated based feature distribution within mini-batch. additionally, uncertainty regularization prevents model excluding potential true positives, thereby improving recall rates. re-ranking. illustrated figure existing cir methods primarily adopt dual branches one branch encodes query, encodes target image perform target image retrieval. architecture ensures efficient inference since embeddings candidate target images pre-computed. model needs embed given test query compare pre-computed candidate image embeddings. however, approach relies solely metric learning module bbc loss function regulate target image manuscript submitted acm xuemeng song al. query. due issues like false negatives modification ambiguity, retrieval performance current cir models still room improvement. address this, several studies proposed re-ranking techniques cir tasks. instance, re-ranking introduces dual-encoder architecture re-rank initial retrieval results obtained conventional dual-branch cir models. specifically, one encoder jointly encodes given query candidate target image, jointly encodes modification text candidate target image. mlp network fuses outputs two encoders compute final ranking score. contrastive loss, similar bbc loss, used optimize re-ranking module. strategy allows candidates target image interact given query deeply comprehensively. importantly, since re-ranking performed subset candidate images selected conventional dual-branch cir models, sophisticated strategy computationally feasible inference. additionally, vqacir re-ranks retrieval results querying multimodal large language model mllm, e.g., llava determine whether candidate images contain desired attributes specified modification text. method serves post-processing approach seamlessly integrated existing cir model. data augmentation mentioned earlier, existing cir datasets typically consist triplets form reference image, modification text, target image. however, creating training samples expensive labor-intensive, significantly limits size benchmark datasets. result, previous research relying solely limited samples faced overfitting issues extent demonstrated poor generalization capabilities. overcome challenge, researchers proposed various data augmentation strategies. image replacement-based. clip-cd introduces clip visual similarity-based data augmentation method replaces reference target images triplets visually similar alternatives generate pseudo triplets, effectively enlarging dataset. notably, establishes lower upper similarity thresholds ensure quality relevance augmented samples. idc model-based. many potential reference-target image pairs existing datasets remain unlabeled, despite highly similar differing minor properties. utilize unlabeled pairs enhance model performance, limn introduces iterative dual self-training paradigm. approach employs dual model cirspecifically, image difference captioning idc model automatically annotate pairs, generating pseudo-triplets improved model training. ensure quality pseudo-triplets generated iteration, cir model trained previous iteration filters triplets low query-target matching scores. llm-based. leveraging advanced image comprehension capabilities large multi-modal model gptv, iudc employs specially designed prompts guide gptv generating attribute-level labels image training triplets. attribute-level labels combined tf-idf features identify potential reference-target image pairs. subsequently, chatgpt used generate corresponding modification texts based attribute-level labels image pairs, enabling construction large volume triplet data. additionally, sda uses chatgpt generate pseudo modification texts editing specific attributes original modification text. gptv generates target image based original reference image generated modification text, thereby creating new triplets assist model training. manuscript submitted acm comprehensive survey composed image retrieval table summarization main zero-shot composed image retrieval approaches. category method year encoder key aspect textual-inversion-based picword clip-l coarse-grained inversion searle clip-bl coarse-grained inversion isearle clip-bl coarse-grained inversion keds clip-l knowledge enhancement context-iw clip-l context-dependent inversion fticir clip-l fine-grained inversion isa blip adaptive inversion lincir clip-lhg self-masking projection pseudo-triplet-based transagg blip-b, clip-bl transformer-based hycir blip-b, clip-b additional training stream mcl clip-l mllm-based magiclens coca-bl, clip-bl transformer-based rtd target-anchored contrastive learning compodiff clip-lg diffusion-based pvlf blip prompt learning mti clip-bl masked learning clip-l masked learning training-free cirevl clip-blg language-level reasoning grb blip coarse-fine reranking ldre clip-blg divergent reasoning seize clip-blg divergent reasoning slerp blip-l, clip-bl spherical linear interpolation weimocir clip-lhg weighted modality fusion query unification-based. fully utilize vlp models like clip mitigating overfitting, dqu-cir introduces two raw query unification methods text-oriented query unification vision-oriented query unification. text-oriented query unification, modification text combined textual description reference image, extracted using vlp model blip-, create purely textual query. vision-oriented query unification, key modification words directly written onto reference image pixel level, forming purely visual query. notably, image encoder vlp model demonstrates strong optical character recognition ocr capabilities, enabling effectively process modified images. reverse objective-based. case blipcir expand dataset incorporating reverse retrieval objective, i.e., retrieving reference image given modification text target image. achieve introducing reverse objective token, rev backward, specify retrieval direction. reverse objective intuitively encourages model learn shift vector reference image target image directions, enhancing ability generalize across tasks. gradient-based. unlike aforementioned data augmentation methods, improves models general- ization ability gradient augmentation rather raw data augmentation. specifically, comprises two components explicit adversarial gradient augmentation implicit isotropic gradient augmentation. explicit adversarial gradient augmentation introduces gradient-oriented regularization term loss function simulate adversarial sample training. implicit isotropic gradient augmentation increases triplet diversity modifying gradients according principle isotropy. zero-shot composed image retrieval although supervised cir approaches achieved favorable performance, rely heavily annotated triplets form reference image, modification text, target image training. however, annotating modification text possible reference image, target image pair time-consuming process. reduce dependence labeled datasets, picword introduces zs-cir, aims perform retrieval without requiring annotated training triplets. manuscript submitted acm xuemeng song al. existing zs-cir methods broadly categorized three groups textual-inversion-based, pseudo-triplet-based, training-free. ease reference, table summarizes methods across three categories. textual-inversion-based methods category begin using text inversion technology map reference image embeddings textual token representations. tokens combined modification text form unified query. query subsequently encoded using text encoder vlp model, like clip, enabling image-text fusion. coarse-grained textual inversion. picword pioneering method category, introducing task zs-cir. leverages collection unlabeled images train lightweight mapping network transforms image embeddings, obtained clip visual encoder, token embeddings compatible clip text encoder. way, image represented pseudo-token-based sentence, photo denotes learnable pseudo-word. mapping network optimized using contrastive loss image feature corresponding pseudo-token-based sentence embedding. around time, searle proposed two approaches optimization-based textual inversion method oti mapping network-based method, aimed learning pseudo-word tokens encapsulate visual content image. unlike picword, methods incorporate category-based semantic regularization align pseudo-word tokens clip token embedding space, ensuring compatibility real textual tokens. building searle, isearle introduces gaussian noise text features oti reduce modality gap text image. moreover, enhance mapping networks ability capture visual contents, isearle implements similarity clustering-based hard negative sampling strategy, ensuring training batch contains proportion visually similar images. furthermore, keds introduces bi-modality knowledge-guided projection network bkp, leverages external database provide relevant image-caption pairs knowledge, enriching mapping function improving generalization ability. additionally, similar searle, recognizing challenges aligning pseudo-word tokens real text concepts using image contrastive training, keds introduces additional training stream explicitly aligns pseudo-word tokens semantics using pseudo triplets uncovered image-caption pairs. overall, aforementioned methods perform coarse-grained textual inversion, simply converts input image single general pseudo-word token, utilizing entire visual content image without differentiation. fine-grained textual inversion. beyond methods, studies explored fine- grained textual inversion enhance performance zs-cir. instance, instead converting entire visual content image, context-iw adaptively selects caption-relevant content textual inversion using context-dependent word mapping network. employs intent view selector map given image task-specific manipulation view visual target extractor collect content related specific view. instead converting image single pseudo-word token, fticir maps image subject-oriented pseudo-word token along several attribute-oriented pseudo-word tokens comprehensively represent image textual form. additionally, introduces tri-wise semantic regularization method based blip-generated image captions, aligning fine-grained pseudo-word tokens real-word token embedding space. isa akin fticir, converts image series sentence tokens rather single pseudo-word token. incorporates spatial attention mechanism-based adaptive token learner select prominent visual patterns image. moreover, isa adopts asymmetric architecture optimize deployment resource-constrained environments. methods manuscript submitted acm comprehensive survey composed image retrieval exhibit promising generalization capabilities unseen datasets, rely fixed pre-defined text prompt e.g., photo training. limitation reduces ability handle diverse textual conditions encountered real-world applications. address issue, lincir introduces self-masking projection, trains language-only mapping network capable projecting given text pseudo-token-based embedding flexibly replacing keywords i.e., consecutive adjectives nouns text projected latent embedding text. minimizing mse loss pseudo-token-based embedding projected latent text embedding, pseudo-tokens keywords effectively encapsulate essential information input text. bridge modality gap inference, lincir introduces random noise addition strategy, adapting language-only mapping network handle visual input seamlessly. pseudo-triplet-based studies category aim address zs-cir automatic pseudo-triplet generation methods. existing pseudo- triplet generation methods zs-cir categorized two main classes llm-based triplet generation mask-based triplet generation. llm-based triplet generation. reduce reliance manual annotation, several studies leverage advanced logical reasoning capabilities llms automatically generate pseudo triplet data. image-text-based. among methods, transagg first use llms generating pseudo-triplets set image-caption pairs, laion-coco dataset. specifically, given image-text pair, treats image reference image generates modification text target image caption based provided caption, using either carefully crafted template llm. drawing inspiration transagg introduces eight types semantic operations cardinality, addition, negation, direct addressing, comparechange, comparative statement, conjunction-based statements, viewpoint, guide modification text generation. uses generated target image caption query retrieve relevant images laion-coco dataset calculating semantic similarity target caption images caption. retrieved images serve target images forming pseudo-triplets, alongside reference image generated modification text. mcl follows similar pseudo-triplet generation strategy transagg instead using target image retrieved based generated target caption, directly uses clip feature generated target caption supervision training cir model. image-based. instead relying image-text pairs, hycir generates pseudo-triplets purely unlabeled image dataset, coco process involves four steps extracting potential reference- target image pairs using image captioning model generate captions images generating modification text using llm based two captions filtering triplets low semantic similarity. notably, ensure generated triplet data compatible previous mainstream zs-cir methods, textual-inversion-based methods, hycir extends existing picword method. introduces additional training stream integrates pseudo-word tokens mapped reference image modification text, forming unified text query. query used supervise text query representation target image representation using contrastive loss. instead relying visual similarity, magiclens extracts potential reference-target image pairs mining images webpage, manuscript submitted acm xuemeng song al. implicit relationships often exist. annotates image detailed descriptions, including alt- texts, image content annotation ica labels, captions generated large multimodal model, pali finally, palm used generate open-ended modification text image pair based detailed descriptions. ensure generated modification text logical, techniques instruction-following few-shot demonstrations chain-of-thought prompting employed. text-based. inspired powerful generation capabilities diffusion models, al. propose generative approach construct pseudo triplets. specifically, aim first generate text triplets, structured reference caption, modification text, target captionusing two strategies collecting large number captions existing caption datasets generating target caption substituting keywords reference caption, deriving modification text based randomly sampled pre-defined template generating text triplets llm i.e., opt-.b fine-tuned text triplets existing image editing study instructpixpix subsequently, reference target images generated based respective captions using text-to-image generation model, e.g., stablediffusion worth noting work also develops diffusion-based cir method, compodiff, handle various modification cases enable control modification strength. additionally, textual triplets generated work adopted subsequent work designs plug-and-play target-anchored text contrastive learning method finetuning clip text encoder, thereby improving performance textual-inversion-based zs-cir. mask-based triplet generation. aforementioned llm-based triplet construction methods efficiently generate large volumes pseudo-triplet data, often require excessive computational resources. address issue, researchers explored mask-based triplet generation strategies resource- efficient. mti representative work area. specifically, given image-caption pair, mti treats given image target image randomly masks certain portions derive corresponding reference image. provided caption, encapsulates predominant content image, used modification text help reconstruct masked image back original form. contrast random masking, introduces class activation map cam-guided masking strategy better mimic complementary roles reference image modification text cir. specifically, begins replacing first noun given caption remove create modification text. calculates cam matrix given image, identifying regions relevant masked noun. regions masked. unlike mti, uses simple color blocks mask image, replaces masked regions corresponding regions another image within batch, ensuring completeness reference image. training-free given flexibility scalability, increasing body research focused solving zs-cir training-free manner modular combinations. approaches leverage existing models, llms vlp models, address cir task without need additional model training. existing methods broadly categorized two branches one focuses transforming cir task caption-to-image retrieval task using powerful llms, manuscript submitted acm comprehensive survey composed image retrieval handled pre-trained encoders vlp models. branch focuses directly mining pre-trained common embedding space vlp models address cir task. task transformation. methods branch employ llms generate target image captions, used retrieve target image via pre-trained encoders vlp models. example, cirevl introduces modular framework vlp model generates description reference image. subsequently, llm combines descriptions modification text infer target images caption, serves basis image retrieval. due nature cir, reference image modification text often involve conflicting semantics, inferred target image caption may contain concepts appear target image. instance, given modification text change dog cat, concept dog appear target image. address this, grb enhances cirevl introducing local concept re-ranking lcr mechanism ensure retrieved images contain correct local concepts. specifically, grb performs initial retrieval based inferred target caption, extracts local concepts present target image using llm e.g., chatgpt-turbo modification text. verify presence local concepts, grb uses llava visual question answering vqa top-retrieved results. predicted probabilities outputting text yes used local scores re-rank retrieved images. moreover, since cir inherently fuzzy retrieval taskwhere target images semantics fully captured input queryldre introduces llm-based divergent compositional reasoning approach. method generates multiple diverse target captions instead single one, capturing wider range possible semantics within target image. complete zs-cir task, ldre also introduces divergent caption ensemble technique combine clip embeddings generated captions retrieve target image accordingly. pre-trained space mining. branch training-free methods focuses leveraging pre-trained common embedding space vlp models, clip blip. vlp models primarily optimized using normalized temperature-scaled cross-entropy loss cosine similarity, results image text embeddings residing joint hypersphere radius determined scaling factor i.e., temperature parameter. weimocir directly employs simple weighted sum combine reference image feature, extracted visual encoder, modification text feature, extracted text encoder, derive query feature. improve retrieval performance, weimocir calculates score candidate image considering query-to-image query- to-caption similarities. utilizes mllm, gemini generate multiple captions candidate image, providing different perspectives image. contrast, slerp applies spherical linear interpolation obtain fused embedding calculating intermediate embeddings reference image embedding, denoted modification text embedding, denoted derived vlp encoders. formulated follows sin sin sin sin cos balancing scalar value within range fused embedding obtained spherical linear interpolation directly used target image retrieval. related tasks composed image retrieval beyond primary task cir, researchers explored various related tasks cater diverse retrieval needs real-world scenarios. here, present five representative related tasks involve different types multimodal manuscript submitted acm xuemeng song al. table summary representative approaches related tasks composed image retrieval. related task method year visual encoder text encoder key aspect attribute-based amnet alex, vggnet memory-augmented eitree resnet- blstm ei-tree fashionsearchnet alexnet attribute localization emasl alexnet attribute localization amgan generator-encoder gan-based adde alexnet, resnet attribute-driven disentangled firam stylegan gan-based sketch-based tsfgir cnn lstm quadruplet deep network task-former clip-b clip-b auxiliary tasks learning scenetrilogy vgg- bi-gru conditional invertible networks stnet clip clip auxiliary tasks learning stdfgir clip-l clip-l textual-inversion-based remote sensing-based weicom clip-l, remoteclip-l clip-l, remoteclip-l weighted average shf resnet- lstm hierarchical fusion dialog-based diir resnet- mlpcnn reinforcement learning cfir resnet-, resnet- self-attention blocks mutual attention strategy fashionntm fashionvlp fashionvlp cascaded memory irr fashionbert clip-b iterative sequence refinement fashion-gpt swin transformer roberta llm-integration llmms clip-l flan llm-integration video-based covr-blip clip, blip clip, blip blip-based fusion ecde blip blip video contextual complement tfr-cvr blip, clip-l blip, clip-l language-level reasoning covr-blip blip- blip- blip-based fusion queries attribute-based, sketch-based, remote sensing-based, dialog-based, video-based. ease reference, summarize methods proposed tasks table attribute-based formal introduction natural language modification-based cir, another flexible image retrieval taskwhere query involves reference image alongside attribute-based modificationshad garnered significant attention. task, constrained predefined attributes, primarily finds applications domains structured attribute sets, fashion face image retrieval. zhao al. pioneered task proposing amnet, comprises memory block storing representations various attribute values. attribute manipulation achieved directly retrieving specific attribute representations memory block fusing representation reference image retrieve target images. similarly, adde also incorporates memory block storing attribute values. unlike amnet, adde devises attribute-driven disentanglement module, uses attributes supervised signals guide learning disentangled image representations. indexing appropriate attribute representations memory block, disentangled representations modified removing, retaining, adding specific attribute values, enabling effective attribute manipulation. recognizing different attributes correlated different regions fashion images, fashionsearchnet employs attribute activation maps extract region-specific attribute features. fashion item represented set attribute features, attribute manipulation accomplished directly replacing specific attribute feature. sharing similar spirit, emasl extracts different part features acquisition attribute features based designed rules. example, upper part associated collar, side parts correlated leftright sleeves. eitree enhances interpretability organizing fashion concepts hierarchical tree structures. guided tree structure, eitree generates meaningful image representations dimension corresponds specific fashion concept. structure allows seamless integration concept-level user feedback, attribute manipulation, interpretable representation fashion items. building success generative models image editing, amgan introduces end-to-end generative attribute manipulation framework. framework generates manuscript submitted acm comprehensive survey composed image retrieval prototype image aligns user-desired attribute modifications reference image improve target image retrieval. amgan consists generator discriminator generator employs visual-semantic pixel-wise consistency constraints, discriminator incorporates semantic learning precise attribute manipulation adversarial metric learning enhance fashion search effectiveness. focusing face image retrieval, firam leverages gans framework design. unlike amgan, generates prototype image directly, firam aims learn sparse orthogonal basis vectors within stylegans latent space. approach disentangles attribute semantics, allowing independent attribute adjustment preference assignment. sketch-based many studies focused combining sketch text descriptions assist users retrieving target images. among these, tsfgir pioneers exploration complementarity text sketch modalities. introduces multi-modal quadruplet deep neural network encourages input sketch text closer corresponding positive target image negative target image. additionally, work contributes dataset consisting sketch-photo-text triplets shoes. model explicit interactions sketch text inputs, task-former extends dual-encoder-based clip framework incorporating sketch encoder two new pretraining objectives multi-label classification, enabling three encoders recognize objects, caption generation, reconstructs input text joint sketch-text embeddings computed simple addition text sketch embeddings. complex scenarios involving rough sketches paired complementary text descriptions, gatti al. present dataset comprising approximately queries natural scene images. also propose multimodal transformer-based framework, stnet. like task-former, stnet employs three clip encoders text, sketch, image modalities. however, beyond contrastive learning, incorporates three task-specific pretraining objectives object classification, sketch-guided object detection, sketch reconstruction. distinct aforementioned methods, stdfgir draws inspiration textual inversion techniques transforming input sketch pseudo-word token, allowing multimodal input directly encoded clip text encoder. address challenge costly data annotation sketch-based fine-grained image retrieval, stdfgir trained using sketch-image pairs. treats difference image embedding sketch embedding proxy missing text query introduces compositionality constraint model relationship. achieve fine-grained matching composed query target image, stdfgir incorporates two innovative loss functions region-aware triplet loss precise alignment sketch-to-photo reconstruction loss enhance representation learning. worth noting stdfgir applied various applications, object-sketch-based scene retrieval, domain attribute transfer, sketchtext-based fine-grained image generation. parallel, scenetrilogy focuses learning flexible joint embedding capable supporting combination modalitiessketches, images, textas queries diverse retrieval captioning tasks. leveraging conditional invertible networks, scenetrilogy disentangles feature representation input sketch, text, photo two components modality-agnostic part modality-specific part. modality-agnostic parts across three modalities aligned using contrastive loss enable cross-modal retrieval, modality-specific parts optimized self-reconstruction, ensuring effective representation learning. remote sensing-based recent years, earth observation via remote sensing experienced significant increase data volume, posing challenges managing extracting pertinent information. enhance search capabilities greater expressiveness manuscript submitted acm xuemeng song al. flexibility, psomas al. introduce concept cir remote sensing build benchmark dataset. pioneer study, focuses attribute-based cir, aims retrieve target images share classes given reference image possess desired attributes specified text description. work designs training-free method, weicom, introduces modality control parameter balancing importance normalized image-oriented text-oriented similarities. pretrained clip remoteclip explored feature embedding. beyond work, wang al. study remote sensing image retrieval natural language-based text feedback. recognizing previous studies cir primarily focused intrinsic attributes target objects, neglecting crucial extrinsic information spatial relationships remote sensing domain, wang al. propose scene graph-aware hierarchical fusion network shf. approach incorporates remote sensing image scene graphs supplementary input data enhancing structured image representation learning target image retrieval. shf employs two-stage multimodal information fusion process. first stage, fuses features remote sensing image scene graph remote sensing reference image multiple levels comprehensively capture visual content. second stage, modification text features fused final scene features first stage using content modulator style modulator, effectively capture apply content style changes. dialog-based task dialog-based cir proposed address challenge users often struggle initially express intentions clearly provide detailed descriptions objects interest. unlike traditional single-turn cir, allows users iteratively refine queries find satisfactory item. necessitates models integrate historical retrieval data current query effectively locate target image. towards end, diir frames task reinforcement learning problem, dialog system rewarded enhancing rank target image within dialog turn. instead using real user interact train dialog system, introduces user simulator based existing relative image captioning model show, attend, tell user simulator trained newly collected dataset amazon mechanical turk, discriminative relative captions manually annotated within shopping chatting scenario. notably, since data annotation training multi-turn simulator quite expensive, work explored single-turn user simulator. enrich decision-making process retrieving target image, cfir designs three-way rnn-based framework, evaluates matching score candidate target image three perspectives visual similarity composed query candidate image, alignment reference-target image difference representation textual feedback representation alignment attribute representations candidate target image textual feedback representation. meanwhile, cfir contributes large-scale multi-turn dataset, named multi-turn fashioniq, fashioniq integrating multiple single-turn triplets. inspired exceptional ability neural turing machines ntms handling complex long-term relationships, fashionntm introduces cascaded memory neural turing machine cm-ntm multi-turn fashion image retrieval. comprising controller, individual readwrite heads, memory blocks, cm-ntm sequentially processes multi-turn queries cascaded mechanism, blocks input comprises current turns query feature output preceding block, supporting modeling intricate relationships within input sequences. beyond previous studies, irr proposes generative conversational composed retrieval framework, formulates task multimodal token sequence alternating reference images modification texts historical turns. irr aims autoregressively predict target image feature based historical session data gpt decoder recently, llmms manuscript submitted acm comprehensive survey composed image retrieval fashion-gpt integrate llms retrieval system realize dialog-based cir. specifically, llmms employs q-former blip translate image information textual pseudo-tokens adopt llm take account query information retrieving target image. adapt model specific task preserving knowledge, llmms employs lora techniques query value matrices self-attention cross-attention layers within efficient fine-tuning. towards building commercial fashion retrieval system, fashion-gpt integrates chatgpt pool retrieval models fashion domain handling users diverse retrieval demands. video-based composed video retrieval covr aims retrieve target videos given reference imagevideo modification text. ventura al. first introduce task develop models, named covr-blip covr-blip, adapting blip blip covr tasks, respectively. since video represented sampled frames, models able address cir covr simultaneously. work also contributes dataset named webvid-covr million triplets leveraging llm automatically generate modification text two similar videos, identified similar captions. one key issue dataset sample modifications mainly regarding static colorshapeobject changes, need temporal understanding. address issue, hummel al. introduce egocvr, manually curated dataset queries. additionally, present train-free method tf-cvr similar previous cir model cirevl tf-cvr involves video captioning model generate caption reference video. uses llm combine video caption modification text obtain target video caption employs existing text-to-video model perform cross-modal retrieval target videos. avoid selecting semantically similar visually unrelated videos, introduce similarity-based filtering strategy first narrow scope candidate videos tf-cvr rank. mine rich query-specific context promoting target video retrieval, ecde introduces novel covr framework. framework uses detailed language descriptions given reference video derived multi-modal conversation model additional input explicitly encode query-specific contextual information. discriminative embedding learning, ecde aligns joint multimodal embedding input query conventional visual embedding target video, also aligns text embedding vision-text joint embedding target video, respectively. benchmarks experiments section, first present public datasets related cir, provide experimental results analyses representative methods. datasets. statistics datasets cir related tasks summarized table fashioniq. fashioniq dataset natural language-based interactive fashion retrieval dataset, crawled amazon.com. provides human-generated captions distinguish similar pairs garment images together. fashion items within dataset belong three categories dress, shirt, toptee. contains images triplets, images triplets training set, images triplets validation set, images triplets test set. challenge dataset, test set publicly accessible. notably, fashioniq comprises two evaluation protocols val-split original-split val-split introduced early-stage cir study, constructs candidate image manuscript submitted acm xuemeng song al. table statistics datasets composed image retrieval related tasks. dataset data type vision scale triplet scale triplet construction datasets composed image retrieval fashioniq imagetext human annotation shoes imagetext human annotation cirr imagetext human annotation imagetext human annotation circo imagetext human annotation genecis imagetext human annotation fashionk imagetext template-base generation mit-states imagetext template-base generation css imagetext template-base generation synthtripletsm imagetext template-base generation lasco imagetext llm-base generation datasets related tasks composed image retrieval shoppingk imageattributes template-base generation webvid-covr videotext llm-base generation fs-coco sketchimagetext template-base generation sketchycoco sketchimagetext template-base generation cstbir sketchimagetext template-base generation patterncom imagetext template-base generation airplane, tennis, whirt imagescene graphtext human annotation multi-turn fashioniq imagetext human annotation vision scale means scale imagessketch-image pairsvideos. triplet scale multi-turn fashioniq refers number sessions. set testing based union reference images target images triplets validation set. original-split recently adopted, directly uses original candidate image set provided fashioniq dataset testing. shoes. shoes dataset originally collected like.com attribute discovery task developed relative caption annotations dialog-based interactive retrieval. annotations gathered via human annotation using interactive interface, allows fine-grained attribute descriptions. dataset includes categories boots, sneakers, high heels, clogs, pumps, rain boots, flats, stilettos, wedding shoes, athletic shoes. overall, dataset comprises images triplets, images triplets training, images triplets testing. cirr. cirr dataset, introduced liu al. open-domain dataset constructed using images sourced natural language reasoning dataset nlvr cirr comprises triplets divided training, validation, test sets allocation ratio alleviate false negative issue, cirr first clusters similar images subsets based visual similarity reference-target image pairs construction. then, subsequent process annotating modification text reference-target image pairs, semantics modification text must differentiate target image similar images within subset. specifically, given subset contains samples high visual similarity target image, testing retrieval subset places greater demands models discriminative ability. bw. birds-to-words dataset consists images birds sourced inaturalist, accompanied paragraphs written humans describe differences pairs images. dataset contains approximately images triplets. notably, text description relatively detailed, average length words, providing rich insights subtle variations across bird images. circo. circo dataset open-domain dataset developed coco unlabeled set address false negative issues prevalent existing datasets. unlike typical cir datasets, circo includes multiple target images per query, thus significantly reducing occurrence false negatives establishing first manuscript submitted acm comprehensive survey composed image retrieval cir dataset multiple ground-truth target images. given circo specifically designed evaluating zero- shot cross-image retrieval models, comprises queries partitioned validation set test set. specifically, queries allocated validation purposes, remaining testing. query includes reference image, modification text, average ground truth target images. utilizing images coco index set, circo provides vastly larger number distractors compared images cirr test set. genecis. genecis open-domain cir dataset serves benchmark evaluating conditional similarity tasks. dataset includes four subsets focus attribute, change attribute, focus object, change object, representing four different tasks. among these, focus object change object constructed based coco dataset, focus attribute change attribute constructed based vaw dataset. unlike open-domain datasets, cirr circo, provide modification text, genecis provides single object name attribute retrieval condition. reduce impact false negatives, gallery selected triplet retrieval candidate set, gallery sizes ranging images. dataset consists triplets. fashionk. fashionk dataset, collected han al. comprises clothing images, categorized five types dress, top, pants, skirt, jacket. image comes compact attribute-like product description, black biker jacket. dataset divided three parts images training set, images validation set, images test set. construct triplets suitable cir task, existing works first create image pairs identifying one-word differences descriptions. modification text generated using templates incorporate differing words, replace red green. based construction method, triplets available training triplets evaluation. mit-states. mit-states dataset features diverse collection objects, scenes, materials various transformed states. contains images, image tagged adjective state label noun object label e.g., new camera cooked beef. dataset encompasses nouns adjectives, noun modified roughly different adjectives average. following image pairs share object labels different attributes states selected reference target images, different attributes states serve modification text. evaluate models capacity handling unseen objects, nouns used test rest training. css. css dataset constructed using clevr toolkit generate synthesized images -by- grid scene, showcasing objects variations color, shape, size. image available simplified blob version detailed rendered version. dataset comprises queries training queries test. query reference image modification, target image. notably, modification texts fall three categories adding, removing, changing object attributes. examples include add red cube remove yellow sphere. synthtripletsm. synthtripletsm large-scale dataset specifically designed cir task. distinct datasets relying human annotation, synthtripletsm uses diffusion models automatically create triplets consisting reference images, modification instructions, target images. adopting approach instruct pixpix dataset initially creates caption triplets modifying reference captions specific instructions converts triplets image-based triplets using diffusion models, resulting highly diverse realistic data. dataset encompasses broad spectrum keywords, enhancing suitability open-domain cir tasks, manuscript submitted acm xuemeng song al. provides rich textual prompts replace target. automated generation process enables creation rare diverse image triplets might commonly appear reality. lasco. lasco large-scale, open-domain dataset consisting natural images, specifically designed cir task. created minimal human effort leveraging vqa dataset lasco leverages complementary image pairs, similar images yield different answers question, transforms question- answer pairs valid transition texts using gpt-. exploiting transition symmetry, dataset amassed images image pairs, organized triplets. shoppingk. shoppingk dataset consists pure clothing images characterized attributes possible attribute values. following pairs images differ one two attributes utilized reference target images. differing attributes serve modification attributes, facilitating construction triplet data attribute-based cir tasks. approach results training triplets testing triplets, forming robust foundation developing evaluating cir models focused nuanced fashion attributes. webvid-covr. webvid-covr large-scale dataset designed covr, containing triplets. video dataset averages seconds duration, modification texts consist roughly words. target video linked triplets, dataset offers rich contextual variations essential effectively training covr models. ensure robust evaluation, introduce meticulously curated test set known webvid-covr-test manually annotated consists triplets. fs-coco. fs-coco large-scale dataset freehand scene sketches paired textual descriptions corresponding images, designed advance research fine-grained scene sketch understanding. consists unique sketches drawn non-experts, training testing, matched photo ms-coco dataset descriptive caption. fs-coco includes object categories coco-stuff provides temporal order information strokes, enables detailed studies scene abstraction salience early vs. late strokes. dataset serves benchmark fine-grained image retrieval, sketch-based captioning, understanding complementary information sketches text. sketchycoco. sketchycoco large-scale composite dataset tailored task automatic image generation scene-level freehand sketches. built coco-stuff dataset includes unique scene- level sketches paired corresponding images textual descriptions, organized sketch-text-image triplets, training remaining testing. additionally, dataset includes triplet examples foreground sketches, images, edge maps covering classes, along background sketch-image pairs covering classes. layered structure facilitates detailed studies scene-level sketch-based generation provides five-tuple data comprehensive training foreground background synthesis tasks. cstbir. composite sketchtext based image retrieval cstbir dataset multimodal dataset specifically designed image retrieval using sketches partial text descriptions. includes natural scene images annotated triplets, containing reference sketch, partial text description, target image. natural images text descriptions sourced visual genome dataset sketches hand-drawn quick, draw! dataset intersecting object categories visual genome quick, draw!, dataset contains intersecting object classes divided training, validation, testing sets aligned visual genomes splits. training set comprises images, sketches, queries. additionally, cstbir includes three distinct test sets test-k, test-k, open-category set. among them, open-category test manuscript submitted acm comprehensive survey composed image retrieval set designed evaluate model performance novel object categories present training, contains novel object categories difficult-to-name corresponding sketches. patterncom. patterncom new benchmark designed evaluating remote sensing cir meth- ods, based patternnet dataset large-scale, high-resolution remote sensing image collection. patterncom focuses selected classes patternnet, incorporating query images corresponding text descriptions define relevant attributes class. example, swimming pools category includes text queries specifying shapes rectangular, oval, kidney-shaped. dataset encompasses six attributes, attribute linked four different classes two five values per class. overall, patterncom contains queries, positive matches ranging per query. airplane, tennis, whirt. airplane, tennis, whirt datasets curated remote sensing cir. dataset organized terms quintets, consisting reference image scene graph, target image scene graph, pair modifier sentences. airplane dataset comprises remote sensing images ucm patternnet nwpu-resisc along pairs modifier sentences describe differences airplane attributes spatial relationships airplanes objects. tennis dataset includes images featuring tennis courts manually annotated modifier sentence pairs. emphasizes target non-target spatial relationships ignoring relationships non-target objects. whirt dataset extensive, consisting images whdld reference-target image pairs. scene graphs provide comprehensive details object attributes spatial relationships, accommodating complex remote sensing scenarios. dataset features high-quality annotations domain experts, serving robust benchmarks advanced retrieval tasks intricate remote sensing environments. multi-turn fashioniq. multi-turn fashioniq extension original fashioniq dataset designed model user interactions multi-turn setting fashion product retrieval. contains sessions structured multi-turn interaction, across three clothing types dress, shirt, toptee. session consists multiple reference images, modification texts, target image, turns ranging sessions constructed linking single-turn triplets fashioniq, matching target image one triplet reference image another, thus forming coherent multi-turn sequences. additionally, dataset expands original attribute data ensure comprehensive coverage, associating target image attributes like texture, fabric, shape, part, style. metric. recall. recall widely used metric cir evaluate effectiveness retrieval systems. often denoted recallr, measuring proportion queries correct target image retrieved within top results. recallcan defined following formula recall denotes total number queries, ris set relevant target images query dis set top retrieved items query represents number target images found top results, represents total number target images query notably, existing cir datasets, query typically corresponds one target image, i.e., except circo. additionally, cirr defines recallto assess frequently desired target image appears top results considering specific subset images. manuscript submitted acm xuemeng song al. table performance comparison among supervised composed image retrieval models fashioniq val split. model dresses shirts topstees average avg. traditional encoder-based methods lsctcir cvprw cirplant iccv val cvpr datir synthtripletgan arxiv valjpm mcr cosmo cvpr acnet icme nsfse tmm sac wacv artemis iclr eer tip manme tcsvt mlclsap tmm mcem tip clvc-net sigir crn-base tip alret-small tmm crn-large tip amc tomm clvc-netmu iclr comqueryformer tmm cmap tomm css-net kbs sdfn icassp vlp encoder-based methods clip-probcr icmr fashionvil eccv fashionvlp cvpr dwc aaai syncmask cvpr plcir-base sigir alret-big tmm plcir-large sigir tg-cir fashionern-small aaai spirit tomm limn tpami limn tpami dqu-cir sigir mean average precision. mean average precision map crucial metric evaluating retrieval systems, particularly cases multiple relevant items. initially employed circo metric integrates precision across various ranks yield single, averaged indicator systems effectiveness retrieving relevant items. formula mapis given map min, rel, pis precision rank relis relevance function. relevance function indicator function equals image rank labeled positive equals otherwise. experimental results. subsection, compare analyze supervised cir zs-cir methods reviewed above. supervised composed image retrieval. in-depth insight results supervised cir methods, provide comparison performance various widely used datasets tables datasets include fashioniq, fashionk, mit-states, css, shoes, cirr. notably, fashioniq dataset includes two evaluation manuscript submitted acm comprehensive survey composed image retrieval table performance comparison among supervised composed image retrieval models fashioniq original split. model dresses shirts topstees average avg. traditional encoder-based methods jvsm eccv tirg cvpr alret-small tmm artemis iclr fashionvlp cvpr neucore nipsw pcasm icme dcnet aaai aacl wacv dscn icmr comqueryformer tmm vlp encoder-based methods fad-vlp emnlp plcir-base sigir combiner cvprw calaclipcir sigir caff cvpr clipcir cvpr combinermu iclr alret-big tmm plcir-large sigir ssn aaai iudc tois clipcir tomm clip-cd ajcai iudcaug tois blipcir wacv shaf icic ranking-aware arxiv fame-vilst cvpr blipcirbi wacv spirit tomm fashionern-small aaai sadn calablipcir sigir fame-vil cvpr case aaai re-rankingr tmlr fashionern-big aaai sprc iclr dqu-cir sigir sprcvqa arxiv sdqur tcsvt sprcspn protocols. however, current approaches erroneously intermixed results different protocols model comparison. address issue, conduct detailed inspection methods available source code. organize comparisons val split original split separately ensure fairness. furthermore, recognizing significant impact different encoders model performance, categorize methods two groups utilizing traditional encoders, resnet lstm, employing vlp encoders e.g., clip blip feature extraction backbones. tables, obtain following observations. vlp encoder-based methods generally achieve much better performance comparison traditional encoder-based methods. reason vlp encoders usually larger traditional encoder methods. moreover, vlp encoders typically pre-trained extensive corpora image-text pairs contrastive learning, thereby possessing excellent capabilities multimodal alignment cross-modal retrieval, crucial context cir. also emphasized methods adopt fusion strategy different types encoders. seen tables airet-big vlp encoder attains significantly better performance airet-small traditional encoder. additionally, even using type traditional encoder vlp encoder, version larger parameters typically manuscript submitted acm xuemeng song al. table performance comparison among supervised composed image retrieval models fashionk shoes. model fashionk shoes avg. avg. traditional encoder-based methods tirg cvpr mgf mmm tirgjpmtri jamma dcnet aaai tis tomm tirgjpmmse lbf-small cvpr lbf-big cvpr mcr sac wacv val cvpr jvsm eccv datir fashionvlp cvpr css-net kbs cosmo cvpr acnet icme gscmr tip eer tip neucore nipsw amc tomm mlclsap tmm css-net kbs comqueryformer tmm crn-base tip clvc-net sigir crn-large tip composeaega tip provla iccvw sdfn icassp composeae wacv alret-small tmm artemis iclr nsfse tmm manme tcsvt cmap tomm dscn icmr aacl wacv mcem tip lgli cvprw vlp encoder-based methods plcir-base sigir alret-big tmm iudc tois iudcaug tois plcir-large sigir fashionern aaai spirit tomm limn tpami limn tpami caff cvpr shaf icic tg-cir dwc aaai dqu-cir sigir delivers better performance. example, lbf-big outperforms lbf-small see table fashionern-big surpasses fashionern-small see table results highlight choice image text encoders plays pivotal role context cir, often surpassing importance image-text fusion target matching method design. regarding image-text fusion, various strategies demonstrated strong performance. here, summarize top-performing methods fashioniq-val, fashioniq-ori, fashionk, shoes, cirr, mit-states, css datasets, respectively. notably, ensure fair comparison, prototype methods evaluated, additional modules manuscript submitted acm comprehensive survey composed image retrieval table performance comparison among supervised composed image retrieval models cirr. avg means average method avg traditional encoder-based methods tirg cvpr artemis iclr cirplant iccv mcem tip neucore nipsw nsfse tmm comqueryformer tmm vlp encoder-based methods clip-probcr icmr combiner cvprw ranking-aware arxiv clipcir cvpr blipcir wacv blipcirbi wacv fashionern aaai clipcir tomm dmot accv spirit tomm vista acl ssn aaai dqu-cir sigir sadn tg-cir tg-cirspn case aaai calablipcir sigir re-ranking tmlr sdqur tcsvt sprc iclr sprc iclr sprcspn sprcvqa arxiv table performance comparison among supervised composed image retrieval models mit-states css. model mit-states css avg. rd-to-d rd-to-d traditional encoder-based methods tirg cvpr mgf mmm tis tomm tirgjpmtri tirgjpmmse tirgga tip jamma lbf-small cvpr lbf-big cvpr mcr composeae wacv lgli cvprw composeaega tip gscmr tip data augmentation reranking disabled. results show dqu-cir mlp-based, sdqur cross-attention- based, sprc self-attention-based, gscmr graph-attention-based achieve best performance across datasets. indicates image-text fusion strategy critical component cir methods, sole determinant final performance. moreover, remains challenging identify universally optimal fusion strategy. models integrate additional target matching techniques data augmentation modules consistently demonstrate superior performance comparison original counterparts. example, sprc-vqa, employs visual question answering re-rank retrieval list, outperforms sprc, demonstrated table table limn, manuscript submitted acm xuemeng song al. table performance comparison among zero-shot composed image retrieval models fashioniq original split. model encoder dresses shirts topstees average avg. textual-inversion-based methods searle iccv clip-b isearle arxiv clip-b picword cvpr clip-l searle-xl iccv clip-l lincir cvpr clip-l keds cvpr clip-l isearle arxiv clip-l context-iw aaai clip-l fticir sigir clip-l lincir cvpr clip-h lincir cvpr clip-g isa iclr blip isa iclr blipcnn isa iclr blipvit slerptat arxiv blip pseudo-triplet-based methods magiclens icml clip-b mti arxiv clip-b picwordhycir arxiv clip-l icip clip-l lincirrtd arxiv clip-l magiclens icml clip-l compodiff tmlr clip-l mti arxiv clip-l magiclens icml coca-b magiclens icml coca-l compodiff tmlr clip-g transagg bmvc blip pvlf acml blip training-free methods ldre sigir clip-b seize clip-b weimocir arxiv clip-l ldre sigir clip-l seize clip-l weimocir arxiv clip-h ldre sigir clip-g weimocir arxiv clip-g seize clip-g slerp arxiv blip grb arxiv blip leveraging augmented pseudo-triplets iterative training, attains better outcomes limn shown table table additionally, composeaega, adopts gradient augmentation regularization approach achieve dataset augmentation effect mitigate overfitting, surpasses composeae, seen table table examples emphasize critical significance exploiting advanced target matching strategies implementing dataset augmentation techniques bolster models generalization capabilities. zero-shot composed image retrieval. zs-cir models, including textual-inversion-based, pseudo-triplet-based, training-free models, evaluated fashioniq, cirr, circo datasets. directly summarize experimental results corresponding papers table table worth noting existing imple- mentations zs-cir approach typically rely generalization capabilities vlp-based encoders commonly tested various backbone versions. therefore, enable comprehensive comparison, list performance results methods utilizing different backbones tables. tables, obtain following observations. certain zero-shot methods yield comparable outcomes supervised methods. example, fashioniq-ori dataset, top-performing zs-cir method, lincir clip-g version, attains score averaged across metrics. significantly, outperforms traditional encoder-based supervised methods also manages achieve performance comparable nearly half number vlp encoder-based manuscript submitted acm comprehensive survey composed image retrieval table performance comparison among zero-shot composed image retrieval models cirr circo. model encoder cirr circo map textual-inversion-based methods searle iccv clip-b isearle arxiv clip-b picword cvpr clip-l searle-xl iccv clip-l keds cvpr clip-l lincir cvpr clip-l isearle arxiv clip-l context-iw aaai clip-l fticir sigir clip-l lincir cvpr clip-h lincir cvpr clip-g isa iclr blipcnn isa iclr blipvit isa iclr blip slerptat arxiv blip pseudo-triplet-based methods mti arxiv clip-b magiclens icml clip-b mti arxiv clip-l mcl icml clip-l lincirrtd arxiv clip-l magiclens icml clip-l compodiff tmlr clip-l picwordhycir arxiv clip-l icip clip-l compodiff tmlr clip-g magiclens icml coca-b magiclens icml coca-l transagg bmvc blip picwordhycir arxiv blip pvlf acml blip training-free methods cirevl iclr clip-b ldre sigir clip-b seize clip-b cirevl iclr clip-l weimocir arxiv clip-l ldre sigir clip-l seize clip-l weimocir arxiv clip-h cirevl iclr clip-g weimocir arxiv clip-g ldre sigir clip-g seize clip-g slerp arxiv blip grb arxiv blip grblcr arxiv blip supervised methods. implies even absence manually annotated triplet data, one still obtain satisfactory retrieval results ingeniously devising pre-training strategy fully activating potential powerful vlp capabilities within cir context. typically, zero-shot methods, equipped large backbone, consistently deliver better performance. example, lincir, isa, magiclens, compodiff, ldre, seize, wiemocir confirm larger-scale variants perform effectively. evidences vlp encoders, possess well-trained multimodal encoding cross-modal retrieval capabilities, significantly influence performance zs-cir. generally, among three types zero-shot methods, overall performance methods based pseudo-triplets better. example, fashioniq-ori dataset, regarding average recall textual-inversion-based methods, pseudo-triplet-based methods, training-free methods, number methods showing performance greater respectively. cirr dataset, based corresponding numbers circo dataset, based map numbers manuscript submitted acm xuemeng song al. ascribed fact pseudo-triplet-based methods still construct triplet data similar supervised training paradigm. although group methods somewhat resource-intensive pseudo-triplet construction stage, overall performance also best. discussion future directions introduction cir tasks garnered significant interest due requirement joint reasoning visual textual information, making challenging endeavor. better retrieve target image within database, cir model comprehend images prototype information semantic information modification text. existing methods domain broadly categorized two main groups supervised cir zs-cir. first discuss issues inherent category point possible technological trends type. additionally, briefly discuss related tasks cir. supervised composed image retrieval future work directions supervised cir given follows. benchmark dataset construction. mentioned earlier, existing cir datasets limited specific domains, e.g., fashion bird species. additionally, false negative issue prevalent current datasets due data annotation strategy, first identifies potential image pairs annotates modification text pair. although open-domain dataset cirr released address false negative issue, still faces two main challenges. first, previous studies shown many cases, simply relying modification text sufficient accurately identify target image cirr. second, cirr provides one-to-one query-target correspondence, align real-world scenarios multiple images dataset may satisfy retrieval requirements. creating open-domain cir datasets multiple ground- truth target images mitigating false negatives remains open challenge requiring exploration. furthermore, scale existing public datasets limited largest dataset contains approximately samples. according scaling law performance generalization capabilities cir improve larger-scale datasets. nevertheless, must acknowledge constructing large-scale triplet datasets resource-intensive time-consuming. fact, magiclens made attempt regard devising pipeline generate triplets web pages self-supervised manner, yielding triplets. unfortunately, dataset neither publicly accessible sufficiently large. therefore, advocate development publicly accessible, large-scale triplet datasets, resources would inject new vitality field cir. llm-based image-text fusion. existing cir methods made efforts devise diverse image-text fusion strategies reviewed above. reasoning capabilities llms mllms explored pseudo triplet generation address zs-cir task, limited cir efforts investigated potential encoding input imagetext queries achieving image-text fusion. approaches use reference image, modification text, target caption triplets, generated data generation pipelines, contrastive learning fine-tune align models encoding capabilities cir tasks, encoding output regarded fused query embedding. despite promising progress, key issue suffer improving encoding capabilities llms may compromise inherent reasoning abilities. developing advanced fine-tuning strategies enhance encoding capabilities llms preserving native reasoning abilities critical direction warrants investigation. manuscript submitted acm comprehensive survey composed image retrieval target matching noisy data. due complex nature cir task, manually annotated datasets may contain noisy triplets. instance, empirically observed triplets existing cir datasets nonsensical, mismatched images textual descriptions modification text descriptions unrelated corresponding images. noisy triplets mislead model hinder ability accurately match queries targets. address issue, model capable learning query-target matching robustly noisy data conditions. necessitates development adaptive mechanisms selectively utilize useful information training data disregarding noise. conclusion, building robust cir model effectively handle noisy data remains open valuable research direction. target matching biased data. mentioned earlier, avoid false negative samples, annotation modification text existing open-domain datasets may sometimes overly specific. lead scenarios simply using text query correctly retrieve target image. consequently, conventional target matching paradigm may overfit spurious correlations text query target image failing accurately learn matching relationship multimodal query target image. limitation negatively impact models generalization capabilities. address issue, key challenge lies mitigating harmful effects preserving beneficial contributions text query context cir. investigating causal relationships among elements cir presents promising research direction tackle challenge. efficient reranking. enhance ranking target images, existing reranking methods cir domain typically conduct sophisticated point-wise evaluations retrieved candidate set, inherently incurs high time cost. ensure desired target image retrieved, size candidate set reranking cannot small. consequently, time cost associated reranking becomes significant cannot overlooked. make reranking suitable real-world retrieval scenarios, developing efficient reranking methods essential key focus future research. furthermore, current reranking retrieval stages operate independently, limited correlation. investigating stages better integrated mutually enhance one another represents another promising research avenue. model interpretability. cir models grow increasingly complex, interpretability decreases, making challenging understand decisions made retrieval process. lack transparency poses significant challenges debugging improving models, well building user trust. future research directions could focus designing hybrid models integrate deep learning interpretable components utilizing post-hoc explanation techniques shed light decision-making process. task synergy. triplets cir also pertain related tasks. example, text-based image editing necessitates reference image along modification text synthesize desired target image, bears high degree relevance format cir. ideally, integrating retrieval generation one framework would mutually enhance both, achieving kill two birds one stone effect. specifically, generation, model needs comprehensively envision desired target image, would beneficial cir. moreover, generated low-quality images regarded type hard negative, boosting metric learning retrieval. additionally, retrieval task mainly centers around discriminative features rather pixel-level generation. consequently, complexity lower model converges readily. this, turn, contribute continuous learning generative process. moreover, retrieval capabilities, one utilize retrieved images references attain retrieval-augmented generation rag manuscript submitted acm xuemeng song al. effect image generation domain. unified framework also affords users flexibility obtain either retrieved real images synthesized ones based one query. zero-shot composed image retrieval compared supervised cir methods, zs-cir approaches eliminate need annotated training triplets. instead, leverage readily available pre-trained data utilize modular combinations training-free manner build models. significant progress achieved, several challenges open questions remain. pseudo triplets generation. researchers explored automatic generation pseudo triplets, enabling cir model training without need manually annotated triplets. however, current methods typically use existing captioning model convert image caption generate modification text llms. approaches may suffer information loss modality transformation, generated image captions often capture coarse-grained attributes e.g., color dress missing fine-grained visual details e.g., logo dress. limitation two main adverse effects constrains identification potential reference-target image pairs results coarse-grained modifications, failing capture challenging fine-grained variations. furthermore, using uniform prompts generate triplets reduce diversity generated samples. contrast, real-world users often express modification requirements various styles. result, building high-quality pseudo triplets training cir models remains significant challenge. mllm-based training-free methods. without additional design pre-training tasks, training-free methods present straightforward promising options field. typically, existing training-free methods predominantly utilize llms summarize caption reference image along modification text, thereby transforming task text-to-image retrieval. however, relying second-hand information reference image might lead bottlenecks attain suboptimal performance. rapid development mllms, promising research avenue explore direct utilization mllms process multimodal queries zs-cir. efficient retrieval. current evaluation protocol primarily concentrates retrieval efficacy neglecting retrieval efficiency. consequently, devised methods merely focus enhancing retrieval performance. however, approaches might practical real-world scenarios. example, certain methods make llm reason multimodal query multiple times enhance retrieval effect, yet incurs high time cost. therefore, striking balance efficacy efficiency crucial ensure research remains relevant academia also holds potential application value. entails two aspects. firstly, method ought lightweight. secondly, similarity computing process, accelerated retrieval techniques, hash learning index structure also explored. few-shot cir. generalization ability existing supervised cir models often limited small scale available datasets, zs-cir completely avoids using labeled triplets. practice, however, annotating small number samples often feasible. consequently, pioneering studies started investigate few-shot cir task, leverages limited number annotated triplets enhance model training. key research question area select high-quality samples annotation maximize benefits manual annotation efforts. manuscript submitted acm comprehensive survey composed image retrieval relation tasks composed image retrieval related tasks cir, current methods remain underdeveloped require refinement fully leverage unique attributes specific sub-task. indicates substantial opportunity improvement tailoring approaches closely distinct characteristics requirements tasks. addition refining current methodologies, exploring broader range novel innovative sub-tasks presents exciting avenue future research. expanding cir tasks could lead groundbreaking advancements field, opening new possibilities application enhancing understanding cir whole. future research focus developing sophisticated models capable seamlessly incorporating diverse modalities, thereby enriching retrieval process improving accuracy relevance results across variety contexts. conclusions paper, provide comprehensive review state-of-the-art methods cir. specifically, begin reviewing supervised cir approaches, focus four main components general framework. explore emerging field zero-shot cir, along several related tasks cir. following this, review various cir datasets, grouping results according datasets providing corresponding analyses. comprehensive endeavors, gained profound insights current challenges cir outlined promising research directions future. comprehensive survey invaluable insightful resource researchers practitioners, significantly propelling rapidly evolving field cir. acknowledgments work supported national natural science foundation china grant natural science foundation shandong province grant zryq zrqf. references lorenzo agnolucci, alberto baldrati, marco bertini, alberto del bimbo. isearle improving textual inversion zero-shot composed image retrieval. arxiv preprint kenan ak, ashraf kassim, joo hwee lim, yew tham. learning attribute representations localization flexible fashion search. proceedings ieee conference computer vision pattern recognition. ieee, kenan ak, joo hwee lim, yew tham, ashraf kassim. efficient multi-attribute similarity learning towards attribute-based fashion search. proceedings ieeecvf winter conference applications computer vision. ieee, rohan anil, andrew dai, orhan firat, melvin johnson, dmitry lepikhin, alexandre passos, siamak shakeri, emanuel taropa, paige bailey, zhifeng chen, al. palm technical report. arxiv preprint muhammad umer anwaar, egor labintcev, martin kleinsteuber. compositional learning image-text query image retrieval. proceedings ieeecvf winter conference applications computer vision. ieee, alberto baldrati, lorenzo agnolucci, marco bertini, alberto del bimbo. zero-shot composed image retrieval textual inversion. proceedings ieeecvf international conference computer vision. ieee, alberto baldrati, marco bertini, tiberio uricchio, alberto del bimbo. conditioned composed image retrieval combining partially fine-tuning clip-based features. proceedings ieee conference computer vision pattern recognition. ieee, alberto baldrati, marco bertini, tiberio uricchio, alberto del bimbo. effective conditioned composed image retrieval combining clip-based features. proceedings ieee conference computer vision pattern recognition. ieee, alberto baldrati, marco bertini, tiberio uricchio, alberto del bimbo. composed image retrieval using contrastive learning task-oriented clip-based features. acm transactions multimedia computing, communications applications oriol barbany, michael huang, xinliang zhu, arnab dhua. leveraging large language models multimodal search. proceedings ieee conference computer vision pattern recognition. ieee, tamara berg, alexander berg, jonathan shih. automatic attribute discovery characterization noisy web data. proceedings european conference computer vision. springer, manuscript submitted acm xuemeng song al. tim brooks, aleksander holynski, alexei efros. instructpixpix learning follow image editing instructions. proceedings ieee conference computer vision pattern recognition. ieee, tim brooks, aleksander holynski, alexei efros. instructpixpix learning follow image editing instructions. proceedings ieee conference computer vision pattern recognition. ieee, tom brown, benjamin mann, nick ryder, melanie subbiah, jared kaplan, prafulla dhariwal, arvind neelakantan, pranav shyam, girish sastry, amanda askell, al. language models few-shot learners. advances neural information processing systems jaeseok byun, seokhyeon jeong, wonjae kim, sanghyuk chun, taesup moon. reducing task discrepancy text encoders zero-shot composed image retrieval. arxiv preprint holger caesar, jasper uijlings, vittorio ferrari. coco-stuff thing stuff classes context. proceedings ieee conference computer vision pattern recognition. ieee, pranit chawla, surgan jandial, pinkesh badjatiya, ayush chopra, mausoom sarkar, balaji krishnamurthy. leveraging style content features text conditioned image retrieval. proceedings ieee conference computer vision pattern recognition. ieee, hila chefer, shir gur, lior wolf. generic attention-model explainability interpreting bi-modal encoder-decoder transformers. proceedings ieeecvf international conference computer vision. ieee, junyang chen hanjiang lai. pretrain like inference masked tuning improves zero-shot composed image retrieval. arxiv preprint junyang chen hanjiang lai. ranking-aware uncertainty text-guided image retrieval. arxiv preprint qianqian chen, tianyi zhang, maowen nie, zheng wang, shihao xu, wei shi, zhao cao. fashion-gpt integrating llms fashion retrieval system. proceedings workshop large generative models meet multimodal applications. acm, chen, josip djolonga, piotr padlewski, basil mustafa, soravit changpinyo, jialin wu, carlos riquelme ruiz, sebastian goodman, xiao wang, tay, al. pali-x scaling multilingual vision language model. arxiv preprint yanbei chen loris bazzani. learning joint visual semantic matching embeddings language-guided retrieval. proceedings european conference computer vision. springer, yanbei chen, shaogang gong, loris bazzani. image search text feedback visiolinguistic attention learning. proceedings ieee conference computer vision pattern recognition. ieee, yiyang chen, zhedong zheng, wei ji, leigang qu, tat-seng chua. composed image retrieval text feedback via multi-grained uncertainty regularization. proceedings international conference learning representations. openreview.net, yanzhe chen, huasong zhong, xiangteng he, yuxin peng, jiahuan zhou, lele cheng. fashionern enhance-and-refine network composed fashion image retrieval. proceedings aaai conference artificial intelligence. aaai press, yanzhe chen, jiahuan zhou, yuxin peng. spirit style-guided patch interaction fashion image retrieval text feedback. acm transactions multimedia computing, communications applications gong cheng, junwei han, xiaoqiang lu. remote sensing image scene classification benchmark state art. proc. ieee patrick john chia, giuseppe attanasio, federico bianchi, silvia terragni, ana rita magalhes, diogo goncalves, ciro greco, jacopo tagliabue. contrastive language vision learning general fashion concepts. scientific reports kyunghyun cho. learning phrase representations using rnn encoder-decoder statistical machine translation. proceedings conference empirical methods natural language processing. acl, pinaki nath chowdhury, ayan kumar bhunia, aneeshan sain, subhadeep koley, tao xiang, yi-zhe song. scenetrilogy human scene-sketch complementarity photo text. proceedings ieee conference computer vision pattern recognition. ieee, pinaki nath chowdhury, aneeshan sain, ayan kumar bhunia, tao xiang, yulia gryaditskaya, yi-zhe song. fs-coco towards understanding freehand sketches common objects context. proceedings european conference computer vision. springer, t-s chua, s-k lim, h-k pung. content-based retrieval segmented images. proceedings acm international conference multimedia. acm, hyung chung, hou, shayne longpre, barret zoph, tay, william fedus, yunxuan li, xuezhi wang, mostafa dehghani, siddhartha brahma, al. scaling instruction-finetuned language models. journal machine learning research niv cohen, rinon gal, eli meirom, gal chechik, yuval atzmon. unicorn, fluffy personalizing frozen vision-language representations. proceedings european conference computer vision. springer, ritendra datta, dhiraj joshi, jia li, james wang. image retrieval ideas, influences, trends new age. comput. surveys ginger delmas, rafael rezende, gabriela csurka, diane larlus. artemis attention-based retrieval text-explicit matching implicit similarity. proceedings international conference learning representations. openreview.net, jia deng, wei dong, richard socher, li-jia li, kai li, fei-fei. imagenet large-scale hierarchical image database. proceedings ieee conference computer vision pattern recognition. ieee, manuscript submitted acm comprehensive survey composed image retrieval jacob devlin. bert pre-training deep bidirectional transformers language understanding. proceedings conference north american chapter association computational linguistics. acl, eric dodds, jack culpepper, simao herdade, yang zhang, kofi boakye. modality-agnostic attention fusion visual search text feedback. arxiv preprint alexey dosovitskiy. image worth words transformers image recognition scale. proceedings international conference learning representations. openreview.net, yongchao du, min wang, wengang zhou, shuping hui, houqiang li. imagesentence based asymmetrical zero-shot composed image retrieval. proceedings international conference learning representations. openreview.net, yali du, yinwei wei, wei ji, fan liu, xin luo, liqiang nie. multi-queue momentum contrast microvideo-product retrieval. proceedings acm international conference web search data mining. acm, chun-mei feng, yang bai, tao luo, zhen li, salman khan, wangmeng zuo, xinxing xu, rick siow mong goh, yong liu. vqacir boosting composed image retrieval visual question answering. arxiv preprint zhangchi feng, richong zhang, zhijie nie. improving composed image retrieval via contrastive learning scaling positives negatives. proceedings acm international conference multimedia. acm, maxwell forbes, christine kaeser-chen, piyush sharma, serge belongie. neural naturalist generating fine-grained image comparisons. proceedings conference empirical methods natural language processing. acl, chaoyou fu, xiang wu, yibo hu, huaibo huang, ran he. dvg-face dual variational generation heterogeneous face recognition. ieee transactions pattern analysis machine intelligence rinon gal, yuval alaluf, yuval atzmon, patashnik, amit haim bermano, gal chechik, daniel cohen-or. image worth one word personalizing text-to-image generation using textual inversion. proceedings international conference learning representations. openreview.net, chengying gao, liu, xu, limin wang, jianzhuang liu, changqing zou. sketchycoco image generation freehand scene sketches. proceedings ieee conference computer vision pattern recognition. ieee, prajwal gatti, kshitij parikh, dhriti prasanna paul, manish gupta, anand mishra. composite sketch text queries retrieving objects elusive names complex interactions. proceedings aaai conference artificial intelligence. aaai press, hongfei ge, yuanchun jiang, jianshan sun, kun yuan, yezheng liu. llm-enhanced composed image retrieval intent uncertainty- aware linguistic-visual dual channel matching model. acm transactions information systems sonam goenka, zhaoheng zheng, ayush jaiswal, rakesh chada, yue wu, varsha hedau, pradeep natarajan. fashionvlp vision language transformer fashion retrieval feedback. proceedings ieee conference computer vision pattern recognition. ieee, yash goyal, tejas khot, douglas summers-stay, dhruv batra, devi parikh. making vqa matter elevating role image understanding visual question answering. proceedings ieee conference computer vision pattern recognition. ieee, alex graves. neural turing machines. arxiv preprint chunbin gu, jiajun bu, zhen zhang, zhi yu, dongfang ma, wei wang. image search text feedback deep hierarchical attention mutual information maximization. proceedings acm international conference multimedia. acm, geonmo gu, sanghyuk chun, wonjae kim, heejae jun, yoohoon kang, sangdoo yun. compodiff versatile composed image retrieval latent diffusion. transactions machine learning research geonmo gu, sanghyuk chun, wonjae kim, yoohoon kang, sangdoo yun. language-only training zero-shot composed image retrieval. proceedings ieee conference computer vision pattern recognition. ieee, xiaoxiao guo, hui wu, cheng, steven rennie, gerald tesauro, rogerio feris. dialog-based interactive image retrieval. advances neural information processing systems david douglas eck. neural representation sketch drawings. proceedings international conference learning representations. openreview.net, xintong han, zuxuan wu, phoenix huang, xiao zhang, menglong zhu, yuan li, yang zhao, larry davis. automatic spatially-aware fashion concept discovery. proceedings ieeecvf international conference computer vision. ieee. xiao han, licheng yu, xiatian zhu, zhang, yi-zhe song, tao xiang. fashionvil fashion-focused vision-and-language representation learning. proceedings european conference computer vision. springer, xiao han, xiatian zhu, licheng yu, zhang, yi-zhe song, tao xiang. fame-vil multi-tasking vision-language model heterogeneous fashion tasks. proceedings ieee conference computer vision pattern recognition. ieee, mehrdad hosseinzadeh yang wang. composed query image retrieval using locally bounded features. proceedings ieee conference computer vision pattern recognition. ieee, bohan hou, haoqiang lin, haokun wen, meng liu, xuemeng song. pseudo-triplet guided few-shot composed image retrieval. arxiv preprint yuxin hou, eleonora vig, michael donoser, loris bazzani. learning attribute-driven disentangled representations interactive fashion retrieval. proceedings ieeecvf international conference computer vision. ieee, manuscript submitted acm xuemeng song al. andrew howard. mobilenets efficient convolutional neural networks mobile vision applications. proceedings ieee conference computer vision pattern recognition. ieee, edward hu, yelong shen, phillip wallis, zeyuan allen-zhu, yuanzhi li, shean wang, wang, weizhu chen. lora low-rank adaptation large language models. proceedings international conference learning representations. openreview.net, zhizhang hu, xinliang zhu, son tran, ren vidal, arnab dhua. provla compositional image search progressive vision-language alignment multimodal fusion. proceedings ieeecvf international conference computer vision. ieee, fuxiang huang lei zhang. language guided local infiltration interactive image retrieval. proceedings ieee conference computer vision pattern recognition. ieee, fuxiang huang, lei zhang, xiaowei fu, suqi song. dynamic weighted combiner mixed-modal image retrieval. proceedings aaai conference artificial intelligence. aaai press, fuxiang huang, lei zhang, yuhang zhou, xinbo gao. adversarial isotropic gradient augmentation image retrieval text feedback. ieee transactions multimedia huang, jiancheng huang, yifan liu, mingfu yan, jiaxi lv, jianzhuang liu, wei xiong, zhang, shifeng chen, liangliang cao. diffusion model-based image editing survey. arxiv preprint thomas hummel, shyamgopal karthik, mariana-iuliana georgescu, zeynep akata. egocvr egocentric benchmark fine-grained composed video retrieval. proceedings european conference computer vision. springer, phillip isola, joseph lim, edward adelson. discovering states transformations image collections. proceedings ieee conference computer vision pattern recognition. ieee, surgan jandial, pinkesh badjatiya, pranit chawla, ayush chopra, mausoom sarkar, balaji krishnamurthy. sac semantic attention composition text-conditioned image retrieval. proceedings ieeecvf winter conference applications computer vision. ieee, young kyun jang, dat huynh, ashish shah, wen-kai chen, ser-nam lim. spherical linear interpolation text-anchoring zero-shot composed image retrieval. arxiv preprint jhamtani berg-kirkpatrick. learning describe differences pairs similar images. proceedings conference empirical methods natural language processing. acl, xintong jiang, yaxiong wang, mengjian li, yujiao wu, bingwen hu, xueming qian. cala complementary association learning augmenting comoposed image retrieval. proceedings international acm sigir conference research development information retrieval. acm, yingying jiang, hanchao jia, xiaobing wang, peng hao. hycir boosting zero-shot composed image retrieval synthetic labels. arxiv preprint justin johnson, bharath hariharan, laurens van der maaten, fei-fei, lawrence zitnick, ross girshick. clevr diagnostic dataset compositional language elementary visual reasoning. proceedings ieee conference computer vision pattern recognition. ieee, jared kaplan, sam mccandlish, tom henighan, tom brown, benjamin chess, rewon child, scott gray, alec radford, jeffrey wu, dario amodei. scaling laws neural language models. arxiv preprint shyamgopal karthik, karsten roth, massimiliano mancini, zeynep akata. vision-by-language training-free compositional image retrieval. proceedings ieee conference computer vision pattern recognition. ieee, jongseok kim, youngjae yu, hoeseong kim, gunhee kim. dual compositional learning interactive image retrieval. proceedings aaai conference artificial intelligence. aaai press, subhadeep koley, ayan kumar bhunia, aneeshan sain, pinaki nath chowdhury, tao xiang, yi-zhe song. youll never walk alone sketch text duet fine-grained image retrieval. proceedings ieee conference computer vision pattern recognition. ieee, ranjay krishna, yuke zhu, oliver groth, justin johnson, kenji hata, joshua kravitz, stephanie chen, yannis kalantidis, li-jia li, david shamma, al. visual genome connecting language vision using crowdsourced dense image annotations. international journal computer vision alex krizhevsky, ilya sutskever, geoffrey hinton. imagenet classification deep convolutional neural networks. commun. acm seungmin lee, dongwan kim, bohyung han. cosmo content-style modulation image retrieval text feedback. proceedings ieee conference computer vision pattern recognition. ieee, matan levy, rami ben-ari, nir darshan, dani lischinski. data roaming quality assessment composed image retrieval. proceedings aaai conference artificial intelligence. aaai press, dafeng yingying zhu. visual-linguistic alignment composition image retrieval text feedback. proceedings ieee international conference multimedia expo. ieee, junnan li, dongxu li, silvio savarese, steven hoi. blip- bootstrapping language-image pre-training frozen image encoders large language models. proceedings international conference machine learning. pmlr, manuscript submitted acm comprehensive survey composed image retrieval junnan li, dongxu li, caiming xiong, steven hoi. blip bootstrapping language-image pre-training unified vision-language understanding generation. proceedings international conference machine learning. pmlr, jianing li, shiliang zhang, tian, meng wang, wen gao. pose-guided representation learning person re-identification. ieee transactions pattern analysis machine intelligence kunpeng li, yulun zhang andkai li, yuanyuan li, yun fu. visual semantic reasoning image-text matching. proceedings ieeecvf international conference computer vision. ieee. mingyong li, zongwei zhao, xiaolong jiang, zheng jiang. clip-probcr clip-based probability embedding combination retrieval. proceedings acm international conference multimedia retrieval. acm, shenshen li. dual-path semantic construction network composed query-based image retrieval. proceedings acm international conference multimedia retrieval. acm, shenshen li, xing xu, xun jiang, fumin shen, xin liu, heng tao shen. multi-grained attention network mutual exclusion composed query-based image retrieval. ieee transactions circuits systems video technology shenshen li, xing xu, xun jiang, fumin shen, zhe sun, andrzej cichocki. cross-modal attention preservation self-contrastive learning composed query-based image retrieval. acm transactions multimedia computing, communications applications wei li, hehe fan, yongkang wong, yang, mohan kankanhalli. improving context understanding multimodal large language models via multimodal composition learning. proceedings international conference machine learning. pmlr, lizi liao, xiangnan he, zhao, chong-wah ngo, tat-seng chua. interpretable multimodal retrieval fashion products. proceedings acm international conference multimedia. acm, haoqiang lin, haokun wen, xiaolin chen, xuemeng song. clip-based composed image retrieval comprehensive fusion data augmentation. proceedings australasian joint conference artificial intelligence. springer, haoqiang lin, haokun wen, xuemeng song, meng liu, yupeng hu, liqiang nie. fine-grained textual inversion network zero-shot composed image retrieval. proceedings international acm sigir conference research development information retrieval. acm, tsung-yi lin, michael maire, serge belongie, james hays, pietro perona, deva ramanan, piotr dollr, lawrence zitnick. microsoft coco common objects context. proceedings european conference computer vision. springer, tsung-yi lin, michael maire, serge belongie, james hays, pietro perona, deva ramanan, piotr dollr, lawrence zitnick. microsoft coco common objects context. proceedings european conference computer vision. springer, fan liu, delong chen, zhangqingyun guan, xiaocong zhou, jiale zhu, qiaolin ye, liyong fu, jun zhou. remoteclip vision language foundation model remote sensing. ieee transactions geoscience remote sensing haotian liu, chunyuan li, qingyang wu, yong jae lee. visual instruction tuning. advances neural information processing systems yinhan liu. roberta robustly optimized bert pretraining approach. arxiv preprint yating liu yan lu. multi-grained fusion conditional image retrieval. multimedia modeling. springer, yikun liu, jiangchao yao, zhang, yanfeng wang, weidi xie. zero-shot composed text-image retrieval. british machine vision conference. bmva press, liu, yutong lin, yue cao, han hu, yixuan wei, zheng zhang, stephen lin, baining guo. swin transformer hierarchical vision transformer using shifted windows. proceedings ieeecvf international conference computer vision. ieee, zheyuan liu, cristian rodriguez-opazo, damien teney, stephen gould. image retrieval real-life images pre-trained vision-and- language models. proceedings ieeecvf international conference computer vision. ieee, zheyuan liu, cristian rodriguez-opazo, damien teney, stephen gould. image retrieval real-life images pre-trained vision-and- language models. proceedings ieeecvf international conference computer vision. ieee, zheyuan liu, weixuan sun, yicong hong, damien teney, stephen gould. bi-directional training composed image retrieval via text prompt learning. proceedings ieeecvf winter conference applications computer vision. ieee, zheyuan liu, weixuan sun, damien teney, stephen gould. candidate set re-ranking composed image retrieval dual multi-modal encoder. transactions machine learning research suvir mirchandani, licheng yu, mengjiao wang, animesh sinha, wenwen jiang, tao xiang, ning zhang. fad-vlp fashion vision- and-language pre-training towards unified retrieval captioning. proceedings conference empirical methods natural language processing. acl, anwesan pal, sahil wadhwa, ayush jaiswal, zhang, yue wu, rakesh chada, pradeep natarajan, henrik christensen. fashionntm multi-turn fashion image retrieval via cascaded memory. proceedings ieeecvf international conference computer vision. ieee, jeffrey pennington, richard socher, christopher manning. glove global vectors word representation. proceedings conference empirical methods natural language processing. acl, khoi pham, kushal kafle, zhe lin, zhihong ding, scott cohen, quan tran, abhinav shrivastava. learning predict visual attributes wild. proceedings ieeecvf conference computer vision pattern recognition. ieee, manuscript submitted acm xuemeng song al. bill psomas, ioannis kakogeorgiou, nikos efthymiadis, giorgos tolias, ondej chum, yannis avrithis, konstantinos karantzalos. composed image retrieval remote sensing. igarss ieee international geoscience remote sensing symposium. ieee, bill psomas, ioannis kakogeorgiou, nikos efthymiadis, giorgos tolias, ondrej chum, yannis avrithis, konstantinos karantzalos. composed image retrieval remote sensing. proceedings ieee international geoscience remote sensing symposium. ieee, leigang qu, meng liu, cao, liqiang nie, tian. context-aware multi-view summarization network image-text matching. proceedings acm international conference multimedia. acm, leigang qu, meng liu, jianlong wu, zan gao, liqiang nie. dynamic modality interaction modeling image-text retrieval. proceedings international acm sigir conference research development information retrieval. acm, alec radford. improving language understanding generative pre-training. alec radford, jong wook kim, chris hallacy, aditya ramesh, gabriel goh, sandhini agarwal, girish sastry, amanda askell, pamela mishkin, jack clark, al. learning transferable visual models natural language supervision. proceedings international conference machine learning. pmlr, colin raffel, noam shazeer, adam roberts, katherine lee, sharan narang, michael matena, yanqi zhou, wei li, peter liu. exploring limits transfer learning unified text-to-text transformer. journal machine learning research jun rao, fei wang, liang ding, shuhan qi, yibing zhan, weifeng liu, dacheng tao. performance improvement come from? reproducibility concern image-text retrieval. proceedings international acm sigir conference research development information retrieval. acm, shaoqing ren, kaiming he, ross girshick, jian sun. faster r-cnntowards real-time object detection region proposal networks. ieee transactions pattern analysis machine intelligence tal ridnik, emanuel ben-baruch, nadav zamir, asaf noy, itamar friedman, matan protter, lihi zelnik-manor. asymmetric loss multi-label classification. proceedings ieeecvf international conference computer vision. ieee, robin rombach, andreas blattmann, dominik lorenz, patrick esser, bjrn ommer. high-resolution image synthesis latent diffusion models. proceedings ieee conference computer vision pattern recognition. ieee, kuniaki saito, kihyuk sohn, xiang zhang, chun-liang li, chen-yu lee, kate saenko, tomas pfister. picword mapping pictures words zero-shot composed image retrieval. proceedings ieee conference computer vision pattern recognition. ieee, gerard salton christopher buckley. term-weighting approaches automatic text retrieval. information processing management patsorn sangkloy, wittawat jitkrittum, diyi yang, james hays. sketch worth thousand words image retrieval text sketch. proceedings european conference computer vision. springer, sanh. distilbert, distilled version bert smaller, faster, cheaper lighter. arxiv preprint zhenfeng shao, yang, weixun zhou. performance evaluation single-label multi-label remote sensing image retrieval using dense labeling dataset. remote sensing minchul shin, yoonjae cho, byungsoo ko, geonmo gu. rtic residual learning text image composition using graph convolutional network. arxiv preprint ken shoemake. animating rotation quaternion curves. proceedings conference computer graphics interactive techniques. acm, chull hwan song, taebaek hwang, jooyoung yoon, shunghyun choi, yeong hyeon gu. syncmask synchronized attentional masking fashion-centric vision-language pretraining. proceedings ieee conference computer vision pattern recognition. ieee, jifei song, yi-zhe song, tao xiang, timothy hospedales. fine-grained image retrieval textsketch input dilemma. british machine vision conference. bmva press, alane suhr, stephanie zhou, ally zhang, iris zhang, huajun bai, yoav artzi. corpus reasoning natural language grounded photographs. proceedings conference association computational linguistics. acl, shitong sun, fanghua ye, shaogang gong. training-free zero-shot composed image retrieval local concept reranking. arxiv preprint yucheng suo, fan ma, linchao zhu, yang. knowledge-enhanced dual-stream zero-shot composed image retrieval. proceedings ieee conference computer vision pattern recognition. ieee, christian szegedy, wei liu, yangqing jia, pierre sermanet, scott reed, dragomir anguelov, dumitru erhan, vincent vanhoucke, andrew rabinovich. going deeper convolutions. proceedings ieee conference computer vision pattern recognition. ieee, yuanmin tang, jing yu, keke gai, jiamin zhuang, gang xiong, yue hu, wu. context-iw mapping images context-dependent words accurate zero-shot composed image retrieval. proceedings aaai conference artificial intelligence. aaai press, ivona tautkute tomasz trzcinski. want product different multimodal retrieval synthetic query expansion. arxiv preprint gemini team, rohan anil, sebastian borgeaud, jean-baptiste alayrac, jiahui yu, radu soricut, johan schalkwyk, andrew dai, anja hauth, katie millican, al. gemini family highly capable multimodal models. arxiv preprint omkar thawakar, muzammal naseer, rao muhammad anwer, salman khan, michael felsberg, mubarak shah, fahad shahbaz khan. composed video retrieval via enriched context discriminative embeddings. proceedings ieee conference computer vision manuscript submitted acm comprehensive survey composed image retrieval pattern recognition. ieee, yuxin tian, shawn newsam, kofi boakye. fashion image retrieval text feedback additive attention compositional learning. proceedings ieeecvf winter conference applications computer vision. ieee, prateksha udhayanan, srikrishna karanam, balaji vasan srinivasan. learning multi-modal gradient attention explainable composed image retrieval. arxiv preprint ashish vaswani, noam shazeer, niki parmar, jakob uszkoreit, llion jones, aidan gomez, ukasz kaiser, illia polosukhin. attention need. advances neural information processing systems sagar vaze, nicolas carion, ishan misra. genecis benchmark general conditional image similarity. proceedings ieee conference computer vision pattern recognition. ieee, petar velikovi, guillem cucurull, arantxa casanova, adriana romero, pietro li, yoshua bengio. graph attention networks. proceedings international conference learning representations. openreview.net, lucas ventura, antoine yang, cordelia schmid, varol. covr- automatic data construction composed video retrieval. ieee transactions pattern analysis machine intelligence lucas ventura, antoine yang, cordelia schmid, varol. covr learning composed video retrieval web video captions. proceedings aaai conference artificial intelligence. aaai press, lucas ventura, antoine yang, cordelia schmid, varol. covr learning composed video retrieval web video captions. proceedings aaai conference artificial intelligence. aaai press, nam vo, jiang, chen sun, kevin murphy, li-jia li, fei-fei, james hays. composing text image image retrieval empirical odyssey. proceedings ieee conference computer vision pattern recognition. ieee, yongquan wan, wenhai wang, guobing zou, bofeng zhang. cross-modal feature alignment fusion composed image retrieval. proceedings ieee conference computer vision pattern recognition. ieee, fei wang, xianzhang zhu, xiaojian liu, yongjun zhang, yansheng li. scene graph-aware hierarchical fusion network remote sensing image retrieval text feedback. ieee transactions geoscience remote sensing jingdong wang, ting zhang, nicu sebe, heng tao shen, al. survey learning hash. ieee transactions pattern analysis machine intelligence peng wang, zining chen, zhicheng zhao, fei su. prompting vision-language fusion zero-shot composed image retrieval. asian conference machine learning. yifan wang, wuliang huang, lei li, chun yuan. semantic distillation neighborhood composed image retrieval. proceedings acm international conference multimedia. acm, yifan wang, liyuan liu, chun yuan, minbo li, jing liu. negative-sensitive framework semantic enhancement composed image retrieval. ieee transactions multimedia hao wei, shuhui wang, zhe xue, shengbo chen, qingming huang. conversational composed retrieval iterative sequence refinement. proceedings acm international conference multimedia. acm, jason wei, xuezhi wang, dale schuurmans, maarten bosma, fei xia, chi, quoc le, denny zhou, al. chain-of-thought prompting elicits reasoning large language models. advances neural information processing systems haokun wen, xuemeng song, xiaolin chen, yinwei wei, liqiang nie, tat-seng chua. simple effective raw-data level multimodal fusion composed image retrieval. proceedings international acm sigir conference research development information retrieval. acm, haokun wen, xuemeng song, xin yang, yibing zhan, liqiang nie. comprehensive linguistic-visual composition network image retrieval. proceedings international acm sigir conference research development information retrieval. acm, haokun wen, xuemeng song, jianhua yin, jianlong wu, weili guan, liqiang nie. self-training boosted multi-factor matching network composed image retrieval. ieee transactions pattern analysis machine intelligence haokun wen, xian zhang, xuemeng song, yinwei wei, liqiang nie. target-guided composed image retrieval. proceedings acm international conference multimedia. acm, hui wu, yupeng gao, xiaoxiao guo, ziad al-halah, steven rennie, kristen grauman, rogerio feris. fashion new dataset towards retrieving images natural language feedback. proceedings ieee conference computer vision pattern recognition. ieee, junda wu, rui wang, handong zhao, ruiyi zhang, chaochao lu, shuai li, ricardo henao. few-shot composition learning image retrieval prompt tuning. proceedings aaai conference artificial intelligence. aaai press, ren-di wu, yu-yen lin, huei-fang yang. training-free zero-shot composed image retrieval via weighted modality fusion similarity. arxiv preprint shin-ting wu, pin-jung chen, po-chun huang, wei-kuan shih, yuan-hao chang. firm-tree multidimensional index structure reprogrammable flash memory. ieee transactions computer-aided design integrated circuits systems yiming wu, hangfei li, fangfang wang, yilong zhang, ronghua liang. self-distilled dynamic fusion network language-based fashion retrieval. proceedings ieee international conference acoustics, speech signal processing. ieee, manuscript submitted acm xuemeng song al. kelvin xu. show, attend tell neural image caption generation visual attention. proceedings international conference machine learning. pmlr, xinxing xu, yong liu, salman khan, fahad khan, wangmeng zuo, rick siow mong goh, chun-mei feng, al. sentence-level prompts benefit composed image retrieval. proceedings international conference learning representations. openreview.net, yahui xu, bin, jiwei wei, yang yang, guoqing wang, heng tao shen. multi-modal transformer global-local alignment composed query image retrieval. ieee transactions multimedia yahui xu, bin, jiwei wei, yang yang, guoqing wang, heng tao shen. align retrieve composition decomposition learning image retrieval text feedback. ieee transactions multimedia yahui xu, jiwei wei, bin, yang yang, zeyu ma, heng tao shen. set diverse queries uncertainty regularization composed image retrieval. ieee transactions circuits systems video technology cairong yan, meng ma, yanting zhang, yongquan wan. dual-path multimodal optimal transport composed image retrieval. multimedia modeling. springer nature singapore, cairong yan, erhe yang, ran tao, yongquan wan, derun ai. shaf semantic-guided hierarchical alignment fusion composed image retrieval. international conference intelligent computing. springer, yang, mang ye, zhaohui cai, kehua su, du. composed image retrieval via cross relation network hierarchical aggregation transformer. ieee transactions image processing sibei yang, guanbin li, yizhou yu. dynamic graph attention referring expression comprehension. proceedings ieeecvf international conference computer vision. ieee. xingyu yang, daqing liu, heng zhang, yong luo, chaoyue wang, jing zhang. decomposing semantic shifts composed image retrieval. proceedings aaai conference artificial intelligence. aaai press, xin yang, xuemeng song, xianjing han, haokun wen, jie nie, liqiang nie. generative attribute manipulation scheme flexible fashion search. proceedings international acm sigir conference research development information retrieval. acm, xin yang, xuemeng song, xianjing han, haokun wen, jie nie, liqiang nie. generative attribute manipulation scheme flexible fashion search. proceedings international acm sigir conference research development information retrieval. acm, yang shawn newsam. bag-of-visual-words spatial extensions land-use classification. proceedings sigspatial international conference advances geographic information systems. acm, yuchen yang, min wang, wengang zhou, houqiang li. cross-modal joint prediction alignment composed query image retrieval. proceedings acm international conference multimedia. acm, yuchen yang, wang, yanfeng wang. sda semantic discrepancy alignment text-conditioned image retrieval. proceedings conference association computational linguistics. acl, zhenyu yang, shengsheng qian, dizhan xue, jiahong wu, fan yang, weiming dong, changsheng xu. semantic editing increment benefits zero-shot composed image retrieval. proceedings acm international conference multimedia. acm, zhenyu yang, dizhan xue, shengsheng qian, weiming dong, changsheng xu. ldre llm-based divergent reasoning ensemble zero-shot composed image retrieval. proceedings international acm sigir conference research development information retrieval. acm, yifei yuan wai lam. conversational fashion image retrieval via multiturn natural language feedback. proceedings international acm sigir conference research development information retrieval. acm, alireza zaeemzadeh, shabnam ghadar, baldo faieta, zhe lin, nazanin rahnavard, mubarak shah, ratheesh kalarot. face image retrieval attribute manipulation. proceedings ieeecvf international conference computer vision. ieee, fangneng zhan, yingchen yu, rongliang wu, jiahui zhang, shijian lu, lingjie liu, adam kortylewski, christian theobalt, eric xing. multimodal image synthesis editing generative era. ieee transactions pattern analysis machine intelligence feifei zhang, mingliang xu, qirong mao, changsheng xu. joint attribute manipulation modality alignment learning composing text image image retrieval. proceedings acm international conference multimedia. acm, feifei zhang, mingliang xu, changsheng xu. geometry sensitive cross-modal reasoning composed query based image retrieval. ieee transactions image processing feifei zhang, mingliang xu, changsheng xu. tell, imagine, search end-to-end learning composing text image image retrieval. acm transactions multimedia computing, communications applications gangjian zhang, shikun li, shikui wei, shiming ge, cai, yao zhao. multimodal composition example mining composed query image retrieval. ieee transactions image processing gangjian zhang, shikui wei, huaxin pang, shuang qiu, yao zhao. composed image retrieval via explicit erasure replenishment semantic alignment. ieee transactions image processing gangjian zhang, shikui wei, huaxin pang, shuang qiu, yao zhao. enhance composed image retrieval via multi-level collaborative localization semantic activeness perception. ieee transactions multimedia gangjian zhang, shikui wei, huaxin pang, yao zhao. heterogeneous feature fusion cross-modal alignment composed image retrieval. proceedings acm international conference multimedia. acm, manuscript submitted acm comprehensive survey composed image retrieval huaying zhang, rintaro yanagi, ren togo, takahiro ogawa, miki haseyama. zero-shot composed image retrieval considering query-target relationship leveraging masked image-text pairs. ieee international conference image processing. jiayan zhang, jie zhang, honghao wu, zongwei zhao, jinyu hu, mingyong li. pcasm text-guided composed image retrieval parallel content style modules. proceedings ieee international conference multimedia expo. ieee, kai zhang, luan, hexiang hu, kenton lee, siyuan qiao, wenhu chen, su, ming-wei chang. magiclens self-supervised image retrieval open-ended instructions. proceedings international conference machine learning. pmlr, susan zhang, stephen roller, naman goyal, mikel artetxe, moya chen, shuohui chen, christopher dewan, mona diab, xian li, victoria lin, al. opt open pre-trained transformer language models. arxiv preprint zhang, zhedong zheng, linchao zhu, yang. collaborative group composed image retrieval via consensus learning noisy annotations. knowledge-based systems zhao, jiashi feng, xiao wu, shuicheng yan. memory-augmented attribute manipulation networks interactive fashion search. proceedings ieee conference computer vision pattern recognition. ieee, shu zhao huijuan xu. neucore neural concept reasoning composed image retrieval. proceedings unireps first workshop unifying representations neural models. pmlr, xiangyu zhao, yuehan zhang, wenlong zhang, xiao-ming wu. unifashion unified vision-language model multimodal fashion retrieval generation. proceedings conference empirical methods natural language processing. acl, xiangyu zhao, yuehan zhang, wenlong zhang, xiao-ming wu. unifashion unified vision-language model multimodal fashion retrieval generation. arxiv preprint yida zhao, yuqing song, qin jin. progressive learning image retrieval hybrid-modality queries. proceedings international acm sigir conference research development information retrieval. acm, wenliang zhong, weizhi an, feng jiang, hehuan ma, yuzhi guo, junzhou huang. compositional image retrieval via instruction-aware contrastive learning. arxiv preprint junjie zhou, zheng liu, shitao xiao, zhao, yongping xiong. vista visualized text embedding universal multi-modal retrieval. proceedings conference association computational linguistics. acl, weixun zhou, shawn newsam, congmin li, zhenfeng shao. patternnet benchmark dataset performance evaluation remote sensing image retrieval. isprs journal photogrammetry remote sensing hongguang zhu, yunchao wei, yao zhao, chunjie zhang, shujuan huang. amc adaptive multi-expert collaborative network text-guided image retrieval. acm transactions multimedia computing, communications applications received february revised march accepted june manuscript submitted acm", "published_date": "2025-02-19T01:37:24+00:00"}
{"id": "2502.08836v1", "title": "Survey on Single-Image Reflection Removal using Deep Learning Techniques", "authors": ["Kangning Yang", "Huiming Sun", "Jie Cai", "Lan Fu", "Jiaming Ding", "Jinlong Li", "Chiu Man Ho", "Zibo Meng"], "summary": "phenomenon reflection quite common digital images, posing significant challenges various applications computer vision, photography, image processing. traditional methods reflection removal often struggle achieve clean results maintaining high fidelity robustness, particularly real-world scenarios. past decades, numerous deep learning-based approaches reflection removal emerged, yielding impressive results. survey, conduct comprehensive review current literature focusing key venues iccv, eccv, cvpr, neurips, etc., conferences journals central advances field. review follows structured paper selection process, critically assess single-stage two-stage deep learning methods reflection removal. contribution survey three-fold first, provide comprehensive summary recent work single-image reflection removal second, outline task hypotheses, current deep learning techniques, publicly available datasets, relevant evaluation metrics third, identify key challenges opportunities deep learning-based reflection removal, highlighting potential rapidly evolving research area.", "full_text": "cs.cv feb survey single-image reflection removal using deep learning techniques kangning yang, huiming sun, jie cai, lan fu, jiaming ding, jinlong li, chiu man ho, zibo meng oppo center abstract phenomenon reection quite common digital im- ages, posing signicant challenges various applications computer vision, photography, image process- ing. traditional methods reection removal often strug- gle achieve clean results maintaining high delity robustness, particularly real-world scenarios. past decades, numerous deep learning-based approaches reection removal emerged, yielding impressive re- sults. survey, conduct comprehensive review current literature focusing key venues iccv, eccv, cvpr, neurips, etc., conferences jour- nals central advances eld. review follows structured paper selection process, critically assess single-stage two-stage deep learning meth- ods reection removal. contribution survey three-fold rst, provide comprehensive summary recent work single-image reection removal second, outline task hypotheses, current deep learning techniques, publicly available datasets, relevant evaluation metrics third, identify key challenges opportunities deep learning-based reection removal, highlighting po- tential rapidly evolving research area. index terms single-image reection removal, deep learning introduction single-image reection removal sirr critical task image processing, focusing recovering true scene behind reections reective surfaces e.g., transparent glasses. years, various techniques proposed solve sirr problem. traditional methods typically re- lied non-learning paradigm. since goal sirr recover transmission image i.e., true scene blended image containing scene reections, ill-posed problem. absence additional informa- tion scene, innite number possible decompositions image therefore, traditional methods often exploit prior knowledge priors constrain solution space guide recovery process. one widely used prior sparsity priors approaches impose gradient sparsity constraints minimum edges corners layer decomposition. fundamental idea require image gradient histogram long- tail distribution another common assumption use smoothness priors prior based ob- servation reection layers likely blurred compared background scene, primarily due differ- ences distance camera. additionally, shih al. introduced concept examining ghosting effects caused double-pane windows. ghosting effects lead multiple reections captured image, challenging separate. address this, proposed mod- eling effects using gaussian mixture models gmm patch-based prior. however, priors often struggle generalize well across different types reections scenes, particularly dealing complex real-world environments. priors based assumption captured image linear combination transmitted scene reection i.e., assumption simple, often deviates signicantly reality, real- world reections complex inuenced fac- tors lighting conditions, surface characteristics, moreover, may contain content real-world scenes, leading overlapping appearance distri- butions make separation two components challenging address issues, researchers re- cently shifted focus toward learning-based methods, particu- larly data-driven deep learning approaches, driven suc- cess deep neural networks tackling versatile computer vision problems. instead relying handcrafted priors, deep learning models trained large, labeled datasets, enabling effectively handle broad spectrum sce- narios. article presents comprehensive survey sirr re- search utilizing deep learning methods. analyze current research trends identify future opportunities. compared existing surveys similar topics, article offers thorough review literature, focusing key journals conferences, aim presenting concise, critical, recent research advances. example, although wan al. provided brief survey, focus primar- ily introducing new datasets sir sir establishing benchmarks different algorithms. amanlou al. conducted survey sirr using deep learning, coverage limited work published contrast, review covers recent ad- vancements organizes research within detailed framework. article organized follows section describes methodology used conduct bibliographic search. section introduces mathematical hypotheses mod- eling sirr. section surveys sirr research three perspectives single-stage, two-stage, multi-stage ap- proaches. section discusses currently available public datasets commonly used evaluation metrics. finally, sec- tion explores future research opportunities sirr eld, followed brief concluding remarks section methodology conduct comprehensive focused survey relevant literature, strategically concentrated bibli- ographic search key conferences journals widely recognized publishing state-of-the-art high-impact research computer vision articial intelligence. instead conducting broad search across available digital li- braries, specically targeted prominent venues considered inuential domains. include cvpr, iccv, eccv, wacv, neurips, tpami, tip, appl. in- tell.. papers presented conferences published journals widely regarded high impact, undergone rigorous peer review earned broad ac- knowledgment within academic community. high citation counts works venues demon- strate signicant inuence acceptance, ensuring studies included survey relevant credible. used following search query capture relevant re- search sirr single image reection removal sin- gle image reection erase single image reection sepa- ration single image reection elimination single- image reection removal single-image reection erase single-image reection separation single-image reection elimination deep learning neural net- work articial intelligence. excluded papers full-text research articles, tutorials, abstracts, workshops, posters, etc. also applied time lter or- der consider publications fan al. introduced rst neural network model solving sirr analyzed papers ensure appropriate survey. finally, papers remained constitute basis literature review. mathematical hypothesis previously mentioned, sirr inherently ill-posed problem. end, researchers proposed various hy- potheses. linear hypothesis linear hypothesis posits captured image perceived superimposition transmission layer reection layer, concept inspired human visual sys- tem early studies, mainly non-learning approaches early deep learning works adopted hypothesis, assuming image containing reections math- ematically modeled sum transmission re- ection layers, i.e., however, assump- tion heuristic reection transmission layers likely degrade due diffusion superposition process may hold true cases involving intense bright reections furthermore, researchers introduced blending scalars build nuanced model, represent scaling factors transmission reection layers, respectively. instance, al. assumes yang al. assumes wan al. treats mixing coefcients balancing transmission reection layers. despite improvements, blending two images using constant values accurately simulate complex real-world reection process. formation reected image depends factors relative position camera image plane lighting conditions non-linear hypothesis address complex process image reection, studies leveraged full potential deep learning efciently incorporate prior information mined labeled training data network structures. studies also in- troduced non-linearity develop sophisticated models better approximate physical mechanisms involved image formation one approach utilizes alpha matting model blending process. formula- tion, alpha blending mask introduced represent relative contribution transmission layer pixel. synthesis process represented denotes element-wise multiplication. similarly, wen al. approximates reection mechanisms approach enables exible accurate separation scene reection layers, particularly cases involving complex light interac- tions semi-transparent surfaces. additionally, zheng al. proposed model represent refractive reec- tive amplitude coefcient maps, respectively. likewise, wan al. considered degradation express- ing formation mixed image fr, represent various degradation pro- cesses learned network structure. recently, guo deliver general formulation rt, represents residue reconstruction process, may arise due factors attenuation, overexposure, etc. reflection removal approaches rst present one-stage sirr approaches sec- tion followed two-stage multi-stage approaches sections learning objective introduced section comparative analysis models table single-stage approaches eld reection removal, academic ap- proaches adopt multi-stage architecture. however, studies also proposed one-stage architectures. given image reections, approaches typically decompose transmission layer andor reection layer fri using single network fri represents network parameters. errnet robustsirr take input output zhang al. ytmt take input output ad- ditionally, robustsirr utilizes multi-resolution inputs alongside enhance feature extraction improve reec- tion removal performance. zhang al. propose utilizing deep neural net- work perceptual losses address problem sirr. errnet enhances fundamental image reconstruction neural network simplifying residual blocks elimination batch normalization, expanding capacity widening network feature maps, enriching input image hypercolumn features extracted pretrained vgg- network incorporate semantic infor- mation better performance. robustsirr presents robust transformer-based model sirr, integrating cross- scale attention modules, multi-scale fusion modules, adversarial image discriminator improve performance. ytmt introduces simple yet effective interactive strat- egy called trash treasure. approach con- structs dual-stream decomposition networks facilitating block-wise communication streams transfer- ring deactivated relu information one stream other, leveraging additive property components. two-stage approaches due inherent ambiguity complexity sirr, solving problem challenging. address this, re- searchers typically use deep learning models cascade sequence manner, helps manage uncertainty es- timating transmission layer also simplifying train- ing sirr systems. academic approaches adopt two-stage architecture, intermediate output, reection layer coarse transmis- sion layer edge map rst estimated, followed reconstruction nal transmission layer andor reection layer. besides, techniques fusing features across different stages vary among studies. methods use convolutional fusion tech- niques others apply image-level feature-level concatenation additionally, certain ap- proaches employ fusion strategies. corrn proposes network uses feature shar- ing tackle problem within cooperative framework, combining image context multi-scale gradient informa- tion. dmgn presents unied framework back- ground restoration, employing residual deep-masking cell progressively rene control information ow. rag module designed improve use esti- mated reection accurate transmission layer predic- tion. ceilnet introduces cascaded pipeline edge prediction followed image reconstruction. dsrnet architecture features two cascaded stages learnable residue module lrm. stage gathers hierarchical semantic information, stage renes decomposition us- ing lrm separate components break linear assumption. structure includes sp-net, decomposes input image predicted transmission layer background reection layer. bt-net elim- inates glass lens effects predicted reection, enhancing image clarity enabling accurate error matching. paper instead eliminating reection components mixed image, goal recover reection scenes mixture. paper addresses sirr incorporating absorption effect ap- proximated using average refractive amplitude coefcient map. proposes two-step solution rst step estimates absorption effect reection-contaminated image, second step recovers transmission image using reection-contaminated image estimated ab- sorption effect. framework consists rdnet rrnet, rdnet utilizes pretrained backbone residual blocks interpolation estimate reection mask, rrnet uses estimate assist reection removal process. paper addresses language-guided reection separation using language descriptions pro- vide layer content. proposes unied framework uses cross-attention contrastive learning align language methods venue scheme cross-stage fusion single-stage zhang al. cvpr errnet cvpr robustsirr cvpr imultiscale ytmt neurips two-stage corrn tpami convolutional fusion dmgn tip convolutional fusion ragnet appl. intell. convolutional fusion ceilnet iccv concat dsrnet iccv residue sp-net bt-net cvpr wan al. cvpr zheng al. cvpr concat zhu al. cvpr concat language-guided cvpr exts feature-level concat multi-stage bdn eccv concat ibcln cvpr ... concat recurrent chang al. wacv concat recurrent lanet iccv ... concat recurrent v-desirr iccv ... in, tn, convolutional fusion recurrent table represent input, reection, transmission, edge map, respectively. subscripts represent intermediate process outputs. absorption effect introduced describe light attenuation passes glass. output residue term, proposed used correct errors additive reconstruction reection transmission layers. language descriptions provide contextual information image layers, assisting addressing ill-posed nature reection separation problem. descriptions image layers, gated network randomized training strategy help resolve layer ambiguity. multi-stage approaches studies extend beyond two-stage architecture using multi-stage cascaded structure. similar two- stage design, multi-stage approach generates intermediate outputs recurrent fashion, eventually reconstructing nal transmission andor reection layer. methods use convolutional fusion techniques others utilize con- catenation dbn introduces cascaded deep neural network simultaneously estimates background reection components. network follows bidirectional approach rst using estimated background predict reec- tion, rening background prediction using estimated reection. dual-estimation strategy improves reection removal performance. ibcln designed reection removal progressively rening estimates transmission reection layers, iteration improving prediction other. utilizing lstm transfer information steps incorporating residual reconstruction loss, ibcln tackles vanishing gradient is- sue improves training across multiple cascade steps. model takes reection-contaminated image sep- arates reection transmission layers. ensure high-quality transmission, three auxiliary techniques em- ployed edge guidance, reection classier, recurrent decomposition. paper presents lanet sirr. employs reection detection module generates proba- bilistic condence map using multi-scale laplacian features. network, designed recurrent model, progressively renes reection removal, laplacian kernel parameters highlighting strong reection boundaries improve detec- tion enhance quality results. v-desirr introduces lightweight model reection removal using innovative scale-space architecture, processes corrupted image two stages low scale sub-network lssnet lowest scale progressive inference stage higher scales. minimize computational com- plexity, stage sub-networks signicantly shallower lssnet, weight sharing across scales enables model generalize high resolutions without need retraining. learning objective train sirr models, several commonly used loss func- tions combined ensure high-quality reection removal. include reconstruction loss, perceptual loss adversarial loss loss functions con- tributes different aspects models learning process reconstruction loss typically dened using loss, directly measures pixel-wise difference be- tween predicted reection-free image ground truth image. loss ensures output image close possible desired reection-free image pixel-wise sense. however, relying solely loss lead overly smooth results, consider high-level perceptual differences. loss formulation follows lrec predicted reection-free image ground truth image. mitigate oversmoothing effect reconstruction loss preserve important structural details, gradient con- sistency loss utilized loss ensures predicted transmission layer retains edge structures ground truth minimizing difference gradients along y-directions lgrad gradient operators along horizontal vertical directions, respectively. combining two losses, sirr achieves balance accurate pixel-wise reconstruction preserva- tion structural details. perceptual loss utilizes pre-trained deep neural network e.g., vgg extract high-level feature representations predicted ground truth images. instead mea- suring pixel-wise differences, loss compares differ- ences feature space, making generated images visually realistic closer human perception. per- ceptual loss expressed lper represents feature map extracted i-th layer pre-trained network. adversarial loss inspired generative adversarial networks gans used improve realism generated images. discriminator introduced distin- guish real reection-free images generated images. adversarial loss formulated ladv elog elog represents discriminator network. generator aims minimize loss, making generated images indistinguishable real ones. addition reconstruction loss, perceptual loss, ad- versarial loss, loss functions also utilized enhance reection removal performance. one example ex- clusion loss encourages separation transmission reection layers minimizing structural correlation. loss enforces gradient decorrela- tion multiple scales. formulated lexcl downsampled versions different scales, frobenius norm, measures correlation gradients. total variation loss regularization technique com- monly used image processing tasks promote smoothness reduce noise artifacts. encourages spatial continu- ity minimizing differences neighboring pix- els, preventing excessive sharp variations. tvloss partic- ularly useful reection removal denoising, super- resolution tasks, helps generate cleaner visually appealing results reducing undesired texture ar- tifacts preserving important image structures. contextual loss used preserve ne-grained details image generation tasks focusing feature sim- ilarity rather direct pixel-wise differences. formula contextual loss typically expressed lcxf, log max cxij represent feature activations pre- trained network generated target images, respec- tively. contextual similarity cxij measures correla- tion feature vectors, ensuring generated im- age retains important structural patterns reference. loss particularly useful reection removal, style transfer, image synthesis, helps maintain perceptual consistency allowing exibility pixel arrangements. addition traditional loss functions, evaluation metrics also directly used loss terms. instance, zheng al. directly incorporates psnr, ssim, si. psnr ensures high-delity reconstruction, ssim pre- serves structural similarity, enhances overall structural consistency. combining metrics, model improves reection removal performance. datasets evaluation metrics data acquisition datasets sirr critical aspect developing effective deep learning models. datasets vary size, image source, type annotations provided. gen- eral, categorized two main types synthetic datasets real-world datasets. ... synthetic datasets synthetic datasets created simulating reection phenomenon using computer graphics techniques. al- lows precise control various factors in- tensity blurriness reection, well presence ghosting effects. common methods creating synthetic datasets include image mixing combining two images different co- efcients represent background reection layers. reection blur applying gaussian blur reection layer mimic out-of-focus effect. brightness adjustment adaptively adjusting bright- ness contrast create realistic reections. physics-based rendering using physics-based meth- ods render reections. ... real-world datasets real-world datasets captured using cameras real- world environments. provides realistic diverse data, also makes challenging obtain accu- rate ground truth images. common methods creating real- world datasets include manual glass removal capturing images without glass obtain ground truth. raw data subtraction subtracting reection mixed image raw data space. flashno-ash pairs capturing images without ash exploit ash-only cues. polarization using polarization cameras capture im- ages different polarization angles. controlled environments capturing images con- trolled environments varying lighting, glass thickness, camera settings. current public datasets several public datasets created facilitate development evaluation deep learning models sirr. datasets vary size, image source, type annotations provided. table summarizes impor- tant datasets sirr, including usage training, testing, both, number image pairs, average resolution, whether collect real synthetic images. datasets, nature real, rela- tively small contain real-world images ground truth transmission layers. others, sir ceil larger include synthetic real- world images. sir dataset particularly notable diversity, includes images varying blur levels glass thicknesses. ceil dataset, hand, designed challenging, includes images strong reections. recent datasets, cdr created address limitations earlier datasets. cdr dataset categorized according reection types contains im- ages perfect alignment mixed transmis- sion images includes misaligned raw ashambient images. largest recent dataset rrw contains high-resolution real-world reection table comparison existing important reection removal datasets. syn synthetic data, real data dataset year usage pair number average resolution realsyn ceil traintest syn rid train syn sir test real real traintest real nature traintest real cdr train real sir train real cid traintest syn rrw train real pairs. dataset particularly valuable training deep learning models, provides large number images. choice dataset depends specic application desired properties reection removal algorithm. general, larger diverse datasets preferred, enable training robust generalizable models. evaluation metrics survey, provide comprehensive summary evaluation metrics commonly employed deep learning- based reection removal. metrics broadly clas- sied two categories quantitative metrics qualitative metrics. ... quantitative metrics quantitative metrics used objectively measure difference predicted transmission layer ground-truth transmission layer. commonly used quantitative metrics include psnr peak signal-to-noise ratio measures dif- ference predicted transmission layer ground-truth transmission layer. ssim structural similarity evaluates similarity predicted ground-truth transmission layers three aspects luminance, contrast, structure. mse mean squared error calculates average squared difference predicted ground- truth transmission layers. local mse lmse evaluates local structure sim- ilarity calculating similarity local patch. normalized cross correlation ncc measures correlation predicted ground-truth transmission layers normalizing overall in- tensity. structure index evaluates structural similar- ity predicted ground-truth transmis- sion layers based covariance variance. ... qualitative metrics qualitative metrics, hand, used subjec- tively evaluate visual quality predicted transmission images. one common qualitative metric perceptual user study human users compare predicted transmission images ground-truth images rate quality. choice evaluation metrics depends specic application desired properties reection re- moval algorithm. general, combination quantitative qualitative metrics used provide comprehensive evaluation algorithms performance. discussion challenges current sirr research one biggest challenges sirr research lack large, high-quality training datasets represent vari- ety reection types across different surfaces lighting conditions. reection removal relies supervised learning, requires well-labeled dataset clear ground-truth images training. however, creating collecting datasets time- labor-consuming. moreover, absence suitable test sets real-world reection sce- narios presents another challenge. test sets in- clude high-quality images also wide range re- ective surfaces, lighting conditions, material properties e.g., building glass, car window, smooth metal surface, rough metal surface, etc.. without comprehensive datasets, model evaluation remains limited often unreliable deploying real world. furthermore, lack datasets sirr research also made exploring complex network architectures less valu- able. limited data, simpler models like unet already achieving state-of-the-art results, making development complex models unnecessary slowing progress evolution sirr network designs. intricate archi- tectures prone overtting small datasets, preventing reaching full potential. consequently, aca- demic research eld stagnating, resulting fewer in- novations network design sirr. addition, inherent complexity sirr task it- self compounds challenges. reections vary type, intensity, interaction scenesome completely obscure background, turning task form image inpainting. wide variety makes difcult clearly dene sirr task involve purely re- moving reections, also need reconstruct missing background details? lack comprehensive task de- nition hinders development consistent methodologies, reliable evaluation metrics, meaningful comparisons. make real progress, academic community must come clearer inclusive task denition better captures complexity reection removal develop specic guidelines handling different reection scenarios. future directions one pressing future directions sirr re- search creation large-scale, high-quality datasets cover wide variety reection types, surfaces, light- ing conditions. efforts also focus curating test sets contain clean ground-truth images without re- ections, also cover real-world reective materials e.g., different types glass metals various environmental factors e.g., lighting variations reections challeng- ing environments. test sets enable accurate training better evaluation metrics, ultimately improving generalization capabilities sirr models. address data scarcity issues, appeal collaborations research institutions, industry, development powerful synthetic data generation methods. hand, future sirr developmentcould greatly benet integration advanced aigc models. utilizing large vision foundation models, researchers en- hance scene understanding improve semantic reasoning, large language models provide deeper, descriptive insights scene content relationships re- ections transmissions. additionally, combining multimodal information e.g., text descriptions, depth information, semantic segmen- tation strengthen reection-transmission separation leveraging complementary insights multimodal re- sources. believe fusion could result context-aware precise reection removal systems. mentioned earlier, clarifying denition sirr task another critical direction. eld needs precisely dene whether goal limited reection removal ex- tends background reconstruction cases severe reec- tions. establishing clear boundaries enable stan- dardized evaluations fair comparisons, ultimately leading development effective solutions. limitations work several limitations. first, due key- words databases chosen search query, related research may missed. addition, research published english, exceeds prescribed time frame, provide enough technical information, included. nevertheless, paper provides comprehensive analysis existing literature, based sample pa- pers sourced key venues. aim review provide readers clear rapid understanding key developments sirr eld, including current state, challenges, future directions. second, in- clude benchmark results paper, different methods datasets often unique training strategies evaluation process. address this, develop unied evaluation framework future, pro- vide fair platform compare publicly available datasets. conclusion article presents overview sirr utilizing deep learning methods. rst describe methodology used bibliographic search. then, provide comprehensive analysis research categorized single-stage, two-stage, multi-stage approaches. next, introduce com- monly used publicly available datasets evaluation met- rics. finally, discuss open challenges future direc- tions sirr research. argue clearer deni- tions larger-scale datasets, current deep learning methods better realize data-driven potential, leading rapid advancements sirr eld near future. references levin, zomet, weiss, separating reections single image using local features, proceedings ieee computer society conference computer vision pattern recognition, cvpr ieee, vol. pp. ii. levin, zomet, weiss, learning perceive transparency statistics natural scenes, advances neural information processing systems, vol. levin weiss, user assisted separation reections single image using sparsity prior, ieee transactions pattern analysis machine intelligence, vol. no. pp. brown, single image layer separation using relative smoothness, proceedings ieee conference computer vi- sion pattern recognition, pp. yang, ma, zheng, cai, xu, fast single im- age reection suppression via convex optimization, proceedings ieeecvf conference computer vision pattern recognition, pp. wan, shi, hwee, kot, depth eld guided reection removal, ieee international conference image processing. ieee, pp. shih, krishnan, durand, freeman, reection re- moval using ghosting cues, proceedings ieee conference computer vision pattern recognition, pp. song, zhang, zhang, luo, fan, ren, lu, robust single image reection removal adversarial attacks, proceedings ieeecvf conference computer vision pattern recognition, pp. wei, yang, fu, wipf, huang, single image reec- tion removal exploiting misaligned training data network enhance- ments, proceedings ieeecvf conference computer vision pattern recognition, pp. wan, shi, duan, tan, kot, benchmark- ing single-image reection removal algorithms, proceedings ieee international conference computer vision, pp. wan, shi, li, hong, duan, kot, bench- marking single-image reection removal algorithms, ieee transac- tions pattern analysis machine intelligence, vol. no. pp. amanlou, suratgar, tavoosi, mohammadzadeh, mosavi, single-image reection removal using deep learning systematic review, ieee access, vol. pp. fan, yang, hua, chen, wipf, generic deep archi- tecture single image reection removal image smoothing, proceedings ieee international conference computer vision, pp. zhang, ng, chen, single image reection separation perceptual losses, proceedings ieee conference computer vision pattern recognition, pp. guo, trash treasure? interactive dual-stream strategy single image reection separation, advances neural information processing systems, vol. pp. guo, single image reection separation via component synergy, proceedings ieeecvf international conference computer vision, pp. wan, shi, li, duan, kot, reection scene separation single image, proceedings ieeecvf con- ference computer vision pattern recognition, pp. li, liu, yi, li, ren, zuo, two-stage single image reection removal reection-aware guidance, applied in- telligence, vol. no. pp. wan, shi, duan, tan, kot, crrn multi- scale guided concurrent reection removal network, proceedings ieee conference computer vision pattern recognition, pp. yang, gong, liu, shi, seeing deeply bidirection- ally deep learning approach single image reection removal, proceedings european conference computer vision, pp. li, yang, he, lin, hopcroft, single image reection removal cascaded renement, proceedings ieeecvf conference computer vision pattern recognition, pp. wen, tan, qin, liu, han, he, single image reection removal beyond linearity, proceedings ieeecvf conference computer vision pattern recognition, pp. dong, xu, yang, bao, xu, lau, location- aware single image reection removal, proceedings ieeecvf international conference computer vision, pp. zheng, shi, chen, jiang, duan, kot, single image reection removal absorption effect, proceedings ieeecvf conference computer vision pattern recognition, pp. wan, shi, li, duan, tan, kot, corrn cooperative reection removal network, ieee transactions pat- tern analysis machine intelligence, vol. no. pp. feng, pei, jia, chen, zhang, lu, deep-masking generative network unied framework background restoration superimposed images, ieee transactions image processing, vol. pp. kim, huo, yoon, single image reection removal physically-based training images, proceedings ieeecvf conference computer vision pattern recognition, pp. zhu, fu, jiang, zhang, sun, chen, zha, li, revisiting single image reection removal wild, pro- ceedings ieeecvf conference computer vision pattern recognition, pp. zhong, hong, weng, liang, shi, language-guided image reection separation, proceedings ieeecvf confer- ence computer vision pattern recognition, pp. chang, lu, cheng, chiu, single image re- ection removal edge guidance, reection classier, recurrent decomposition, proceedings ieeecvf winter conference applications computer vision, pp. prasad, boregowda, mitra, chowdhury, al., v-desirr fast deep embedded single image reection removal, proceed- ings ieeecvf international conference computer vision, pp. johnson, alahi, fei-fei, perceptual losses real-time style transfer super-resolution, computer visioneccv european conference, amsterdam, netherlands, october proceedings, part springer, pp. goodfellow, pouget-abadie, mirza, xu, warde-farley, ozair, courville, bengio, generative adversarial nets, advances neural information processing systems, vol. lei, huang, qi, zhao, sun, yan, chen, categorized reection removal dataset diverse real-world scenes, proceedings ieeecvf conference computer vision pattern recognition, pp. wan, shi, li, hong, duan, kot, benchmarking single-image reection removal algorithms, ieee transacations pattern analysis machine intelligence, wang, xu, wan, he, shi, duan, background scene recovery image looking colored glass, ieee transactions multimedia,", "published_date": "2025-02-12T22:57:06+00:00"}
{"id": "2502.07869v1", "title": "EventEgo3D++: 3D Human Motion Capture from a Head-Mounted Event Camera", "authors": ["Christen Millerdurai", "Hiroyasu Akada", "Jian Wang", "Diogo Luvizon", "Alain Pagani", "Didier Stricker", "Christian Theobalt", "Vladislav Golyanik"], "summary": "monocular egocentric human motion capture remains significant challenge, particularly conditions low lighting fast movements, common head-mounted device applications. existing methods rely rgb cameras often fail conditions. address limitations, introduce eventegod, first approach leverages monocular event camera fisheye lens human motion capture. event cameras excel high-speed scenarios varying illumination due high temporal resolution, providing reliable cues accurate human motion capture. eventegod leverages lnes representation event streams enable precise reconstructions. also developed mobile head-mounted device hmd prototype equipped event camera, capturing comprehensive dataset includes real event observations controlled studio environments in-the-wild settings, addition synthetic dataset. additionally, provide holistic dataset, include allocentric rgb streams offer different perspectives hmd wearer, along corresponding smpl body model. experiments demonstrate eventegod achieves superior accuracy robustness compared existing solutions, even challenging conditions. moreover, method supports real-time pose updates rate hz. work extension eventegod approach cvpr advances state art egocentric human motion capture. details, visit project page", "full_text": "eventegod human motion capture head-mounted event camera christen millerdurai,, hiroyasu akada, jian wang, diogo luvizon, alain pagani, didier stricker, christian theobalt, vladislav golyanik visual computing artificial intelligence, max planck institute informatics, sic, stuhlsatzenhausweg saarbrucken, saarland, germany. augmented vision, german research center artificial intelligence dfki, trippstadter str. kaiserslautern, rhineland-palatinate, germany. contributing authors christen.millerduraidfki.de hakadampi-inf.mpg.de jianwangmpi-inf.mpg.de dluvizonmpi-inf.mpg.de alain.paganidfki.de didier.strickerdfki.de theobaltmpi-inf.mpg.de golyanikmpi-inf.mpg.de abstract monocular egocentric human motion capture remains significant challenge, particularly conditions low lighting fast movements, common head-mounted device applications. existing methods rely rgb cameras often fail conditions. address limitations, introduce eventegod, first approach leverages monocular event camera fisheye lens human motion capture. event cameras excel high-speed scenarios varying illumination due high temporal resolution, providing reliable cues accurate human motion capture. eventegod leverages lnes representation event streams enable precise reconstructions. also developed mobile head-mounted device hmd prototype equipped event camera, capturing comprehensive dataset includes real event observations controlled studio environments in-the-wild settings, addition synthetic dataset. additionally, provide holistic dataset, include allocentric rgb streams offer different perspectives hmd wearer, along corresponding smpl body model. experiments demonstrate eventegod achieves superior accuracy robustness compared existing solutions, even challenging conditions. moreover, method supports real-time pose updates rate hz. work extension eventegod approach cvpr advances state art egocentric human motion capture. details, visit project page keywords event-based vision, human pose estimation, egocentric vision, vrar. introduction head-mounted devices hmds hold significant potential become next major platform mobile pervasive computing, offering diverse applications many fields education, driving, personal assistance systems, gaming. hmds enhance user flexibility, allowing individuals move freely explore surroundings seamlessly. result, cs.cv feb egocentric human pose estimation emerged active research area, numerous studies focusing recovering human poses using down-facing fisheye rgb cameras mounted hmds rhodin al, al, zhao al, wang al, akada al, wang al, tome al, liu al, al, wang al, kang al, kang lee, although experimental prototypes demonstrated high human pose estimation accuracy, setups several limitations. firstly, rgb cameras prone over- under-exposure motion blur, especially low-light conditions rapid movements, common hmd applications. secondly, cameras consume relatively high power, making less efficient mobile devices. furthermore, recording image frames synchronously demands high data processing throughput, significant burden real-time applications. limitations particularly problematic hmds, efficient reliable performance crucial. light challenges, work motivated observation many challenges associated rgb-based hmds mitigated use event cameras. event cameras record streams asynchronous per-pixel brightness changes high temporal resolution order microseconds, support increased dynamic range consume less power order tens rgb cameras, consume watts gallego al, leverage benefits, build lightweight hmd integrates event camera fisheye lens. setup allows precise capture fast dynamic movements much lower power consumption, making well-suited real-time applications. building advantages, develop lightweight hmd equipped event camera fisheye lens, enabling precise capture fast dynamic movements notably lower power consumption. details event camera efficiency found app. however, existing rgb-based pose estimation techniques, particularly learning-based methods, cannot straightforwardly repurposed event streams. also, methods typically slow ideal real-time applications. dedicated approaches required fully leverage advantages event cameras, demonstrated recent progress event-based reconstruction across various scenarios al, rudnev al, zou al, jiang al, millerdurai al, furthermore, egocentric hmd setup utilising event camera introduces two additional challenges. firstly, moving event camera generates significant amount background events, making difficult isolate user-specific events required accurate pose estimation. secondly, event cameras fail generate events situations hmd user remains stationary motion detected. previous work, eventegod millerdurai al, addressed challenges introducing lightweight neural network processes egocentric event streams estimate human pose real time. incorporating confidence scores, network assigns higher weights human-generated events background events, enabling robust pose estimation even presence significant background noise. additionally, frame buffer mechanism introduced maintain stable pose predictions even limited number events captured due lack motions. paper, substantially extend eventegod millerdurai al, eventegod, includes several key improvements additions. firstly, improve pose estimation accuracy eventegod framework millerdurai al, incorporating additional supervision projection loss bone loss. secondly, addition synthetic dataset eed-s studio-recorded real dataset eed-r included eventegod, introduce new in-the-wild real dataset eed-w ground truth poses, providing additional data fine-tuning evaluating method outdoor environments. thirdly, provide allocentric rgb views smpl loper al, body annotations real datasets, thereby providing comprehensive dataset advancing research. inclusion in-the-wild data ensures robustness real-world conditions, smpl body annotations provide dense human correspondences, making datasets human pose estimation real-time demo prediction eed hmd datasets low light fast motion input fig. eventegod builds upon work eventegod millerdurai al, real-time human motion capture egocentric event streams photograph new head-mounted device hmd custom-designed egocentric fisheye event camera top visualisations synthetically rendered dataset real dataset recorded hmd bottom real-time demo achieving pose update rate visualisation real event streams top corresponding human poses third-person perspective. valuable future research applicable wide range applications. remainder paper organised follows. section reviews related work egocentric human motion capture, event-based reconstruction, alternative sensors human pose estimation. section provides detailed description eventegod method, focusing neural network architecture newly introduced losses. section describes design implementation mobile head-mounted device prototype synthetic dataset. additionally, outline recording procedures real datasets, including studio in-the-wild settings. section presents comprehensive evaluation method synthetic real datasets. finally, section discusses limitations approach, section offers concluding remarks. related work next review related methods egocentric human pose estimation event-based reconstruction. egocentric human pose estimation human pose estimation egocentric monocular stereo rgb views actively studied last decade. earliest approaches optimisation-based rhodin al, field promptly adopted neural architectures following state art human pose estimation. thus, follow-up methods used two-stream cnn architecture al, auto-encoders monocular tome al, stereo inputs zhao al, akada al, kang al, another work focused automatic calibration fisheye cameras widely used egocentric setting zhang al, recent papers leverage human motion priors temporal constraints predictions global coordinate frame wang al, reinforcement learning improved physical plausibility estimated motions yuan kitani, luo al, semi-supervised gan-based human pose enhancement external views wang al, depth estimation wang al, scene-conditioned denoising diffusion probabilistic models zhang al, khirodkar address slightly different setting use multi-stream transformer capture multiple humans front-facing egocentric views. meanwhile, wang focus egocentric whole-body motion capture single fisheye camera, utilising fisheyevit feature extraction, specialised networks hand tracking, diffusion-based model refining motion estimates. works demonstrated promising results pushed field forward. they, however, designed synchronously operating rgb cameras and, henceas every rgb-based methodsuffer inherent limitations sensors detailed sec. thus, support real-time frame rates al, tome al, moreover, unreasonable expect rgb-based approaches easily adapted event streams. contrast, propose approach first time accounts new data type context egocentric vision events estimates human poses high pose update rates. last least, none existing datasets training evaluation egocentric human pose estimation techniques related problems rhodin al, al, tome al, wang al, zhang al, wang al, pan al, khirodkar al, wang al, provide event streams frames framerate sufficient generate events event steam simulators rebecq al, evaluate train approach, synthesise record necessary datasets i.e., synthetic, real, background augmentation required investigate event-based human pose estimation hmds. event-based methods reconstruction substantial discrepancies rgb frames asynchronous event data spurred development specialised pose estimation methods, ranging purely event-based approaches rudnev al, nehvi al, zou al, wang al, xue al, chen al, rudnev al, millerdurai al, rgb-event hybrid methods al, zou al, park al, jiang al, although hybrid solutions offer complementary information, also significantly increase bandwidth usage, power consumption, computational overheadfactors become especially problematic battery-powered head-mounted displays. comparison bandwidth usage power consumption rgb event cameras, please see app. consequently, work adopts purely event-based paradigm. within event-based domain, nehvi track non-rigid objects polygonal meshes parametric models differentiable event stream simulator. rudnev synthesise dataset human hands train neural hand pose tracker kalman filter. introduce lightweight lnes representation events learning improvement upon event frames. next, xue optimise parameters hand model associating events mesh faces using expectation-maximisation framework assuming events predominantly triggered hand contours. works represent events spatiotemporal points space encode either point clouds chen al, millerdurai al, consequently, approaches slow due different reasons iterative optimisation computationally expensive operations point clouds, notable exception eventhands rudnev al, achieving khz hand pose update rates. work, leverage lnes rudnev al, operates independently input event count, facilitates real-time inference, efficiently processed using neural components e.g. cnn layers. unlike previously discussed approaches, method specifically designed egocentric setting achieves highest accuracy among methods compared. particular, incorporate novel residual mechanism propagates events event history previous frame current one, prioritising events triggered around human. also helpful events triggered due lack motion. alternate sensors human pose estimation inertial measurement units imus widely used human pose estimation, often relying multiple sensorstypically sixstrategically placed head, arms, pelvis, legs track body movements von marcard al, huang al, al, jiang al, al, systems deliver reasonable accuracy, tend cumbersome inflexible due large number sensors required associated calibration demands. recent advancements reduced reliance multiple sensors, systems using three imus aliakbarian al, winkler al, jiang al, lee al, jiang al, zheng al, jiang al, typically mounted head hands, making practical applications virtual reality vr. however, even fewer sensors, systems remain prone issues like sensor drift frequent recalibration rapid motion, limiting effectiveness high-dynamic scenarios. another line research fuses imus additional modalities rgb data gilbert al, von marcard al, malleson al, guzov al, al, dai al, depth maps helten al, offering improved global positioning fine-grained pose estimates. yet, vision-based methods remain sensitive low-light environments, occlusions, motion blur, particularly subjects move rapidly operate challenging lighting. although diffusion-based approaches al, al, guzov al, yielded smoother poses, rely future frames achieve robust predictions, making unsuitable real-time usage. contrast, propose purely event-camera-based approach, operates high frame rates i.e. fps exhibits robustness challenging conditions like low light fast motion. mounting single event camera head-mounted display hmd, eliminate need additional body-worn sensors, thus simplifying setup avoiding drift issues. setup handles large lighting variations also naturally accommodates rapid head body movements, making especially well-suited real-time, egocentric human pose estimation. eventegod approach approach estimates human poses egocentric monocular event camera fisheye lens. first explain event camera model sec. describe proposed framework sec. event camera preliminaries event cameras capture event streams, i.e. temporal sequence contains discrete packets asynchronous events indicate brightness change pixel sensor. event tuple form xi, yi, ti, i-th index representing event fired pixel location xi, corresponding timestamp polarity timestamps modern event cameras temporal resolution. event generated change logarithmic brightness pixel location xi, exceeds predefined threshold i.e., lxi, yi, lxi, yi, represents previous triggering time pixel location. indicates brightness decreased otherwise, increased modern neural computer vision architectures rudnev al, lan al, jiang al, require event streams converted regular representation, usually end, adopt locally normalised event surfaces lnes rudnev al, aggregate event tuples compact representation function time windows. time window size constructed collecting events first event relative given time window ek, events time window stored lnes frame denoted rhw event within time window, ek, update lnes frame lxi, yi, tit event occurring pixel location updates corresponding pixel lnes frame. lq- input events sec. residual event propagation module sec. egocentric pose module sec. head mounted device sec. segmentation decoder confidence decoder encoder lq- pose hm-to-d lifting block frame buffer cq- heatmap decoder predicted pose fig. overview eventegod approach. hmd captures egocentric event stream, converted series lnes frames rudnev al, inputs neural architecture estimate poses hmd user. residual event propagation module repm emphasises events triggered around human considering temporal context observations realised frame buffer event decay based event confidence. repm, hence, helps encoder-decoder lnes heatmaps heatmap lifting module estimate accurate human poses. method supervised ground-truth human body masks, heatmaps human poses. note visualisation. visualisation purposes, convert -channel lnes frame -channel rgb image mapping positive-polarity channel red channel, negative-polarity channel blue channel, setting green channel zero. architecture eventegod approach takes consecutive lnes frames ln, lqr inputs regresses camera-centric human body pose per lnes frame, denoted jn, jqr include joints head, neck, shoulders, elbows, wrists, hips, knees, ankles, feet. proposed framework includes two modules see fig. first, egocentric pose module epm estimates coordinates human body joints. subsequently, residual event propagation module repm propagates events previous lnes frame current one. repm module allows framework focus events triggered around human background retain human pose events generated due absence motions. egocentric pose module epm regress joints input two steps joint heatmap estimation heatmap-to-d lifting. joint heatmap estimation. estimate joint heatmaps, develop u-net-based architecture ronneberger al, here, utilise blaze blocks bazarevsky al, layers encoder decoder achieve real-time performance. encoder decoder five layers see fig. encoder takes input heatmap decoder generates joint heatmaps different resolution sizes layer. then, average create heatmaps body joints final output. details heatmap averaging scheme, please refer app. segmentation decoder heatmap decoder encoder confidence decoder hm-to-d lifting block fig. network architecture eventegod. encoder takes current lnes frame input. heatmap decoder predicts heatmaps body joints, fed hm-to-d lifting block regress joint locations. segmentation decoder generates human body mask, confidence decoder subsequently produces feature map acts human body mask create confidence map, highlighting important regions egocentric view. network supervised using mean square error mse ground-truth heatmaps predicted ones hq,b vq,b hq,b vq,b, hq,b hq,b predicted ground-truth heatmaps b-th joint vq,b visibility b-th joint number body joints element-wise multiplication. visibility mask vq,b ensures joints visible thus relevant pose estimation contribute loss calculation. particularly important scenarios joints may occluded view, arms extended feet positioned behind torso. applying visibility mask allows network training focus joints detectable input lnes frames instead occluded out-of-view joints. heatmap-to-d lifting module. following previous works tome al, pavlakos al, heatmap-to-d hm-to-d lifting module takes estimated heatmaps input outputs joints module based three convolutional layers three dense layers see fig. supervise module using three distinct loss terms mse joints loss, mse joints reprojected joints reprojection loss, error bone orientations bone lengths bone loss. loss computed using ground-truth joint positions estimated ones frame index ljd jq,r vq,r jq,r vq,r, number body joints, vq,r visibility r-th joint jq,r jq,r predicted ground-truth r-th joint, respectively. reprojection loss denoted ljd, compares projections predicted ground-truth joints, formulated ljd jq,r vq,r jq,r vq,r, camera projection function fisheye lens, projecting joints joints. bone loss, denoted lba, captures difference predicted ground-truth bone orientations lengths, allowing network learn spatial relationships joints bones. bone orientations, use negative cosine similarity loss, defined cos pq,l, pq,l number bones, pq,l l-th predicted bone vector, pq,l corresponding ground-truth bone vector, cos ab. formulation, cos, penalizes misalignment predicted bone ground-truth counterpart. bone lengths, compute mse predicted ground-truth bone vectors, denoted lbl lbl pq,l pq,l. overall bone loss lba computed combining orientation length losses lba bllbl, bl. weights assigned orientation length losses, respectively. overall, combined supervision loss joints, denoted ljoints, defined ljoints jdljd jdljd balba set weight loss jd., jd., ba. residual event propagation module repm contrast stationary camera setups, egocentric cameras mounted head-mounted displays hmds experience diverse movement, affects number events capture. intense movements hmd users often result large number events, significant portion coming background. conversely, minimal motion results events. address issues, introduce residual event propagation module repm. repm helps network focus events generated human body incorporating information previous frames. focusing human-generated events, network ensures events given higher importance background events. simultaneously, propagating information previous frames helps maintain stable pose estimates even events observed. repm comprises segmentation decoder, confidence decoder, frame buffer. segmentation decoder estimates human body masks. next, confidence decoder takes body masks inputs produce feature maps. feature maps used body masks produce confidence maps indicate regions egocentric view place importance on. lastly, frame buffer stores past input frame corresponding confidence map, providing weighting important regions current frame see top part fig. segmentation decoder. segmentation decoder estimates human body mask hmd user egocentric lnes views. architectures module heatmap decoder except final layer outputs human body masks. use feature maps multiple layers encoder inputs segmentation decoder see fig. segmentation decoder supervised cross-entropy loss lseg logsq log sq, predicted ground-truth segmentation masks, respectively. confidence decoder. confidence decoder four-layer convolution network takes human body mask input produces feature map feature map used combination produce cq- fig. visualisation frame buffering human-weighted event generation. frame buffer holds previous input frame previous confident map weighted added current lnes frame produce observe events generated subject highlighted background events. confidence map sigmoidsq fq, sigmoid sigmoid operation. frame buffer. frame buffer stores previous confidence map previous input frame note initialise frame buffer zeros first frame. compute current input frame lq, retrieve frame buffer using following expression denotes lnes frame current time represents element-wise addition. normalise values range note, resized applying eqn. see fig. exemplary visualisation components used eqn. loss terms supervision overall, method supervised heatmap loss eqn. joint loss ljoints eqn. segmentation loss lseg eqn. follows jointsljoints hlh seglseg, set weight loss joints, seg.. egocentric setup datasets work, introduce three new datasets eed-r, eed-w, eed-s. datasets used train, evaluate, fine-tune eventegod method. eed-r eed-w real-world datasets captured using head-mounted device hmd. eed-r dataset recorded studio environment controlled lighting background conditions. contrast, eed-w includes indoor outdoor environments real world, offering broader range scenarios accurately represent real-world conditions. eed-s large-scale synthetic dataset camera parameters applied real-world camera. eed-s provides diverse array human poses within wide variety virtual backgrounds. together, datasets support comprehensive approach developing refining eventegod method. moreover, pre-training synthetic dataset fine-tuning real-world datasets allows model handle diverse realistic conditions. real-world data capture section, first describe experimental head-mounted device hmd used create real-world datasets, i.e. eed-r eed-w sec. ... next, outline calibration process hmd setup sec. ... detail procedure generating ground truth data using calibrated hmd sec. ... finally, describe details captured datasets, including diversity coverage sec. ... head-mounted device hmd prototypical device consisting bicycle helmet dvxplorer mini event camera attached helmet .cm away users head strap allows firm attachment head. see fig. use fisheye lens, lensagon bfmsc field view wide field view effectively covers scenarios users arms fully extended. total weight device .kg. device used laptop backpack external power supply real-time on-device computing. compact design flexibility hmd allow users freely move heads perform rapid motions. fig. real-world setup. head-mounted device equipped event camera fisheye lens. camera calibration intrinsic calibration. record event stream moving checkerboard, described muglikar convert stream sequence images using evid rebecq al, intrinsic calibration, utilise scaramuzza projection model scaramuzza al, account radial distortion wide field view fisheye lens head-mounted device hmd. specifically, use matlabs camera calibrator tool mathworks, obtain projection model parameters. extrinsic calibration. obtain egocentric poses smpl loper al, parameters hmd user, first track hmds position motion recording. achieved calibrating hmd equipped checkerboard reference marker allocentric rgb multi-camera setup. step enables track hmds position within coordinate frame multi-camera setup, i.e. world coordinate frame. subsequently, perform hand-eye calibration compute hmd coordinate frame. finally, convert poses smpl parameters world coordinate frame hmds coordinate frame. obtain checkerboard images necessary hand-eye calibration, first generate events checkerboard convert events images using evid rebecq al, ensure uniform event distribution, slide checkerboard diagonally event capture process. final position checkerboard sliding motion serves reference chequerboard position calibration procedure. additional details hand-eye calibration, please refer app. c.. ground truth generation obtain human poses smpl loper al, body parameters using multi-view motion capture setups, captury easymocap specifically, captury rgb-based multi-view motion capture system provides accurate human joint positions easymocap used derive smpl parameters multi-view rgb streams. subsequently, transform human poses smpl parameters world coordinate frame hmd coordinate frame. details accuracy generated ground-truth, please refer app. c.. additionally, generate egocentric human body masks, joint coordinates, joint visibility masks. joint visibility mask indicates whether joint visible occluded egocentric view. details generation human body masks, egocentric joint coordinates, joint visibility masks, refer readers app. note additional metadata. dataset release includes smpl body parameters, meshes, allocentric multi-view rgb streams. supplementary data provided solely future research purposessuch shape estimation clothing reconstructionand used training evaluation framework. allocentric rgb streams egocentric event stream body joints allocentric rgb streams egocentric event stream smpl body meshes eed-r eed-w body joints fig. visualisation example data eed-r left eed-w right datasets. real-world datasets following procedure previous sections, create real-world datasets, eed-r eed-w. eed-r. eed-r studio dataset consists everyday movements, performed different manners various participants. ask twelve subjectspersons different body shapes skin tonesto wear hmd perform different motions e.g. fast multi-view motion capture studio allocentric rgb cameras recording fps. see left part fig. sequence encompasses following motions walking, crouching, pushups, boxing, kicking, dancing, interaction environment, crawling, sports jumping. sports category, participants perform specific activitiesplaying basketball, participating tug war, playing golf. meanwhile, interaction environment category, subjects perform actions picking objects table, sitting chair, moving chair. total, collect sequences containing approximately poses spanning around minutes. sequences include fast-paced actions boxing, kicking, dancing, sports, jumping, comprising approximately frames, well slower-paced activities remaining frames. figure illustrates visibility joint derived smpl body see app. details generation process. observe lower-body joints predominantly occluded out-of-view due camera constraints, visibility ankles. experiments, use eight sequences poses training, two sequences poses validation, two sequences poses testing. eed-w. eed-w in-the-wild dataset recorded varying lighting conditions three different scenes indoor environments, outdoor areas concrete flooring, outdoor areas grass. capture various motions six subjects multi-view motion capture setup allocentric rgb cameras recording fps. see right part fig. motion types eed-w similar specified eed-r. resulted nine sequences totaling approximately poses minutes, roughly frames containing fast-paced motion. shown fig. in-the-wild dataset exhibits lower overall joint visibility compared eed-r. primarily frequent head movements outdoor activities cause parts body intermittently move cameras field view, thereby increasing occlusions. experiments, use five sequences poses training, two sequences poses validation, two sequences poses testing. synthetic data setup addition real-world datasets, propose eed-s, large-scale synthetic dataset. following, first describe virtual human character wearing hmd virtual scenes sec. ... next, explain rendering generation egocentric event stream sec. ... outline ground truth generation proposed dataset sec. ... finally, introduce event augmentation strategy aimed reducing domain gap real-world datasets sec. ... fig. joint visibility smpl body proposed datasets. visibility percentage computed proportion samples joint visible egocentric perspective. synthetic rgb frame simulated event stream human body mask body joints fig. visualisations sample data eed-s. virtual human character background scene utilise smpl body models virtual human users hmd, following body textures randomly sampled surreal dataset varol al, animations driven motions cmu mocap dataset cmu, generating event data, sample motions high frame rates gehrig al, using linear interpolation smpl parameters. background scene, use sized -dimensional plane textures sampled lsun dataset al, scenes illuminated four randomly placed point lights within -meter radius hmd. rendering event stream generation render egocentric views using fisheye camera positioned near virtual humans face, emulating real-world hmd setup. apply random perturbations fisheye camera position account head size variations hmd movement. allows simulating real-world scenarios camera position relative users head may slightly shift. use real-world intrinsic camera parameters sec. render rgb frames human body masks. rendered rgb frames processed vide gehrig al, generate event streams. sample data eed-s shown fig. total, synthesise motion sequences containing approximately human poses events. shown fig. joint visibility predominantly reduced lower body, head remains largely unobstructed. experiments, use sequences poses training, sequences poses validation, sequences poses testing. details configurations used create synthetic dataset, refer readers app. ground truth generation extract body joints smpl model, including head, neck, shoulders, elbows, wrists, hips, knees, ankles, feet. additionally, derive joints, human body masks, visibility masks outlined app. c.. event augmentation original lnes frame background lnes frame augmented lnes frame fig. example scenario event augmentation technique. original lnes frame left augmented lnes frame background events middle create augmented lnes frame right. models trained synthetic data often fail generalise effectively real-world scenarios diverse backgrounds. address issue, propose event-wise augmentation technique background events synthetic dataset, eed-s see fig. first, capture sequences outdoor indoor scenes without humans handheld event camera, creating background event streams. streams converted background lnes frames center image fig. subsequently, apply human body mask eed-s lb, obtaining background lnes frame without region corresponding human body original lnes frame, denoted la. finally, add original lnes frames eed-s generate augmented frame laug right image fig. laug serves input network. experimental evaluation section describes implementation details experiments sec. results including numerical comparisons related methods sec. ablation study validating contributions core method modules sec. well comparisons terms runtime architecture parameters sec. finally, show real-time demo sec. implementation details implement method pytorch paszke al, use adam optimiser kingma ba, batch size eed-s dataset, adopt learning rate iterations. eed-r dataset, train network learning rate iterations. eed-w dataset, use learning rate iterations. modules eventegod architecture jointly trained. network supervised using recent ground-truth human pose within time window constructing lnes frame, i.e. ground-truth pose aligned latest event lnes. set experiments. additional details lnes frames constructed, please refer app. performance metrics reported single geforce rtx real-time demo performed laptop equipped single quadro gpu housed backpack illustrated fig. -b. compare method eventegod eventegod millerdurai al, cvpr version work. addition, adapt three existing pose estimation methods problem setting tome egocentric rgb-based methods modify first convolution layer accept lnes representation. specifically, replace original -channel input convolution, designed rgb images, -channel input convolution layer compatible lnes representation. rudnev event-based method takes lnes input estimates hand poses modify output layer regress human poses. specifically, modify output linear layer predict body joints fair comparison, adopt training strategy, i.e. learning rates iterations, competing methods ours. follow previous works al, zhao al, akada al, wang al, report mean per joint position error mpjpe mpjpe procrustes alignment kendall, pa-mpjpe. comparisons related state art experiment eed-s. firstly, evaluate approach test set synthetic eed-s dataset. ensure fair comparison, train method competing methods tome al, al, rudnev al, millerdurai al, training set eed-s dataset. table observe method achieves lowest mpjpe average, outperforming previous work millerdurai al, well competing methods. method demonstrates superior performance estimating lower body joints, offering improvement rudnev gains exceeding ankle foot joints. robustness particularly notable given significant radial distortion caused fisheye lens setup, makes feet appear much smaller input compared upper body. despite distortion, method effectively estimates position feet small joint areas, highlighting accuracy reliability challenging conditions. experiment eed-r. experiment, first pretrain methods eed-s dataset. fine-tune methods using eed-r dataset evaluate performance eed-r test set. eed-s dataset includes wide range human motions, domain gap synthetic real-world cases. gap arises factors uncontrolled diverse movement patterns, well wearer-specific variability, including differences posture movement style. fine-tuning pose estimation methods real-world data reveal potential real-world scenarios. table observe method significantly outperforms comparison methods large margin. specifically, method achieves improvements mpjpe average compared best-competing method, i.e. eventegod millerdurai al, also worth noting method demonstrates superiority competing methods especially complex motions involving interaction environment, crawling, kicking, sports dancing. motions often come fast-paced jittery movements hmd, generating substantial background event noise. notably, method excels handling challenging scenarios. tome al. al. rudnev al. eventegoours mpjpe mpjpe mpjpe mpjpe event stream rgb view fig. qualitative results eed-r. mpjpe values shown figures. pose predictions ground-truth poses visualised red green, respectively. fig. shows visual outputs approach compared methods. input lnes frame noisy events generated hand sometimes exhibit close proximity generated background. scenarios, competing methods often struggle, predicting incorrect hand positions. however, method estimates reasonably accurate poses even presence noisy background events. experiment image-based reconstructions eed-r. experiment, first convert event streams image sequences using rebecq train evaluate rgb-based methods al, tome al, reconstructed image sequences. table observe method, directly processes event streams, significantly outperforms rgb-based methods large method metric head neck shoulder elbow wrist hip knee ankle foot avg. tome mpjpe pa-mpjpe mpjpe pa-mpjpe rudnev mpjpe pa-mpjpe millerdurai mpjpe pa-mpjpe eventegod mpjpe pa-mpjpe table numerical comparisons eed-s dataset mm. denotes standard deviation mpjpe pa-mpjpe across body joints. eventegod outperforms competing methods, particularly lower body joints, achieving best mpjpe. additionally, method improves lower body performance compared eventegod millerdurai al, method metric walk crouch pushup boxing kick dance inter. env. crawl sports jump avg. tome mpjpe pa-mpjpe mpjpe pa-mpjpe rudnev mpjpe pa-mpjpe millerdurai mpjpe pa-mpjpe eventegod mpjpe pa-mpjpe table numerical comparisons eed-r dataset mm. denotes standard deviation mpjpe pa-mpjpe across actions. eventegod outperforms existing approaches activities substantial margin achieves improvement rudnev method metric walk crouch pushup boxing kick dance inter. env. crawl sports jump avg. tome mpjpe pa-mpjpe mpjpe pa-mpjpe rudnev mpjpe pa-mpjpe millerdurai mpjpe pa-mpjpe eventegod mpjpe pa-mpjpe table numerical comparisons eed-w dataset mm. denotes standard deviation mpjpe pa-mpjpe across actions. method, eventegod, outperforms existing approaches lowest mpjpe activities. see improvement rudnev interaction environment inter. env., showing robustness method events generated environment. tome al. al. rudnev al. eventegoours mpjpe mpjpe mpjpe mpjpe event stream rgb view fig. qualitative results eed-w. mpjpe values shown figures. pose predictions ground-truth poses visualised red green, respectively. margin. specifically, achieve average improvement mpjpe compared best-performing rgb-based method, tome method mpjpe pa-mpjpe tome eventegod table numerical comparisons eed-r dataset mm. methods marked process reconstructed images obtained event streams using rebecq method marked processes event streams directly. performance gap likely attributed artefacts introduced image reconstruction process. significant motion person background, event camera produces large number events, leading relatively clear reconstructions see fig. however, scenarios sparse eventssuch slower minimal motionthe reconstructed images degrade dramatically, making difficult rgb-based methods accurately estimate human poses. figure illustrates issue although event data captures lower body e.g. right leg, details lost reconstructed images, leading poorer performance rgb-based methods. contrast, method, leverages raw event streams, continues produce reasonably accurate poses even challenging conditions. additional details conversion process, refer readers app. tome al. al. eventegoours event stream rgb view reconstructed image mpjpe mpjpe mpjpe fig. qualitative results eed-r. mpjpe values shown figures. pose predictions ground-truth poses visualised red green, respectively. tome process reconstructed images obtained event streams, whereas eventego directly processes event streams. experiment eed-w. also interested pose estimation performance in-the-wild real-world scenarios, i.e. eed-w. therefore, experiment, initially pretrain methods eed-s dataset fine-tune using training set eed-w evaluation test set eed-w. tab. observe approach achieves best mpjpe pa-mpjpe scores among methods. compared competing methods, significant performance improvement, ranging improvement rudnev improvement tome mpjpe. furthermore, achieve high accuracy specific motions, crawling, crouching, pushups, boxing. reflects strength handling diverse complex human activities. additionally, achieve lowest standard deviation errors average. result indicates method robust across different types motion, consistently providing accurate pose estimations wide range activities. fig. shows visual outputs approach compared methods. comparison methods fail handle substantial amount events generated background scene. challenging scenario, however, method estimates reasonably accurate poses. ablation study next perform ablation study systematically evaluate contributions core modules method shown tab. seg. conf. ljd lba mpjpe pa-mpjpe iii vii table ablation study approach. seg. segmentation decoder, frame buffer, conf. confidence decoder, ljd reprojection loss, lba bone loss visibility mask. report mpjpe pa-mpjpe evaluated eed-r dataset. first row represents baseline includes egocentric pose module epm. tab. first define baseline method egocentric pose module epm without repm next systematically examine impact repm. adding segmentation decoder baseline improves performance mpjpe. incorporating frame buffer along segmentation decoder iii enables past events mpjpe mpjpe mpjpe mpjpe mpjpe fig. qualitative ablation study approach eed-r. reference rgb view, baseline epm only, inclusion segmentation decoder seg. inclusion frame buffer seg. inclusion confidence decoder conf. seg. inclusion reprojection, bone losses, visibility mask mpjpe values shown figures. pose predictions ground-truth poses visualised red green, respectively. reprojection predicted joints shown yellow. propagate current frame, resulting improvement mpjpe. additionally, introducing confidence decoder significantly enhances performance, e.g. pa-mpjpe. results validate effectiveness component repm. also introduce reprojection loss refine alignment predicted poses observed event streams, yielding additional improvement mpjpe improvement pa-mpjpe. integration bone loss visibility mask vii improves methods accuracy. specifically, incorporating bone loss ensures anatomically plausible bone orientations lengths, resulting additional improvement mpjpe. furthermore, applying visibility mask full model excludes occluded out-of-view joints joint supervision. prevents model directly learning positions invisible joints. instead, model estimates positions based bone orientations lengths. approach enables accurate pose predictions leveraging spatial relationships joints bones even cases occlusion partial views. integrating losses, method achieves best mpjpe pa-mpjpe scores, improvements respectively, compared baseline. dataset config. ljd lba mpjpe pa-mpjpe eed-s eed-w table ablation study additional losses. ljd reprojection loss lba bone loss. report mpjpe pa-mpjpe evaluated eed-s eed-w datasets visibility masks enabled. validate findings across datasets, evaluate reprojection bone loss terms eed-s eed-w tab. let represent model without additional losses. adding reprojection loss consistently reduces errors percentage points datasets, indicating enforcing tight alignment estimated poses projections helps refine pose predictions. furthermore, adding bone loss supervision yields additional improvements mpjpe, larger reduction observed eed-w. greater improvement likely due frequent severe occlusions in-the-wild dataset see fig. combining bone loss supervisory signals, model effectively recovers joint positions utilising information nearby visible joints. enables inference anatomically consistent poses, even scenarios parts human body occluded. configuration mpjpe pa-mpjpe without augmentation augmentation table comparison approach without event augmentation. lower values indicate better performance. also examine impact event augmentations pretraining eed-s, shown tab. disabling augmentations degrades generalisation performance eed-r, resulting increase mpjpe. result highlights importance event augmentation capturing variability real-world event noise preventing model overfitting training datas limited noise patterns. finally, present hyperparameter tuning study tab. vary loss terms weight factor method exhibits minimal sensitivity changes average, mpjpe varies approximately mm, suggesting contribution term remains stable broad range loss weightings. also provide qualitative ablation studies core modules approach fig. fig. fig. fig. observe baseline highly susceptible noisy events. significantly affects network outputs, especially hand pose high mpjpe value. although issue mitigated adding segmentation decoder extent, still struggles estimate correct hand position. introduction frame buffer results significant performance improvement utilise residual events previous frame weighted human body mask. moreover, additional inclusion confidence decoder improves visual quality pose estimation. finally, supervising framework reprojection loss, bone loss visibility masks plays key role producing best visual outputs. mpjpe mpjpe fig. qualitative ablation study reprojection loss eed-r. reference rgb view, model without loss, inclusion reprojection loss. mpjpe values shown figures. pose predictions ground-truth poses visualised red green, respectively. reprojection predicted joints shown yellow. fig. visually examine impact reprojection loss challenging motion, dancing. similarly, fig. analyse influence bone loss visibility masks another demanding motion, namely crawling. despite significant occlusions egocentric views, proposed components enable accurate estimation human body poses demonstrate effectiveness handling complex scenarios. runtime performance eventegod eventegod millerdurai al, support real-time human pose update rates hz. tab. see methods lowest number parameters floating point operations flops compared competing methods. rudnev fastest approach third-best terms accuracy. achieve second-highest number pose updates per seg ljd lbl weights mpjpe pa-mpjpe current iii current current current current vii current table ablation study loss hyperparamters. joint loss, heatmap loss, seg segmentation loss, reprojection loss, bone orientation loss bone length loss. highlights loss ablated, indicates losses enabled respective current weights. report mpjpe pa-mpjpe evaluated eed-r dataset. mpjpe mpjpe mpjpe fig. qualitative ablation study bone loss visibility mask eed-r. reference rgb view, model without bone loss visibility mask, bone loss, bone loss visibility mask. mpjpe values displayed. predicted poses red, ground-truth poses green, reprojections yellow. second. result highlights approach well-suited mobile devices due low memory computational requirements well low power consumption, due event camera. since rudnev use direct regression joints, method faster, methods use heatmaps intermediate representation estimate joints. furthermore, operations rudnev well parallelisable, explains high pose update rate. meanwhile, tome designed event streams achieve lower accuracy. real-time demo event cameras provide high temporal event resolution operate low-lighting conditions due excellent high dynamic range properties. eventegod runs real-time pose update rates, design real-time demo setup see fig. third-person view. portable hmd enables wide range movements, on-device computing laptop housed backpack allows capture in-the-wild sequences. showcase two challenging scenarios, i.e. fast motions poorly lit fig. qualitative results method in-the-wild motion sequences. waving, clapping boxing. method accurately regresses poses even low-light conditions. although rgb stream experiences significant motion blur due fast movement hands seen approach effectively utilises event stream capture poses. method params flops pose update rate tome rudnev millerdurai eventegod table comparisons model efficiency number parameters, flops, runtime pose update rate. eventegod millerdurai al, eventegod maintain number parameters flops, achieving lowest values metrics still maintaining good pose update rate. enhancements eventegod improve accuracy without increasing complexity, refining eventegod framework. environment would lead increased exposure time motion blur images captured mainstream rgb cameras. fig. illustrates challenging motions performed demo, highlighting method accurately estimates poses motion. notably, fig. -a, fast-paced waving motion depicted, method successfully recovers poses dynamic scenario. limitations eventegod achieves substantial progress event-based egocentric pose estimation, particularly challenging scenarios involving fast motion low-light conditions, surpasses traditional rgb-based methods producing robust pose estimates. nevertheless, several factors constrain theoretical upper bound event-only approach. first, event cameras detect changes brightness rather absolute intensities. cause jitter estimated poses subtle shifts clothing generate unexpected events, less pronounced issue rgb-based methods. second, despite inherent advantages event cameras, sensor noise, spurious events, environmental artefacts e.g. flickering lights degrade performance. finally, repm module mitigates effects minimal motion aggregating events, extended periods little user movement yield fewer events, allowing sensor noise dominate destabilise pose estimates. furthermore, framework employs locally-normalised event surfaces lnes sec. convert event stream representation. step may introduce uncertainties multiple events triggered pixel location within time window overwrite other, potentially discarding valuable spatiotemporal details. alternative methods, proposed chen millerdurai aim preserve event streams spatiotemporal representation could enhance performance event-based systems. nonetheless, important note methods developed static event cameras. transitioning moving event cameras, new challenges arise, particularly significant increase number events generated background. event sampling strategies offer potential solution issue, effectiveness importance sampling specifically targeting events generated human body remains unexplored area. addressing challenge could present promising direction future research event-based pose estimation using egocentric cameras. conclusion work, present eventegod, enhanced framework egocentric human motion capture event cameras. building upon existing eventegod framework, eventegod introduces additional loss functions new in-the-wild dataset eed-w. expanded datasets eed-s, eed-r, eed-w incorporating parametric human models, well allocentric multi-view rgb recordings eed-r eed-w datasets. expanded diverse dataset provides comprehensive resource support advance future research field. experimental results demonstrate eventegod achieves state-of-the-art accuracy real-time pose update rates, excelling scenarios involving rapid motions low-light conditionsareas egocentric event sensing proves particularly advantageous. method effectively handles sparse noisy event inputs, maintaining robust performance across wide range challenging conditions. findings highlight potential event-based cameras egocentric vision tasks pave way future research areas motion analysis, action recognition, human-computer interaction. acknowledgement. research partially funded erc consolidator grant dreply nr. project fluently nr. hiroyasu akada also supported nakajima foundation. data availability. datasets used papereed-r, eed-w, eed-sare publicly available accessed project page de. appendix efficiency event cameras evaluate efficiency event cameras along two dimensions power consumption hmd equipped event camera, bandwidth required transmit event data fixed time window energy efficiency event cameras. measure power draw hmd using precision usb power analyser record watts milliamperes ma. average, device consumes ma, notably lower typical rgb cameras often exceed furthermore, significant variation power usage observed stationary fast-motion scenarios, whether indoors outdoors. stability, despite rapid head movements dynamic backgrounds, highlights suitability event cameras continuous, real-time egocentric applications. event camera bandwidth requirements. measure bandwidth consumption representative eed-w sequence featuring outdoor, in-the-wild conditions generate large number events wearers body background. fig. plots per-frame bandwidth usage sequence, showing average approximately bytes per frame. event -byte tuple ts, require bytes, requires bytes, requires byte. events accumulated time window ms, matching fps rate allocentric rgb cameras. comparison, rgb frame encodes pixel bytes rgb, resulting bytes per frameabout times higher event-stream data. even lower resolution matches event camera, rgb data requires times bandwidth event stream. appendix joint heatmap estimation estimate joint heatmaps using heatmap decoder. produce heatmaps fig. bandwidth comparison event streams rgb frames. different resolutions layers decoder. specifically, utilise layers extracting first feature maps layer. feature map corresponds heatmap body joint. heatmaps upsampled common resolution upsampling, average heatmaps selected layers produce final heatmaps represent joint heatmaps body joints. appendix real world data capture head mounted device calibration obtain ground-truth pose hmd user, first calibrate hmd using allocentric rgb multi-camera setup. calibration allows determine hmds position multi-camera setups coordinate frame i.e. world coordinate frame. finally, compute world-to-device transformation matrix, denoted mwe, maps world coordinate frame hmd coordinate frame. lets obtain users pose within hmds coordinate system. position hmd world coordinate frame obtained hand-eye calibration, following approach rhodin process, checkerboard, referred head-checkerboard, mounted top hmd. checkerboard surrogate event cameras position, enabling precise tracking hmd within world coordinate system. compute mwe matrix two steps. first, obtain transformation world head-checkerboard coordinate frame, denoted mwc. next, calculate transformation head-checkerboard event camera, denoted mce. specifically, mwe, defined mwe mce mwc mwc matrix obtained solving pose head-checkerboard world coordinate frame. apply pnp algorithm itseez, images obtained multi-view rgb setup pose computation. meanwhile, mce matrix obtained following steps generate checkerboard image using event camera first capture event stream checkerboard placed bottom hmd, referred floor-checkerboard, keeping hmd stationary. create uniform distribution events vertical horizontal directions, checkerboard slid diagonally. captured event stream converted image sequences using evid rebecq al, sequences, select image captures last position floor-checkerboard slide. finally, compute pose, me, hmd coordinate system using pnp algorithm. visualisation shown fig. c-c. maintaining positions floor-checkerboard hmd previous step, use external rgb camera capture image sequence includes head-checkerboard floor-checkerboard. select images calibration patterns checkerboards detected. selected images, compute poses head-checkerboard floor-checkerboard relative external rgb camera using pnp algorithm. finally, mce matrix obtained following transformation mce mh. visualisation calibrated setup shown fig. c-d. accuracy ground truth acquire human poses smpl loper al, parameters using two multi-view motion capture pipelines captury accurate joints easymocap smpl parameter recovery. eed-r dataset. captured state-of-the-art commercial system captury, fps high illumination, eed-r uses cameras minimise motion blur maximise tracking accuracy. setup aligns prior literature multi-view pose capture al, wang al, akada al, wang al, millerdurai al, ensures robust reference poses. eed-w dataset. contrast, eed-w filmed fps using cameras outdoor settings, leveraging captury technology. although fewer cameras employed, system remains sufficient accurate ground-truth capture, following best practices used prior works outdoor fig. hand-eye calibration determining event camera position relative checkerboard head-mounted device coordinate frame head-checkerboard obtained using external rgb camera. coordinate frame floor-checkerboard obtained using external rgb camera. coordinate frame floor-checkerboard obtained using event camera. hand-eye calibration performed, event camera localised respect head-checkerboard. environments elhayek al, mehta al, al, datasets, event egocentric event stream synchronised allocentric rgb frames frames timestamp. together, eed-r eed-w provide diverse, well-calibrated benchmarks, facilitating robust evaluations egocentric human pose estimation. ground truth generation obtain human poses smpl loper al, parameters within world coordinate frame using multi-view rgb camera setup see fig. subsequently, apply world-to-device transformation matrix mwe convert human poses smpl parameters world coordinate frame hmd coordinate frame. specifically, use following transformations mwe mwe here, represents world human pose, represents egocentric human pose, world smpl mesh egocentric smpl mesh. additionally, derive egocentric joint coordinates, represented projecting egocentric poses using intrinsics event camera. fig. visualisation calibrated hmd human body pose. employ multi-view camera setup simultaneously track human body pose position checkerboard world coordinate frame. poses obtained subsequently projected onto coordinate frame hmd. establish coordinate frame hmd, determine suitable transformation matrix maps points checkerboards coordinate frame hmds coordinate frame. given known position checkerboard, transformation matrix allows derive egocentric pose. also, generate human body masks visibility masks joint, addition obtaining human poses smpl parameters. joint visibility mask indicates whether joint visible occluded egocentric view. use blender create human body masks joint visibility masks. first set smpl body model user egocentric virtual camera intrinsic parameters position real-world event camera. render human body masks, use mist render layers blenders cycles renderer. next, obtain joint visibility masks shooting rays virtual camera body joint. ray intersects smpl body first time, query nearest vertices intersection. nearest vertices belong corresponding body part targeted body joint, mark body joint visible. conversely, nearest vertices belong relevant body part, body joint considered occluded. additionally, joint occluded, also mark corresponding joint occluded. body parts identified using predefined human part segmentation mesh provided loper appendix reconstructing images event stream utilise evid rebecq generate image reconstructions event stream. frame duration event window set align ground-truth frame timing eed-r. shown fig. reconstructed images often exhibit artefacts, particularly scenarios minimal human motion. instance, low-motion actions walking left part fig. reconstructed images fail accurately capture human figure. contrast, high-motion actions, punching right part fig. reconstructed images recover human figure properly. ensure precise synchronization, event window aligned corresponding ground-truth frame number, maintaining consistency ground-truth poses reconstructed images. appendix synthetic data generation simulate human motions captured event cameras, linearly interpolated smpl body parameters surreal dataset event stream reconstructed images event stream fig. exemplary event streams corresponding image reconstructions. reconstructed images lose significant details human body, especially motion human minimal. frequency hz. dataset created generating rgb frames human body masks image mist render layers blenders cycles renderer blender, body joints used training eventegod method, denoted jn, derived smpl body joints represented sn, specifically, map joints follows denotes i-th smpl joint index set joint corresponds specific body part head, neck, right shoulder, right elbow, right wrist, left shoulder, left elbow, left wrist, right hip, right knee, right ankle, right foot, left hip, left knee, left ankle, left foot, respectively. appendix input representation use lnes representation rudnev al, aggregate events time window without applying temporal overlap. nevertheless, conducted experiments using explicitly overlapping lnes frames temporal resolution matching networks runtime performance fpswhich yields mpjpe pa-mpjpe eed-r dataset. results nearly identical default setting using non-overlapping lnes frames mpjpe pa-mpjpe furthermore, using overlapping configuration temporal resolution, obtain mpjpe pa-mpjpe .corresponding reduction mpjpe reduction pa-mpjpe compared default configuration. however, since network process frames effective rate approximately per frame ms, configuration feasible real-time operation. references akada wang shimada unrealego new dataset robust egocentric human motion capture. european conference computer vision eccv akada wang golyanik human pose perception egocentric stereo videos. computer vision pattern recognition cvpr aliakbarian cameron bogo flag flow-based avatar generation sparse observations. proceedings ieeecvf conference computer vision pattern recognition, bazarevsky grishchenko raveendran blazepose on-device real-time body pose tracking. arxiv preprint blender blender modelling rendering package. blender foundation, blender institute, amsterdam, url captury capturystudio markerless mocap humans pre-recorded, multi-view video footage. chen shi efficient human pose estimation via event point cloud. international conference vision cmu cmu graphics lab motion capture database. url dai zhang liu hmd-poser on-device real-time human motion tracking scalable sparse observations. proceedings ieeecvf conference computer vision pattern recognition, kips pumarola avatars grow legs generating smooth human motion sparse tracking inputs diffusion model. cvpr dvxplorer mini dvxplorer mini specification. boarddvxplorer-mini.pdf easymocap easymocap make human motion capture easier. zjudveasymocap elhayek aguiar jain marconiconvnet-based marker-less motion capture outdoor indoor scenes. ieee transactions pattern analysis machine intelligence gallego delbruck orchard event-based vision survey. ieee transactions pattern analysis machine intelligence gehrig gehrig hidalgo-carrio video events recycling video datasets event cameras. computer vision pattern recognition cvpr gilbert trumble malleson fusing visual inertial sensors semantics human pose estimation. international journal computer vision guzov mir sattler human poseitioning system hps human pose estimation self-localization large scenes body-mounted sensors. proceedings ieeecvf conference computer vision pattern recognition, guzov jiang hong hmd environment-aware motion generation single egocentric head-mounted device. arxiv preprint helten muller seidel hp, real-time body tracking one depth camera inertial sensors. proceedings ieee international conference computer vision, huang kaufmann aksan deep inertial poser learning reconstruct human pose sparse inertial measurements real time. acm transactions graphics tog itseez open source computer vision library. jiang streli qiu avatarposer articulated full-body pose tracking sparse motion sensing. european conference computer vision, springer, jiang streli meier egoposer robust real-time ego-body pose estimation large scenes. arxiv e-prints arxiv jiang zhang evhandpose event-based hand pose estimation sparse supervision. ieee transactions pattern analysis machine intelligence jiang zhou wang complementing event streams rgb frames hand mesh reconstruction. proceedings ieeecvf conference computer vision pattern recognition, jiang streli luo manikin biomechanically accurate neural inverse kinematics human motion estimation. european conference computer vision, springer, jiang gopinath transformer inertial poser real-time human motion reconstruction sparse imus simultaneous terrain generation. siggraph asia conference papers, kang lee attention-propagation network egocentric heatmap pose lifting. proceedings ieeecvf conference computer vision pattern recognition cvpr kang lee zhang egodpose capturing cues binocular egocentric views. siggraph asia conference papers. association computing machinery, new york, ny, usa, org.., url doi.org.. kendall survey statistical theory shape. statistical science khirodkar bansal ego-humans ego-centric multi-human benchmark. international conference computer vision iccv kingma adam method stochastic optimization. international conference learning representations iclr lan yin basu tracking fast learning slow event-based speed adaptive hand tracker leveraging knowledge rgb domain. arxiv preprint lee starke questenvsim environment-aware simulated motion tracking sparse sensors. acm siggraph conference proceedings, lensagon bfmsc lensagon bfms specification. lensation.depdfbfms.pdf liu ego-body pose estimation via ego-head pose estimation. computer vision pattern recognition cvpr liu ego-body pose estimation via ego-head pose estimation. proceedings ieeecvf conference computer vision pattern recognition, liu yang egofishd egocentric pose estimation fisheye camera via self-supervised learning. ieee transactions multimedia loper mahmood romero smpl skinned multi-person linear model. acm trans graphics proc siggraph asia luo hachiuma yuan dynamics-regulated kinematic policy egocentric pose estimation. advances neural information processing systems neurips malleson gilbert trumble real-time full-body motion capture video imus. international conference vision dv, ieee, mathworks matlab version ra. url mehta sotnychenko mueller single-shot multi-person pose estimation monocular rgb. vision dv, sixth international conference on, ieee, url singleshotmultiperson millerdurai akada wang eventegod human motion capture egocentric event streams. proceedings ieeecvf conference computer vision pattern recognition, millerdurai luvizon rudnev pose estimation two interacting hands monocular event camera. international conference vision muglikar gehrig gehrig calibrate event camera. conference computer vision pattern recognition cvpr workshops nehvi golyanik mueller differentiable event stream simulator non-rigid tracking. cvpr workshop event-based vision pan charron yang aria digital twin new benchmark dataset egocentric machine perception. international conference computer vision iccv park moon hand sequence recovery real blurry images event stream. european conference computer vision, springer, paszke gross massa pytorch imperative style, high-performance deep learning library. advances neural information processing systems neurips pavlakos zhu zhou learning estimate human pose shape single color image. computer vision pattern recognition cvpr rebecq gehrig scaramuzza esim open event camera simulator. conference robot learning corl rebecq ranftl koltun events-to-video bringing modern computer vision event cameras. computer vision pattern recognition cvpr rebecq ranftl koltun high speed high dynamic range video event camera. ieee transactions pattern analysis machine intelligence rhodin richardt casas egocap egocentric marker-less motion capture two fisheye cameras. acm transactions graphics tog ronneberger fischer brox u-net convolutional networks biomedical image segmentation. international conference medical image computing computer-assisted intervention rudnev golyanik wang eventhands real-time neural hand pose estimation event stream. international conference computer vision iccv rudnev elgharib theobalt eventnerf neural radiance fields single colour event camera. computer vision pattern recognition cvpr scaramuzza martinelli siegwart toolbox easily calibrating omnidirectional cameras. ieeersj international conference intelligent robots systems iros tome peluse agapito xr-egopose egocentric human pose hmd camera. international conference computer vision iccv tome alldieck peluse selfpose egocentric pose estimation headset mounted camera. ieee transactions pattern analysis machine intelligence varol romero martin learning synthetic humans. computer vision pattern recognition cvpr von marcard pons-moll rosenhahn human pose estimation video imus. ieee transactions pattern analysis machine intelligence von marcard rosenhahn black mj, sparse inertial poser automatic human pose estimation sparse imus. computer graphics forum, wiley online library, wang liu estimating egocentric human pose global space. international conference computer vision iccv wang liu estimating egocentric human pose wild external weak supervision. computer vision pattern recognition cvpr wang luvizon scene-aware egocentric human pose estimation. computer vision pattern recognition cvpr wang cao luvizon egocentric whole-body motion capture fisheyevit diffusion-based motion refinement. proceedings ieeecvf conference computer vision pattern recognition, wang cao luvizon egocentric whole-body motion capture fisheyevit diffusion-based motion refinement. computer vision pattern recognition cvpr wang chaney daniilidis evacd event-based apparent contours models via continuous visual hulls. european conference computer vision eccv winkler questsim human motion tracking sparse sensors simulated avatars. siggraph asia conference papers, golyanik eventcap monocular capture high-speed human motions using event camera. computer vision pattern recognition cvpr chatterjee zollhoefer mocap real-time mobile motion capture cap-mounted fisheye camera. ieee transactions visualization computer graphics xue leutenegger event-based non-rigid reconstruction contours. british machine vision conference bmvc zhou transpose real-time human translation pose estimation six inertial sensors. acm transactions graphics tog zhou habermann physical inertial poser pip physics-aware real-time human motion tracking sparse inertial sensors. proceedings ieeecvf conference computer vision pattern recognition, zhou habermann egolocate real-time motion capture, localization, mapping sparse body-mounted sensors. acm transactions graphics tog zhang song lsun construction large-scale image dataset using deep learning humans loop. arxiv preprint yuan kitani ego-pose estimation forecasting real-time control. international conference computer vision iccv zhang zhang egobody human body shape motion interacting people head-mounted devices. european conference computer vision eccv zhang zhang probabilistic human mesh recovery scenes egocentric views. international conference computer vision iccv zhang gevers automatic calibration fisheye camera egocentric human pose estimation single image. winter conference applications computer vision zhao wei mahmud egoglass egocentric-view human pose estimation eyeglass frame. international conference vision zheng wen realistic full-body tracking sparse observations via joint-level modeling. proceedings ieeecvf international conference computer vision, zou guo zuo eventhpe event-based human pose shape estimation. international conference computer vision iccv", "published_date": "2025-02-11T18:57:05+00:00"}
{"id": "2412.20066v2", "title": "MaIR: A Locality- and Continuity-Preserving Mamba for Image Restoration", "authors": ["Boyun Li", "Haiyu Zhao", "Wenxin Wang", "Peng Hu", "Yuanbiao Gou", "Xi Peng"], "summary": "recent advancements mamba shown promising results image restoration. methods typically flatten images multiple distinct sequences along rows columns, process sequence independently using selective scan operation, recombine form outputs. however, paradigm overlooks two vital aspects local relationships spatial continuity inherent natural images, discrepancies among sequences unfolded totally different ways. overcome drawbacks, explore two problems mamba-based restoration methods design scanning strategy preserving locality continuity facilitating restoration, aggregate distinct sequences unfolded totally different ways. address problems, propose novel mamba-based image restoration model mair, consists nested s-shaped scanning strategy nss sequence shuffle attention block ssa. specifically, nss preserves locality continuity input images stripe-based scanning region s-shaped scanning path, respectively. ssa aggregates sequences calculating attention weights within corresponding channels different sequences. thanks nss ssa, mair surpasses baselines across challenging datasets, achieving state-of-the-art performance tasks image super-resolution, denoising, deblurring dehazing. code available", "full_text": "mair locality- continuity-preserving mamba image restoration boyun li, haiyu zhao, wenxin wang, peng hu, yuanbiao gou, peng, college computer science, sichuan university, china. national key laboratory fundamental algorithms models engineering numerical simulation, sichuan university, china. liboyun.gm, haiyuzhao.gm, wangwenxin.gm, penghu.ml, gouyuanbiao, pengx.gmgmail.com abstract recent advancements mamba shown promising re- sults image restoration. methods typically flatten images multiple distinct sequences along rows columns, process sequence independently using selective scan operation, recombine form outputs. however, paradigm overlooks two vital as- pects local relationships spatial continuity in- herent natural images, discrepancies among sequences unfolded totally different ways. over- come drawbacks, explore two problems mamba- based restoration methods design scanning strategy preserving locality continuity fa- cilitating restoration, aggregate dis- tinct sequences unfolded totally different ways. ad- dress problems, propose novel mamba-based image restoration model mair, consists nested s-shaped scanning strategy nss sequence shuffle attention block ssa. specifically, nss preserves locality continuity input images stripe-based scanning region s-shaped scanning path, respec- tively. ssa aggregates sequences calculating at- tention weights within corresponding channels differ- ent sequences. thanks nss ssa, mair surpasses baselines across challenging datasets, achieving state-of-the-art performance tasks image super- resolution, denoising, deblurring dehazing. code available scu-cvpr-mair. introduction image restoration aims recover visually appealing high- quality images given degraded correspondences, e.g., noisy, blurry, hazy images. recent years, meth- ods based convolutional neural networks cnns corresponding authors zigma mair localmamba vmambavim figure scanning strategies existing mamba-based meth- ods proposed method. vmambavim uses z-shaped scan path flatten image sequences, locality continuity image disrupted. zigma utilizes s-shaped path maintain spatial continuity, ignores locality. localmamba leverages window-based scanning region preserve locality. however, z-shaped scanning path within across windows disrupts spatial continuity. contrast, mair divides images multiple non-overlapping stripes, adopts s-shaped scanning path within across stripes, thus simultaneously preserves locality continuity. transformers significantly advanced image restora- tion effectively capturing locality i.e., fine-grained pat- terns correlations small regions continuity i.e., smooth, gradual transitions across larger areas inherent natural images. specific, cnns capture locality continuity elaborately designed small ker- nels sliding strides, respectively. transformers capture local window partitions adjacent window communications e.g., window shifts window expan- sions. however, like coin two sides, success cs.cv mar cnns transformers preserving locality continu- ity comes cost ability capture long-range dependencies. consider limited region input image time due localized kernels windows, making challenging model relationships span across larger sections image. therefore, highly expected develop method able capture long-range dependencies well preserving locality continuity inherent natural images. mamba novel selective state space model garnered significant attention due promising performance long sequence modeling maintaining nearly linear complexity. mambas core algorithm, selective scan operation sso, inherently designed sequences, directly appli- cable processing images. address problem, mamba-based restoration methods typically involve step pipeline flattening image multiple se- quences along rows columns processing se- quence independently using sso iii aggregating processed sequences form output image. how- ever, paradigm still faces two demerits process- ing images. first, transforming image sequences, disrupts locality continuity inherent image, illustrated fig. a-c. second, generally aggregates processed sequences via pixel-wise summation, overlook- ing distinct contexts among sequences unfolded totally different ways. work, present novel locality- continuity- preserving mamba image restoration mair, consists nested s-shaped scanning strategy nss sequence shuffle attention block ssa. specifically, nss preserves locality stripe-based scanning region, continuity via s-shaped scanning path shift- stripe mechanism. ssa aggregates processed sequences calculating attention weights within corresponding chan- nels sequences. thanks corporation nss ssa, mair enjoys following merits. firstly, mair involves cost-free solution preserve locality continuity in- herent natural images, ensuring structural coherence avoiding computational overhead. secondly, mair captures complex dependencies across distinct sequences, facilitat- ing leverage complementary information for- ward reversed rows columns. summarize, contributions innovations work work, present mair, approach effi- ciently captures long-range dependencies preserv- ing locality continuity inherent natural images. mamba, introduce nss, cost-free solution preserve locality continuity, ssa, module capture dependencies across distinct sequences. mair obtains state-of-the-art performance four tasks across benchmarks comparing baselines. related works section, briefly review related works image restoration vision mamba. image restoration according focus paper, existing methods classified three categories, i.e., cnn-, transformer- mamba-based methods. introduce first two categories here, last one detailed sec. cnn-based method benefiting ability cap- turing locality continuity natural images, cnn-based methods achieved promising results various tasks image restoration, image super-resolution image denoising image deblurring however, since localized receptive fields, cnns inherently limited capturing long-range dependencies. transformer-based method transformers theoret- ically capable capturing global dependencies however, avoid impractical quadratic complexity im- ages, existing methods tend partition local regions input image different windows, calcu- late attentions within across windows. instance, swinir computes attentions within local windows shifts windows layers. hat divides im- ages overlapping windows enhance interaction neighbor windows. although methods ensured structural coherence i.e., locality continuity natural images avoided computational overhead, fell another dilemma failing fully capture long- range dependencies due limited window sizes. vision mamba due mambas demonstrated superiority long-sequence modeling studies introduced high- low-level vision tasks. enable sso process images, methods tend flatten images multiple sequences along different directions. instance, vmamba proposes cross-scan strategy flattens input images along rows columns. however, existing scanning strategies disrupt structure coherence essential image restoration. recently, mamba-based restoration methods be- gun recognize importance structure coherence, tend introduce extra coherence-preserving modules. instance, mambair uvm-net enhances lo- cality additional cnn layers, introduces extra computational costs. although studies devote designing scanning strategy preserve locality continuity, preserve one them. rmg rmg rmb rmb conv conv reconstruction extraction layernorm vmm scale layernorm scale mlp linear dwconv silu mairm layernorm linear silu linear visual mamba module vmm overall architecture mair residual mamba block rmb residual mamba group rmg figure illustrations mair. overall architecture mair, highlighting core component, residual mamba group rmg. rmg primarily composed residual mamba block rmb, visual mamba module vmm plays pivotal role. module module nested s-shaped scanning shift-stripe mechanism figure illustrations nested s-shaped scanning strategy nss shift-stripe mechanism. contrast, mair provides cost-free solution preserve locality continuity. methods section, first introduce overall architecture mair, elaborate nss ssa assembled mair module mairm. overall architecture network structure following previous works mair built three stages, namely, shallow feature extraction stage, deep feature extraction stage recon- struction stage. specifically, shallow feature extrac- tion stage, given degraded image rhw first employ convolution layer extract shallow feature rchw represent height width number channels. that, fed deep feature extraction stage produce deep feature rchw illustrated fig. deep feature extraction stage stacked multiple residual mamba groups rmgs, rmg consists sev- eral residual mamba blocks rmbs. within rmb, visual mamba module vmm introduced capture long-range dependencies, composed proposed mairm. finally, reconstruct high-quality image based fd. specifically, image super- resolution, introduce pixel-shuffle layer ups convolution layer reconstruct high- resolution image upsfs fd. tasks require upsampling e.g., denoising, deblurring dehazing, employ single convolution layer resid- ual connection construct high-quality result, formulated loss function image super-resolution, use loss optimize network following formulated target image. image denoising, deblur- ring dehazing, adopt charbonnier loss, i.e., hyper-parameter set empirically. mair module elaborated above, mairm serves core module mair, involves three-step pipeline. specific, mairm first flattens features four sequences nss along four distinct directions following then, mairm employs sso capture long-range depen- dencies. finally, mairm aggregates processed sequences shuffled sequences group conv. weighted summation input features pooled sequences unshuffled weights attention weights output feature pooling concat shuffle chunk group group unshuffle group figure illustration sequence shuffle attention ssa. input features xik rdhw first pooled concatenated form rl, sequence undergoes sequence shuffle operation results shuffled sequences rl, whose channels split group. then, group convolution sequence unshuffle operation applied, producing unshuffled weights rl, chunked reshaped attention weights rd. finally, output feature rdhw computed performing weighted summation input features using attention weights. ssa form outputs. mathematically, input feature fi,j, output feature i,j formulated i,j mi,jfi,j, ssa i,j sso i,j nss i,j fi,j, mi,j, nss i,j sso i,j ssa i,j mairm, nss, sso ssa j-th rmb i-th rmg, respectively. nss nss designed extract locality- continuity- preserving sequences input features. motivated observation illustrated fig. one could find local- mamba preserves locality restricted scanning region, zigma preserves continuity shaped scanning path. thus, shown fig. de- sign nested s-shaped scanning strategy, divides features multiple non-overlapping stripes uses shaped scanning path within across stripes maintain locality continuity. better leverage spatial infor- mation, extract sequences four different scanning directions top-left bottom-right, bottom-right top-left, top-right bottom-left, bottom-left top-right, fol- lowing previous works besides, nss includes shift-stipe mechanism preserve locality continuity boundary regions ad- jacent stripes. depicted fig. two successive modules, first module partitions features multiple non-overlapping stripes stripe width ws. sec- ond module, employ shift-stripe operation, set first last stripe widths others width ws. consequently, boundary regions previous module fully covered single stripe module. ssa ssa aggregates processed sequences cal- culating attentions within corresponding channels. de- sign enables capture complex dependencies across dis- tinct sequences, thus better leveraging complementary in- formation different scanning directions. shown fig. supposing sequence number sso- processed sequences first apply spatial aver- age pooling reduce computational cost, concatenate catap pooled feature d-th channel k-th se- quence, number channel mairm. then, employ sequence shuffle operation rearrange features ssx that, employ group convolution group size four obtain channel-wise attention weights unshuffle weights back original order, i.e., sug sequence unshuffle operation. unshuf- fled weights chunked chunk chunk refers chunk operation. finally, adopt weight summation based generate output, formulated xi, output sequence ssa. experiments section, evaluate mair four representative image restoration tasks, i.e., image super-resolution, image denoising, image deblurring, image dehazing. following, first show quantitative results, conduct analysis studies verify reasonability. experi- mental settings presented supplementary ma- terials. table quantitative results classic image super-resolution. best second best results red blue. methods scale set set urban manga psnr ssim psnr ssim psnr ssim psnr ssim psnr ssim san han ignn nlsa elan ipt swinir srformer mambair mair mair san han ignn nlsa elan ipt swinir srformer mambair mair mair san han ignn nlsa elan ipt swinir srformer mambair mair mair mair mambair srformer swinir image ignn nlsa han san bicubic figure visual comparison image super-resolution results manga dataset. mair demonstrates superior visual quality, particularly preserving fine details textures. table quantitative results lightweight image super-resolution. best second best results red blue. methods scale params macs set set urban manga psnr ssim psnr ssim psnr ssim psnr ssim psnr ssim carn imdn lapar-a latticenet swinir mambair-tiny mair-tiny mambair-small mair-small carn imdn lapar-a latticenet swinir mambair-tiny mair-tiny mambair-small mair-small carn imdn lapar-a latticenet swinir mambair-tiny mair-tiny mambair-small mair-small results image super-resolution section, conduct experiments classic lightweight image super-resolution. datasets following previous works em- ploy dfk divk flickrk training set classic image super-resolution, divk train- ing set lightweight image super-resolution. eval- uation, employ following five datasets test sets, i.e., set set urban manga following existing works low-resolution images downsampled corre- sponding high-resolution images via bicubic interpolation. baselines compare method competitive baselines. specifically, adopt four cnn-based methods i.e., san han ignn nlsa four transformer-based methods i.e., elan ipt swinir srformer one mamba-based method i.e., mambair baselines clas- sic super-resolution. lightweight super-resolution, four cnn-based methods i.e., carn imdn la- par latticenet two transformer-based methods i.e., swinir srformer one mamba- based method i.e., mambair introduced quantitative qualitative comparisons. similar mambair, offers two versions lightweight super- resolution, mair also available two configurations mair-tiny mair-small. results classic super-resolution, shown tab. fig. one could observe mair achieves best result almost quantitative comparisons. instance, method surpasses mambair .db.db terms psnr urban, srformer .db, .db .db terms psnr ur- ban, manga, respectively, demonstrates superiority mair. light-weight sr, mair also exhibits advancement compared baselines reported tab. taking scale examples, mair-small sur- passes mambair-small .db terms psnr manga fewer parameters macs. mair-tiny outperforms mambair-tiny swinir .db .db terms psnr urban fewer parame- ters, verifies efficiency effectiveness proposed method. results image denoising section, evaluate mair synthetic gaus- sian noise real-world noise. datasets synthetic noise removal, train mair dfwb, consists divk, flickrk, water- loo exploration dataset wed bsd evaluation, utilize bsd kodak, mcmas- ter urban test set. following generate noisy images manually adding white gaus- table quantitative results gaussian color image denoising. best second best results red blue. methods bsd kodak mcmaster urban ircnn ffdnet dncnn drunet swinir restormer code art mambair mair mair table quantitative results real image denoising. best second best results red blue. deamnet mprnet nbnet dagl uformer mambair mair psnr ssim mair art restormer swinir noisy image drunet code dncnn ffdnet noisy figure visual comparison image denoising results urban dataset. mair effectively removes noise images produces detailed textures closely match ground truth. sian noise clean images three distinct noise lev- els, i.e., real-world image denoising, model trained tested sidd-medium dataset, provides high-resolution noisy-clean im- age pairs training additional image pairs test. baselines compare mair representa- tive methods. specific, adopt four cnn-based methods i.e., ircnn ffdnet dncnn drunet four transformer-based methods i.e., swinir restormer code art one mamba-based method i.e., mambair baselines synthetic noise removal. real-world image denoising, four cnn-based methods i.e., deam- net mprnet nbnet dagl two transformer-based methods i.e., uformer restormer one mamba-based method i.e., mam- bair introduced comparisons. results depicted tab. mair demonstrates superior performance synthetic real-world im- age denoising compared baselines. taking results ur- ban examples, mair averagely outperforms mam- bair .db terms psnr, indicates superiority image denoising. similar results derived qualitative comparisons shown fig. mair could keep detailed textures restored images, closely ground truth. results image deblurring section, evaluate mair motion deblurring verify effectiveness proposed method. input mt-rnn dmphn code mimo mprnet blurry image restormer nafnet mair figure visual comparison motion deblurring results gopro dataset. mair demonstrates superior performance effectively removing motion blur preserving precise fine details textures, closely matching ground truth. table quantitative results image motion deblurring. best second best results red blue. macs table evaluated patches followed method params macs gopro hide srn dbgan mt-rnn dmphn code mimo mprnet restormer uformer cu-mamba nafnet mair datasets following previous works em- ploy gopro dataset training consists blurry-clean image pairs. evaluation, use two com- mon datasets, i.e., gopro test set hide con- sist blurry-clean pairs, respectively. baselines adopt competitive image deblur- ring baselines comparisons. detail, adopt six cnn-based deblurring methods i.e., srn db- gan dmphn mimo mprnet nafnet three transformer-based methods i.e., code restormer uformer one rnn- based method i.e., mt-rnn one mamba-based method i.e., cu-mamba baselines. results shown tab. proposed mair surpasses baselines psnr gopro hide. de- tail, mair outperforms restormer .db gopro dataset .db hide dataset terms psnr. although nafnet achieves similar quantitative results gopro, mair surpasses nafnet hide dataset .db terms psnr. illustrated fig. mair demonstrates ability handling heavily degraded areas, i.e., mair could effectively remove blur restore details around wheels. results image dehazing section, evaluate mair image dehazing ver- ify effectiveness mair. datasets following existing works employ reside dataset training testing. indoor scenes, train mair indoor training set consists hazy-clean pairs, test indoor synthetic objective testing set sots-indoor involving pairs. outdoor scenes, train mair outdoor train- ing set ots, contains image pairs, evaluate outdoor synthetic objective testing set sots- outdoor involving images. addition, verify mair general cases, also train model reside- test sots-mix, mix indoor outdoor images. baselines adopt eight competitive methods base- lines. specifically, adopt five cnn-based image dehaz- ing methods i.e., aodnet gdn msbdn ffanet aecrnet two transformer-based methods i.e., dehamer dehazeformer one mamba-based method i.e., uvm-net baselines. results shown tab. fig. mair surpasses baselines quantitative qualita- tive comparisons. taking quantitative results examples, mair significantly outperforms dehazeformer uvm- net .db .db terms psnr outdoor scenes. although uvm-net slightly higher psnr indoor scenes, mair takes params macs uvm-net, verifies effective- ness efficiency. analysis experiments section, first conduct ablation studies verify effectiveness nss ssa. then, introduce anal- ysis experiments verify observations investigate table quantitative results image dehazing. best second best results red blue. macs table evaluated patches followed method aodnet gdn msbdn ffanet aecrnet dehamer dehazeformer uvm-net mair params ,.m macs sots- psnr indoor ssim sots- psnr outdoor ssim sots- psnr mix ssim mair dehazeformer dehamer ffanet hazy image msbdn aecrnet gdn aodnet input figure visual comparison image dehazing results sots dataset. mair effectively remove haze restore content colors closely match ground truth. table ablation study nss, tested lightweight super- resolution scale factor results urban dataset presented, demonstrates effectiveness nss. baseline nss zigma psnr ssim impact stripe width overall performance. ... ablation studies first conduct ablation study analyze effectiveness nss. detail, five configurations conducted, re- placing nss z-shaped scanning strategy denoted nss, removing shift stripe denoted ss, iii replacing nss scanning strategy local- mamba denoted lm, replacing nss scanning strategy zigma denoted zigma replacing nss peano-hilbert curve denoted ph. illustrated tab. nss important improve performance mair. investigate effectiveness ssa, remove ssa aggregate sequences sequences-wise table ablation study ssa lightweight super-resolution scale factor results urban dataset demonstrate effectiveness ssa. mair ssa uvm seqgat psnr ssim cagat fpixgat dwpixgat psnr ssim addition termed ssa, ssm termed uvm, iii sequence-wise gating termed seqgat, channel-wise gating termed cagat, pixel-wise gat- ing fully connected convolution termed fpix- gat. pixel-wise gating depth-wise convolution termed dwpixgat. worth noting keep size different models similar fair comparisons. shown tab. ssa effective others. ... verification observations verify observations shown fig. conduct visual comparisons among different scanning strategies. shown fig. proposed methods maintain locality input mambair z-shaped localmamba window-based zigma s-shaped mair nested s-shaped figure visual comparisons different scanning strategies, illustrating windows-based scanning path overlooks continuity different regions e.g., relationship different layers scarf, resulting wrong textures, s-shaped scanning path leads distortion local regions, causing scarfs texture appear warped. iii z-shaped scanning path suffers them. contrast, mair avoids aforementioned problems achieves visually appealing results. table analyses stripe widths. experiment conducted urban dataset scale factor lightweight super- resolution tasks, illustrates changes stripe width af- fect restored image quality. psnr ssim continuity produce visual pleasant results. ... results different stripe width investigate influence stripe width, train lightweight model stripe width eval- uate urban dataset. presented tab. psnr ssim values quite similar different settings, except cases largest small- est stripe widths. indicates proposed method ex- hibits robustness changes stripe width, maintain- ing high-quality image restoration across range stripe widths. conclusion paper, propose mair, novel state space model image restoration preserve local depen- dencies spatial continuity input images. end, propose two designs nested s-shaped scanning strat- egy nss sequences shuffle attention ssa. nss designed extract locality- continuity-preserving se- quences images, ssa adaptively aggregates sequences. thanks cooperation, mair ad- dresses limitations existing mamba-based restoration methods also improves image quality without introduc- ing extra computations. extensive experiments across four tasks benchmarks comparing baselines vali- date superiority mair, demonstrating robustness effectiveness various image restoration tasks. acknowledgments work supported part nsfc grant ub, part fundamen- tal research funds central universities grant part sichuan science technology planning project grant nsftd. references abdelrahman abdelhamed, stephen lin, michael brown. high-quality denoising dataset smartphone cameras. proceedings ieee conference com- puter vision pattern recognition, pages namhyuk ahn, byungkon kang, kyung-ah sohn. fast, accurate, lightweight super-resolution cascading residual network. proceedings european confer- ence computer vision eccv, pages marco bevilacqua, aline roumy, christine guillemot, marie line alberi-morel. low-complexity single-image super-resolution based nonnegative neighbor embedding. hanting chen, yunhe wang, tianyu guo, chang xu, yiping deng, zhenhua liu, siwei ma, chunjing xu, chao xu, wen gao. pre-trained image processing transformer. ieee conference computer vision pattern recogni- tion, pages virtual, keyan chen, bowen chen, chenyang liu, wenyuan li, zhengxia zou, zhenwei shi. rsmamba remote sens- ing image classification state space model. ieee geo- science remote sensing letters, liangyu chen, xiaojie chu, xiangyu zhang, jian sun. simple baselines image restoration. european confer- ence computer vision, pages springer, xiangyu chen, xintao wang, jiantao zhou, qiao, chao dong. activating pixels image super- resolution transformer. proceedings ieeecvf con- ference computer vision pattern recognition, pages shen cheng, yuzhi wang, haibin huang, donghao liu, haoqiang fan, shuaicheng liu. nbnet noise basis learning image denoising subspace projection. proceedings ieeecvf conference computer vi- sion pattern recognition, pages sung-jin cho, seo-won ji, jun-pyo hong, seung-won jung, sung-jea ko. rethinking coarse-to-fine approach sin- gle image deblurring. proceedings ieeecvf inter- national conference computer vision, pages tao dai, jianrui cai, yongbing zhang, shu-tao xia, lei zhang. second-order attention network single image super-resolution. ieee conference computer vision pattern recognition, pages tri dao albert gu. transformers ssms gen- eralized models efficient algorithms structured state space duality. international conference machine learning icml, rui deng tianpei gu. cu-mamba selective state space models channel learning image restoration. arxiv preprint hang dong, jinshan pan, lei xiang, zhe hu, xinyi zhang, fei wang, ming-hsuan yang. multi-scale boosted de- hazing network dense feature fusion. ieee con- ference computer vision pattern recognition, pages seattle, wa, yuanbiao gou, boyun li, zitao liu, songfan yang, peng. clearer multi-scale neural architecture search im- age restoration. advances neural information processing systems, albert tri dao. mamba linear-time sequence modeling selective state spaces. arxiv preprint albert gu, karan goel, christopher re. efficiently modeling long sequences structured state spaces. arxiv preprint chun-le guo, qixin yan, saeed anwar, runmin cong, wenqi ren, chongyi. image dehazing transformer transmission-aware position embedding. proceed- ings ieeecvf conference computer vision pattern recognition, hang guo, jinmin li, tao dai, zhihao ouyang, xudong ren, shu-tao xia. mambair simple baseline image restoration state-space model. arxiv preprint vincent tao hu, stefan andreas baumann, ming gui, olga grebenkova, pingchuan ma, johannes fischer, bjorn ommer. zigma dit-style zigzag mamba diffusion model. arxiv preprint jia-bin huang, abhishek singh, narendra ahuja. single image super-resolution transformed self-exemplars. proceedings ieee conference computer vision pattern recognition, pages tao huang, xiaohuan pei, you, fei wang, chen qian, chang xu. localmamba visual state space model windowed selective scan. arxiv preprint zheng hui, xinbo gao, yunchu yang, xiumei wang. lightweight image super-resolution information multi- distillation network. proceedings acm inter- national conference multimedia, pages boyi li, xiulian peng, zhangyang wang, jizheng xu, dan feng. aod-net all-in-one dehazing network. ieee international conference computer vision, pages venice, italy, boyi li, wenqi ren, dengpan fu, dacheng tao, dan feng, wenjun zeng, zhangyang wang. benchmarking single image dehazing beyond. ieee transactions image processing, boyun li, xiao liu, peng hu, zhongqin wu, jiancheng lv, peng. all-in-one image restoration unknown corruption. ieee conference computer vision pattern recognition, pages new orleans, la, wenbo li, kun zhou, qi, nianjuan jiang, jiangbo lu, jiaya jia. lapar linearly-assembled pixel-adaptive re- gression network single image super-resolution be- yond. advances neural information processing systems, jingyun liang, jiezhang cao, guolei sun, kai zhang, luc van gool, radu timofte. swinir image restoration using swin transformer. international conference computer vision workshops, virtual, bee lim, sanghyun son, heewon kim, seungjun nah, kyoung lee. enhanced deep residual networks single image super-resolution. ieee conference computer vision pattern recognition workshop, pages bee lim, sanghyun son, heewon kim, seungjun nah, kyoung lee. enhanced deep residual networks single image super-resolution. proceedings ieee confer- ence computer vision pattern recognition workshops, pages xiaohong liu, yongrui ma, zhihao shi, jun chen. grid- dehazenet attention-based multi-scale network im- age dehazing. international conference computer vi- sion, pages seoul, korea, yue liu, yunjie tian, yuzhong zhao, hongtian yu, lingxi xie, yaowei wang, qixiang ye, yunfan liu. vmamba visual state space model. arxiv preprint liu, yutong lin, yue cao, han hu, yixuan wei, zheng zhang, stephen lin, baining guo. swin transformer hierarchical vision transformer using shifted windows. proceedings ieeecvf international conference computer vision, pages xiaotong luo, yuan xie, yulun zhang, yanyun qu, cui- hua li, yun fu. latticenet towards lightweight image super-resolution lattice block. european conference computer vision, pages kede ma, zhengfang duanmu, qingbo wu, zhou wang, hongwei yong, hongliang li, lei zhang. waterloo ex- ploration database new challenges image quality as- sessment models. ieee transactions image processing, david martin, charless fowlkes, doron tal, jitendra malik. database human segmented natural images application evaluating segmentation algorithms measuring ecological statistics. international conference computer vision, pages vancouver, canada, yusuke matsui, kota ito, yuji aramaki, azuma fujimoto, toru ogawa, toshihiko yamasaki, kiyoharu aizawa. sketch-based manga retrieval using manga dataset. mul- timedia tools applications, yiqun mei, yuchen fan, yuqian zhou. image super- resolution non-local sparse attention. ieee con- ference computer vision pattern recognition, pages chong mou, jian zhang, zhuoyuan wu. dynamic atten- tive graph learning image restoration. ieee interna- tional conference computer vision, seungjun nah, tae hyun kim, kyoung lee. deep multi-scale convolutional neural network dynamic scene deblurring. ieee conference computer vi- sion pattern recognition, pages honolulu, hi, ben niu, weiwei wen, wenqi ren, xiangde zhang, lian- ping yang, shuzhen wang, kaihao zhang, xiaochun cao, haifeng shen. single image super-resolution via holis- tic attention network. european conference computer vision, pages dongwon park, dong kang, jisoo kim, young chun. multi-temporal recurrent neural networks progres- sive non-uniform single image deblurring incremental temporal training. european conference computer vi- sion, pages springer, qin, zhilin wang, yuanchao bai, xiaodong xie, huizhu jia. ffa-net feature fusion attention network single image dehazing. aaai conference artificial intelligence, pages new york, ny, chao ren, xiaohai he, chuncheng wang, zhibo zhao. adaptive consistency prior based deep network image denoising. proceedings ieeecvf conference computer vision pattern recognition, pages christos sakaridis, dengxin dai, luc van gool. seman- tic foggy scene understanding synthetic data. interna- tional journal computer vision, ziyi shen, wenguan wang, xiankai lu, jianbing shen, haibin ling, tingfa xu, ling shao. human-aware motion deblurring. proceedings ieeecvf inter- national conference computer vision, pages jimmy smith, andrew warrington, scott linder- man. simplified state space layers sequence modeling. eleventh international conference learning rep- resentations, yuda song, zhuqing he, hui qian, xin du. vision transformers single image dehazing. ieee transactions image processing, xin tao, hongyun gao, xiaoyong shen, jue wang, ji- aya jia. scale-recurrent network deep image deblurring. proceedings ieee conference computer vision pattern recognition, pages radu timofte, eirikur agustsson, luc van gool, ming- hsuan yang, lei zhang. ntire challenge single image super-resolution methods results. proceed- ings ieee conference computer vision pattern recognition workshops, pages zhendong wang, xiaodong cun, jianmin bao, wengang zhou, jianzhuang liu, houqiang li. uformer general u-shaped transformer image restoration. proceedings ieeecvf conference computer vision pat- tern recognition cvpr, pages haiyan wu, yanyun qu, shaohui lin, jian zhou, ruizhi qiao, zhizhong zhang, yuan xie, lizhuang ma. con- trastive learning compact single image dehazing. ieee conference computer vision pattern recogni- tion, pages virtual, syed waqas zamir, aditya arora, salman khan, munawar hayat, fahad shahbaz khan, ming-hsuan yang, ling shao. multi-stage progressive image restoration. ieee conference computer vision pattern recognition, pages virtual, syed waqas zamir, aditya arora, salman khan, mu- nawar hayat, fahad shahbaz khan, ming-hsuan yang. restormer efficient transformer high-resolution im- age restoration. ieee conference computer vision pattern recognition, pages new orleans, la, roman zeyde, michael elad, matan protter. single image scale-up using sparse-representations. curves surfaces international conference, avignon, france, june revised selected papers pages springer, hongguang zhang, yuchao dai, hongdong li, piotr ko- niusz. deep stacked hierarchical multi-patch network image deblurring. proceedings ieeecvf con- ference computer vision pattern recognition, pages jiale zhang, yulun zhang, jinjin gu, yongbing zhang, linghe kong, xin yuan. accurate image restoration attention retractable transformer. iclr, kai zhang, wangmeng zuo, yunjin chen, deyu meng, lei zhang. beyond gaussian denoiser residual learning deep cnn image denoising. ieee transactions image processing, kai zhang, wangmeng zuo, shuhang gu, lei zhang. learning deep cnn denoiser prior image restoration. ieee conference computer vision pattern recogni- tion, pages kai zhang, wangmeng zuo, lei zhang. ffdnet toward fast flexible solution cnn based image denoising. ieee transactions image processing, kaihao zhang, wenhan luo, yiran zhong, lin ma, bjorn stenger, wei liu, hongdong li. deblurring realistic blurring. proceedings ieeecvf conference computer vision pattern recognition, pages kai zhang, yawei li, wangmeng zuo, lei zhang, luc van gool, radu timofte. plug-and-play image restora- tion deep denoiser prior. ieee transactions pat- tern analysis machine intelligence, lei zhang, xiaolin wu, antoni buades, xin li. color demosaicking local directional interpolation nonlocal adaptive thresholding. journal electronic imaging, xindong zhang, hui zeng, shi guo, lei zhang. efficient long-range attention network image super- resolution. european conference computer vision, pages springer, yulun zhang, kunpeng li, kai li, lichen wang, bineng zhong, yun fu. image super-resolution using deep residual channel attention networks. european confer- ence computer vision, pages appear eccv yulun zhang, yapeng tian, kong, bineng zhong, yun fu. residual dense network image restoration. ieee transactions pattern analysis machine intelli- gence, haiyu zhao, yuanbiao gou, boyun li, dezhong peng, jiancheng lv, peng. comprehensive delicate efficient transformer image restoration. proceed- ings ieeecvf conference computer vision pattern recognition, pages zhuoran zheng chen wu. u-shaped vision mamba single image dehazing. arxiv preprint shangchen zhou, jiawei zhang, wangmeng zuo, chen change loy. cross-scale internal graph neural network image super-resolution. neural information process- ing systems, neurips yupeng zhou, zhen li, chun-le guo, song bai, ming-ming cheng, qibin hou. srformer permuted self-attention single image super-resolution. ieee conference computer vision pattern recognition, pages lianghui zhu, bencheng liao, qian zhang, xinlong wang, wenyu liu, xinggang wang. vision mamba efficient visual representation learning bidirectional state space model. arxiv preprint", "published_date": "2024-12-28T07:40:39+00:00"}
{"id": "2412.16947v1", "title": "Separating Drone Point Clouds From Complex Backgrounds by Cluster Filter -- Technical Report for CVPR 2024 UG2 Challenge", "authors": ["Hanfang Liang", "Jinming Hu", "Xiaohuan Ling", "Bing Wang"], "summary": "increasing deployment small drones tools conflict disruption amplified threat, highlighting urgent need effective anti-drone measures. however, compact size drones presents significant challenge, traditional supervised point cloud image-based object detection methods often fail identify small objects effectively. paper proposes simple uav detection method using unsupervised pipeline. uses spatial-temporal sequence processing fuse multiple lidar datasets effectively, tracking determining position uavs, detect track uavs challenging environments. method performs front rear background segmentation point clouds global-local sequence clusterer parses point cloud data spatial-temporal density spatial-temporal voxels point cloud. furthermore, scoring mechanism point cloud moving targets proposed, using time series detection improve accuracy efficiency. used mmaud dataset, method achieved place cvpr challenge, confirming effectiveness method practical applications.", "full_text": "separating drone point clouds complex backgrounds cluster filter technical report cvpr challenge hanfang liang, jinming hu, xiaohuan ling, bing wang abstract increasing deployment small drones tools conflict disruption amplified threat, highlighting urgent need effective anti-drone measures. however, compact size drones presents significant challenge, traditional supervised point cloud image-based object detection methods often fail identify small objects effectively. paper proposes simple uav detection method using unsupervised pipeline. uses spatial-temporal sequence processing fuse multiple lidar datasets effectively, tracking determining position uavs, detect track uavs challenging environments. method performs front rear background segmentation point clouds global-local sequence clusterer parses point cloud data spatial-temporal density spatial-temporal voxels point cloud. furthermore, scoring mechanism point cloud moving targets proposed, using time series detection improve accuracy efficiency. used mmaud dataset, method achieved place cvpr challenge, confirming effectiveness method practical applications. introduction drones received huge attention various real-world applications, surveillance, military applications, sur- veying mapping monitoring, etc. regarding drone mon- itoring, currently mainly vision-based detection solutions. however, uav far away target, difficult detect position uav solely relying images, also difficult obtain spatial location information uav solely relying images. compared two-dimensional image detection uavs, three-dimensional spatial position detection estimation uavs challenging. necessary detect track target uav, also estimate spatial position uav. main challenges first, drone flying high-altitude orbit, image features drone weak occupy pixels. time, easily affected environment lighting difficult identify. secondly, point cloud drone scanned lidar sparse, drone cannot scanned every frame, resulting point cloud characteristics drone work supported organization hanfang liang school jianghan university wuhan, china. hanfangliangstu.jhun.edu.cn jinming school jianghan university wuhan, china. stu.jhun.edu.cn xiaohuan ling school jianghan university wuhan, china. qq.com bing wang corresponding author school jianghan university wuhan, china. wangbingjhun.edu.cn sparse mav point cloud small mav pixels fig. challenging examples image point cloud detection. point cloud, scanning points drone sparse continuous time dimension. many time frames, drone small detected. picture, drone tiny, dozen pixels. unstable. third, point cloud data provided multiple lidars, lidar data contain lot noise, making difficult correctly identify track drones. goal build unsupervised uav point cloud detection method, segment point cloud uav trajectory unknown point cloud space, retain point cloud containing uav trajectory, restore combined timestamp information. original trajectory drone. article, mainly classify point cloud point sets clustering idea. particular, divide clusterer two parts. global-local clusterer classifies overall point cloud, initially divides excludes cs.cv dec buildings. large objects objects trees based clustering results, two attributes spatiotemporal density spatiotemporal voxels separated, moving targets spatiotemporal sequence processed select point cloud corresponding uav trajectory. finally, filter spline fitting operation performed point cloud, spatial position uav restored based timestamp interpolation. main contributions work follows provide simple fast unsupervised detection method detecting drone trajectories positions point cloud data. method uses point cloud sources detect drones uses lidar data mmaud dataset. rely complex deep learning algorithms quickly deployed edge devices. propose spatio-temporal voxel spatio-temporal density analysis method point cloud moving targets, scoring mechanism evaluate confidence point cloud isolate correct trajectory point set. method achieved place cvpr challenge, confirming effectiveness reliability method tracking uavs determining spatial location uavs. compared selected deep learning algo- rithms conducted ablation experiments formalize speed usefulness method uav detec- tion. ii. related works section reviews literature field uav de- tection tracking. due limited research tracking small objects uavs based lidar, focus vision-based lidar-based uav tracking vision-based uav tracking currently many vision-based uav detection methods received attention many applied tasks. rapid development deep learning, many studies uav detection methods based deep learning. zheng al. evaluated eight state-of-the-art deep learning algorithms mav detection det-fly dataset. isaac-medina al. evaluated four state- of-the-art deep learning algorithms three representative mav datasets mav-vid, drone-vs-bird, anti-uav. order improve accuracy target detection, liu al. implemented special data augmentation method pruned convolution channels skip layers yolov small uav detection. rui al. proposed novel comprehensive method combines transfer learn- ing adaptive fusion based simulated data improve small target detection performance. motion-assisted micro air vehicle mav detection meth- ods aim detect mavs combining motion features appearance features. existing motion-assisted mav detection methods divided two categories fixed cameras mobile cameras. seidaliyeva al. used fixed camera monitor sky used background subtraction cnn-based object classification mav detection. xie al. used method fuse spatiotemporal characteristics target detection long-range flying drones-. zheng al. used appearance features exclude non-mav moving targets, used motion- based classification algorithms distinguish mavs distractors. mav detection moving cameras challenging fixed cameras motion back- ground coupled motion target. al. proposed uav-to-uav video dataset general archi- tecture small mav detection cameras mounted mobile mav platforms. authors detect moving mavs subtracting adjacent frames use hybrid classifier identify mavs. ashraf al. proposed two-stage segmentation method. first stage, authors utilize convolutional networks channel-wise pixel-level attention extract contextual information overlapping patches. then, convolutional network channel-pixel-level attention used learn spatiotemporal cues discover first-stage omission detection. method proposes uav detector based feature super-resolution, based motion information extraction dense optical flow. however, still challenging air-to-air mav detection complex environments. lidar-based uav tracking lidar systems commonly used detect track objects, unmanned aerial vehicles uavs present unique challenges detecting tracking uavs due small size, shape, diverse materials, high speeds, unpredictable motion challenges proposed new method tracking drones using lidar point clouds. consider speed distance drone adjust lidar frame integration time, parameters impact dealing density size point cloud. sedat dogru al. suggest detection accom- plished using fewer lidar beams long probabilistic analysis detection performed appropriate settings ensured. tracking small number hit points continuously, limitations lidar technology overcome moving sensor increase field view improve coverage. razlaw, al. proposed method combines segmentation methods simple object models utilizing temporal information over- come limitations lidar technology improve uav detection tracking capabilities. wang, al. used euclidean distance clustering particle filtering algorithms complete uav detection tracking. sier al. proposed concept lidar camera track drones without prior knowledge data content fusion images point cloud data generated single lidar sensor. using custom yolov model fig. first superimpose point clouds sequence obtain global point cloud, separate data different lidars, perform denoising processing data dji livox avia. picture box right radar data processed noise reduction, rendered distinguished according point cloud density point cloud height. greater density point cloud higher altitude, red color becomes red trajectory picture right real trajectory drone. seen noise well processed trajectory point cloud drone preserved. trained panoramic images, able integrate computer vision capabilities directly onto lidar itself. although deep learning methods made great progress relative traditional methods, methods either time-consuming effective target large enough background simple, different scenarios different drone types data-driven deep learning methods require large uav data sets. there- fore, still many challenges deep learning mav detection method based image appearance, difficulty detection complex background environments difficulty detection small objects. time, image-based detection, difficult estimate three- dimensional position drone, making difficult meet needs many practical applications. use lidar technology offers multiple ways improve drone detection tracking explore new tech- nologies overcome unique challenges posed small, fast-moving unpredictable objects. time, lidar point cloud detection method also needs face problems point cloud noise, sparsity lidar point cloud, discontinuity small targets point cloud sequence. compared detecting position drone image, article focuses using unsupervised methods detect pose trajectory drone, well detecting predicting spatio- temporal coordinates drone. iii. methods section presents details proposed method. effectively detect mavs challenging conditions, propose clustering-based point cloud unsupervised spatial-temporal sequence uav trajectory detection method. consists three parts. sect. first denoise point clouds data different lidars sect. design global clusterer local clusterer sect. introduce scoring mechanism evaluate confidence clustering result filter clustering categories separation. uav trajectory sect. combine processed uav point cloud time frame regression fit uav spatial position. point clouds denoise detected point cloud data comes two lidar sen- sors, dji livox avia dji livox mid. combining two radars, lidar scanning range close full coverage ground sky obtained. however, lidar data sparse signal, dji livox avia radar accompanied lot noise, range meters, small targets drone similar noise. data lot noise used directly, leading disastrous consequences. therefore, first need denoise lidar data dji livox avia. first superimpose continuous lidar sequences superimpose sequences. find noise density sensor belongs sparsest category. based this, exclude noise points based density, ensuring high accuracy. shown picture excluded noise points updated lidar sequence facilitate use subsequent local global clustering methods, introduced next section. global-local point set clusterings lidar data sparse signal accompanied lot noise. stationary object, points scanned different times still large spatial differences. however, time dimension increases, point set density stationary object increases. increase significantly. inspired previous clustering ideas, point set initially divided frame frame ... frame denoised point cloud temporal sequence extracted point cloud dbscan global cluster local cluster ... cluster cluster cluster cluster cluster cluster mse evaluaon voxel density scoring iou trajectory esmaon global-local cluster fig. proposed algorithm architecture. given continuous point cloud input sequence, first classify global clustering local clustering obtain different categories using dbscan. number point clouds voxel spatial information calculated global local categories respectively. cross-compare calculate spatial coincidence degree temporal density changes different clusters. final scoring mechanism calculates spatial coincidence degree relative density score categories exclude point clouds uav, restores trajectory uav spline fitting interpolation, uses mse calculate error true value. point sets different densities density distance clustering. dbscan, density associated point ob- tained counting number points within specified radius area around point. points density specified threshold constructed clusters. among existing clustering algorithms, chose dbscan algo- rithm ability discover clusters arbitrary shapes, linear, concave, elliptical, etc. moreover, compared clustering algorithms, re- quire shape clusters determined advance. dbscans proven ability handle large data. key issue method correctly exclude point sets different densities retain correct point set containing uav trajectories. considering stationary object surfaces accumulate lidar point clouds time dimension increases. moving objects volume, probability scanned lidar continuous time periods. therefore, point cloud density moving object relatively stable continuous time frames. addition, moving objects, volume point cloud voxel space change time dimension changes. therefore, method processes spatio-temporal sequence two perspectives point cloud density voxel volume, filters point cloud uav trajectory. define total number frames sequence point set frame pi,i ,,...,n, overall point set global. pglobal pi,i ,,...,n number frames local point set contains time frame frames, starting frame local point set set frame, frame ,,...,nframes.the local point set pframe local pframe local frameframes iframe define category cluster global point set pglobal global, category cluster local point set pframe local ck,frame local particular, point set ck,frame local new cluster set obtained dbscan re-clustering, corresponding points global within period time frame ,,...,nframes. voxel space category cluster global global point set global global, voxel space category cluster ck,frame local local point set pframe local k,frame local number points category cluster local point set pframe local changes time numk,frame local first superimpose point cloud global time frame obtain pglobal, use dbscan perform clus- tering obtain global. define use closer denser clustering parameters minpts, record volume class time. prime information global density point set also recorded. although low threshold bring false targets, reliable targets obtained applying confidence ranking scoring mechanism, introduced detail section calculate density global point cloud point set pglobal according global. global numk global global considering target mav cause drastic changes spatial position continuous time, local point clusterer used time. local point clusterer, first calculate density k,frame local point cloud point set frame local according ck,frame local simultaneously calculate spatial intersection union iou overlapping areas voxels. define iou voxels category cluster ck,frame local frame ioui,j k,frame local numk,frame local k,frame local ioui,j k,i local k,j local k,i local k,j local ,,...,nframes,i calculate ratio local density global density relative density frame frame k,frame local global point, global-local clusterer, relative density cluster point set frame iou voxels ioui,j obtained. scoring mechanism section, use density voxel co- incidence obtained previous section. moving object, adjacent time dimension, spatial position changes, voxel position space also change accordingly. local voxel coincidence smaller voxel coincidence stationary objects. time, time dimension increases, density point clouds accumulated surface stationary objects also increase. therefore, point cloud density stationary objects larger difference global- local relative density density point clouds moving objects relatively consistent overall situation within local time period. based inference, designed scoring mechanism evaluate confidence point clouds density voxel dimensions.and order make value stable retain changing trend value, use logarithmic function map voxel iou balanced scale. first define voxel coincidence score cluster local point set frame ck,frame local ck,frame local scorek iou. scorek iou log ioui,j define score point set density matching global k,frame local scorek dens. scorek dens erframe total score formula recorded scorek, hyperparameter set us. scorek scorek dens scorek iou according calculation formula, category highest score filtered out. specifically, target highest confidence selected final target. is, drone trajectory. order confirm practicality confidence representation scoring mechanism, randomly selected several groups se- quences compare separated trajectory point clouds true values, shown figure significantly removed cluttered point clouds background, retaining characteristics uav trajectory point cloud. trajectory prediction final trajectory based time frame, use spline fitting uav point cloud, interpolate based time frame spatial position corresponding time frame. time frame background segmented, may multiple point clouds frame. data collection sorting, even multiple points corresponding timestamp, sorted consecutive blocks list chronolog- ical order. means timestamp, corresponding point cloud data processed instead one point. define k-th point cloud frame segmenting background sort point clouds time frame according timestamp merge point set puav ,...,pk among them, points point set puav selected control points, three- dimensional spline expressed ibi basis function spline function defined ,ui finally, three-dimensional curve interpolated fitted order time frames obtain uav spatial coordinates required time nodes. table comparison different methods methods modality backbone sda mse yolovs image resnet cascade r-cnn image fpn r-cnn image grid r-cnn image vit image image centernet point cloud point cloud pointnet point cloud point cloud point cloud iv. experiment dataset evaluate performance proposed algorithm, tested proposed algorithm mmaud challenging dataset. mmaud dataset briefly introduced below. mmaud dataset provides multi-modal dataset integrates visual, lidar array, radar, audio array sensors, high-precision ground truth. dataset contains seconds multimodal data divided different sequences. sequence contains sufficient visual, lidar, audio radar data identification purposes. example images shown along visualized point cloud. evaluation metrics implementation details cvpr challenge, use mse error evaluate accuracy algorithm. time, introduce sequence detection accuracy sda indicator. sda detectedsequencetime allsequencetime image sequences, situations target cannot detected, case, obviously difficult predict spatial position coordinates drone. therefore, target cannot detected, predicting completely irrelevant spatial position prediction coordinate meaningless, mse method using camera modal data large. therefore, sda indicator also reflects detection ability different al- gorithms small drones complex backgrounds case. detection results method shown figure convenience display, superimpose trajectories entire sequence. green trajectory drone trajectory point cloud segmented background global local clustering methods red trajectory real spatial position drone blue trajectory spatial position drone predicted method. method achieved good results eliminating noise extracting correct drone trajectory point cloud space. uav trajectory point fig. green point cloud figure uav point cloud separated background method, red real trajectory uav, blue predicted uav trajectory. conclusions paper, propose unsupervised approach mav point cloud detector ground-to-space detection mavs challenging conditions. method employs spatial-temporal global local clustering point cloud se- quences extracting effective uav point cloud trajectories sparse noisy point clouds. method place cvpr challenge, confirming effectiveness method. moreover, method interpretable. time, use mmaud dataset, evaluate several representative deep learning algorithms, analyze experimental re- sults. future, order detect different drones environment, types drones classified based distribution drone point cloud trajectories deep learning technology. references important reader therefore, citation must complete correct. possible, references commonly available publications. references zheng, chen, lv, li, lan, zhao, air-to-air visual detection micro-uavs experimental evaluation deep learning, ieee robotics automation letters, vol. no. pp. isaac-medina, poyser, organisciak, willcocks, breckon, shum, unmanned aerial vehicle visual detection tracking using deep neural networks performance benchmark, proceedings ieeecvf international conference com- puter vision, pp. liu, fan, ouyang, li, real-time small drones detection based pruned yolov, sensors, vol. no. rui, youwei, huafei, hongyu, comprehensive approach uav small object detection simulation-based transfer learning adaptive fusion, arxiv preprint seidaliyeva, akhmetov, ilipbayeva, matson, real- time accurate drone detection video static background, sensors, vol. no. xie, gao, wu, shi, chen, small low-contrast target detection data-driven spatiotemporal feature fusion implementa- tion, ieee transactions cybernetics, vol. no. pp. xie, yu, wu, shi, chen, adaptive switching spatial-temporal fusion detection remote flying drones, ieee transactions vehicular technology, vol. no. pp. zheng, zheng, zhang, chen, chen, zhao, detection, localization, tracking multiple mavs panoramic stereo camera networks, ieee transactions automation science engineering, vol. no. pp. li, ye, kolsch, wachs, bouman, fast robust uav uav detection tracking video, ieee transactions emerging topics computing, vol. no. pp. ashraf, sultani, shah, dogfight detecting drones drones videos, proceedings ieeecvf conference computer vision pattern recognition, pp. wang, wang, zhou, meng, shi, low resolution, high precision uav detection super-resolution motion information extraction, icassp ieee international conference acoustics, speech signal processing icassp. ieee, pp. lei, deng, wang, yang, yuan, audio array-based uav trajectory estimation lidar pseudo-labeling, ieee international conference acoustics, speech, signal processing, qingqing, xianjia, queralta, westerlund, adaptive lidar scan frame integration tracking known mavs point clouds, international conference advanced robotics icar. ieee, pp. dogru marques, drone detection using sparse lidar mea- surements, ieee robotics automation letters, vol. no. pp. razlaw, quenzel, behnke, detection tracking small objects sparse laser range data, international conference robotics automation icra. ieee, pp. wang, peng, liu, liang, study target detection tracking method uav based lidar, global reliability prognostics health management phm-nanjing. ieee, pp. sier, yu, catalano, queralta, zou, westerlund, uav tracking lidar camera sensor gnss-denied environ- ments, international conference localization gnss icl-gnss. ieee, pp. liang, yang, hu, yang, liu, yuan, unsupervised uav trajectories estimation sparse point clouds, ieee international conference acoustics, speech, signal processing, xiao, hu, xu, he, tame temporal audio-based mamba enhanced drone trajectory estimation classification, ieee international conference acoustics, speech, signal processing,", "published_date": "2024-12-22T09:44:43+00:00"}
{"id": "2412.12716v5", "title": "Unsupervised UAV 3D Trajectories Estimation with Sparse Point Clouds", "authors": ["Hanfang Liang", "Yizhuo Yang", "Jinming Hu", "Jianfei Yang", "Fen Liu", "Shenghai Yuan"], "summary": "compact uav systems, advancing delivery surveillance, pose significant security challenges due small size, hinders detection traditional methods. paper presents cost-effective, unsupervised uav detection method using spatial-temporal sequence processing fuse multiple lidar scans accurate uav tracking real-world scenarios. approach segments point clouds foreground background, analyzes spatial-temporal data, employs scoring mechanism enhance detection accuracy. tested public dataset, solution placed cvpr challenge, demonstrating practical effectiveness. plan open-source designs, code, sample data research community github.comlianghanfangunlidar-uav-est.", "full_text": "unsupervised uav trajectories estimation sparse point clouds hanfang liang jianghan university, china. hanfangliangstu.jhun.edu.cn yizhuo yang nanyang technological university, singapore. yizhuoe.ntu.edu.sg jinming jianghan university, china. stu.jhun.edu.cn jianfei yang nanyang technological university, singapore. jianfei.yangntu.edu.sg fen liu nanyang technological university, singapore. fen.liuntu.edu.sg shenghai yuan nanyang technological university, singapore. corresponding- author shyuanntu.edu.sg abstractcompact uav systems, advancing delivery surveillance, pose significant security challenges due small size, hinders detection traditional methods. paper presents cost-effective, unsupervised uav detec- tion method using spatial-temporal sequence processing fuse multiple lidar scans accurate uav tracking real-world scenarios. approach segments point clouds foreground background, analyzes spatial-temporal data, employs scoring mechanism enhance detection accuracy. tested public dataset, solution placed cvpr challenge, demonstrating practical effectiveness. plan open-source designs, code sample data research community github.comlianghanfangunlidar-uav-est. index termstrajectory estimation, uav detection, point clouds, unsupervised introduction drones revolutionized various industries allowing precise fertilization agriculture allowing detailed inspec- tion hard-to-reach structures however, potential malicious drone use major concern. used unauthorized surveillance drug trafficking smuggling even deployment grenades war zones. threat highlights urgent need advanced detection systems detect hostile drones effectively. detecting compact uavs challenging. existing solutions rely uav control signals detect, easily bypassed changing frequencies, using networks, fully autonomous drones visual-based methods struggle small objects high altitudes. narrow field-of-view cameras mounted buildings operated manually see drone impractical field operations. wide field view cameras monitor larger area, often capture pixels drone shown fig. radar detect drones effectively, cheaper models noisy expensive ones expensive power demanding audio-based detection work supported national research foundation, singapore, medium-sized center advanced robotics technology innovation. sampled uav pointcloud older recent ground truth predicted trajectory visual hard detect lidar parally work system help police identify track uav drug fig. illustration detecting tracking compact drones using single low-cost sparse lidar identify threats. intuitive often less effective, com- mercial drones quiet distance. lidar detect drones, data sparse long ranges general, perfect solution drone detection. work aims accurately detect drones regardless control signal frequency autonomy, including small drones high altitudes, without manual operation. ensures practicality wide field operations affordability use single person single vehicle, shown fig. paper, propose concurrent clustering method analyzing point clouds low-cost lidar system. first, perform global-local clustering exclude large static objects. then, refine clustering using spatiotemporal density voxel attributes identify moving targets isolate uav trajectory. finally, use spline fitting reconstruct uavs spatial trajectory, enhancing detection accuracy, reducing noise, eliminating irrelevant data clearer insights drone movements. main contributions work follows unsupervised trajectory estimation propose fast, unsupervised method detecting drone trajectories positions lidar point cloud data without labels supervised learning. spatio-temporal analysis spatio-temporal voxel cs.cv jan fig. system overview algorithm uses dbscan cluster point clouds, compares spatial-temporal changes, filters non-uav data, estimates uav trajectories spline fitting, measuring error mse. density analysis method, scoring mechanism, isolates correct trajectory point set. extensive benchmarking benchmarked tested various modalities different methods validate performance system. best knowledge, first benchmark kind anti-uav study. open-source plan open-source de- sign, codes, scripts, processed data benefit community general public github.com lianghanfangunlidar-uav-est. international recognition proposed method improved iteration award-winning solution cvpr challenge, enhancing cost- effectiveness, robustness reliability practical field ap- plications. ii. related works section reviews limited literature uav detection tracking, focusing key approaches. vision-based uav detection evolved deep learning, addressing challenges highlighted studies det-fly mav-vid, drone-vs-bird, anti-uav methods improved accuracy augmenting data optimizing yolov small uav detection transfer learning adaptive fusion using simulated data motion-assisted mav detection integrates motion ap- pearance features using fixed mobile cameras. fixed camera methods employ background subtraction cnn- based classification mobile cameras utilize spatio- temporal characteristics struggle dynamic environments. another approach combines appearance motion-based classification distinguish mavs distrac- tors albeit facing challenges similar moving objects. detection moving cameras complex due background target motion mixing together. methods using uav-to-uav datasets hybrid classifiers contend background interference. two-stage segmentation feature super-resolution offer advancements grapple issues like motion blur occlusions complex set- tings. lidar systems, widely used object detection track- ing, face unique challenges uavs due small size, shape variability, diverse materials, high speeds, unpredictable movements. one method adjusts integration time lidar frame based drone speed distance improve density size point cloud, approach intricate sensitive parameter settings another strategy reduces lidar beams probabilistic analysis repositions sensor wider coverage, yet struggles continuous tracking small points segmentation methods combined object models temporal information improve effectiveness uav de- tection tracking effectiveness, though constrained segmentation object model accuracy euclidean distance clustering particle filtering algorithms offer ac- curate yet computationally efficient solutions, albeit sensitive data noise outliers summary, several methods address challenges uav detection tracking lidar, method presents distinct limitations complexities, underscoring need ongoing research development domain. iii. proposed methods section outlines unsupervised spatial-temporal ap- proach based clustering detect mavs challenging conditions. overview system shown fig. global-local point set clusterings let denote sequence lidar scan frames, denoting number frames. represents set points single scan number points set denoted card denotes cluster subset points denotes density points, denotes voxels set points. local representation, pfi represents set points within i-th frame pfi denotes j-th cluster category points pfi. global representation, denotes frame sequences frame frame denotes k-th category cluster merging points dji mavic pro predicted trajectory dji mavic pro prediction error dji predicted trajectory dji phantom predicted trajectory dji prediction error dji phantom prediction error fig. figure shows sampled points, ground truth, predicted trajectory, showing accuracy solution. distinguish results global local clustering, represents k-th cluster points n-th frame, clustering derived sequence frames indicates size voxel occupied cluster space. let operator denotes density points cluster derived set points context frame sequence first superimpose point cloud global time frames obtain use dbscan perform clustering obtain let denote cardi- nality calculate density point cloud global point set. local point cluster, first calculate density point cloud point set simultaneously calculate spatial intersection union iou overlapping areas voxels. define iou voxels category cluster frame iou i,j iou i,j pfi pfj pfi pfj calculate ratio local density global density relative density ri,j ri,j point, global-local clustering, relative density cluster point set ri,j iou voxels iou i,j obtained. scoring mechanism trajectory prediction moving objects, positions voxels change time frames, causing lower alignment compared stationary objects. stationary surfaces show increase point cloud density time, moving objects maintain consistent density. propose scoring mechanism based density voxel shifts, using logarithmic function stabilize scale voxel iou. define voxel coincidence score cluster local point set frame pfi pfj iou.define score point set density matching iou log ioui,j eri,j iou based proposed scoring scheme, category highest score identified final target highest confidence. final trajectory based time frame, use spline fitting uav point cloud interpolate based time frame determine spatial position corresponding time points. define cloud frame kth point segmenting background sort point clouds time frame according timestamp merge set points puav ..., among them, points set points puav selected control points, three-dimensional spline expressed basis function spline. three- dimensional curve interpolated fitted order time frames obtain uav spatial coordinates required time nodes. iv. experiment dataset evaluated algorithm difficult part mmaud namely mmaud mmaud se- quences, featuring visual, lidar array, radar, audio table benchmark wide-area drone estimation mmaud challenging dataset methods modality training bandwidth day rmse night rmse rmse rmse day night visualnet visual supervised .mpts darknet visual supervised .mpts yolovs visual supervised .mpts audionet audio supervised .mhz vorasnet audio supervised .mhz voxelnet lidar supervised .mpts pointnet lidar supervised .mpts pointpillars lidar supervised .mpts votenet lidar supervised .mpts second lidar supervised .mpts spvnas lidar supervised .mpts lidar unsupervised .mpts rmse error predicted actual values. smaller, better estimation. rmse denotes average error day night. mpts denotes mega sampling points per second input. denotes fails detect. best results boldened, second-best results underlined array sensors, seconds multi-modal data sequences. sequence includes millions sam- pling points visual, lidar, audio, radar data. mmaud sequences, detections easy uavs typically fly within meters. however, mmaud mmaud sequences, -meter range makes smaller uavs harder detect lidar. evaluation metrics evaluate algorithm using rmse error, directly evaluates system prediction accuracy various conditions. varying lighting conditions, better understand performance baseline method. overall visual performance seen fig. green represents drone trajectories segmented global local clus- tering, red denotes actual drone positions, blue indicates predicted positions. approach excels noise reduction precise drone trajectory extraction point cloud data. result discussion proposed solution demonstrates robust performance un- der various lighting conditions, shown table traditional supervised lidar-based methods often expect dense data large object sizes end worst performance due sparse lidar data reflected compact uavs. visual-based approaches perform well day denser sampling points exhibit significant perfor- mance drops night. audio-based methods show consistent performance day night, overall accuracy low. proposed solution manages perform robust drone pose estimation day night, even sparse point clouds. shows practical solution early warning applications uavs. conclusion future works paper introduces unsupervised approach ro- bust ground-based uav detection using spatial-temporal global-local clustering sparse point cloud sequences. method extracts precise uav trajectories sparse noisy data. plan open-source design, codes, scripts, sampled data. future work, aim integrate ac- tive countermeasures, leveraging uavs emp devices, effectively neutralize drone threats using proposed perception inputs. references muqing cao, kun cao, xiuxian li, shenghai yuan, yang lyu, thien- minh nguyen, lihua xie, distributed multi-robot sweep coverage region unknown workload distribution, autonomous intelli- gent systems, vol. no. pp. yang lyu, muqing cao, shenghai yuan, lihua xie, vision-based plane estimation following building inspection autonomous uav, ieee trans. syst. man. cybern. muqing cao, yang lyu, shenghai yuan, lihua xie, online trajec- tory correction tracking facade inspection using autonomous uav, ieee international conference control automation icca. ieee, yang lyu, muqing cao, shenghai yuan, lihua xie, vision based autonomous uav plane estimation following building inspection, arxiv preprint mahdi abolfazli esfahani, han wang, benyamin bashari, keyu wu, shenghai yuan, learning extract robust handcrafted features single observation via evolutionary neurogenesis, applied soft computing, yang lyu, shenghai yuan, lihua xie, structure priors aided visual-inertial navigation building inspection tasks auxiliary line features, ieee taes, kun cao, muqing cao, shenghai yuan, lihua xie, direct differential dynamic programming based framework trajectory generation, ieee ral, yang lyu, thien-minh nguyen, liu liu, muqing cao, shenghai yuan, thien hoang nguyen, lihua xie, spins structure priors aided inertial navigation system, journal field robotics, zheng si, chao liu, jianyu liu, yinhao zhou, application snns model based multi-dimensional attention drone radio frequency signal classification, icassp, ryan wallace, kristy kiernan, tom haritos, john robbins, jon loffi, evaluating small uas operations national airspace system interference using aeroscope, journal aviation technology engineering, vol. no. pp. shenghai yuan, han wang, lihua xie, survey localization systems algorithms unmanned systems, us, thien-minh nguyen, muqing cao, shenghai yuan, yang lyu, thien hoang nguyen, lihua xie, viral-fusion visual-inertial- ranging-lidar sensor fusion approach, ieee tro, angelo coluccia, alessio fascista, lars sommer, arne schumann, anastasios dimou, dimitrios zarpalas, nabin sharma, drone- vs-bird detection grand challenge icassp, icassp, sahaj mistry, shreyas chatterjee, ajeet verma, vinit jakhetiya, badri subudhi, sunil jaiswal, drone-vs-bird drone detection using yolov csrt tracker, icassp, pengcheng dong, chuntao wang, zhenyong lu, kai zhang, wenbo wan, jiande sun, s-feature pyramid network attention model drone detection, icassp, nan jiang, kuiran wang, xiaoke peng, xuehui yu, qiang wang, junliang xing, guorong li, jian zhao, guodong guo, zhenjun han, anti-uav large multi-modal benchmark uav tracking, arxiv preprint zhao, wang, li, jin, fan, wang, wang, yong, deng, guo, al., anti-uav workshop challenge methods results., arxiv preprint jian zhao, jianan li, lei jin, jiaming chu, zhihao zhang, jun wang, jiangqiang xia, kai wang, yang liu, sadaf gulshad, al., anti-uav workshop challenge methods results, arxiv preprint shenghai yuan han wang, autonomous object level segmentation, proc. icarcv, pp. han wang, shenghai yuan, keyu wu, heterogeneous stereo human vision inspired method general robotics sensing, tencon ieee region conference. ieee, mahdi abolfazli esfahani, keyu wu, shenghai yuan, han wang, new approach train convolutional neural networks real-time dof camera relocalization, ieee international conference control automation icca. ieee, mahdi abolfazli esfahani, keyu wu, shenghai yuan, han wang, towards utilizing deep uncertainty traditional slam, ieee international conference control automation icca, mahdi abolfazli esfahani, keyu wu, shenghai yuan, han wang, local understanding global regression monocular visual odometry, international journal pattern recognition artificial intelligence, mahdi abolfazli esfahani, han wang, keyu wu, shenghai yuan, unsupervised scene categorization, path segmentation landmark extraction traveling path, proc. icarcv. ieee, yizhuo yang, shenghai yuan, lihua xie, overcoming catastrophic forgetting semantic segmentation via incremental learning, proc. icarcv. ieee, tete ji, shenghai yuan, lihua xie, robust rgb-d slam dynamic environments autonomous vehicles, proc. icarcv. ieee, yuan he, jia zhang, rui xi, xin na, yimiao sun, beibei li, de- tection identification non-cooperative uav using cots mmwave radar, acm transactions sensor networks, chenxing wang, jiangmin tian, jiuwen cao, xiaohong wang, deep learning-based uav detection pulse-doppler radar, ieee transactions geoscience remote sensing, sara al-emadi, abdulla al-ali, amr mohammad, abdulaziz al- ali, audio based drone detection identification using deep learning, iwcmc. ieee, pp. allen lei, tianchen deng, han wang, jianfei yang, shenghai yuan, audio array-based uav trajectory estimation lidar pseudo- labeling, proc. icassp, zhenyuan xiao, huanran hu, guili xu, junwei he, tame temporal audio-based mamba enhanced drone trajectory estimation classification, proc. icassp, zhenyuan xiao, yizhuo yang, guili xu, xianglong zeng, shenghai yuan, av-dtec self-supervised audio-visual fusion drone trajectory estimation classification, arxiv preprint matous vrba, viktor walter, vaclav pritzl, michal pliska, tomas baca, vojtech spurny, daniel hert, martin saska, onboard lidar-based flying object detection, ieee tro, hanfang liang, jinming hu, xiaohuan ling, bing wang, sepa- rating drone point clouds complex backgrounds cluster filter technical report cvpr challenge, arxiv preprint zheng, zhang chen, dailin lv, zhixing li, zhenzhong lan, shiyu zhao, air-to-air visual detection micro-uavs experimental evaluation deep learning, ieee robot. autom. lett., brian isaac-medina, matt poyser, daniel organisciak, chris willcocks, toby breckon, hubert shum, unmanned aerial vehicle visual detection tracking using deep neural networks performance benchmark, cvpr, hansen liu, kuangang fan, qinghua ouyang, li, real-time small drones detection based pruned yolov, sensors, chen rui, guo youwei, zheng huafei, jiang hongyu, compre- hensive approach uav small object detection simulation-based transfer learning adaptive fusion, arxiv preprint ulzhalgas seidaliyeva, daryn akhmetov, lyazzat ilipbayeva, eric matson, real-time accurate drone detection video static background, sensors, vol. no. pp. jiayang xie, chengxing gao, junfeng wu, zhiguo shi, jiming chen, small low-contrast target detection data-driven spatiotemporal feature fusion implementation, ieee trans. cybern., jiayang xie, jin yu, junfeng wu, zhiguo shi, jiming chen, adaptive switching spatial-temporal fusion detection remote flying drones, transactions vehicular technology, zheng, canlun zheng, xiaoyu zhang, fei chen, zhang chen, shiyu zhao, detection, localization, tracking multiple mavs panoramic stereo camera networks, tase, jing li, dong hye ye, mathias kolsch, juan wachs, charles bouman, fast robust uav uav detection tracking video, ieee transactions emerging topics computing, muhammad waseem ashraf, waqas sultani, mubarak shah, dog- fight detecting drones drones videos, cvpr, hanzhuo wang, xingjian wang, chengwei zhou, wenchao meng, zhiguo shi, low resolution, high precision uav detection super-resolution motion information extraction, icassp, qingqing, xianjia, jorge pena queralta, tomi westerlund, adaptive lidar scan frame integration tracking known mavs point clouds, icar. ieee, sedat dogru lino marques, drone detection using sparse lidar measurements, ieee robot. autom. lett., jan razlaw, jan quenzel, sven behnke, detection tracking small objects sparse laser range data, ieee icra, hong wang, peng, liansheng liu, jun liang, study target detection tracking method uav based lidar, global reliability prognostics health management. ieee, yizhuo yang, shenghai yuan, jianfei yang, thien hoang nguyen, muqing cao, thien-minh nguyen, han wang, lihua xie, av- fdti audio-visual fusion drone threat identification, jai, zhang, zhongyin guo, jianqing wu, yuan tian, haotian tang, xinming guo, real-time vehicle detection based improved yolo sustainability, neel vora, wu, jian liu, phuc nguyen, dronechase mobile automated cross-modality system continuous drone tracking, proc. workshop micro aerial vehicle netw. syst. appl., yin zhou oncel tuzel, voxelnet end-to-end learning point cloud based object detection, cvpr, pp. charles qi, hao su, kaichun mo, leonidas guibas, pointnet deep learning point sets classification segmentation, cvpr, pp. alex lang, sourabh vora, holger caesar, lubing zhou, jiong yang, oscar beijbom, pointpillars fast encoders object detection point clouds, cvpr, pp. zhipeng ding, han, marc niethammer, votenet deep learning label fusion method multi-atlas segmentation, miccai, yan yan, yuxing mao, li, second sparsely embedded convolutional detection, sensors, haotian tang, zhijian liu, shengyu zhao, yujun lin, lin, hanrui wang, song han, searching efficient architectures sparse point-voxel convolution, eccv, shenghai yuan, yizhuo yang, thien hoang nguyen, thien-minh nguyen, jianfei yang, fen liu, jianping li, han wang, lihua xie, mmaud comprehensive multi-modal anti-uav dataset modern miniature drone threats, icra,", "published_date": "2024-12-17T09:30:31+00:00"}
{"id": "2412.11186v1", "title": "Efficient Quantization-Aware Training on Segment Anything Model in Medical Images and Its Deployment", "authors": ["Haisheng Lu", "Yujie Fu", "Fan Zhang", "Le Zhang"], "summary": "medical image segmentation critical component clinical practice, state-of-the-art medsam model significantly advanced field. nevertheless, critiques highlight medsam demands substantial computational resources inference. address issue, cvpr medsam laptop challenge established find optimal balance accuracy processing speed. paper, introduce quantization-aware training pipeline designed efficiently quantize segment anything model medical images deploy using openvino inference engine. pipeline optimizes training time disk storage. experimental results confirm approach considerably enhances processing speed baseline, still achieving acceptable accuracy level. training script, inference script, quantized model publicly accessible", "full_text": "efficient quantization-aware training segment anything model medical images deployment haisheng lu, yujie fu, fan zhang, zhang university electronic science technology china, chengdu, china luhaisheng, fuyujiestd.uestc.edu.cn, fan.zhang,lezhanguestc.edu.cn abstract. medical image segmentation critical component clin- ical practice, state-of-the-art medsam model significantly advanced field. nevertheless, critiques highlight medsam de- mands substantial computational resources inference. address issue, cvpr medsam laptop challenge estab- lished find optimal balance accuracy processing speed. paper, introduce quantization-aware training pipeline de- signed efficiently quantize segment anything model medical images deploy using openvino inference engine. pipeline optimizes training time disk storage. experimental results confirm approach considerably enhances processing speed baseline, still achieving acceptable accuracy level. train- ing script, inference script, quantized model publicly accessible keywords quantization-aware training segment anything model. introduction drawing inspiration remarkable achievements foundation models natural language processing, researchers meta fair introduced versatile foundation model image segmentation, termed segment anything model sam widely recognized foundation models domain often confront challenges stemming limited data diversity. despite consider- able scale dataset utilized train sam referred sa-b dataset, comprising one billion masks, models performance fell short med- ical image segmentation tasks shortfall attributed part composition sa-b dataset, primarily comprises photographs natural scenes captured cameras, thus lacking nuanced features character- istic medical images. response challenge, al. curated diverse extensive medical image segmentation dataset encompassing modalities, upon fine-tuned sam refined model, dubbed medsam, represents significant step forward addressing discrepancy. however, cs.cv dec al. despite advancements, medsam still grapples several unresolved chal- lenges. instance, training dataset suffers extreme modality im- balances, model encounters difficulties accurately segmenting vessel-like branching structures, practicality text prompts remains limited. focus cvpr medsam laptop challenge enhancing inference speed medsam. segment anything model comprises three core components image encoder responsible transforming input images image embeddings, prompt encoder converts prompts prompt embeddings, mask decoder tasked generating low-resolution masks image embeddings prompt embeddings. notably, initial proto- type medsam, image encoder notably resource-intensive two components. consequently, various alternative backbones proposed replace original image encoder, vit-tiny archi- tecture adopted mobilesam efficientvit efficientvit-sam challenges baseline model litemedsam incorporates distilled vit-tiny image encoder, albeit slight adjustments compared mobilesam. sum- mary parameters different submodules provided table table parameters different submodules litemedsam medsam parameters image encoder prompt encoder mask decoder litemedsam medsam addition optimizing backbones sam, pursued alterna- tive approach expedite inference quantization. quantization offers several benefits, including reducing parameter sizes, increasing inference speed, de- creasing power consumption inference. two primary paradigms quantizing neural networks post-training quantization ptq quantization-aware training qat ptq involves converting pre- trained floating-point model directly low-precision one calibrating model using batch calibration data. method generally faster since require re-training, precision quantized model largely depends calibration process. hand, qat integrates quan- tization de-quantization nodes computational graph, enabling training model preserving accuracy quantization. ensure prediction accuracy, chose qat quantize sam. attention blocks transformers serve principal components backbone sam. several methods proposed enhance accu- racy quantized transformers. al. introduced information rectification module distribution-guided distillation scheme tailored fully quantized vision transformers liu al. discovered incorporating fixed uniform noise values quantized significantly mitigate quantization er- rors provable conditions study, chosen leverage xilinx brevitas framework framework offers excellent workflow, qmedsam encompassing quantization-aware training development inference engines. main contributions paper listed follows propose quantized litemedsam model comparable average ac- curacy, alleviate imbalance across different modalities. optimized online dataset proposed replace offline baseline, yield- ing significant reduction disk storage requirement. experiments proposed prove small subset training dataset maintain accuracy quantized model, making efficient training. quantized model deployed openvino inference engine, en- abling compete effectively models challenge. method preprocessing dataset comprises three types medical images grayscale images, rgb images, images. images split individual clips along z-axis, clip treated grayscale image. standardize grayscale format rgb format, grayscale images duplicated across red, green, blue channels. subsequently, rgb images resized, padded dimensions finally normalized. important note baseline approach, rgb images undergo normalization padding zeros. case, padded value equivalent minimum value image instead zero. weve implemented optimizations dataloader enhance effi- ciency training inference. training process, base- line approach, compressed npz files decompressed along z-axis, demands approximately disk storage. overhead signifi- cantly disproportionate size original dataset, around gb. mitigate inefficiency, propose indexing clip along z-axis employing binary search algorithm locate target clip necessary. adopting strategy, distribute decompression time across batch training data, resulting substantial savings disk stor- age. additionally, considering machine typically processes one batch data approximately one second, computational cost decompression becomes negligible. terms inference, baseline method iterates prompt box individually. however, boxes intersect along z-axis, base- line recalculates image features. given image encoder constitutes computationally intensive aspect sam, propose preprocess boxes boxes corresponding clips. approach ensures image embedding clip computed once, optimizing com- putational resources. addition, challenge limit docker al. running memory. experiments show litemedsam exceed memory limit number boxes approaches since maximum number boxes propose block partition algorithm along batch axis boxes. algorithm allows users specify maximum running batch size prevent exceeding memory limit. proposed method propose quantize baseline model litemedsam using qat. neu- ral networks consist various components beyond matrix multiplications, within operations peak computational complexity resides. therefore, nearly every qat method focuses quantizing inputs weights matrix multiplications, linear layers, convolution layers, attention blocks. contrast, operations involving biases, activation layers, normalization layers typically performed per element. quantiza- tion layers selective, proposed quantized model, opt retain layers floating-point, matrix multiplications image encoder mask decoder quantized. reason choose quantize prompt encoder lies fact parameter size times smaller two modules, indicated table common quantized sub-structures illustrated figure since quantization non-differentiable, employ straight-through es- timator ste methodology, demonstrated previous works ste, incoming gradients directly passed threshold operation become outgoing gradients. quantization node, propose -bit symmetric per-tensor signed integer activations quantizer learned floating-point scale factor. scale factor initialized runtime statistics. model inference post-processing upon completion quantization-aware training, brevitas provides exceptional toolchains exporting quantized models diverse backends. standard quantizelinear-dequantizelinear qcq representation quantization onnx exists, brevitas extended quantizelinear- clip-dequantizelinear qcdq. extension, researchers confine range quantized values. therefore, propose exporting quantized litemedsam onnx qcdq representation. numerous inference engines support onnx format, compatible qcdq. given challenge mandates cpu inference, narrow options onnx runtime openvino. experiment inference speed two inference engines detailed section based results, ultimately opt openvino. model caching also supported openvino. strategy reduce resulting delays appli- cation startup, making considerably suitable accelerating challenge qmedsam fig. common quantized sub-layers. quantized linear layer quantized convo- lutional layer quantized attention block. circles figure represent correspond- ing calculations stands matrix multiplication, stands convolution, stands transpose. operations involving quantization represented round rect- angles figure. inputs output sub-layers depicted figure floating-point tensors. al. sam generates mask provided image prompt. binarize floating-point values either crop padding, subse- quently resize low-resolution mask original dimensions input image. experiments dataset sampler employed challenge dataset training, evaluation dataset obtained partitioning ratio one-tenth. dataset comprises modalities, sizes prior partitioning training evaluation datasets summarized table evident issue arises significant imbalance sample numbers across modalities.to address imbalance prevent bias overfitting quantized model, well expedite training, propose randomly sampling clips modality epoch. additionally, samples undergo random horizontal vertical flips data augmentation. table samples modalities training dataset including additional datasets released post-challenge task. modalities counted num- ber clips z-axis. modalities pet samples modalities endoscopy x-ray dermoscopy samples modalities oct mammography fundus microscopy samples metrics loss functions accuracy model evaluated using dice similarity coefficient dsc normalized surface distance nsd, efficiency measured running time analysis. metrics collectively utilized compute ranking. training phase, mainly employ combination dice loss focal loss. decision based robustness demonstrated compound loss functions various medical image segmentation tasks, evidenced prior research training protocols training procedure includes three stages. qmedsam stage one, goal train quantized image encoder keeping floating-point prompt encoder mask decoder frozen. apart loss function mentioned section distill image encoder medsam introduce distillation loss. loss calculated product mean squared error intersection union ratio across image embeddings generated teacher student models. stage two, propose train quantized mask decoder concate- nating best-trained quantized image encoder stage one floating-point prompt encoder. final stage, whole model undergoes end-to-end fine-tuning fitting dataset. stage, propose employing linear learning rate warm-up epochs, commencing initial learning rate. additional training details summarized table warm-up period followed cosine anneal- ing scheduler epochs. minimum learning rate cosine annealing scheduler set initial learning rate, half-period cosine function determined quantization-aware training process completed, evaluate checkpoint epoch evalua- tion dataset select best-performing one. additional training details summarized table table training protocols. values separated vertical bars table correspond stages pre-trained model litemedsam baseline batch size ddp world size samples modality optimizer sgd momentum. total epochs initial learning rate warm-up epochs cosine annealing epochs training time hours environment settings development environments requirements presented table al. table development environments requirements. system ubuntu lts cpu intelr xeonr gold cpu.ghz ram gpu nvidia geforce rtx cuda version programming language python deep learning framework pytorch specific dependencies brevitas code results discussion inference speeds different engines challenge evaluates models intel xeon cpu ct.ghz, use intel core i-h cpu ct.ghz offers comparable performance identical environment. test variant single image prompt box. inference speeds various methods detailed table table inference speed different litemedsam variants. method inference time litemedsam inferenced pytorch litemedsam exported onnx inferenced onnx runtime litemedsam exported onnx inferenced openvino quantized litemedsam inferenced onnx runtime quantized litemedsam inferenced openvino results indicate quantized model exhibit fastest runtime. hardware optimized quantized op- erations, resulting slower execution compared standard floating-point op- erations. comparison purposes, inference speeds floating-point quantized versions medsam substantially larger litemed- sam provided table interestingly, case quantized model outperforms floating-point model. given comprehensive advantages quantization, evident de- ploying quantized litemedsam openvino inference engine effec- tively addresses requirement medical image segmentation laptop. quantitative results validation set table presents performance proposed three stages comparison baseline model public validation dataset. qmedsam table inference speed different medsam variants. method inference time medsam inferenced pytorch medsam exported onnx inferenced onnx runtime medsam exported onnx inferenced openvino quantized medsam inferenced onnx runtime quantized medsam inferenced openvino average, quantized model scores comparably dsc slightly higher nsd. highlight modalities significant differences accuracy. particular, quantized model degraded performance around us, shows gains approximately im- provement pet microscope. evident that, certain extent, proposed method effectively addressed performance imbalance baseline model across various modalities, caused datasets inherent imbalance. table quantitative evaluation results validation dataset. stage stage stage baseline dsc nsd dsc nsd dsc nsd dsc nsd pet x-ray dermoscopy endoscopy fundus microscope average comparison inference speeds specific cases baseline proposed method presented table results highlight notable acceleration achieved quantization method. qualitative results validation set two sets successful segmentation results depicted figure observed proposed quantized model performs better matching roi floating-point counterpart. figure illustrates two sets chal- lenging cases. cases, segmentation results proposed quantized model align closely ground truth roi compared baseline. however, since baseline prediction results significantly distant ground truth, correction unsuccessful. al. table quantitative efficiency terms inference running time seconds. mle stands memory limit exceeded. case size objects baseline proposed dboxct dboxct dboxct dboxmr dboxmr dboxmr dboxpet dboxus dboxx-ray dboxdermoscopy dboxendoscopy dboxfundus dboxmicroscope dboxmicroscope mle fig. good segmentation results. image box ground truth baseline proposed method. qmedsam fig. bad segmentation results. image box ground truth baseline proposed method. ablation study training segment anything model scratch requires huge mass data. however, proposed quantization-aware training procedure starts pre-trained model. reducing number samples modality, especially larger modalities, certainly benefits saving training time. however, still raises questions influence precision quan- tized model. section propose ablation study explore balance efficiency accuracy. describe variation samples different modalities clearly, use nsm represent number samples modality total samples modality denoted nmm, complete set modalities denoted strategy proposed method described nsm min nmi. ablation study introduces strategy enlarges nsm one-tenth nmm, particular, nsm max nmm min nmi metrics three stages ablation study summarized table compared table provide average metrics pro- posed method last row table results indicate increasing result significant improvement, underscoring efficiency proposed qat pipeline terms training time. al. table evaluation results ablation study validation dataset. stage stage stage dsc nsd dsc nsd dsc nsd pet x-ray dermoscopy endoscopy fundus microscope average proposed results final testing set testing results summarized table proposed quantized model exhibits marginal decrease much balance average accuracy. additionally, inference efficiency significantly optimized backbone. compared table observe models perfor- mance different modalities varies validation set testing set. however, trend balance across modalities remains consistent. table evaluation results test dataset. proposed baseline dsc nsd runtime dsc nsd runtime x-ray endoscopy fundus microscope oct pet average limitation future work experimental results shown significant decrease performance certain modalities larger amounts data, accuracy least accurate modalities still lags far behind average. hence accurate modality- balanced quantization expected. hand, floating-point model qmedsam runs faster openvino inference engine. explain bit above, beyond that, brevitas also provides excellent workflow export quantized model finn dataflow acceleration xilinx fpgas. quan- tized models promise faster energy-efficient inference customized hardware platform. conclusion paper, present efficient pipeline quantizing litemedsam deploying openvino inference engine. objective experiments conclusively shown method significantly accelerates baseline maintaining acceptable level accuracy. future endeavors focus en- hancing speed floating-point backbone, alleviating im- balance across different modalities, deploying quantized model cus- tomized hardware platforms. acknowledgements express gratitude data owners mak- ing medical images publicly available, codalab hosting challenge platform. disclosure interests. authors competing interests declare relevant content article. references hubara, i., nahshan, y., hanani, y., banner, r., soudry, accurate post train- ing quantization small calibration sets. international conference ma- chine learning. pp. pmlr jacob, b., kligys, s., chen, b., zhu, m., tang, m., howard, a.g., adam, h., kalenichenko, quantization training neural networks efficient integer- arithmetic-only inference. corr abs. abs. kirillov, a., mintun, e., ravi, n., mao, h., rolland, c., gustafson, l., xiao, t., whitehead, s., berg, a.c., lo, w.y., dollar, p., girshick, segment anything. proceedings international conference computer vision. pp. le, b.h., nguyen-vu, d.k., nguyen-mau, t.h., nguyen, h.d., tran, m.t. medficientsam robust medical segmentation model optimized inference pipeline limited clinical settings. submitted cvpr segment any- thing medical images laptop aafrki, review li, y., xu, s., zhang, b., cao, x., gao, p., guo, q-vit accurate fully quantized low-bit vision transformer. advances neural information processing systems al. liu, y., yang, h., dong, z., keutzer, k., du, l., zhang, noisyquant noisy bias-enhanced post-training activation quantization vision transformers. proceedings ieeecvf conference computer vision pattern recog- nition. pp. liu, z., cheng, k.t., huang, d., xing, e.p., shen, nonuniform-to-uniform quantization towards accurate quantization via generalized straight-through es- timation. proceedings ieeecvf conference computer vision pattern recognition. pp. liu, z., wang, y., han, k., zhang, w., ma, s., gao, post-training quantization vision transformer. advances neural information processing systems ma, j., chen, j., ng, m., huang, r., li, y., li, c., yang, x., martel, a.l. loss odyssey medical image segmentation. medical image analysis ma, j., he, y., li, f., han, l., you, c., wang, segment anything medical images. nature communications pappalardo, xilinxbrevitas. pfefferle, a.t., purucker, l., hutter, daft data-aware fine-tuning founda- tion models efficient effective medical image segmentation. submit- ted cvpr segment anything medical images laptop review shen, m., liang, f., gong, r., li, y., li, c., lin, c., yu, f., yan, j., ouyang, quantization-aware training high performance extremely low-bit ar- chitecture search. proceedings ieeecvf international conference computer vision. pp. xu, z., escalera, s., pavo, a., richard, m., tu, w.w., yao, q., zhao, h., guyon, codabench flexible, easy-to-use, reproducible meta-benchmark platform. patterns zhang, c., han, d., qiao, y., kim, j.u., bae, s.h., lee, s., hong, c.s. faster segment anything towards lightweight sam mobile applications. arxiv preprint zhang, j., zhou, y., saab, post-training quantization neural networks provable guarantees. corr abs. zhang, z., cai, h., han, efficientvit-sam accelerated segment anything model without performance loss. cvpr workshop efficient large vision models", "published_date": "2024-12-15T13:35:07+00:00"}
{"id": "2412.07247v1", "title": "Driving with InternVL: Oustanding Champion in the Track on Driving with Language of the Autonomous Grand Challenge at CVPR 2024", "authors": ["Jiahan Li", "Zhiqi Li", "Tong Lu"], "summary": "technical report describes methods employed driving language track cvpr autonomous grand challenge. utilized powerful open-source multimodal model, internvl-., conducted full-parameter fine-tuning competition dataset, drivelm-nuscenes. effectively handle multi-view images nuscenes seamlessly inherit internvls outstanding multimodal understanding capabilities, formatted concatenated multi-view images specific manner. ensured final model could meet specific requirements competition task leveraging internvls powerful image understanding capabilities. meanwhile, designed simple automatic annotation strategy converts center points objects drivelm-nuscenes corresponding bounding boxes. result, single model achieved score final leadboard.", "full_text": "driving internvl oustanding champion track driving language autonomous grand challenge cvpr jiahan li, zhiqi li, tong nanjing university december abstract technical report describes methods employed driving language track cvpr autonomous grand challenge. utilized powerful open-source multimodal model, internvl-., conducted full-parameter fine-tuning competition dataset, drivelm-nuscenes. effectively handle multi-view images nuscenes seamlessly inherit internvls outstanding multimodal understanding capabilities, formatted concatenated multi-view images specific manner. ensured final model could meet specific requirements competition task leveraging internvls powerful image understanding capabilities. meanwhile, designed simple automatic annotation strategy converts center points objects drivelm-nuscenes corresponding bounding boxes. result, single model achieved score final leadboard. introduction competition primarily aimed evaluate perception, prediction, planning capabilities multimodal models autonomous driving scenarios. specifically, drivelm src designed series diverse natural language questions based various autonomous driving scenarios, models scored based responses. different types questions evaluated using different scoring strategies. notably, competition placed greater emphasis perception capabilities multimodal models. model correctly perceives specific object eligible answer related questions. following sections, continue introduce competition dataset, methodologies, final results. dataset drivelm-nuscenes src, cbl consists question-answer pair training split. shown tab. show examples questions drivelm-nuscenes dataset. designed special format represent key objects, consisting object id, camera name, objects center coordinates, example c,cam back,.,.. chose change representation objects center point objects bounding box following two reasons representation capability center point strong bounding box, provide precise positional information object. multimodal models like internvl cww inherently possess perception capabilities perform grounding detection reference captioning using bounding boxes specific format. competition, used segment anything kmr model convert object center points object bounding boxes. specifically, used objects center point point prompt obtain multiple candidate masks point. observed largest mask cs.cv dec typically corresponds complete object need. therefore, consistently selected largest mask derived final bounding box coordinates mask. method works well cases. however, objects center point main body object, may produce incorrect bounding boxes. situation occur traffic light objects. tag question moving status object c,cam back,.,.? please select correct answer following options going ahead. stopped. back up. turn left. actions could ego vehicle take based c,cam back,.,.? take action whats probability? important objects current scene? objects considered future reasoning driving decision. object ego vehicle notice first ego vehicle getting next possible location? state object first noticed ego vehicle action ego vehicle take? object ego vehicle notice second ego vehicle getting next possible location? state object perceived ego vehicle second action ego vehicle take? object ego vehicle notice third? state object perceived ego vehicle third action ego vehicle take? table model selected internvl-. base model, shown fig. consists internlm-b language model, internvit, connector, extensively pre-trained multimodal data. handle high-resolution images, internvl employs dynamic high-resolution training approach effectively adapts varying resolutions aspect ratios input images. method leverages flexibility segmenting images tiles, enhancing models ability process detailed visual information accommodating diverse image resolutions. although internvl multi- image inference capabilities, trained default using single image. since sample nuscenes corresponds six images also extended temporally, performed concatenation operation multi-view images reduce number images internvl needs process. specifically, first added text image indicate orientation, cam front. resized image pixels. six images arranged single composite image grid. resizing ensures easier subsequent image segmentation preserves integrity individual image much possible. final concatenated image size shown figure complete image divided twelve sub-images, view corresponding two sub-images. additionally, entire image resized thumbnail processing. finally, image transformed image tokens vit-mlp pixel shuffle. time, also include layout descriptions system prompt system prompt autonomous driving assistant. receive image con- sists six surrounding camera views. layout follows first row contains three images front left, front, front right. second row contains three images back left, back, back right. task analyze images provide insights actions based visual data. important note since large language model predicts bounding box coordinates predicting next token, internvl normalizes box coordinates integers therefore, image concatenation, also process bounding box coordinates accordingly meet internvls requirements. figure overall architecture. figure concatenated image. finally, performed full-parameter fine-tuning internvl-. using gpus. train model learning rate one epoch. utilize deepspeed zero- strategy save memory batchsize temporal fusion also conducted preliminary explorations temporal expansion, using image previous keyframe. corresponding input prompt system system message user previous images image, current images im- agequestion assistant experiment experimental results shown table temporal version internvldrive-t errors due data format issues achieved lower score, requires exploration. best single model internvldrive-v achieves final score version trained subset training set, around full data. based sub-dataset, model actually achieves higher score except chatgpt score. emplying ensemble result, actually obtain much higher final score. method accuracychatgptbleu bleu bleu bleu rouge cider matchfinal score internvldrive-v internvldrive-v internvldrive-t table results drivelm dataset. references cbl holger caesar, varun bankiti, alex lang, sourabh vora, venice erin liong, qiang xu, anush krishnan, pan, giancarlo baldan, oscar beijbom. nuscenes multimodal dataset autonomous driving. cww zhe chen, jiannan wu, wenhai wang, weijie su, guo chen, sen xing, zhong muyan, qinglong zhang, xizhou zhu, lewei lu, al. internvl scaling vision foundation models aligning generic visual-linguistic tasks. arxiv preprint kmr alexander kirillov, eric mintun, nikhila ravi, hanzi mao, chloe rolland, laura gustafson, tete xiao, spencer whitehead, alexander berg, wan-yen lo, al. seg- ment anything. proceedings ieeecvf international conference computer vision, pages src chonghao sima, katrin renz, kashyap chitta, chen, hanxue zhang, chengen xie, ping luo, andreas geiger, hongyang li. drivelm driving graph visual question answering. arxiv preprint", "published_date": "2024-12-10T07:13:39+00:00"}
{"id": "2412.01383v2", "title": "Second FRCSyn-onGoing: Winning Solutions and Post-Challenge Analysis to Improve Face Recognition with Synthetic Data", "authors": ["Ivan DeAndres-Tame", "Ruben Tolosana", "Pietro Melzi", "Ruben Vera-Rodriguez", "Minchul Kim", "Christian Rathgeb", "Xiaoming Liu", "Luis F. Gomez", "Aythami Morales", "Julian Fierrez", "Javier Ortega-Garcia", "Zhizhou Zhong", "Yuge Huang", "Yuxi Mi", "Shouhong Ding", "Shuigeng Zhou", "Shuai He", "Lingzhi Fu", "Heng Cong", "Rongyu Zhang", "Zhihong Xiao", "Evgeny Smirnov", "Anton Pimenov", "Aleksei Grigorev", "Denis Timoshenko", "Kaleb Mesfin Asfaw", "Cheng Yaw Low", "Hao Liu", "Chuyi Wang", "Qing Zuo", "Zhixiang He", "Hatef Otroshi Shahreza", "Anjith George", "Alexander Unnervik", "Parsa Rahimi", "S\u00e9bastien Marcel", "Pedro C. Neto", "Marco Huber", "Jan Niklas Kolf", "Naser Damer", "Fadi Boutros", "Jaime S. Cardoso", "Ana F. Sequeira", "Andrea Atzori", "Gianni Fenu", "Mirko Marras", "Vitomir \u0160truc", "Jiang Yu", "Zhangjie Li", "Jichun Li", "Weisong Zhao", "Zhen Lei", "Xiangyu Zhu", "Xiao-Yu Zhang", "Bernardo Biesseck", "Pedro Vidal", "Luiz Coelho", "Roger Granada", "David Menotti"], "summary": "synthetic data gaining increasing popularity face recognition technologies, mainly due privacy concerns challenges associated obtaining real data, including diverse scenarios, quality, demographic groups, among others. also offers advantages real data, large amount data generated ability customize adapt specific problem-solving needs. effectively use data, face recognition models also specifically designed exploit synthetic data fullest potential. order promote proposal novel generative methods synthetic data, investigate application synthetic data better train face recognition systems, introduce frcsyn-ongoing challenge, based face recognition challenge era synthetic data frcsyn, originally launched cvpr ongoing challenge provides researchers accessible platform benchmark proposal novel generative methods synthetic data, novel face recognition systems specifically proposed take advantage synthetic data. focus exploring use synthetic data individually combination real data solve current challenges face recognition demographic bias, domain adaptation, performance constraints demanding situations, age disparities training testing, changes pose, occlusions. interesting findings obtained second edition, including direct comparison first one, synthetic databases restricted dcface gandiffface.", "full_text": "second frcsyn-ongoing winning solutions post-challenge analysis improve face recognition synthetic data ivan deandres-tame, ruben tolosana, pietro melzi, ruben vera-rodriguez, minchul kim, christian rathgeb, xiaoming liu, luis gomez, aythami morales, julian fierrez, javier ortega-garcia, zhizhou zhong, yuge huang, yuxi mi, shouhong ding, shuigeng zhou, shuai he, lingzhi fu, heng cong, rongyu zhang, zhihong xiao, evgeny smirnov, anton pimenov, aleksei grigorev, denis timoshenko, kaleb mesfin asfaw, cheng yaw low, hao liu, chuyi wang, qing zuo, zhixiang he, hatef otroshi shahreza, anjith george, alexander unnervik, parsa rahimi, sebastien marcel, pedro neto, marco huber, jan niklas kolf, naser damer, fadi boutros, jaime cardoso, ana sequeira, andrea atzori, gianni fenu, mirko marras, vitomir struc, jiang yu, zhangjie li, jichun li, weisong zhao, zhen lei, xiangyu zhu, xiao-yu zhang, bernardo biesseck, pedro vidal, luiz coelho, roger granada, david menotti abstractsynthetic data gaining increasing pop- ularity face recognition technologies, mainly due privacy concerns challenges associated obtaining real data, including diverse scenarios, quality, demographic groups, among others. also offers advantages real data, large amount data generated ability customize adapt specific problem- solving needs. effectively use data, face recognition models also specifically designed exploit synthetic data fullest potential. order promote proposal novel generative methods synthetic data, investigate application synthetic data better train face recognition systems, introduce frcsyn- ongoing challenge, based face recognition challenge era synthetic data frcsyn, originally launched cvpr ongoing challenge provides researchers accessible platform benchmark proposal novel gen- frcsyn challenge organizers ruben tolosana, ivan deandres-tame, pietro melzi, ruben vera-rodriguez, minchul kim. christian rathgeb, xiaoming liu, aythami morales, julian fierrez javier ortega-garcia. information related author included end article. link frcsyn challenge erative methods synthetic data, novel face recognition systems specifically proposed take advantage synthetic data. focus exploring use synthetic data individually combination real data solve current challenges face recognition demographic bias, domain adaptation, performance constraints demanding situations, age disparities be- tween training testing, changes pose, occlusions. interesting findings obtained second edition, including direct comparison first one, synthetic databases restricted dcface gandiffface. index termsfrcsyn, face recognition, synthetic data, generative ai, demographic bias, benchmark, privacy introduction face biometrics popular area within computer vision pattern recognition, finding applications across various domains person recognition healthcare learning among others. recent years, fast development deep learning, significant cs.cv mar gandiface dcface idi-face fig. examples synthetic identities variations different demographic groups using gandiffface left, dcface middle idiff-face right. advances made areas like face recogni- tion surpassing previous benchmarks. however, technology still faces challenges several research directions, including explainability demographic bias privacy robustness adverse conditions aging pose variations illu- mination changes occlusions synthetic data gained popularity good solution mitigate drawbacks allowing generation large number facial images different non-existent identities, variability terms demographic attributes scenario conditions. context, re- fer demographics whole societies smaller groups defined criteria ethnicity, sex, age several generative approaches presented last couple years synthesis face images, considering state-of- the-art deep learning methods. popular approaches generate synthetic facial images generative adversarial networks gans dif- fusion models combination however, less common approaches rely models still achieve high level realism image generation examples synthetic face images generated using methods shown figure beyond generation synthetic faces, another critical aspect lies understanding potential ap- plications benefits synthetic data enhanc- ing technology. recent studies highlighted performance gap systems trained synthetic data trained real data nevertheless, results achieved edition face recognition challenge era synthetic data frcsyn empha- size relevance synthetic data, either alone merged real data, mitigating challenges fr, demographic bias notably, frcsyn-ongoing, synthetic data dcface gandiffface methods allowed training systems. additionally, together novel generative methods, improving technology involves refining design training processes address domain gaps real synthetic data certain scenarios. in- stance, observations frcsyn-ongoing revealed teams considered similar deep learning architectures e.g., resnet- loss functions e.g., adaface commonly used systems trained real data. moreover, use synthetic facial data limited fr. emerging popularity foundational models synthetic facial data also leveraged provide large models general understanding human face looks like, serving pretraining many tasks additionally, fact data labeled generated allows creation novel datasets various tasks attribute detection, facial expression recognition, more. although field synthetic faces still many public datasets lines research, several studies already highlighted benefits synthetic data biometric tasks facial expression recognition signature verification action recognition pose estimation order promote development novel face generative methods creation syn- thetic face databases, well investigate application synthetic data better train sys- tems, organized frcsyn-ongoing challenge, based frcsyn challenge part cvpr edition, introduce new sub-tasks allowing participants train systems using synthetic data generated preferred generative frame- works, offering flexibility compared edition additionally, new sub-tasks varied experimental settings included explore systems trained constrained unconstrained scenarios regarding amount synthetic training data. frcsyn challenge aims address following research questions limitations technology trained synthetic data? synthetic data help alleviate current lim- itations technology? questions become increasingly rele- vant discontinuation popular real databases due privacy concerns intro- duction new regulatory laws. foundation present article estab- lished earlier publication current version notably extending ex- tensive description analysis top synthetic face generation methods systems presented far frcsyn-ongoing, including key march, march, graphical representations proposed systems improve understanding reader, incorporating additional metrics evaluation proposed systems order analyze different operational scenarios, iii presenting in-depth analysis performance achieved various demographic groups databases used evaluation, together novel figures tables, direct comparison results obtained edition ones obtained edition highlighting interesting findings. remainder article organized fol- lows. section describes databases considered frcsyn-ongoing. section iii explains experimental setup challenge, including different tasks sub-tasks, experimental protocol, metrics, restrictions. section iv, describe approaches proposed top- participating teams. section presents best results achieved far different tasks sub-tasks frcsyn-ongoing, emphasizing key results challenge. finally, section vi, provide conclusions, highlighting potential future research directions field. ii. second frcsyn-ongoing databases synthetic databases one main novelties frcsyn- ongoing restrictions terms generative methods used create synthetic data. unlike frcsyn-ongoing, synthetic data created using dcface gan- diffface available, edition allow participants use generative framework choice create synthetic data, limiting sub-tasks number synthetic face images used train systems details section iii-a. reference, registration challenge, provide participants list possible state-of-the-art generative frameworks. completeness, summarize next table popular approaches available beginning challenge table description possible generative methods synthetic databases used participants frcsyn-ongoing. identities, img images database framework imgid img dcface diffusion model gandiffface gan diffusion model idiff-face uniform diffusion model idiff-face two-stage diffusion model digiface-m d-model idpm diffusion model sface gan synface gan iti-gen clip dcface framework entirely based diffusion models, composed sampling stage generation synthetic identities xid, mixing stage generation images xid,sty identities xid sampling stage style selected style bank images xsty. gandiffface framework com- bines stylegan diffusion model, i.e., dreambooth generate fully syn- thetic databases desired properties human face realism, controllable de- mographic distributions, realistic intra- class variations e.g., changes pose, expres- sion, occlusions. graphical examples shown figure idiff-face framework uses dif- fusion model conditioned identity context, allows model either generate variations existing authentic images us- ing authentic embeddings generate novel synthetic identities using synthetic face em- beddings. authors presented two distinct datasets one generating identity context two-stage process, synthetic uniform representation. digiface-m framework gen- erate large-scale synthetic face images many unique subjects based paramet- ric model rendering. considers method introduced wood al. tackling ethical labeling problems associated generation synthetic data. idpm framework considers dif- fusion model perform inversion model generating new images gaus- sian noise various backgrounds, lighting, poses, expressions preserving identity. sface framework uses condi- tional gan synthetically generate face im- accurate-face-recognition-using-synthetic-data table description real databases considered evaluation frcsyn-ongoing. identities, img images database framework imgid img casia-webface real bupt-balancedface real agedb real cfp-fp real rof real ages adaptive discriminator augmen- tation increase diversity training database. synface framework uses disco- facegan generate face images different identities mixup face gener- ator. iti-gen framework uses clip generate embeddings translate visual attribute differences natural language differences perform text-to- image generation inclusive. possible generative frame- works, corresponding synthetic databases available, used participants. but, indicated before, purpose frcsyn- ongoing promote proposal novel gen- erative methods creation better syn- thetic databases improve performance systems. important mention frcsyn-ongoing, synthetic data exclusively used training stage technology, repli- cating realistic operational scenarios. real databases training systems participants allowed use casia-webface real data depending sub-task, please see section iii-a. database contains face images real identities collected web. final evaluation proposed systems, consider four real databases used frcsyn challenge consider key challenges demographic bias, pose variations, aging, occlusions. summarize next table bupt-balancedface designed ad- dress performance disparities across different ethnic groups. relabel according fairface classifier provides labels ethnicity white, black, asian, indian gender male, female. consider eight demographic groups obtained possible combinations four ethnic groups genders. aware groups comprehensively represent entire spectrum real world ethnic diversity. never- theless, selection categories, imperfect, primarily driven need align demographic categorizations used bupt-balancedface facilitate easier consistent evaluation. agedb contains facial images featuring subjects different ages different environmental contexts. cfp-fp presents facial images sub- jects great changes pose, frontal profile images, different en- vironmental contexts. rof consists occluded faces upper face occlusion, due sunglasses, lower face occlusion, due masks. finally, important highlight that, differ- ent databases considered training eval- uation, also intend analyze generalization ability proposed systems. table iii tasks sub-tasks frcsyn-ongoing respective metrics databases. trade-off. gap gap real. avg average verification accuracy. standard deviation. flops floating point operations per second. syn accuracy proposed model. real accuracy baseline model. identity. task synthetic data demographic bias mitigation baseline training casia-webface ranking trade-off, see section iii-c details. metrics gap real nsy sub-task constrained training exclusively synthetic data train maximum face images e.g., ids images per id. eval bupt-balancedface sub-task unconstrained training exclusively synthetic data train restrictions terms number face images. eval bupt-balancedface. sub-task constrained training real synthetic data train casia-webface, maximum face synthetic images. eval bupt-balancedface. task synthetic data overall performance improvement baseline training casia-webface. ranking average accuracy, see section iii-c details. metrics gap real nsy sub-task constrained training synthetic data train maximum face images. eval bupt-balancedface, agedb cfp-fp rof sub-task unconstrained training synthetic data train restrictions terms number face images. eval bupt-balancedface, agedb, cfp-fp, rof. sub-task constrained training real synthetic data train casia-webface, maximum face synthetic images. eval bupt-balancedface, agedb, cfp-fp, rof. restrictions flops gflops specified databases used training. generative models cannot used generate supplementary data. iii. second frcsyn-ongoing setup due success frcsyn- ongoing also decided run edition codalab, open-source framework designed conducting scientific competitions benchmarks. platform, participants find competitions requirements limitations submit scores automatically obtain evaluation metrics system, position challenge leaderboard. table iii provides overview key aspects experimental protocol, metrics restrictions sub-task. detailed explanations found respective subsections. tasks similar frcsyn-ongoing edition also explore application synthetic data training systems, specific focus addressing two critical aspects current technology mitigating demographic bias, enhancing overall performance challenging conditions include variations age pose, presence occlusions, diverse demographic groups. investigate two areas, consider two different tasks, comprising three sub-tasks. sub-task considers different types realsynthetic amounts data train- ing systems. consequently, edition comprises different sub-tasks. task first task focuses using synthetic data mitigate demographic biases within systems. evaluate performance systems, create sets mated non- mated comparisons using subjects bupt- balancedface database consider eight demographic groups defined section ii-b, result combination four ethnicities white, black, asian, indian two genders male female, ensuring balanced represen- tation across groups comparison lists. non-mated comparisons, exclusively pair subjects within demographic group, hold greater relevance compared non-mated comparisons involving subjects different de- mographic groups. task second proposed task focuses using synthetic data enhance overall perfor- mance systems challenging conditions. assess effectiveness proposed systems, use lists mated non-mated comparisons selected subjects different evaluation databases, one designed address specific challenges fr. specifically, bupt-balancedface used consider diverse demographic groups, whereas agedb, cfp-fp, rof assess age, pose, occlusion challenges respectively. experimental protocol training six sub-tasks introduced frcsyn-ongoing mutually independent. implies participants flexibility participate number sub-tasks based preferences. selected sub-task, participants required develop system train twice using authorized real databases exclusively, i.e., casia-webface following specific requirements chosen sub-task, summarized table iii. ac- cording protocol, participants must provide baseline system proposed system specific sub-task. baseline system plays critical role evaluating impact synthetic data training serves reference point comparing proposed model conventional practice training real databases. maintain consistency, baseline system, trained exclusively real data, proposed system, trained according specifications selected sub-task, must architecture training protocol. evaluation sub-task, participants re- ceived comparison files comprising mated non-mated comparisons, used evaluate performance proposed systems. task involves single comparison file containing balanced comparisons different demo- graphic groups bupt database, task comprises four comparison files, cor- responding every specific real-world databases considered i.e., bupt, agedb cfp-fp rof evaluation sub- task, participants required submit two files per database codalab platform scores baseline system, scores proposed system. finally, sub-task, participants must submit file including deci- sion threshold system i.e., baseline proposed. submitted scores must fall within range lower scores indicating non- mated comparisons, vice versa. evaluation metrics evaluate systems using protocol based lists mated non-mated comparisons sub-task database. scores thresholds provided participants, calculate binary decision verification accuracy. addi- tionally, calculate gap real gap follows gap real syn syn, real representing verification accuracy baseline system syn verification accuracy proposed system, trained synthetic real synthetic data. metrics false non-match rate fnmr fixed operational point, area roc curve popular analysis systems real-world applications, also computed scores provided participants. next, explain participants ranked different tasks. task rank participants determine winners sub-tasks closely examine trade-off average avg standard deviation verification ac- curacy across eight demographic groups defined section ii-b. define trade-off metric follows avg sd. metric involves plotting average accuracy x-axis standard deviation y-axis space. multiple -degree parallel lines drawn identify winning team, whose performance located far right lines. proposed metric, reward systems achieve good levels performance fairness simultane- ously, unlike common benchmarks based recognition performance. standard deviation verification accuracy across demographic groups common metric assessing bias reported work addressing demographic bias mitigation. task rank participants establish winners sub-tasks examine average verification accuracy four different databases designated evaluation, described section ii. approach enables assess four main challenges technologies representation diverse demographic groups, impact aging recognition, iii variations facial pose, challenges made occlusions. evaluation provides comprehensive overview systems real operational scenarios. restrictions participants freedom choose system task long number float- ing point operations per second flops system exceed gflops. threshold established facilitate exploration innovative architectures encourage use diverse models preventing dominance excessively large models. participants also free use preferred training modality, requirement specified databases used training. generative models cannot used generate supplementary data. participants allowed use non-face databases pre-training purposes use traditional data augmentation techniques using authorized training databases. maintain integrity evaluation process, organizers reserve right disqualify par- ticipants anomalous results detected participants fail adhere challenges rules. iv. second frcsyn-ongoing systems description frcsyn-ongoing, encourage par- ticipants propose novel generative methods creation synthetic data. besides, also give participants freedom choose architecture training methods. table summarizes team key information terms proposed synthetic data system. table serves quick reference, detailed explanations teams approach methodology found corresponding subsections. teams arranged average ranking sub-tasks frcsyn-ongoing. general, see teams decided use synthetic data dc- face idiff-face databases, improving also original data cleaning selection approaches, among sophisticated tech- niques. also, regarding technologies, based resnet iresnet architectures, adaface arcface main used losses. however, teams proposed methods generate synthetic facial images, well train models. next, describe specific details top- proposed systems frcsyn-ongoing. table description key information terms proposed synthetic data system. ldm latent diffusion model, ddpm diffusion probabilistic model, ddim denoising diffusion implicit models, hdt hourglass diffusion transformer team country sub task fig. synthetic database synthetic data team improvements model training method admis china novel used ldm create faces. ldm uses embeddings context. contexts generated ddpm used cosine similarity ensure quality ddim accelerate sampling. finally oversampled enhance consistency. appliyed dropout dimensions feature embeddings. loss arcface backbone iresnet- opdai china dcface generated new facesid photomaker replaced randomly dcface. used different heads different databases calculated final loss average. loss adaface backbone iresnet- usa novel used hdt model generate synthetic images. trained identity embeddings style embeddings processed vqvae also used stylenat generate variability images generated hdt. trained models, one color, geometric augmentations, facemix-b second horizontal flipping augmentation. final score obtained combining outputs. loss uniface backbone iresnet- k-ibs-ds korea dcface modified adaface following slackedface changing initialization replacing l-norm p-norm. used different backbones final score combination them. loss slackedface backbone iresnet- squeeze-and-excitation blocks ctai china dcface gandiff cleaned data using iresnet squeeze-and-excitation blocks. also used dbscan segregate intra-class noise removed ids far class center. used two models trained different losses occlusion augmentation. final score obtained combining outputs. loss adaface cosface backbone iresnet- idiap-synthdistill switzerland novel used end-to-end method generates synthetic images training model. base data generated stylegan model training, dynamically generated images based training loss. used pretrained model train model distillation new model using synthetic data generated. loss synthdistill backbone iresnet- inesc-igd portugal germany dcface idiff labeled images public databases ethnicity balanced final data. trained two models using loss applying randaug occlusion augmentation. final score obtained combining outputs models. loss elasticcosfac-plus backbone resnet- unica-igd-lsi italy germany slovenia dcface used public data dcface generated data idiff-face framework exfacegan took similarity mean real synthetic samples added loss value. loss cosface backbone resnet- srcn aivl china dcface idiff balanced dcface ethnicity trained idiff-face balanced well. trained two models inference, ingested one original image flipped image. merged two resulting vectors used resulting vector get distance scores. loss adaface backbone resnet- cbsr-samsung china dcface de-overlaped dcface casia. created validation dataset using subset dcface, adding masks sunglasses positioning vertical bars images. loss adaface backbone resnet- bovifocr-ufpr brazil dcface used randaugment, random erasing random flip augmentation techniques. loss arcface backbone resnet- gaussian distribution unconditional context image embedding context synthetic images conditionallatent idi-face based sample noise sample noise sample crop extract embedding context filter intra -class similarity pretrained sample gather crop iresnet- arcface fig. framework proposed admis team. admis sub-tasks team comprises members fudan university tencent youtu lab, china. used latent diffusion model ldm based idiff-face synthesize faces. idiff-face uses noise embedding sampled gaussian distribution serve ldms, admis team uses identity embeddings contexts, extracted faces pretrained elasticface model iresnet- backbone. trained ldm casia- webface database. ldm takes embeddings context, considered uncon- ditional denoising diffusion probabilistic model ddpm trained ffhq database context generator. specifically, used ddpm generate faces arbitrary identities, known context faces. exploited elasticface model extract embeddings context faces. encourage quality distinctiveness identity later ldm- generated faces, filtered embeddings setting minimum cosine similarity threshold arbitrary pairs embeddings. yields embeddings discriminative iden- tities. furthermore, accelerated sampling process ldm denoising diffusion im- plicit models ddim training model, generated images context. adopted oversampling strategy dcface performed five times enhance consistency. result, contexts used sub-tasks sub-tasks sub- tasks expanded sub-tasks casia-webface database. baseline proposed models used iresnet- architectures. applied arcface loss batch size initial learning rate epochs. learning rate divided epochs also used random cropping augmentation training. proposed architecture described figure code frcsyn admis opdai sub-tasks team comprises members interactive entertainment group netease inc., china. initially used dcface database, generating dcface casia-webface data generation partial training pose occlusion adaface head adaface head iresnet photo maker final loss head loss head loss fig. framework proposed opdai team. face images large pose variations occlusions using photomaker given input images, photomaker generate diverse personalized photos based text prompt preserving identity information input image. randomly replaced images original dcface data ensure total number samples meets requirement photomaker inference, adopted batch size used random prompts including age, pose, image quality ensure diversity generated samples. sub-tasks combined data version dcface, sub-tasks merged casia-webface sub- tasks merge denoise samples different databases, following partial approach consists sparse variant model parallel architecture training models. regarding model, obtained loss different databases indepen- dent adaface heads, calculating final loss average multiple heads. baseline proposed models based iresnet- ar- chitectures, horizontal flipping. proposed architecture described figure code cvpr.git sub-tasks team comprises members inc, usa. generate synthetic data, used two models trained webfacem first model based hourglass diffusion transformers hdt combines scalability transformer ar- chitectures efficiency convolutional nets trained focusing conditional flow matching following classifier- free guidance approach identity embeddings used directly, whereas style embeddings processed vector quantised-variational autoencoder vqvae specifically, head position, embeddings allocated vq- vae processing, age facial expression attributes represented embeddings each. inference, combinations embed- dings randomly selected. second model used generate synthetic data based style- nat enhanced model. network trained using auxiliary sources supervision pre-trained network prototype mem- ory identity supervision pre-trained face attribute classification network style su- pervision. create synthetic data, used classifier weights trained prototype memory get identity embeddings, input image prototype memory hourglass diusion transformer identity vector stylenat identity style synthetic dataset original mirrored combine outputs final score model model data generation model fig. framework proposed team. randomly selected uniformly sampled clusters obtained using means, get demographic diversity. iden- tity, generated face images using two generative models. first stage, identity embeddings used hdt model get images identity. images included training dataset. identity style embeddings taken remaining images used condition generate different images stylenat model. images also put training dataset. regarding model, trained iresnet- uniface loss epochs. one network trained color, geometric augmentations, facemix-b one using random horizontal flipping. two networks combined ensemble, first one received original image, second one mirrored copy. used model sub-tasks sub- tasks combined synthetic data casia-webface data, training two models, one mixed data, casia-webface. proposed architecture described figure k-ibs-ds sub-tasks team com- prises members korea advanced institute science technology institute basic science, south korea. used dcface face images depending sub-task. regarding model, used several iresnet models layers squeeze-and-excitation blocks architectural components designed enhance representational power convolutional neural networks dynamically adjusting channel- wise features. inspired slackedface made two modifications enhance adaface classifier used renormalized uniform initialization reliable weight initializa- tion uniformity across identity prototypes unit sphere replaced l-norm powered-norm p-norm, face recognizability index integrates l-norm learned embedding proximity. training stage line including optimizer, learning rate, etc. sub-tasks first subjects casia- webface assigned training, remaining ones performance validation using random pairs challenging conditions identified based poorest l-norm values dropout fully connected batch normalization p-norm powered norm index data augmentation realistic synthetic face examples aggressive huber loss embedding mlp do-fc-bn regression mlp do-fc-bn flattened feature vectors iresnetse feature maps cross-entropy loss face embedding emb. adaface classifer estimated emb. proximity learned emb. proximity p-norm l-emb. norm adaface proposed initiallization adaface original initialization p-norm l-norm best recognizability p-norm l-norm poorest recognizability fig. framework proposed k-ibs-ds team. final score obtained aggregating compar- ison scores different iresnet models, along horizontally flipped instances score fusion. training test sets realigned using retinaface followed similarity transformation. sub-tasks, aggressive data augmentations applied, including random hor- izontal flipping, photometric operations, cropping, resizing, addition sunglasses masks. proposed architecture described figure code frcsyn ctai sub-tasks team comprises members china telecom ai, china. an- alyzing popular synthetic data, found intra-class inter-class noise widely present. data cleaning effectively remove bad ex- amples synthetic data retain important im- ages large amount synthetic data. order select optimal synthetic data, first trained iresnet- model squeeze-and-excitation blocks using casia- webface extract features synthetic im- ages dcface gandiffface digiface-m subsequently, used db- scan clustering method segregate intra-class noise removed ids class center feature cosine similarity greater finally, used cleaned synthetic data merged casia- gaussian distribution stylegan generator mapping network loss stylegan generator small loss similar images diverse images high loss data generation model iresnet cosface loss fig. framework proposed idiap-synthdistill team. webface finetune iresnet- second data refinement. final refined synthetic dataset, sampled face images retaining many ids possible build synthetic training set. regarding model, particular sub-task achieved highest position among sub-tasks, trained iresnet- adaface loss cosface loss mask occlusion augmentation casia-webface refined synthetic data. used ensemble model trained synthetic data. furthermore, data augmentation considered enhance features. code idiap-synthdistill sub-tasks team comprises members idiap re- search institute, epfl, universite lausanne, switzerland. proposed method based synthdistill end-to-end approach, generating synthetic images training model training loop. instead using pre-trained model separate step, directly used training loop supervision, new student model trained fully using syn- thetic data generated stylegan model generating synthetic images, trained style- gan casia-webface database dynamically generated synthetic images training based training loss. dynamic image generation, used training loss every iteration feedback find difficult synthetic image batch re-sampled new batch synthetic images intermediate latent space stylegan near latent vector difficult sample. loss value high, re-sampled relatively small standard deviation around diffi- cult sample generated similar images, loss value small re-sampled higher standard deviation, generating images variations. throughout process, generated images resized fed models. used pre-trained model iresnet- architecture trained cosface loss subset web- facem database trained new model student network architecture using synthetic data dynamic synthetic image generation approach. used adam optimizer initial learning rate trained student model loss function thresholding, subset dcface used determine optimal threshold maximizing verification accuracy, using -fold cross-validation approach based random se- lection identities comparison pairs. proposed architecture described figure code synthdistill indian asian white black ethnicity classier idi-face dataset dcface dataset ethnicity labelled dataset sampled dataset average score resnet- resnet- elasticcosfac loss dataset identities images casia-webface dataset model data generation elasticcosfac loss fig. architecture proposed inesc-igd team. inesc-igd sub-tasks team com- prises members inesc tec universidade porto, portugal, fraunhofer igd, germany. training dataset, merged dcface idiff-face uniform, idiff-face two-stage databases labeled data ethnicity labels using similar approach sub- tasks created synthetic training dataset containing face images sampling balanced identities, terms ethnicity labels. sub-tasks created synthetic training dataset containing face images sampling identities training datasets. sub-tasks two instances resnet- trained, one casia-webface subset synthetic datasets e.g., images identities. sub-tasks, trained resnet- elasticcosfac-plus loss using settings presented testing phase sub-tasks feature embeddings obtained trained models weighted sum score-level fusion used. training sub-tasks, training datasets augmented using randaug occluded augmentation probabilities occluded augmentation followed proto- col proposed leading occlusions eyes, lower face, upper face, combination eyes occlusion others. occluded augmen- tations boosted performance, synthetic data lower frequency natural occlusions beard makeup proposed architecture described figure code recognition unica-igd-lsi sub-tasks team comprises members fraunhofer igd, ger- many, university cagliari, italy, univer- sity ljubljana, slovenia. used dc- face synthetic database led remarkable performance gains well-known evaluation benchmarks face verification, combined real data also, considered synthetic data generated idiff-face exface- gan sub-tasks exfacegan data generated using iden- tity disentanglement approach pretrained gan- control regarding model, trained resnet- network using cosface loss margin penalty scale term resnet- real embeddings classication layer casia-webface dcface synthetic randaugment absolute angular dierence real synthetic samples idi-face exfacegan synthetic synthetic cosface loss final loss fig. framework proposed unica-igd-lsi team. similarity mean difference real- synthetic-only samples real synt scaled added loss value. trained model epochs batch size initial learning rate divided epochs. training phase, synthetic samples aug- mented using randaugment operations magnitude following sub- tasks selected synthetic dataset combined casia-webface obtaining total images identities. proposed architecture described figure code srcn aivl sub-task team com- prises members samsung electronics china centre, university science technol- ogy, iie, cas, mais, casia, china. selected samples dcface database labeled ethnicity subject, racial distribution gap may lead bad performance testing. based approach, trained idiff-face casia-webface database generating synthetic face images specific races. regarding system, used two custom resnet- trained adaface loss function. models trained epochs initial learning rate batch size adjusted predefined milestones. training data un- derwent preprocessing, including padding crop augmentation, low-resolution augmentation, photometric augmentation, random grayscale, normalization. threshold determined -fold optimal threshold validation set. inference, data preprocessing involved mtcnn alignment resizing data cropping alignment, fed image flipped image two models. finally, obtaining two feature embeddings, combined performed similarity calculation embeddings. face diusion sample generation sample selection stage face recognition data generation model idi-face flip crop crop feature feature feature feature final feature iresnet iresnet data augmentation fig. framework proposed srcn aivl team. model real data synthetic data intra-class cleansing de-overlap data generation mask sunglasses raw adaface head adaface head iresnet iresnet occluded occluded final score head score head score fig. framework proposed cbsr-samsung team. proposed architecture described figure code cbsr-samsung sub-tasks team comprises members samsung elec- tronics china centre, iie, cas, mais, casia, china. first trained model using casia-webface then, used de- overlap dcface casia, dcface trained using real database. synthetic dataset, compared performance models trained three synthetic databases, including gandiffface dcface, idiff-face finally selected dcface synthetic training set. created validation dataset in- cluding three subsets three different testing sce- narios random sample pairs dcface, simu- lating age variability demographic groups agedb bupt-balanced databases, respectively randomly positioned vertical bar masks images simulate self-occlusion due considered cfp-fp database iii add mask sunglasses images detecting landmarks via facex-zoo simulating rof database done following validation subsets consist positive pairs negative pairs. finally, concatenated subsets validation set. subsequently, conducted intra-class cluster- ing datasets using dbscan threshold removed samples separated class center. regarding model, merged refined datasets trained iresnet- adaface loss addition, adopted two augmentation strategies, i.e., photo- metric augmentation rescaling. that, trained two models using occlusion augmenta- tion probability, respectively. finally, submitted average similarity score two models. proposed architecture shown figure bovifocr-ufpr sub-tasks team comprises members federal uni- data aug flip erase data generation model resnet arcface loss fig. framework proposed bovifocr-ufpr team. versity parana, federal institute mato grosso, unico idtech, brazil. chose dcface synthetic database randomly removed identities images per reduce number follow rules. model, used resnet- backbone, trained arcface loss function. images used training augmented using random flip probability also applied random erasing randaugment additional augmentations. validate model subsampled images dcface, generating genuine impostor pairs, used pairs select best threshold classify proposed model output scores. proposed architecture described figure code second frcsyn-ongoing results next, describe sections v-a v-b main results achieved tasks respectively. results analyzed section v-c focusing specific demographic groups indi- vidual databases. finally, discuss section v-d common trends among different teams compare results obtained edition. present table ranking key results frcsyn-ongoing. task bias mitigation table shows results achieved partic- ipants task focused demographic bias mitigation. teams ranked descending order to, tends correlate ascending order i.e., less biased systems. notably, winner sub-tasks to, demonstrates significant negative gap value -., show- ing higher performance training system synthetic data compared real data i.e., casia-webface furthermore, sub- task observe teams negative gap values considered diffusion models generation synthetic data i.e., uses hdt, srcn aivl combines dcface idiff- face, ctai combines dcface gandiff- face, showing generation method may work better real data scenarios limited data. next, removing limitation number synthetic images i.e., sub-task value systems increases, leads performance fairness improvement simultaneously. instance, admis team ranked top- sub-task value increases sub-task i.e., improvement compared sub-task table results teams ranked among top- least one sub-task, ordered average rank sub-tasks. team, report ranking metric position across sub-tasks. best result sub-task highlighted bold. mark team participate sub-task. trade-off, avg average accuracy task bias mitigation task overall improvement team task task task task avg task avg task avg admis opdai k-ibs-ds ctai idiap-synthdistill inesc-igd unica-igd-lsi srcn aivl cbsr-samsung bovifocr-ufpr also, gap value decreases obtaining better results increasing amount synthetic data comparison limited real data i.e., casia-webface. another example opdai team, raised top- top- positions sub-task value increases i.e., improvement sub-tasks gap value reduced findings emphasize potential generating large number synthetic face images different demographic groups mitigate bias existing technology. finally, analyze sub-task case using both, real synthetic data, training process. general, observe considerable improvements terms values, along higher negative gap values top- teams, e.g., admis to, gap, k-ibs-ds to, gap, unica-igd-lsi to, gap. moreover, table vii shows performance demographic group baseline models, i.e., trained real data, models proposed top-ranked teams sub-tasks task team achieves higher performance across demographic groups except white males females vs. avg, vs. avg, respectively. believe produced due class imbalance real data demo- graphic group, also leads achieving best performance case training real data, due overfitting. similarly, admis team achieves higher performance across de- mographic groups except black males vs. avg. moreover, analyze scenario training synthetic data, overall performance increase, bias demographic groups also reduced std sub-tasks std sub-task implies better model terms fairness across demographic groups. mainly produced synthetic data control balanced data respect groups. finally, noteworthy compare best results achieved sub-task i.e., unconstrained synthetic data, sub-task i.e., constrained synthetic real data. team achieves sub-task whereas admis achieves sub- task showing unlimited synthetic data training even outperform systems trained table ranking three sub-tasks considered task sub-task, highlight bold best team according trade-off. trade-off, avg average accuracy, standard deviation accuracy, fnmr false non-match rate, fmr false match rate, auc area curve, gap gap real. sub-task bias mitigation synthetic data constrained pos. team avg fnmrfmr auc gap admis srcn aivl opdai ctai k-ibs-ds sub-task bias mitigation synthetic data unconstrained pos. team avg fnmrfmr auc gap admis opdai inesc-igd k-ibs-ds ctai sub-task bias mitigation synthetic real data constrained pos. team avg fnmrfmr auc gap admis k-ibs-ds unica-igd-lsi opdai inesc-igd cbsr-samsung limited synthetic real data. results motivate use synthetic data demographic bias mitigation, improving time privacy real identities seen network. task overall improvement table viii provides results achieved par- ticipants task focusing bias mitigation also challenges age, pose, occlusions. teams ranked descending order based average verification accuracy across four databases. notably, sub-tasks, avg lower achieved task bupt-balancedface database, showing additional challenges introduced agedb cfp-fp rof databases. trend also observed gap results, tend worse sub-tasks compared sub-tasks suggesting far difficult emulate conditions real databases synthetic data itself. example, sub-task although top- teams achieve high avg results, exhibit considerable positive gap value i.e., opdai table vii comparison baseline model trained exclusively real data proposed model trained synthetic data finalist task performance demographic group represented avg sub task team model asian female asian male black female black male indian female indian male white female white male avg std gap proposed baseline admis proposed baseline avg, gap avg, gap admis avg, gap, showing model trained real data i.e., casia-webface adapts better adverse image conditions aging, pose, occlusions. focusing sub-task team ranked top-, i.e., idiap-synthdistill, achieves much better results compared best result sub-task i.e., vs. avg, proving unlimited synthetic data improve performance system. finally, sub-task teams report better avg higher negative gap values e.g., k-ibs-ds achieves avg, gap, proving synthetic data combined real data alleviate existing limitations within technology. furthermore, table compares performance across different evaluation databases, highlight- ing variations baseline models, trained real data, models proposed top-ranked teams sub-tasks task .in sub-task model proposed opdai trained limited synthetic data due competition rules, underper- forms compared baseline across databases gap. suggests synthetic samples insufficient fully replace real data. moreover, sub-task training synthetic data unlimited, performance model proposed idiap-synthdistill almost identical baseline gap. suggests large synthetic dataset closely replicate distribution real data. also observe agedb bupt databases, baseline still out- performs proposed model, indicates synthetic data fully capture differences aging demographic groups. finally, sub-task combines real unlimited synthetic data, model proposed k-ibs-ds outperforms baseline databases gap. supports idea previous experiments, synthetic data complements real data, improving generalization. demographic groups evaluation databases section provides in-depth analysis results terms different demographic groups individual databases considered frcsyn-ongoing. figure left, shows de- tection error tradeoff det curves sub-tasks including results achieved top- team demographic group. completeness, information graphical representations teams found challenge codalab platform. sub-tasks team achieves first place, rd, demonstrates high per- formance across different demographic groups considered accuracy de- mographic groups. however, slight gender bias observed improvements male female labels ethnicities. regarding ethnicity, proposed model showed better results subjects in- dian ethnicity accuracy indian male accuracy indian female. finally, sub-task winning team, admis, also per- forms well across demographic groups table viii ranking three sub-tasks considered task sub-task, highlight bold best team according average accuracy. avg average accuracy, fnmr false non-match rate, fmr false match rate, auc area curve, gap gap real. sub-task overall improvement synthetic data constrained pos. team avg fnmrfmr auc gap opdai admis k-ibs-ds ctai bovifocr-ufpr sub-task overall improvement synthetic data unconstrained pos. team avg fnmrfmr auc gap idiap-synthdistill admis opdai k-ibs-ds ctai sub-task overall improvement synthetic real data constrained pos. team avg fnmrfmr auc gap k-ibs-ds opdai ctai cbsr-samsung admis accuracy. however, exists variabil- ity performance different demographic groups. example, asian females in- dian females lowest accuracy white females highest accuracy figure right shows det curves sub-tasks including results achieved top- team database. an- alyzing model proposed opdai team sub-task spread curves indicates variability system performance across different databases, results agedb accuracy outperforming others. table comparison baseline model trained exclusively real data, proposed model using synthetic data finalist task performance database represented avg sub-task team model agedb bupt cfp-fp rof avg std gap opdai proposed baseline idiap synthdistill proposed baseline k-ibs-ds proposed baseline moreover, sub-task idiap-synthdistill model significantly improves performance sub-task agedb cfp-fp databases i.e., accuracy, respectively. finally, sub-task curves k-ibs-ds model closely aligned agedb accuracy, bupt ac- curacy, cfp-fp accuracy, show- ing consistent reliable performance across databases. however, curve rof database remains worst sub-task i.e., accuracy sub- tasks respectively, reflecting difficult database emulate synthetic data. post-challenge analysis comparison edition analyzing contributions eleven top teams, observe prevalence well- established methodologies. notably, teams used dcface either independently con- junction synthetic databases gandiffface digiface-m idiff- face dcface dual condition face gener- ator based diffusion model, designed create facial images subject various styles maintaining identity consistency. key com- ponent patch-wise style extractor, extracts style features image mini- mizing identity information. forces model rely separate input identity data. unlike previous approaches like stylegan dcface retains essential spatial details, pose, ensur- ing greater variability subjects identity. results images similar style enhance performance facial recognition models identifying subjects. furthermore, several teams, including cbsr-samsung, inesc-igd, ctai adopted interesting approaches involving syn- thetic data cleaning selection. approaches include de-overlapping data dcface, trained casia-webface data could similar, deteriorating training, balancing data respect demographic information, iii removing images far class center using clustering techniques dbscan. interesting highlight idiap-synthdistill teams con- sidered novel methods generate synthetic data. specifically, team used hdt generate synthetic facial images along iden- tity style embeddings, used stylenat model generate variability synthetic data. another example idiap- synthdistill team, proposed end-to-end method dynamically generated facial images stylegan trained model model distillation. regarding backbone architecture, teams opted either resnet iresnet mainly widespread adop- tion state-of-the-art methodologies. ar- chitectures use residual connections improve training, resnet, skip connections bypass one layers address vanishing gradient problem, iresnet optimizes informa- false match rate false non-match rate sub-task bias mitigation white male acc white female acc black male acc black female acc asian male acc asian female acc indian male acc indian female acc false match rate false non-match rate sub-task overall improvement bupt acc rof acc agedb acc cfp-fp acc false match rate false non-match rate sub-task bias mitigation white male acc white female acc black male acc black female acc asian male acc asian female acc indian male acc indian female acc false match rate false non-match rate sub-task overall improvement bupt acc rof acc agedb acc cfp-fp acc false match rate false non-match rate sub-task bias mitigation white male acc white female acc black male acc black female acc asian male acc asian female acc indian male acc indian female acc false match rate false non-match rate sub-task overall improvement bupt acc rof acc agedb acc cfp-fp acc fig. det curves task left task right. sub-tasks left-top, left-middle, left-bottom, including results achieved top- team i.e., rd, rd, admis, respectively demographic group. sub-tasks right-top, right-middle, right-bottom, including results achieved top- team i.e., opdai, idiap- synthdistill, k-ibs-ds, respectively demographic group. table description best results achieved frcsyn-ongoing. sub-tasks frcsyn-ongoing included table novel sub-tasks available edition. sub-task bias mitigation frcsyn-ongoing frcsyn-ongoing team gap team gap lens sub-task bias mitigation frcsyn-ongoing frcsyn-ongoing team gap team gap cbsr admis sub-task overall improvement frcsyn-ongoing frcsyn-ongoing team avg gap team avg gap bovifocr opdai sub-task overall improvement frcsyn challenge frcsyn-ongoing team avg gap team avg gap bovifocr k-ibs-ds tion flow network, allowing training extremely deep architectures without in- creasing model complexity. finally, selection loss functions also similar among teams, adaface arcface prevalent choices. losses use angular margin-based loss functions improve facial feature discrimination, arcface margin fixed samples, adaface adapts margin dynamically based quality image. nevertheless, exceptions team used recent uniface unica team considered cosface aditionally, compare results achieved frcsyn-ongoing results edition table shows best results achieved editions challenge, including also gap values. important remark sub-tasks frcsyn- ongoing included analysis novel sub-tasks available edition. notably, two observations made main metric ranking teams i.e., avg shows improvements across cases edition task e.g., vs. sub-task vs. sub- task task e.g., vs. avg sub-task vs. avg sub-task terms gap value, models edition follow similar trend compared edition, achieving sub-tasks negative gap values, remarking benefits training using synthetic data. particular, sub-task much higher negative gap value observed edition i.e., vs. -.. result, together higher value, seems due generation better synthetic data team, proposal novel generative methods, indicated before. addition, several conclusions drawn. first, improvement main metric associated freedom select methodology generate synthetic data train models, well application data cleaning selection techniques. observe increasing gap value associated also enhancement models, due proposal different architectures loss functions, indicated before. finally, observe comes quality generated synthetic data, higher qual- ity imply better performance recogni- tion tasks. teams use dcface main dataset training, even though generates images lower resolution less detail qualitative perspective. nevertheless, analyzing results achieved using database, conclude models might require highly detailed images learn match identities, least databases considered challenge. sug- gests instead focusing realism, synthetic data generate diverse images help models better learn class variability reducing noise. vi. conclusion frcsyn-ongoing presented com- prehensive exploration applications syn- thetic data fr, effectively addressing existing limitations field. edition, two addi- tional sub-tasks introduced, showing impressive results achieved using unlimited synthetic data, even outperforming cases scenario training real data. increased number participants last edition, witnessed considerable performance im- provement sub-tasks comparison edition possible thanks proposal novel methods generate select better synthetic data, well models loss functions. approaches compared across variety sub-tasks, many reproducible thanks materials made available participating teams. future studies include recent tech- niques make sure databases available challenge used participants. also perform detailed analysis results comparison recent challenges topic, sdfr evaluate diverse set databases include challenges, like quality, surveillance, large distance images finally, plan focus explainability models frame- works generate synthetic images debiasing face recognition models using synthetic datasets important task challenge, found use synthetic data furhter increase performance. however, concept bias complex often subjective. constitutes fair representation vary signif- icantly depending cultural context, individual experiences, even personal beliefs. therefore, debiasing efforts approached accurate understanding multifaceted nature bias. simply generating synthetic data reflect particular demographic distribution might fully address complexities real-world inequalities. rather seeking eliminate bias, perhaps productive approach pursue research direction transparency, interpretability, controllability. translates research questions allow researchers easily define bias allow fine-tune mod- els accordingly. ultimately, goal develop face recognition systems accurate also fair ethical. acknowledgements study supported inter-action pid- ob-i micinnfeder, catedra enia uam- veridas responsable nextgenerationeu prtr tsi---, agreement dggcuamfuam biometrics cybersecurity, powerai sipji- funded comunidad madrid grant agreement promotion research technology transfer uam. also supported german federal ministry education research hessian ministry higher education, research, science, arts within joint support national research center applied cybersecurity athene. k-ibs-ds supported institute basic science, republic korea ibs-r- unica-igd-lsi supported aris program p-b. references wang deng, deep face recognition survey, neurocomputing, vol. pp. du, shi, zeng, x.-p. zhang, mei, elements end-to-end deep face recognition survey recent advances, acm comput. surv., vol. no. pp. bisogni, castiglione, hossain, narducci, umer, impact deep learning approaches facial expression recognition healthcare industries, ieee transactions industrial informatics, vol. no. pp. gomez, morales, orozco-arroyave, daza, fierrez, improving parkinson detection using dynamic features evoked expressions video, proc. ieeecvf conference computer vision pattern recognition workshops, daza, gomez, morales, fierrez, tolosana, cobos, ortega-garcia, matt multimodal at- tention level estimation e-learning platforms, proc. aaai workshop artificial intelligence education, deng, guo, xue, zafeiriou, arcface additive angular margin loss deep face recognition, proc. ieeecvf conference computer vision pattern recognition, kim, jain, liu, adaface quality adap- tive margin face recognition, proc. ieeecvf conference computer vision pattern recognition, deandres-tame, tolosana, vera-rodriguez, morales, fierrez, ortega-garcia, good chatgpt face biometrics? first look recog- nition, soft biometrics, explainability, ieee access, vol. pp. crum, tinsley, boyd, piland, sweet, kelley, bowyer, czajka, explain salience-based explainability synthetic face detection models, ieee transactions artificial intelligence, pp. shen, yang, tang, zhou, interfacegan interpreting disentangled face representation learned gans, ieee transactions pattern analysis machine intelligence, vol. no. pp. terhorst, kolf, huber, kirchbuchner, damer, moreno, fierrez, kuijper, comprehensive study face recognition biases beyond demographics, ieee transactions technology society, vol. no. pp. melzi, rathgeb, tolosana, vera-rodriguez, morales, lawatsch, domin, schaubert, synthetic data mitigation demographic biases face recognition, proc. ieee international joint conference biometrics, morales, fierrez, vera-rodriguez, tolosana, sensitivenets learning agnostic representations application face images, ieee transactions pattern analysis machine intelligence, vol. no. pp. melzi, shahreza, rathgeb, tolosana, vera- rodriguez, fierrez, marcel, busch, multi- ive privacy enhancement multiple soft-biometrics face embeddings, proc. ieeecvf winter conference applications computer vision workshops, melzi, rathgeb, tolosana, vera, busch, overview privacy-enhancing technologies bio- metric recognition, acm computing surveys, zhao, yan, feng, towards age-invariant face recognition, ieee transactions pattern analysis machine intelligence, vol. no. pp. valle, buenaposada, baumela, multi-task head pose estimation in-the-wild, ieee transactions pattern analysis machine intelligence, vol. no. pp. tran, yin, liu, representation learning rotating faces, ieee transactions pattern analysis machine intelligence, vol. no. pp. mudunuri biswas, low resolution face recognition across variations pose illumination, ieee transactions pattern analysis machine intel- ligence, vol. no. pp. qiu, gong, li, liu, tao, endend occluded face recognition masking corrupted fea- tures, ieee transactions pattern analysis machine intelligence, vol. no. pp. melzi, rathgeb, tolosana, vera-rodriguez, lawatsch, domin, schaubert, gandiffface controllable generation synthetic datasets face recognition realistic variations, proc. ieeecvf international conference computer vision workshops, kim, liu, jain, liu, dcface synthetic face generation dual condition diffusion model, proc. ieeecvf conference computer vision pattern recognition, boutros, grebe, kuijper, damer, idiff-face synthetic-based face recognition fizzy identity-conditioned diffusion model, proc. ieeecvf international conference computer vision, boutros, struc, fierrez, damer, syn- thetic data face recognition current state future prospects, image vision computing, vol. joshi, grimmer, rathgeb, busch, bremond, dantcheva, synthetic data human analysis survey, ieee transactions pattern analysis machine intelligence, vol. no. pp. sun, zhang, sun, tan, demographic analysis biometric data achievements, challenges, new frontiers, ieee transactions pattern analysis machine intelligence, vol. no. pp. goodfellow, pouget-abadie, mirza, xu, warde-farley, ozair, courville, bengio, generative adversarial nets, proc. advances neural information processing systems, ho, jain, abbeel, denoising diffusion proba- bilistic models, proc. advances neural information processing systems, bae, gorce, baltrusaitis, hewitt, chen, valentin, cipolla, shen, digiface- million digital face images face recognition, proc. ieeecvf winter conference applications computer vision, zhang, chen, chai, wu, lagun, beeler, torre, iti-gen inclusive text-to-image generation, proc. ieeecvf international conference computer vision, qiu, yu, gong, li, liu, tao, synface face recognition synthetic data, proc. ieeecvf international conference computer vision, melzi, tolosana, vera-rodriguez al., frcsyn- ongoing benchmarking comprehensive evaluation real synthetic data improve face recognition systems, information fusion, vol. frcsyn challenge wacv face recog- nition challenge era synthetic data, proc. ieeecvf winter conference applications computer vision workshops, he, zhang, ren, sun, deep residual learning image recognition, proc. ieeecvf conference computer vision pattern recognition, radford, kim, hallacy, ramesh, goh, agarwal, sastry, askell, mishkin, clark, krueger, sutskever, learning transferable visual models natural language supervision, proc. international conference machine learning, oquab, darcet, moutakanni, vo, szafraniec, khalidov, fernandez, haziza, massa, el- nouby al., dinov learning robust visual features without supervision, arxiv preprint wang, wu, tang, he, guo, zhu, bai, zhao, wu, al., hulk universal knowledge translator human-centric tasks, arxiv preprint tang, chen, xie, chen, wang, ci, bai, zhu, yang, yi, zhao, ouyang, hu- manbench towards general human-centric perception projector assisted pretraining, proc. ieeecvf conference computer vision pattern recognition, khirodkar, bagautdinov, martinez, zhaoen, james, selednik, anderson, saito, sapiens foundation human vision models, proc. european conference computer vision, bozorgtabar, rad, ekenel, j.-p. thiran, using photorealistic face synthesis domain adapta- tion improve facial expression analysis, proc. ieee international conference automatic face gesture recognition, tolosana, delgado-santos, perez-uribe, vera- rodriguez, fierrez, morales, deepwritesyn on-line handwriting synthesis via deep short-term rep- resentations, proc. aaai conference artificial intelli- gence, vol. pp. hwang, jang, park, cho, i.-j. kim, eldersim synthetic data generation platform hu- man action recognition eldercare applications, ieee access, vol. pp. varol, romero, martin, mahmood, black, laptev, schmid, learning synthetic humans, proc. ieee conference computer vision pattern recognition, deandres-tame, tolosana, melzi, vera- rodriguez, kim, rathgeb, liu, morales, fierrez, ortega-garcia, zhong, huang, mi, ding, zhou, he, fu, cong, zhang, xiao, smirnov, pimenov, grigorev, tim- oshenko, asfaw, low, liu, wang, zuo, he, shahreza, george, unnervik, rahimi, marcel, neto, huber, kolf, damer, boutros, cardoso, sequeira, atzori, fenu, marras, struc, yu, li, li, zhao, lei, zhu, x.-y. zhang, biesseck, vidal, coelho, granada, menotti, second edition frcsyn challenge cvpr face recog- nition challenge era synthetic data, proc. ieeecvf conference computer vision pattern recognition, kansy, rael, mignone, naruniec, schroers, gross, weber, controllable inversion black-box face recognition models via diffusion, proc. ieeecvf international conference computer vision workshops, boutros, huber, siebke, rieber, damer, sface privacy-friendly accurate face recognition using synthetic data, proc. ieee international joint conference biometrics, karras, aittala, laine, harkonen, hellsten, lehtinen, aila, alias-free generative adversar- ial networks, advances neural information processing systems, vol. pp. ruiz, li, jampani, pritch, rubinstein, aberman, dreambooth fine tuning text-to-image diffusion models subject-driven generation, proc. ieeecvf conference computer vision pattern recognition, wood, baltrusaitis, hewitt, dziadzio, cashman, shotton, fake till make face analysis wild using synthetic data alone, proc. ieeecvf international conference computer vision, deng, yang, chen, wen, tong, dis- entangled controllable face image generation via imitative-contrastive learning, proc. ieeecvf conference computer vision pattern recognition, yi, lei, liao, li, learning face rep- resentation scratch, arxiv preprint wang deng, mitigating bias face recog- nition using skewness-aware reinforcement learning, proc. ieeecvf conference computer vision pattern recognition, moschoglou, papaioannou, sagonas, deng, kotsia, zafeiriou, agedb first manually collected, in-the-wild age database, proc. ieeecvf conference computer vision pattern recognition workshops, sengupta, j.-c. chen, castillo, patel, chel- lappa, jacobs, frontal profile face verifica- tion wild, proc. ieeecvf winter conference applications computer vision, erakn, demir, ekenel, recog- nizing occluded faces wild, proc. international conference biometrics special interest group, karkkainen joo, fairface face attribute dataset balanced race, gender, age bias measurement mitigation, proc. ieeecvf winter conference applications computer vision, duta, liu, zhu, shao, improved residual networks image video recognition, proc. international conference pattern recognition, rombach, blattmann, lorenz, esser, ommer, high-resolution image synthesis la- tent diffusion models, proceedings ieeecvf conference computer vision pattern recognition, song, meng, ermon, denoising diffusion implicit models, proc. international conference learning representations, li, cao, wang, qi, m.-m. cheng, shan, photomaker customizing realistic human photos via stacked embedding, proc. ieeecvf conference computer vision pattern recognition, crowson, baumann, birch, abraham, kaplan, shippole, scalable high-resolution pixel-space image synthesis hourglass diffusion transformers, arxiv preprint van den oord, vinyals, kavukcuoglu, neural discrete representation learning, proc. advances neural information processing systems, walton, hassani, xu, wang, shi, stylenat giving head new perspective, arxiv preprint garaev, smirnov, galyuk, lukyanets, facemix transferring local regions data augmen- tation face recognition, proc. neural information processing, zhou, jia, li, shen, duan, uniface unified cross-entropy loss deep face recognition, proc. ieeecvf international conference computer vision, low, chai, park, ann, cha, slackedface learning slacked margin low-resolution face recognition, proc. british machine vision conference, hu, shen, sun, squeeze-and-excitation networks, proc. ieee conference computer vision pattern recognition, wang, wang, zhou, ji, gong, zhou, li, liu, cosface large margin cosine loss deep face recognition, proc. ieee conference computer vision pattern recognition, karras, laine, aila, style-based gener- ator architecture generative adversarial networks, proc. ieeecvf conference computer vision pattern recognition, otroshi shahreza, george, marcel, knowl- edge distillation face recognition using synthetic data dynamic latent sampling, ieee access, vol. pp. boutros, damer, kirchbuchner, kuijper, elasticface elastic margin loss deep face recogni- tion, proc. ieeecvf conference computer vision pattern recognition workshops, boutros, klemt, fang, kuijper, damer, exfacegan exploring identity directions gans learned latent space synthetic identity generation, ieee international joint conference biometrics, an, zhu, gao, xiao, zhao, feng, wu, qin, zhang, zhang, fu, partial training million identities single machine, proc. ieeecvf international conference computer vision workshops, zhu, huang, deng, ye, huang, chen, zhu, yang, lu, du, zhou, webfacem benchmark unveiling power million-scale deep face recognition, proc. ieeecvf conference computer vision pattern recognition, ronneberger, fischer, brox, u-net con- volutional networks biomedical image segmentation, proc. medical image computing computer-assisted intervention, tong, fatras, malkin, huguet, zhang, rector-brooks, wolf, bengio, improving generalizing flow-based generative models minibatch optimal transport, transactions machine learning research, salimans, classifier-free diffusion guid- ance, arxiv preprint smirnov, garaev, galyuk, lukyanets, prototype memory large-scale face representation learning, ieee access, vol. pp. deng, guo, ververas, kotsia, zafeiriou, retinaface single-shot multi-level face localisation wild, proc. ieeecvf conference computer vision pattern recognition, karras, laine, aittala, hellsten, lehtinen, aila, analyzing improving image quality stylegan, proc. ieeecvf conference computer vision pattern recognition, neto, caldeira, cardoso, se- queira, compressed models decompress race biases quantized models forget fair face recognition, proc. international conference biometrics special interest group, neto, boutros, pinto, damer, sequeira, cardoso, bengherabi, bousnat, boucheta, hebbadj, erakn, demir, ekenel, queiroz vidal, menotti, ocfr competition occluded face recognition synthetically generated structure-aware occlusions, proc. ieee international joint conference biometrics, neto, mamede, albuquerque, goncalves, sequeira, massively annotated datasets as- sessment synthetic real data face recognition, arxiv preprint atzori, boutros, damer, fenu, marras, enough, make reducing authentic data demand face recognition synthetic faces, proc. international conference automatic face gesture recognition, shoshan, bhonker, kviatkovsky, medioni, gan-control explicitly controllable gans, proc. ieeecvf international conference computer vision, boutros, klemt, fang, kuijper, damer, unsupervised face recognition using unlabeled syn- thetic data, proc. international conference automatic face gesture recognition, zhang, zhang, li, qiao, joint face de- tection alignment using multitask cascaded convolu- tional networks, ieee signal processing letters, vol. no. pp. wang, liu, hu, shi, mei, facex-zoo pytorch toolbox face recognition, proc. acm international conference multimedia, ngan, grother, hanaoka, ongo- ing face recognition vendor test frvt part face recognition accuracy face masks using post-covid- algorithms, dealcala, morales, mancera, fierrez, tolosana, ortega-garcia, data model? membership inference test application face images, arxiv preprint shahreza, ecabert, george, unnervik, marcel, domenico, borghi, maltoni, boutros, vogel, damer, angela sanchez-perez, enriquemas-candela, calvo-zaragoza, biesseck, vidal, granada, menotti, deandres-tame, cava, concas, melzi, tolosana, vera- rodriguez, perelli, orru, marcialis, fierrez, sdfr synthetic data face recognition competition, proc. ieee international conference automatic face gesture recognition, liu, ashbaugh, chimitt, hassan, hassani, jaiswal, kim, mao, perry, ren, su, varghaei, wang, zhang, chan, ross, shi, wang, jain, liu, farsight physics- driven whole-body biometric system large distance altitude, proc. ieeecvf winter conference applications computer vision, vii. biography section ivan deandres-tame ph.d. universidad autonoma madrid madrid, spain. contact ivan.deandresuam.es. ruben tolosana associate professor universidad autonoma madrid madrid, spain. contact ruben.tolosanauam.es. pietro melzi got ph.d. universidad autonoma madrid madrid, spain. contact pietro.melziuam.es. ruben vera-rodriguez associate professor universi- dad autonoma madrid madrid, spain. contact ruben.verauam.es. minchul kim ph.d. michigan state university, michigan, usa. contact kimmincmsu.edu. christian rathgeb full professor hochschule darmstadt darmstadt, germany. contact christian.rathgebh- da.de. xiaoming liu full professor michigan state university, michigan, usa. contact liuxmcse.msu.edu. luis f.gomez got ph.d. universidad autonoma madrid madrid, spain. contact luisf.gomezuam.es. aythami morales associate professor universidad autonoma madrid madrid, spain. contact aythami.moralesuam.es. julian fierrez full professor universidad autonoma madrid madrid, spain. contact rjulian.fierrezuam.es. javier ortega-garcia full professor universidad autonoma madrid madrid, spain. contact javier.ortegauam.es. zhizhou zhong m.s. student fudan university shanghai, china. contact zzzhongm.fudan.edu.cn. yuge huang senior researcher tencent youtu lab shanghai, china. contact yugehuangtencent.com. yuxi ph.d. fudan university shanghai, china. contact yxmifudan.edu.cn. shouhong ding principal researcher tencent youtu lab shanghai, china. contact ericshdingtencent.com. shuigeng zhou full professor fudan university shanghai, china. contact sgzhoufudan.edu.cn. shuai software engineer interactive entertainment group netease inc guangzhou, china. contact heshuaicorp.netease.com. lingzhi software engineer interactive entertain- ment group netease inc guangzhou, china. contact fulingzhicorp.netease.com. heng cong software engineer interactive entertain- ment group netease inc guangzhou, china. contact conghengcorp.netease.com. rongyu zhang software engineer interactive enter- tainment group netease inc guangzhou, china. contact zhangrongyucorp.netease.com. zhihong xiao software engineer interactive enter- tainment group netease inc guangzhou, china. contact xiaozhihongcorp.netease.com. evgeny smirnov engineer inc. barcelona, spain. contact evgeny.smirnovidrnd.net. anton pimenov engineer inc. barcelona, spain. contact pimenovidrnd.net. aleksei grigorev engineer inc. barcelona, spain. contact grigorievidrnd.net. denis timoshenko engineer inc. new york, usa. contact timoshenkoidrnd.net. kaleb mesfin asfaw undergraduate student researcher korea advanced institute science technology daejeon, south korea. contact kalebmeskaist.ac.kr. cheng yaw low research associate insti- tute basic science daejeon, south korea. contact chengyawlowibs.re.kr. hao liu m.s china telecom ai, china. contact liuhchinatelecom.cn chuyi wang m.s china telecom ai, china. contact wangcychinatelecom.cn. qing zuo ph.d. china telecom ai, china. contact zuoqchinatelecom.cn zhixiang ph.d. china telecom ai, china. contact hezxchinatelecom.cn. hatef otroshi shahreza ph.d. student epfl lausanne, switzerland research assistant idiap research institute martigny, switzerland. contact hatef.otroshiidiap.ch. anjith george research associate idiap re- search institute martigny, switzerland. contact an- jith.georgeidiap.ch. alexander unnervik ph.d. student epfl lau- sanne, switzerland research assistant idiap re- search institute martigny, switzerland. contact alex.unnervikidiap.ch. parsa rahimi ph.d. student epfl lausanne, switzer- land research assistant idiap research institute martigny, switzerland. contact parsa.rahimiidiap.ch. sebastien marcel senior researcher head biometrics security privacy group idiap research institute martigny, switzerland professor unil lausanne, switzerland. contact sebastien.marcelidiap.ch. pedro neto research assistant inesc tec porto, portugal. contact pedro.d.carneiroinesctec.pt marco huber research associate fraunhofer igd darmstadt, germany. contact marco.huberigd.fraunhofer.de jan niklas kolf research associate fraunhofer igd darmstadt, germany. contact jan.niklas.kolfigd.fraunhofer.de naser damer research associate fraunhofer igd darm- stadt, germany. contact naser.damerigd.fraunhofer.de fadi boutros research associate fraunhofer igd darm- stadt, germany. contact fadi.boutrosigd.fraunhofer.de. jaime cardoso full professor feup porto, portugal. contact jaime.cardosofe.up.pt ana sequeira assistant researcher inesc tec porto, portugal. contact ana.f.sequeirainesctec.pt andrea atzori ph.d. student university cagliari cagliari, italy. contact andrea.atzoriunica.it. gianni fenu ph.d. student university cagliari cagliari, italy. contact gianni.fenuunica.it. mirko marras ph.d. student university cagliari cagliari, italy. contact mirko.marrasacm.org. vitomir struc full professors university ljubljana ljubljana, slovenia. contact vitomir.strucfe.uni-lj.si jiang engineer samsung electronics china centre, china. contact jiang.yusamsung.com. zhangjie student university science technology china, china. contact lizhangjiemail.ustc.edu. jichun engineer samsung electronics china centre, china. contact jichun.lisamsung.com. weisong zhao student institute information engineer- ing, chinese academy sciences beijing, china. contact zhaoweisongiie.ac.cn. zhen lei professor mais, institute automation, chinese academy sciences beijing, china. contact zleinlpr.ia.ac.cn. xiangyu zhu associated professor institute automa- tion, chinese academy sciences beijing, china. contact xiangyu.zhuia.ac.cn xiao-yu zhang professor institute information engineering, chinese academy sciences beijing, china. contact zhangxiaoyuiie.ac.cn bernardo biesseck ph.d. student federal univer- sity parana, curitiba, brazil. contact bjg- biesseckinf.ufpr.br. pedro vidal undergraduate student federal university parana, curitiba, brazil. contact pbqvinf.ufpr.br. luiz coelho engineer unico idtech belo horizonte, brazil. contact luiz.coelhounicio.io. roger granada ph.d.ml engineer unico idtech porto alegre, brazil. contact roger.granadaunico.io. david menotti associate professor federal university parana, curitiba, brazil. contact menottiinf.ufpr.br.", "published_date": "2024-12-02T11:12:01+00:00"}
{"id": "2411.17027v1", "title": "D$^2$-World: An Efficient World Model through Decoupled Dynamic Flow", "authors": ["Haiming Zhang", "Xu Yan", "Ying Xue", "Zixuan Guo", "Shuguang Cui", "Zhen Li", "Bingbing Liu"], "summary": "technical report summarizes second-place solution predictive world model challenge held cvpr- workshop foundation models autonomous systems. introduce d-world, novel world model effectively forecasts future point clouds decoupled dynamic flow. specifically, past semantic occupancies obtained via existing occupancy networks e.g., bevdet. following this, occupancy results serve input single-stage world model, generating future occupancy non-autoregressive manner. simplify task, dynamic voxel decoupling performed world model. model generates future dynamic voxels warping existing observations voxel flow, remaining static voxels easily obtained pose transformation. result, approach achieves state-of-the-art performance openscene predictive world model benchmark, securing second place, trains faster baseline model. code available", "full_text": "d-world efficient world model decoupled dynamic flow haiming zhang,, yan, ying xue, zixuan guo, shuguang cui, zhen li, bingbing liu huawei noahs ark lab, chinese university hong kong, shenzhen haimingzhanglink.cuhk.edu.cn abstract technical report summarizes second-place so- lution predictive world model challenge held cvpr- workshop foundation models au- tonomous systems. introduce d-world, novel world model effectively forecasts future point clouds decoupled dynamic flow. specifically, past semantic occupancies obtained via existing occupancy networks e.g., bevdet. following this, occupancy results serve input single-stage world model, generating fu- ture occupancy non-autoregressive manner. simplify task, dynamic voxel decoupling performed world model. model generates future dynamic vox- els warping existing observations voxel flow, remained static voxels easily obtained pose transformation. result, approach achieves state-of-the-art performance openscene predictive world model benchmark, securing second place, trains faster baseline model. code available d-world. introduction predictive world model aims forecast future states using past observations, playing crucial role achiev- ing end-to-end driving systems. cvpr pre- dictive world model challenge, participants required use past image inputs predict point cloud future frames. challenge presents two main difficulties first effectively train large-scale data. given openscene dataset contains million frames, designed model must efficient. second challenge predict faithful point clouds sore visual inputs. address issues, designed novel so- lution extends beyond baseline model. regarding problem found official baseline model project lead. i.e., vidar requires long training times uses historical frames predict future frames autoregressive manner. address this, designed solution divides entire training process two parts. first part trains occupancy prediction model single-frame prediction, second part uses past occupancy data predict future point clouds. specifically, first stage, utilize existing occupancy network, bevdet predicts semantic occupancy encoding occupancy states semantic informa- tion within volume. second stage, generative world model takes past occupancy results input generates future occupancy states, ren- dered point clouds via differentiable volume rendering. training paradigm, increased training speed given significant development occupancy net- works autonomous driving community recently aforementioned problem ii, focus construct world model maps past occupancy re- sults future ones. framework leverages advan- tages potential single-stage video prediction en- abling prediction multiple future volumes non- autoregressive manner. moreover, found directly predicting occupancy frame results unsatis- factory performance due majority voxels empty. address issue, use semantic informa- tion predicted occupancy network decouple vox- els dynamic static categories. world model predicts voxel flow dynamic objects warps voxels accordingly. static objects, since global positions remain unchanged, easily ob- tain pose transformation. leveraging components, d-world surpasses baseline model large margin, achieving chamfer distance single model securing place challenge. proposed method method comprises two stages, overall architec- ture depicted fig. given historical camera im- cs.cv nov past occupancy spacetime tokens enc past ego poses positional encoding spatial-temporal transformer future occupancy flow decoder enc positional encoding pose decoder future ego poses queries future poses rendering future point cloud occupancy head pose tokens encoded tokens warping refinement d-to-d view transform voxel features stage multi-view images stage feature current frame figure overall pipeline d-world. first stage, train single-frame occupancy network, second stage, train world model takes past occupancy input, forecasting future point clouds. ages timestamps, first stage predicts occupancy frame-by-frame, aiming recover rich dense repre- sentation images. second stage, ap- proach point cloud forecasting task. instead forecasting future point cloud inefficient au- toregressive manner like vidar design novel versatile point cloud forecasting framework op- erates non-autoregressive manner decoupled dy- namic flow. stage vision-based occupancy prediction section, introduce architecture occu- pancy network, takes visual images input pre- dicts occupancy state semantics single frame. image encoder. image encoder designed encode input multi-camera images high-level features. image encoder comprises backbone high-level feature extraction neck multi-resolution feature aggregation. default, use classical imagenet pre- trained resnet- backbone ablation studies, swin-transformer-b backbone submission. although employing stronger image backbone en- hance prediction performance, considered trade-offs resource usage training time, ultimately decided using huge backbones internimage- view transformation. utilize lss view trans- formation, densely predicts depth pixel classification method, allowing project image features space. moreover, introduce temporal information model, adopt technique proposed dynamically warping fusing one his- torical volume feature produce fused feature. ocupancy head. adopt semantic scene comple- tion module proposed occupancy head, contains several convolutional blocks learn local ge- ometric representation. features different blocks concatenated aggregate information. finally, linear projection utilized map features dimen- sions, number classes. losses. alleviate class-imbalance issue occupancy prediction, utilize class-weighted cross-entropy lo- vasz losses. multi-task training losses combina- tion occupancy prediction loss depth loss. stage occupancy forecasting section, introduce process future point cloud forecasting. framework consists occupancy encoder, flow decoder, flow guided warping refine, rendering process. initially, occupancy data preprocessed spacetime tokens. spatial-temporal transformer effec- tively captures spatial structures local spatiotem- poral dependencies within tokens. following en- coding historical tokens, flow decoder employed predict future flow voxel grid. then, warping refinement generate final occupancy density. fully leverage temporal information across entire sequence, utilize non-autoregressive approach de- coding, achieves impressive forecasting performance alongside high efficiency. finally, differentiable volume rendering process used generate point cloud predicted occupancy. occupancy encoding. given sequence historically observed frames occupancy nht occupancy rhwd, first encode occu- pancy sequence spacetime tokens. here, represent resolution surrounding space cen- tered ego car. voxel assigned one classes, denoting whether occupied semantic category occupied with. reduce computational burden, transform input tokens convd matmul scale softmax matmul convd matrix multiplication warping feature current frame forecasted future flow refinement warped feature refined feature inner structures salt warping refinement dynamic static figure inner structure salt warping refinement. detailed structures salt, replace mlp ffn feed forward network vanilla transformer con- volutions convolutions respectively capturing spatial- temporal dependencies. decouple flow dy- namic static flow warp feature current frame forecasting future frame. refinement module refines coarse warping features. occupancy bev representation. take single- frame occupancy example, first uses learnable class embedding map occupancy occupancy embedding rhwdc. then, reshapes occupancy embedding along height dimension ob- tain bev representation rhwdc. bev embedding decomposed non-overlapping patches rhw hp, wp, resolution image patch. that, lightweight encoder composed several convolution layers, i.e., convd-groupnorm-silu, fol- lowed extract patch embeddings. considering sequence patch embeddings, obtain historical occupancy spacetime tokens rnhhw ego pose encoding. represent ego pose relative displacements adjacent frames ground plane. given historical ego poses, employ multiple linear layers followed relu activation function ob- tain ego tokense rnhc. spatial-temporal transformer. spatial-temporal transformer jointly models evolution surround- ing scene plans future trajectory ego vehicle. inspired previous works video prediction incorporate several spatial-aware local-temporal salt attention blocks within spatial-temporal transformer. shown fig. salt block, convolution layers first utilized generate query map paired key-value embeddings spacetime tokens, effectively preserving structural information spatial-aware cnn operation. subsequently, standard multi-head at- tention mechanism employed capture temporal cor- relations tokens. approach allows learning temporal correlations preserving spa- tial information sequence. furthermore, replace traditional feed-forward network ffn layer convolutional neural network dcnn introduce local temporal clues enhanced sequential modeling. decoupled dynamic flow. illustrated fig. fig. design decoupled dynamic flow sim- plify occupancy forecasting problem. specifically, flow decoderwhich comprises multiple stacked salt blocksprocesses encoded historical bev features forecasts absolute future flows respect cur- rent ego coordinate. utilizing occupancy semantics, decouple dynamic static grids, forecasting fu- ture voxel features via warping operation. dy- namic voxels, transform absolute flow fu- ture timestamp using future ego poses, ensuring align- ment current frame. static ones, directly transform future ego poses. finally, apply refinement module composed several simple cnns enhance coarse warped features. rendering losses. utilize rendering pro- cess losses vidar optimizing point cloud forecasting, ray-wise cross-entropy loss maximize response points along corresponding ray. pose regression, use loss training. experiments experimental setups dataset. conduct experiments openscene dataset derived nuplan dataset due scenes openscene lacking corresponding occupancy labels, ignore scenes ex- periments. submission, challenge utilizes online server provides historical images along normal- ized ray directions point forecasting. metric. challenge, model evaluation con- ducted using chamfer distance cham- fer distance quantifies similarity predicted ground-truth point clouds computing average nearest-neighbor distance points one set set, directions. training strategies. training process, stages trained adamw optimizer gradient clipping cyclic learning rate policy. initial learn- ing rates stage stage ii, respec- tively. stage utilize total batch size dis- tributed across nvidia gpus. stage ii, total batch size reduced leveraging nvidia gpus. ablation studies, stage trained us- ing nvidia gpus total batch size method training split test split chamfer distance avg vidar baseline mini mini d-world vanilla mini mini d-world mini mini vidar baseline mini online server d-world vanilla mini online server d-world vanilla full online server d-world full online server table poin cloud forecasting performance. best results setting highlighted bold. d-world vanilla denotes model without decoupled dynamic flow. method hours gpu mem. vidar total vidar total d-world stage-i d-world vanilla stage-ii d-world stage-ii d-world total proportion table training efficiency comparisons. experiments trained gpus epochs mini training set. indi- cates efficient version vidar inferior performance. stages trained epochs. network details. stage input image resolution incorporating common data augmentation tech- niques flipping rotation, applied im- ages space. resolution generated voxel grid prior feeding predicted occupancy stage ii, apply grid sampling operations align occupancy annotations range -m, -m, -m, lidar point cloud range -.m, -.m, -.m, .m, .m, .m. quantitative results main results ablation study. main results pre- sented tab. addition showing overall per- formance model d-world, also demonstrate performance model without decoupled dynamic flow d-world vanilla. method demonstrates su- perior performance across timestamps compared baseline model, performance enhance- ments observed upon introduction decoupled dy- namic flow. best submission ranks leader- board, achieving chamfer distance stages trained full dataset. training efficiency. validate efficiency approach, compare training hours gpu memory usage across different models, shown tab. base- line method, vidar, requires gpu memory hours training. even efficient version supervise future frames, still demands high gpu memory considerable training time hours. contrast, although method necessi- tates pre-training occupancy prediction model, world method miou iou chamfer distance vidar baseline version version version version version use version version version use table results analysis. effects occupancy prediction performance. model trained approximately hours gpu memory conditions. addi- tionally, model, even decoupled dynamic flow, maintains reasonable training hours gpu memory. effects occupancy performance. results using different occupancy performances presented tab. mini dataset used train. first train world model binary occupancy predic- tion empty occupied inputs. results ver- sion version denote performance world model occupancy performance changes. find world model performs better occupancy performance improved. furthermore, introducing decoupled dynamic flow semantic occupancy inputs yields additional performance enhancements, shown versions interestingly, performance significantly improve even ground truth occupancy miou iou used input. analysis indicates due inher- ently sparse nature point cloud forecasting, pri- marily requires predicting foremost visible surfaces objects space, whereas iou evaluation occu- pancy encompasses entire dense space. conclusion report, present solution d-world predictive world model challenge held conjunction cvpr workshop. reformulating visual point cloud forecasting predictive world model vision- based occupancy prediction point cloud forecasting via decoupled dynamic flow, solution demonstrates ex- emplary forecasting performance significant potential. references holger caesar, juraj kabzan, kok seang tan, whye kit fong, eric wolff, alex lang, luke fletcher, oscar beijbom, sammy omari. nuplan closed-loop ml-based plan- ning benchmark autonomous vehicles. arxiv preprint openscene contributors. openscene largest up-to- date occupancy prediction benchmark autonomous driving. openscene, yingruo fan, zhaojiang lin, jun saito, wenping wang, taku komura. faceformer speech-driven facial anima- tion transformers. proceedings ieeecvf conference computer vision pattern recognition, pages junjie huang, guan huang, zheng zhu, yun ye, dalong du. bevdet high-performance multi-camera object de- tection bird-eye-view. arxiv preprint tarasha khurana, peiyun hu, david held, deva ra- manan. point cloud forecasting proxy occu- pancy forecasting. cvpr, yinhao li, han bao, zheng ge, jinrong yang, jianjian sun, zeming li. bevstereo enhancing depth estimation multi-view object detection dynamic temporal stereo. arxiv preprint zhiqi li, wenhai wang, hongyang li, enze xie, chong- hao sima, tong lu, qiao, jifeng dai. bevformer learning birds-eye-view representation multi-camera images via spatiotemporal transformers. european con- ference computer vision, pages springer, liu, yutong lin, yue cao, han hu, yixuan wei, zheng zhang, stephen lin, baining guo. swin transformer hierarchical vision transformer using shifted windows. proceedings ieeecvf international conference computer vision, pages shuliang ning, mengcheng lan, yanran li, chaofeng chen, qian chen, xunlai chen, xiaoguang han, shuguang cui. mimo need strong multi-in-multi-out base- line video prediction. arxiv preprint arxiv ofir press, noah smith, mike lewis. train short, test long attention linear biases enables input length extrapolation. arxiv preprint wenhai wang, jifeng dai, zhe chen, zhenhang huang, zhiqi li, xizhou zhu, xiaowei hu, tong lu, lewei lu, hongsheng li, al. internimage exploring large-scale vi- sion foundation models deformable convolutions. arxiv preprint yan, jiantao gao, jie li, ruimao zhang, zhen li, rui huang, shuguang cui. sparse single sweep lidar point cloud segmentation via learning contextual shape priors scene completion. proceedings aaai conference artificial intelligence, pages zetong yang, chen, yanan sun, hongyang li. visual point cloud forecasting enables scalable autonomous driving. arxiv preprint haiming zhang, yan, dongfeng bai, jiantao gao, pan wang, bingbing liu, shuguang cui, zhen li. radocc learning cross-modality occupancy knowledge ren- dering assisted distillation. proceedings aaai con- ference artificial intelligence, pages", "published_date": "2024-11-26T01:42:49+00:00"}
