{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f088764c",
   "metadata": {},
   "source": [
    "<h3>Step 1: Fetch CVPR Papers and create JSON dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb917ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohit\\AppData\\Local\\Temp\\ipykernel_12008\\179593903.py:74: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in tqdm(search.results(), desc=\"Processing Papers\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data collection and preprocessing pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Papers: 50it [04:10,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully processed 50 papers.\n",
      "\n",
      "--- Example of Processed Paper ---\n",
      "Title: SLRTP2025 Sign Language Production Challenge: Methodology, Results, and Future Work\n",
      "Cleaned Summary: sign language production slp task generating sign language video spoken language inputs. field seen range innovations last years, introduction deep learning-based approaches providing significant improvements realism naturalness generated outputs. however, lack standardized evaluation metrics slp approaches hampers meaningful comparisons across different systems. address this, introduce first sign language production challenge, held part third slrtp workshop cvpr competitions aims evaluate archi\n",
      "Cleaned Full Text (start): slrtp sign language production challenge methodology, results, future work harry walsh, fish, ozge mercanoglu sincan, mohamed ilyes lakhal, richard bowden, neil fox, bencie woll, kepeng wu, zecheng li, weichao zhao, haodong wang, wengang zhou, houqiang li, shengeng tang, jiayi he, wang, ruobei zhang, yaxiong wang, lechao cheng, meryem tasyurek, tugce kiziltepe, hacer yalim keles university surrey, university birmingham, university college london, university science technology china, hefei univer\n",
      "--- End of Example ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "import os\n",
    "import pandas as pd\n",
    "import pymupdf # PyMuPDF\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Setup: NLTK Stopwords ---\n",
    "# This ensures the stopwords list is available for the cleaning function.\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK stopwords...\")\n",
    "    nltk.download('stopwords')\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "# --- Text Cleaning Function ---\n",
    "# A robust function to clean the text extracted from PDFs.\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Performs comprehensive cleaning of extracted text.\n",
    "    - Removes common PDF artifacts like headers, footers, and arXiv identifiers.\n",
    "    - Strips out URLs and citation numbers (e.g., [1, 2]).\n",
    "    - Normalizes whitespace and converts to lowercase.\n",
    "    - Removes stopwords for noise reduction.\n",
    "    \"\"\"\n",
    "    # Remove arXiv identifiers and page numbers\n",
    "    text = re.sub(r'arXiv:\\S+', '', text)\n",
    "    text = re.sub(r'Page \\d+ of \\d+', '', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove citation brackets like [1], [2, 3], etc.\n",
    "    text = re.sub(r'\\[\\d+(?:,\\s*\\d+)*\\]', '', text)\n",
    "    \n",
    "    # Remove special characters and digits, keeping basic punctuation\n",
    "    text = re.sub(r'[^a-zA-Z\\s.,!?-]', '', text)\n",
    "    \n",
    "    # Normalize whitespace (tabs, newlines, etc.) to a single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    # Note: For modern embedding models, this step is often skipped as stopwords can provide context.\n",
    "    # We are including it here to follow your requested preprocessing style.\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = text.split()\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    "    \n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# --- Main Data Collection Logic ---\n",
    "pdf_dir = \"cvpr_papers_pdf\"\n",
    "os.makedirs(pdf_dir, exist_ok=True)\n",
    "\n",
    "search_query = \"cat:cs.CV AND (ti:CVPR OR abs:CVPR)\"\n",
    "search = arxiv.Search(\n",
    "  query = search_query,\n",
    "  max_results = 50, # Fetching 50 papers for a good-sized dataset\n",
    "  sort_by = arxiv.SortCriterion.SubmittedDate\n",
    ")\n",
    "\n",
    "# This list will hold the final, cleaned data for all papers\n",
    "all_papers_data = []\n",
    "\n",
    "print(\"Starting data collection and preprocessing pipeline...\")\n",
    "\n",
    "for result in tqdm(search.results(), desc=\"Processing Papers\"):\n",
    "    try:\n",
    "        paper_id = result.entry_id.split('/')[-1]\n",
    "        pdf_filename = f\"{paper_id}.pdf\"\n",
    "        pdf_path = os.path.join(pdf_dir, pdf_filename)\n",
    "        \n",
    "        # 1. Download the PDF\n",
    "        result.download_pdf(dirpath=pdf_dir, filename=pdf_filename)\n",
    "        \n",
    "        # 2. Extract Text using PyMuPDF\n",
    "        full_text = \"\"\n",
    "        with pymupdf.open(pdf_path) as doc:\n",
    "            for page in doc:\n",
    "                full_text += page.get_text() + \" \"\n",
    "        \n",
    "        if not full_text.strip():\n",
    "            print(f\"Warning: No text extracted for {paper_id}. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        # 3. Clean the Extracted Text\n",
    "        cleaned_full_text = clean_text(full_text)\n",
    "        \n",
    "        # Also clean the summary for consistency\n",
    "        cleaned_summary = clean_text(result.summary)\n",
    "        \n",
    "        # 4. Store the Processed Data\n",
    "        all_papers_data.append({\n",
    "            \"id\": paper_id,\n",
    "            \"title\": result.title,\n",
    "            \"authors\": [author.name for author in result.authors],\n",
    "            \"summary\": cleaned_summary,\n",
    "            \"full_text\": cleaned_full_text,\n",
    "            \"published_date\": result.published.isoformat()\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process paper {result.entry_id}. Error: {e}\")\n",
    "\n",
    "print(f\"\\nSuccessfully processed {len(all_papers_data)} papers.\")\n",
    "\n",
    "if all_papers_data:\n",
    "    print(\"\\n--- Example of Processed Paper ---\")\n",
    "    print(\"Title:\", all_papers_data[0]['title'])\n",
    "    print(\"Cleaned Summary:\", all_papers_data[0]['summary'][:500])\n",
    "    print(\"Cleaned Full Text (start):\", all_papers_data[0]['full_text'][:500])\n",
    "    print(\"--- End of Example ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e18697",
   "metadata": {},
   "source": [
    "<h3>Step 2: Saving the processed into jsonl file</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b675fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving processed data to processed_data\\cvpr_papers_cleaned.jsonl...\n",
      "Successfully saved the data.\n",
      "\n",
      "Verifying the saved file...\n",
      "Successfully read back the first paper's title: SLRTP2025 Sign Language Production Challenge: Methodology, Results, and Future Work\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Save Processed Data to JSON Lines file\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Define the directory and filename for the output\n",
    "output_dir = \"processed_data\"\n",
    "output_filename = \"cvpr_papers_cleaned.jsonl\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "print(f\"Saving processed data to {output_path}...\")\n",
    "\n",
    "# Write the data to a JSON Lines file\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    for paper_data in all_papers_data:\n",
    "        # Convert each paper's dictionary to a JSON string and write it as a new line\n",
    "        f.write(json.dumps(paper_data) + '\\n')\n",
    "\n",
    "print(\"Successfully saved the data.\")\n",
    "\n",
    "\n",
    "print(\"\\nVerifying the saved file...\")\n",
    "with open(output_path, 'r', encoding='utf-8') as f:\n",
    "    first_line = f.readline()\n",
    "    first_paper = json.loads(first_line)\n",
    "    print(\"Successfully read back the first paper's title:\", first_paper['title'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2646412",
   "metadata": {},
   "source": [
    "<h3>Chunking the cleaned data for building FAISS Vector database</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ba69df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50 papers from processed_data/cvpr_papers_cleaned.jsonl\n",
      "\n",
      "Starting to chunk documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Papers: 100%|██████████| 50/50 [00:00<00:00, 124.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created a total of 2310 chunks from 50 papers.\n",
      "\n",
      "--- Example of a Chunk ---\n",
      "Paper ID: 2508.06951v1\n",
      "Title: SLRTP2025 Sign Language Production Challenge: Methodology, Results, and Future Work\n",
      "Chunk Content (start): Title: SLRTP2025 Sign Language Production Challenge: Methodology, Results, and Future Work...\n",
      "--- End of Example ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Load Data and Chunk Documents\n",
    "import json\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Load the processed data from the JSONL file ---\n",
    "processed_data_path = \"processed_data/cvpr_papers_cleaned.jsonl\"\n",
    "papers = []\n",
    "with open(processed_data_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        papers.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(papers)} papers from {processed_data_path}\")\n",
    "\n",
    "# --- Initialize the Text Splitter ---\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,      # The maximum number of characters in a chunk\n",
    "    chunk_overlap=200,    # The number of characters to overlap between chunks to maintain context\n",
    "    length_function=len,  \n",
    "    add_start_index=True, # This helps in identifying the location of a chunk in the original text\n",
    ")\n",
    "\n",
    "# --- Process each paper and create chunks ---\n",
    "all_chunks = []\n",
    "print(\"\\nStarting to chunk documents...\")\n",
    "\n",
    "for paper in tqdm(papers, desc=\"Chunking Papers\"):\n",
    "    # We create a combined text with important metadata for better context during retrieval\n",
    "    text_to_split = f\"Title: {paper['title']}\\n\\nSummary: {paper['summary']}\\n\\nFull Text: {paper['full_text']}\"\n",
    "    \n",
    "    # Create the chunks\n",
    "    chunks = text_splitter.create_documents([text_to_split])\n",
    "    \n",
    "    # Add paper-specific metadata to each chunk\n",
    "    for chunk in chunks:\n",
    "        chunk.metadata['paper_id'] = paper['id']\n",
    "        chunk.metadata['title'] = paper['title']\n",
    "        \n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "print(f\"\\nCreated a total of {len(all_chunks)} chunks from {len(papers)} papers.\")\n",
    "\n",
    "# --- Verification (Optional) ---\n",
    "# Let's inspect a chunk to see what it looks like\n",
    "if all_chunks:\n",
    "    print(\"\\n--- Example of a Chunk ---\")\n",
    "    example_chunk = all_chunks[0]\n",
    "    print(f\"Paper ID: {example_chunk.metadata['paper_id']}\")\n",
    "    print(f\"Title: {example_chunk.metadata['title']}\")\n",
    "    print(f\"Chunk Content (start): {example_chunk.page_content[:500]}...\")\n",
    "    print(\"--- End of Example ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497850bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 2310 chunks to processed_data\\cvpr_papers_chunks.jsonl...\n",
      "Successfully saved the chunks.\n",
      "\n",
      "Verifying the saved chunks file...\n",
      "Successfully read back the first chunk's metadata: {'start_index': 0, 'paper_id': '2508.06951v1', 'title': 'SLRTP2025 Sign Language Production Challenge: Methodology, Results, and Future Work'}\n"
     ]
    }
   ],
   "source": [
    "#Save the processed chunks to a file\n",
    "import json\n",
    "import os\n",
    "\n",
    "# --- Convert Document objects to a list of dictionaries ---\n",
    "# This is necessary because LangChain's Document object is not directly JSON serializable.\n",
    "chunks_to_save = [\n",
    "    {\n",
    "        \"page_content\": chunk.page_content,\n",
    "        \"metadata\": chunk.metadata\n",
    "    } \n",
    "    for chunk in all_chunks\n",
    "]\n",
    "\n",
    "# --- Save to a JSON Lines file ---\n",
    "output_dir = \"processed_data\"\n",
    "output_filename = \"cvpr_papers_chunks.jsonl\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_path = os.path.join(output_dir, output_filename)\n",
    "\n",
    "print(f\"Saving {len(chunks_to_save)} chunks to {output_path}...\")\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    for chunk_dict in chunks_to_save:\n",
    "        f.write(json.dumps(chunk_dict) + '\\n')\n",
    "\n",
    "print(\"Successfully saved the chunks.\")\n",
    "\n",
    "# --- Verification (Optional) ---\n",
    "print(\"\\nVerifying the saved chunks file...\")\n",
    "with open(output_path, 'r', encoding='utf-8') as f:\n",
    "    first_line = f.readline()\n",
    "    if first_line:\n",
    "        first_chunk = json.loads(first_line)\n",
    "        print(\"Successfully read back the first chunk's metadata:\", first_chunk['metadata'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2547f4b",
   "metadata": {},
   "source": [
    "<h3>Building FAISS Vector Database</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8af0355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading chunks from processed_data/cvpr_papers_chunks.jsonl...\n",
      "Successfully loaded 2310 chunks.\n",
      "\n",
      "Loading embedding model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohit\\AppData\\Local\\Temp\\ipykernel_8272\\75465820.py:27: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings_function = HuggingFaceEmbeddings(\n",
      "e:\\Kishan Reddy\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model loaded.\n",
      "\n",
      "Building FAISS index from loaded document chunks... (This will take a few minutes)\n",
      "\n",
      "FAISS index built and saved to 'faiss_cvpr_index'\n",
      "\n",
      "Verifying the vector store with a test query...\n",
      "\n",
      "Test Query: 'What are some novel approaches to 3D human motion modeling?'\n",
      "Top 3 similar chunks found:\n",
      "\n",
      "--- Result 1 ---\n",
      "Source Paper Title: EventEgo3D++: 3D Human Motion Capture from a Head-Mounted Event Camera\n",
      "Chunk Content: Title: EventEgo3D++: 3D Human Motion Capture from a Head-Mounted Event Camera...\n",
      "--------------------\n",
      "\n",
      "--- Result 2 ---\n",
      "Source Paper Title: EventEgo3D++: 3D Human Motion Capture from a Head-Mounted Event Camera\n",
      "Chunk Content: data fine-tuning evaluating method outdoor environments. thirdly, provide allocentric rgb views smpl loper al, body annotations real datasets, thereby providing comprehensive dataset advancing research. inclusion in-the-wild data ensures robustness real-world conditions, smpl body annotations provide dense human correspondences, making datasets human pose estimation real-time demo prediction eed hmd datasets low light fast motion input fig. eventegod builds upon work eventegod millerdurai al, real-time human motion capture egocentric event streams photograph new head-mounted device hmd custom-...\n",
      "--------------------\n",
      "\n",
      "--- Result 3 ---\n",
      "Source Paper Title: EventEgo3D++: 3D Human Motion Capture from a Head-Mounted Event Camera\n",
      "Chunk Content: human pose estimation event-based reconstruction. egocentric human pose estimation human pose estimation egocentric monocular stereo rgb views actively studied last decade. earliest approaches optimisation-based rhodin al, field promptly adopted neural architectures following state art human pose estimation. thus, follow-up methods used two-stream cnn architecture al, auto-encoders monocular tome al, stereo inputs zhao al, akada al, kang al, another work focused automatic calibration fisheye cameras widely used egocentric setting zhang al, recent papers leverage human motion priors temporal co...\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "#Load Chunks, Embed, and Build FAISS Index\n",
    "import json\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document # Needed to reconstruct Document objects\n",
    "\n",
    "# --- Load the saved chunks from the JSONL file ---\n",
    "chunks_path = \"processed_data/cvpr_papers_chunks.jsonl\"\n",
    "loaded_chunks = []\n",
    "\n",
    "print(f\"Loading chunks from {chunks_path}...\")\n",
    "with open(chunks_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        # Recreate the LangChain Document object from the dictionary\n",
    "        chunk = Document(page_content=data['page_content'], metadata=data['metadata'])\n",
    "        loaded_chunks.append(chunk)\n",
    "\n",
    "print(f\"Successfully loaded {len(loaded_chunks)} chunks.\")\n",
    "\n",
    "# --- Initialize the Embedding Model ---\n",
    "# This part remains the same.\n",
    "print(\"\\nLoading embedding model...\")\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_kwargs = {'device': 'cuda'} # Use 'cpu' if you don't have a GPU\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings_function = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "print(\"Embedding model loaded.\")\n",
    "\n",
    "# --- Build the FAISS Vector Store from the loaded chunks ---\n",
    "print(\"\\nBuilding FAISS index from loaded document chunks... (This will take a few minutes)\")\n",
    "vector_store = FAISS.from_documents(\n",
    "    documents=loaded_chunks, # We use the chunks we just loaded from the file\n",
    "    embedding=embeddings_function\n",
    ")\n",
    "\n",
    "# --- Save the FAISS Index to Disk ---\n",
    "faiss_index_path = \"faiss_cvpr_index\"\n",
    "vector_store.save_local(faiss_index_path)\n",
    "\n",
    "print(f\"\\nFAISS index built and saved to '{faiss_index_path}'\")\n",
    "\n",
    "# --- Verification (Optional) ---\n",
    "# This part also remains the same.\n",
    "print(\"\\nVerifying the vector store with a test query...\")\n",
    "query = \"What are some novel approaches to 3D human motion modeling?\"\n",
    "results = vector_store.similarity_search(query, k=3)\n",
    "\n",
    "print(f\"\\nTest Query: '{query}'\")\n",
    "print(\"Top 3 similar chunks found:\")\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\n--- Result {i+1} ---\")\n",
    "    print(f\"Source Paper Title: {doc.metadata['title']}\")\n",
    "    print(f\"Chunk Content: {doc.page_content[:600]}...\")\n",
    "    print(\"-\" * 20)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
