{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "911d4b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading chunks from E:\\Kishan Reddy Generation\\cvpr_papers_chunks.jsonl...\n",
      "Loaded 2310 chunks.\n",
      "Generating 200 examples using 'llama3'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Data:   2%|▎         | 5/200 [00:26<15:57,  4.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping malformed JSON object: {'question': 'What are the two main reasons why applying SSL methods to cell images is challenging?', 'context': 'robust feature extractor well xw, enabling learned representation capture cellular phenotypic features causal effects applied perturbations. although ssl methods successful natural images, applying methods cell images challenging two main reasons first, cell images provide multiple types information compared natural images. instance, natural image datasets imagenet representations generated based single image. contrast, cell images require integration data various positions multiple channels. second, significant distribution gap exists cell images natural images. effective ssl methods heavily rely data augmentation strategies, random cropping color jittering must carefully modified cell im- ages due distinct properties channels. data preprocessing original dataset consists -bit tiff images. re- duce disk space usage accelerate data loading training, first convert images -bit format, aligning natural image standards, using ix, ix, mini maxi mini next, compute mean variance channel,'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Data:   8%|▊         | 17/200 [01:36<16:44,  5.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping malformed JSON object: {'instruction': 'What is the primary focus of this research paper?', 'context': 'however, cocoop identified coop struggles generalizing out-of-distribution data recommends conditioning prompt input images. effective, methods require access annotated training data, limits zero- shot adaptation pre-trained models like ours. tackle challenge, recent research introduced tpt technique enables adaptive prompt learning inference time, using one test sample. tpt optimizes prompt minimizing entropy confidence selection model consistent predictions test sample. difftpt innovates test-time prompt tuning leveraging pre-trained diffusion models augment diver- sity test data samples used tpt. promptalign fine-tunes multi-modal prompts test-time aligning distribution statistics obtained mul- tiple augmented views single test image training data distribution statistics. although previous studies primarily concentrated refining prompt templates improve accuracy, largely neglected calibration except paper focuses critical under-explored challenge calibrat- ing vlms zero-short,'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Data:  22%|██▏       | 43/200 [12:48<57:25, 21.95s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping malformed JSON object: {'instruction': 'Generate a question-and-answer pair from the provided context.', 'context': 'context-aware question understanding. proceedings acm international conference multimedia, page association computing ma- chinery, haoyu zhang, meng liu, zan gao, xiaoqiang lei, yinglong wang, liqiang nie. multimodal dialog system rela- tional graph-based context-aware question understanding. proceedings acm international conference multimedia, pages haoyu zhang, meng liu, yuhong li, ming yan, zan gao, xiaojun chang, liqiang nie. attribute-guided collab- orative learning partial person re-identification. ieee transactions pattern analysis machine intelligence, haoyu zhang, meng liu, yaowei wang, cao, weili guan, liqiang nie. uncovering hidden connections iterative tracking reasoning video-grounded dialog. arxiv preprint haoyu zhang, meng liu, zixin liu, xuemeng song, yaowei wang, liqiang nie. multi-factor adaptive vision selec- tion egocentric video question answering. proceedings international conference machine learning, pages pmlr, haoyu zhang, yuquan xie, yisen feng, zaijing li,'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Data:  23%|██▎       | 46/200 [13:00<26:18, 10.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping malformed JSON object: {'instruction': \"What is the reference for the concept of 'less-forgetting learning deep neural networks'?\", 'context': 'proceed- ings ieee conference computer vision pattern recognition, pages hamed hemati, andrea cossu, antonio carta, julio hurtado, lorenzo pellegrini, davide bacciu, vincenzo lomonaco, damian borth. class-incremental learning repetition, geoffrey hinton, oriol vinyals, jeff dean. distill- ing knowledge neural network. arxiv preprint heechul jung, jeongwoo ju, minju jung, junmo kim. less-forgetting learning deep neural networks, diederik kingma jimmy ba. adam method stochastic optimization. international conference learning representations, iclr san diego, ca, usa, may conference track proceedings, zhizhong derek hoiem. learning without forgetting. computer vision eccv pages cham, springer international publishing. zhizhong derek hoiem. learning without forgetting. ieee transactions pattern analysis machine intelli- gence, vincenzo lomonaco, lorenzo pellegrini, andrea cossu, an- tonio carta, gabriele graffieti, tyler hayes, matthias lange, marc masana, jary pomponi,'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Data:  26%|██▌       | 52/200 [13:20<11:10,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping malformed JSON object: {'question': \"Who are the authors of the paper 'Spectrum-guided multi-granularity referring video object segmentation'?\", 'response': 'pages miao, mohammed bennamoun, yongsheng gao, ajmal mian', 'context': 'vision pattern recognition, pages miao, mohammed bennamoun, yongsheng gao, ajmal mian. spectrum-guided multi-granularity referring video object segmentation. proceedings ieeecvf international conference computer vision, pages nikhila ravi, valentin gabeur, yuan-ting hu, ronghang hu, chaitanya ryali, tengyu ma, haitham khedr, roman radle, chloe rolland, laura gustafson, eric mintun, junt-ing pan, kalyan vasudev alwala, nicolas carion, chao-yuan wu, ross girshick, piotr dollar, christoph feichtenhofer. sam segment anything images videos. arxiv preprint seonguk seo, joon-young lee, bohyung han. urvos unified referring video object segmentation network large-scale benchmark. computer visioneccv european conference, glasgow, uk, august proceedings, part pages springer, jiannan wu, jiang, peize sun, zehuan yuan, ping luo. language queries referring video object seg- mentation. proceedings ieeecvf conference computer vision pattern recognition, pages shilin yan, renrui zhang, ziyu guo'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Data:  35%|███▌      | 70/200 [14:09<06:52,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping malformed JSON object: {'question': 'What type of learning techniques are used in the proposed feature fusion attention network for image dehazing?', 'context': 'Title: Feature Fusion Attention Network with CycleGAN for Image Dehazing, De-Snowing and De-Raining\\nSummary: paper presents novel approach image dehazing combining feature fusion attention ffa networks cyclegan architecture. method leverages supervised unsupervised learning techniques effectively remove haze images preserving crucial image details. proposed hybrid architecture demonstrates significant improvements image quality metrics, achieving superior psnr ssim scores compared traditional dehazing methods. extensive experimentation reside densehaze cvpr dataset, show approach effectively handles synthetic real-world hazy images. cyclegan handles unpaired nature hazy clean images effectively, enabling model learn mappings even without paired data.', 'response': 'The proposed method leverages both supervised and unsupervised learning techniques.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Data:  51%|█████     | 102/200 [15:32<05:38,  3.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping malformed JSON object: {'instruction': 'What is the topic of the second paper mentioned in the context?', 'context': 'yuqin cao, wei sun, zicheng zhang, yingjie zhou, zhichao zhang, haon- ing wu, weixia zhang, xiaohong liu, xiongkuo min, guangtao zhai. aigiqa-k large database ai-generated image quality assessment. proceedings ieeecvf conference computer vision pattern recognition workshops, xin li, yeying jin, xin jin, zongwei wu, bingchen li, yufei wang, wenhan yang, li, zhibo chen, bihan wen, robby tan, radu timofte, al. ntire challenge day night raindrop removal dual-focused images methods results. proceedings ieeecvf conference computer vision pattern recognition cvpr workshops, xin li, xijun wang, bingchen li, kun yuan, yizhen shao, suhang yao, ming sun, chao zhou, radu timofte, zhibo chen. ntire challenge short-form ugc video quality assessment enhancement kwaisr dataset study. proceedings ieeecvf conference computer vision pattern recognition cvpr work- shops, xin li, kun yuan, bingchen li, fengbin guan, yizhen shao, zihao yu, xijun wang, yiting lu, wei luo, suhang yao, ming sun, chao zhou,'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Data:  56%|█████▋    | 113/200 [16:08<05:25,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping malformed JSON object: {'instruction': 'What is the name of the conference where the CVPR workshops take place?', 'context': 'pattern recognition cvpr workshops, marcos conde, radu timofte, al. raw image reconstruc- tion rgb smartphones. ntire challenge re- port. proceedings ieeecvf conference com- puter vision pattern recognition cvpr workshops, yubin deng, chen change loy, xiaoou tang. aesthetic- driven image enhancement adversarial learning. acm mm, pages egor ershov, sergey korchagin, alexei khalin, artyom pan- shin, arseniy terekhin, ekaterina zaychenkova, georgiy lobarev, vsevolod plokhotnyuk, denis abramov, elisey zhdanov, sofia dorogova, yasin mamedov, nikola banic, georgii perevozchikov, radu timofte, al. ntire challenge night photography rendering. proceedings ieeecvf conference computer vision pat- tern recognition cvpr workshops, yuqian fu, xingyu qiu, bin ren yanwei fu, radu timofte, nicu sebe, ming-hsuan yang, luc van gool, al. ntire challenge cross-domain few-shot object detection methods results. proceedings ieeecvf con- ference computer vision pattern recognition cvpr workshops, shuhao han,'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Data:  62%|██████▏   | 124/200 [16:30<02:25,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping malformed JSON object: {'question': 'What is the purpose of combining outputs in the proposed framework?', 'context': '\"create synthetic data, used classifier weights trained prototype memory get identity embeddings, input image prototype memory hourglass diusion transformer identity vector stylenat identity style synthetic dataset original mirrored combine outputs final score model model data generation model fig. framework proposed team.\"', 'response': 'The purpose of combining outputs in the proposed framework is to generate synthetic data.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Data:  76%|███████▌  | 152/200 [17:39<02:11,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping malformed JSON object: {'instruction': 'Generate a question-and-answer pair based on the provided context.', 'context': 'xuejing liu, jialin wang, wenbin ge, sibo song, kai dang, peng wang, shijie wang, jun tang, al. qwen. -vl technical report. arxiv preprint tianqi chen carlos guestrin. xgboost scalable tree boosting system. proceedings acm sigkdd international conference knowledge discovery data mining, pages zhongzhi chen, guang liu, bo-wen zhang, fulong ye, qinghong yang, ledell wu. altclip altering lan- guage encoder clip extended language capabilities. arxiv preprint zewen chen, juan wang, bing li, chunfeng yuan, wei-hua xiong, rui cheng, weiming hu. teacher-guided learning blind image quality assessment. proceedings asian conference computer vision, pages zewen chen, juan wang, wen wang, sunhan xu, hang xiong, yun zeng, jian guo, shuxun wang, chunfeng yuan, bing li, al. seagull no-reference image quality assess- ment regions interest via vision-language instruction tuning. arxiv preprint zhe chen, weiyun wang, yue cao, yangzhou liu, zhang-wei gao, erfei cui, jinguo zhu, shenglong ye, hao tian,'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Data:  82%|████████▎ | 165/200 [18:17<02:08,  3.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping malformed JSON object: {'instruction': \"What is the name of the authors who proposed the 'Segment Anything' approach in the proceedings of the International Conference on Computer Vision?\", 'context': 'berg. referitgame refer- ring objects photographs natural scenes. proceedings conference empirical methods natural language processing emnlp. oct. pp. mehar khurana, neehar peri, deva ramanan, james hays. shelf-supervised multi-modal pre-training object detection. arxiv preprint alexander kirillov, eric mintun, nikhila ravi, hanzi mao, chloe rolland, laura gustafson, tete xiao, spencer whitehead, alexander berg, wan-yen lo, al. segment anything. proceedings ieeecvf international conference computer vision. pp. kirsten korosec. waymo robotaxi got trapped chick-fil-a drive-through. accessed apr. url got-trapped-in-chick-fil-a-drive-through. kirsten korosec. waymo doubled weekly robotaxi rides less year. techcrunch url techcrunch com waymo doubled-its-weekly-robotaxi-rides-in-less-than-a-year. lee, kang, hwang, yoon. typical accident scenarios urban area obtained clustering association rule mining real-world accident reports. heliyon jan. doi .j.heliyon..e. jae-keun lee, jin-hee lee,'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Data:  90%|█████████ | 181/200 [19:02<00:58,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping malformed JSON object: {'instruction': 'What is the primary goal of the proposed HECOFUSE framework?', 'context': 'inconsistency arises modality absent certain nodes, rendering naive fusion strategies suboptimal. third, system robustness must guaranteed un- der partial sensor failures interacting nodes varying capabilities, requiring adaptable fusion mechanisms degrade gracefully. address challenges, propose hecofuse, unified cooperative perception framework tailored het- erogeneous vehicleinfrastructure settings. hecofuse extracts birds-eye-view BEV features node via modality-specific encoders, performs inter-node fusion novel hierarchical attention-based mecha- nism dynamically weighs channel-wise spatial features according sensor quality. parallel, adaptive spatial resolution module adjusts feature-map scales based sensor configuration balance computational cost information fidelity. training, randomly sample nine representative heterogeneous configurations, enabling model learn robust fusion strategies generalized across sensor combinations. main contributions threefold introduce hecofuse,'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Data:  98%|█████████▊| 197/200 [19:43<00:08,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Skipping malformed JSON object: {'question': 'What is the name of the team that proposed the face generation framework mentioned in the context?', 'context': '\"comprises members interactive entertainment group netease inc., china. initially used dcface database, generating dcface casia-webface data generation partial training pose occlusion adaface head adaface head iresnet photo maker final loss head loss head loss fig. framework proposed opdai team. face images large pose variations occlusions using photomaker given input images, photomaker generate diverse personalized photos based text prompt preserving identity information input image. randomly replaced images original dcface data ensure total number samples meets requirement photomaker inference, adopted batch size used random prompts including age, pose, image quality ensure diversity generated samples. sub-tasks combined data version dcface, sub-tasks merged casia-webface sub- tasks merge denoise samples different databases, following partial approach consists sparse variant model parallel architecture training models. regarding model, obtained loss different databases indepen- dent\"', 'response': 'The opdai team proposed the face generation framework.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Data: 100%|██████████| 200/200 [19:54<00:00,  5.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Synthetic dataset generation complete.\n",
      "Data saved to E:\\Kishan Reddy Generation\\cvpr_finetuning_dataset.jsonl\n",
      "IMPORTANT: Please manually review the generated file for quality before fine-tuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# generate_dataset.py\n",
    "import ollama\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# --- Configuration ---\n",
    "INPUT_CHUNKS_FILE = r\"E:\\Kishan Reddy Generation\\cvpr_papers_chunks.jsonl\"\n",
    "OUTPUT_DATASET_FILE = r\"E:\\Kishan Reddy Generation\\cvpr_finetuning_dataset.jsonl\"\n",
    "NUM_EXAMPLES_TO_GENERATE = 200 # Target number of training examples\n",
    "OLLAMA_MODEL = 'llama3' # The model we'll use for generation\n",
    "\n",
    "# --- System Prompt ---\n",
    "# This prompt guides the LLM to generate data in the correct format.\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "From the following context taken from a computer vision research paper, please generate one high-quality question-and-answer pair that would be suitable for fine-tuning a large language model.\n",
    "\n",
    "**Instructions:**\n",
    "1.  The question should be a specific, technical question that can be answered *only* from the provided context.\n",
    "2.  The answer should be a concise, clear, and direct response based *only* on the information in the context.\n",
    "3.  Do not make up information.\n",
    "4.  Your output MUST be a single, valid JSON object with the keys \"instruction\", \"context\", and \"response\".\n",
    "\n",
    "**Context:**\n",
    "\"{context}\"\n",
    "\n",
    "**JSON Output:**\n",
    "\"\"\"\n",
    "\n",
    "# --- Main Script ---\n",
    "\n",
    "# 1. Load the source chunks\n",
    "print(f\"Loading chunks from {INPUT_CHUNKS_FILE}...\")\n",
    "with open(INPUT_CHUNKS_FILE, 'r', encoding='utf-8') as f:\n",
    "    chunks = [json.loads(line) for line in f]\n",
    "print(f\"Loaded {len(chunks)} chunks.\")\n",
    "\n",
    "# We'll select a random subset of chunks to generate from, to get variety\n",
    "if len(chunks) > NUM_EXAMPLES_TO_GENERATE:\n",
    "    chunks_to_process = random.sample(chunks, NUM_EXAMPLES_TO_GENERATE)\n",
    "else:\n",
    "    chunks_to_process = chunks\n",
    "\n",
    "# 2. Initialize the Ollama client\n",
    "client = ollama.Client()\n",
    "\n",
    "# 3. Generate the dataset\n",
    "print(f\"Generating {NUM_EXAMPLES_TO_GENERATE} examples using '{OLLAMA_MODEL}'...\")\n",
    "generated_examples = []\n",
    "\n",
    "with open(OUTPUT_DATASET_FILE, 'w', encoding='utf-8') as f_out:\n",
    "    for chunk_data in tqdm(chunks_to_process, desc=\"Generating Data\"):\n",
    "        context = chunk_data['page_content']\n",
    "        \n",
    "        # Skip chunks that are too short to have meaningful content\n",
    "        if len(context.split()) < 50:\n",
    "            continue\n",
    "            \n",
    "        prompt = PROMPT_TEMPLATE.format(context=context)\n",
    "        \n",
    "        try:\n",
    "            # Call the local Ollama model\n",
    "            response = client.chat(\n",
    "                model=OLLAMA_MODEL,\n",
    "                messages=[{'role': 'user', 'content': prompt}],\n",
    "                format='json' # Use Ollama's built-in JSON mode\n",
    "            )\n",
    "            \n",
    "            # The response content should be a JSON string\n",
    "            json_response_str = response['message']['content']\n",
    "            \n",
    "            # Parse the JSON string into a Python dictionary\n",
    "            data = json.loads(json_response_str)\n",
    "            \n",
    "            # Validate the keys\n",
    "            if all(k in data for k in [\"instruction\", \"context\", \"response\"]):\n",
    "                # Write the valid example directly to the output file\n",
    "                f_out.write(json.dumps(data) + '\\n')\n",
    "            else:\n",
    "                print(f\"Warning: Skipping malformed JSON object: {data}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nAn error occurred: {e}\")\n",
    "            print(f\"Problematic context: {context[:200]}...\")\n",
    "            continue\n",
    "\n",
    "print(f\"\\nSynthetic dataset generation complete.\")\n",
    "print(f\"Data saved to {OUTPUT_DATASET_FILE}\")\n",
    "print(\"IMPORTANT: Please manually review the generated file for quality before fine-tuning.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
